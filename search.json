[
  {
    "objectID": "posts/challenge1_MekhalaKumar.html",
    "href": "posts/challenge1_MekhalaKumar.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_MekhalaKumar.html#datasets-used",
    "href": "posts/challenge1_MekhalaKumar.html#datasets-used",
    "title": "Challenge 1",
    "section": "Datasets used",
    "text": "Datasets used\nTwo datasets have been used: Railroad_2012_clean_county and birds.\n\n\nCode\n#1st Dataset Railroad_2012_clean_county\nlibrary(readr)\nrailroad_2012_clean_county <- read_csv(\"_data/railroad_2012_clean_county.csv\")\nview(railroad_2012_clean_county)\n#2nd Dataset birds\nbirds <- read_csv(\"_data/birds.csv\")\nView(birds)"
  },
  {
    "objectID": "posts/challenge1_MekhalaKumar.html#description-of-datasets",
    "href": "posts/challenge1_MekhalaKumar.html#description-of-datasets",
    "title": "Challenge 1",
    "section": "Description of Datasets",
    "text": "Description of Datasets\nThe first dataset is about the number of employees in each company. There are 3 variables as can be seen using the colnames command- state, county, number of employees. The number of employees is a continuous variable. The data was gathered from several states as seen in the table.\nThe second dataset has 14 columns and 30977 observations. From colnames, we get to know that the dataset gives us the values of the dietary energy intake for different countries across different years. Data types of the columns, value could actually be converted into double type. There were around 11000 missing values found and removed from the data. Many countries were included in this dataset and there are 6 types of birds but only one domain of animals present.\nA plot was created to visualise the changes in the Value across the years.It can be seen that the values have increased over time. A plot was also created to visualise the changes in a specific country, in this case, the USA.\n\n\nCode\n#1st dataset Railroad_2012_clean_county\n\ncolnames(railroad_2012_clean_county)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nCode\nstates<-select(railroad_2012_clean_county,state)\ntable(states)\n\n\nstate\n AE  AK  AL  AP  AR  AZ  CA  CO  CT  DC  DE  FL  GA  HI  IA  ID  IL  IN  KS  KY \n  1   6  67   1  72  15  55  57   8   1   3  67 152   3  99  36 103  92  95 119 \n LA  MA  MD  ME  MI  MN  MO  MS  MT  NC  ND  NE  NH  NJ  NM  NV  NY  OH  OK  OR \n 63  12  24  16  78  86 115  78  53  94  49  89  10  21  29  12  61  88  73  33 \n PA  RI  SC  SD  TN  TX  UT  VA  VT  WA  WI  WV  WY \n 65   5  46  52  91 221  25  92  14  39  69  53  22 \n\n\nCode\nprop.table(table(states))\n\n\nstate\n          AE           AK           AL           AP           AR           AZ \n0.0003412969 0.0020477816 0.0228668942 0.0003412969 0.0245733788 0.0051194539 \n          CA           CO           CT           DC           DE           FL \n0.0187713311 0.0194539249 0.0027303754 0.0003412969 0.0010238908 0.0228668942 \n          GA           HI           IA           ID           IL           IN \n0.0518771331 0.0010238908 0.0337883959 0.0122866894 0.0351535836 0.0313993174 \n          KS           KY           LA           MA           MD           ME \n0.0324232082 0.0406143345 0.0215017065 0.0040955631 0.0081911263 0.0054607509 \n          MI           MN           MO           MS           MT           NC \n0.0266211604 0.0293515358 0.0392491468 0.0266211604 0.0180887372 0.0320819113 \n          ND           NE           NH           NJ           NM           NV \n0.0167235495 0.0303754266 0.0034129693 0.0071672355 0.0098976109 0.0040955631 \n          NY           OH           OK           OR           PA           RI \n0.0208191126 0.0300341297 0.0249146758 0.0112627986 0.0221843003 0.0017064846 \n          SC           SD           TN           TX           UT           VA \n0.0156996587 0.0177474403 0.0310580205 0.0754266212 0.0085324232 0.0313993174 \n          VT           WA           WI           WV           WY \n0.0047781570 0.0133105802 0.0235494881 0.0180887372 0.0075085324 \n\n\nCode\n#2nd dataset birds\n\ndim(birds) \n\n\n[1] 30977    14\n\n\nCode\ncolnames(birds)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nCode\nstr(birds)\n\n\nspec_tbl_df [30,977 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Domain Code     : chr [1:30977] \"QA\" \"QA\" \"QA\" \"QA\" ...\n $ Domain          : chr [1:30977] \"Live Animals\" \"Live Animals\" \"Live Animals\" \"Live Animals\" ...\n $ Area Code       : num [1:30977] 2 2 2 2 2 2 2 2 2 2 ...\n $ Area            : chr [1:30977] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ Element Code    : num [1:30977] 5112 5112 5112 5112 5112 ...\n $ Element         : chr [1:30977] \"Stocks\" \"Stocks\" \"Stocks\" \"Stocks\" ...\n $ Item Code       : num [1:30977] 1057 1057 1057 1057 1057 ...\n $ Item            : chr [1:30977] \"Chickens\" \"Chickens\" \"Chickens\" \"Chickens\" ...\n $ Year Code       : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Year            : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Unit            : chr [1:30977] \"1000 Head\" \"1000 Head\" \"1000 Head\" \"1000 Head\" ...\n $ Value           : num [1:30977] 4700 4900 5000 5300 5500 5800 6600 6290 6300 6000 ...\n $ Flag            : chr [1:30977] \"F\" \"F\" \"F\" \"F\" ...\n $ Flag Description: chr [1:30977] \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Domain Code` = col_character(),\n  ..   Domain = col_character(),\n  ..   `Area Code` = col_double(),\n  ..   Area = col_character(),\n  ..   `Element Code` = col_double(),\n  ..   Element = col_character(),\n  ..   `Item Code` = col_double(),\n  ..   Item = col_character(),\n  ..   `Year Code` = col_double(),\n  ..   Year = col_double(),\n  ..   Unit = col_character(),\n  ..   Value = col_double(),\n  ..   Flag = col_character(),\n  ..   `Flag Description` = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nCode\nbirds <- transform(birds,value1 = as.numeric(Value))\nsum(is.na(birds))\n\n\n[1] 12845\n\n\nCode\nbirds<-na.omit(birds)\ndim(birds)\n\n\n[1] 19168    15\n\n\nCode\narea<-select(birds,Area)\ntable(area)\n\n\nArea\n                                         Afghanistan \n                                                  31 \n                                              Africa \n                                                 290 \n                                             Albania \n                                                  62 \n                                             Algeria \n                                                 208 \n                                      American Samoa \n                                                  53 \n                                            Americas \n                                                 232 \n                                              Angola \n                                                  49 \n                                 Antigua and Barbuda \n                                                  53 \n                                           Argentina \n                                                 132 \n                                             Armenia \n                                                  39 \n                                                Asia \n                                                 290 \n                                           Australia \n                                                  47 \n                           Australia and New Zealand \n                                                 232 \n                                             Austria \n                                                  63 \n                                          Azerbaijan \n                                                  46 \n                                             Bahamas \n                                                  57 \n                                             Bahrain \n                                                  48 \n                                          Bangladesh \n                                                  46 \n                                            Barbados \n                                                  79 \n                                             Belarus \n                                                  68 \n                                             Belgium \n                                                  36 \n                                              Belize \n                                                 151 \n                                               Benin \n                                                  24 \n                                             Bermuda \n                                                  44 \n                                              Bhutan \n                                                  29 \n                    Bolivia (Plurinational State of) \n                                                 125 \n                              Bosnia and Herzegovina \n                                                  80 \n                                            Botswana \n                                                  29 \n                                              Brazil \n                                                  94 \n                                   Brunei Darussalam \n                                                  35 \n                                            Bulgaria \n                                                  59 \n                                        Burkina Faso \n                                                  40 \n                                             Burundi \n                                                  45 \n                                          Cabo Verde \n                                                  38 \n                                            Cambodia \n                                                  65 \n                                            Cameroon \n                                                  31 \n                                              Canada \n                                                 146 \n                                           Caribbean \n                                                 232 \n                                      Cayman Islands \n                                                  31 \n                            Central African Republic \n                                                  69 \n                                     Central America \n                                                 174 \n                                        Central Asia \n                                                 108 \n                                                Chad \n                                                  58 \n                                               Chile \n                                                  53 \n                                China, Hong Kong SAR \n                                                 124 \n                                    China, Macao SAR \n                                                  58 \n                                     China, mainland \n                                                 168 \n                                            Colombia \n                                                  45 \n                                             Comoros \n                                                  58 \n                                               Congo \n                                                  41 \n                                        Cook Islands \n                                                  55 \n                                          Costa Rica \n                                                  43 \n                                       Côte d'Ivoire \n                                                  28 \n                                             Croatia \n                                                  27 \n                                                Cuba \n                                                   3 \n                                              Cyprus \n                                                 196 \n                                             Czechia \n                                                   1 \n                                      Czechoslovakia \n                                                   1 \n               Democratic People's Republic of Korea \n                                                  60 \n                    Democratic Republic of the Congo \n                                                  17 \n                                             Denmark \n                                                   2 \n                                            Dominica \n                                                  58 \n                                  Dominican Republic \n                                                  41 \n                                      Eastern Africa \n                                                 232 \n                                        Eastern Asia \n                                                 290 \n                                      Eastern Europe \n                                                 232 \n                                             Ecuador \n                                                 147 \n                                               Egypt \n                                                  92 \n                                         El Salvador \n                                                  31 \n                                   Equatorial Guinea \n                                                 112 \n                                             Eritrea \n                                                  23 \n                                             Estonia \n                                                  90 \n                                            Eswatini \n                                                  22 \n                                            Ethiopia \n                                                   8 \n                                        Ethiopia PDR \n                                                  16 \n                                              Europe \n                                                 290 \n                         Falkland Islands (Malvinas) \n                                                  25 \n                                                Fiji \n                                                 171 \n                                             Finland \n                                                  16 \n                                              France \n                                                  74 \n                                       French Guyana \n                                                  80 \n                                    French Polynesia \n                                                 115 \n                                               Gabon \n                                                  49 \n                                              Gambia \n                                                  30 \n                                             Georgia \n                                                  43 \n                                             Germany \n                                                  40 \n                                               Ghana \n                                                  17 \n                                              Greece \n                                                  25 \n                                             Grenada \n                                                  51 \n                                          Guadeloupe \n                                                 140 \n                                                Guam \n                                                  48 \n                                           Guatemala \n                                                  29 \n                                              Guinea \n                                                  26 \n                                       Guinea-Bissau \n                                                  44 \n                                              Guyana \n                                                  48 \n                                               Haiti \n                                                 218 \n                                            Honduras \n                                                  33 \n                                               India \n                                                  79 \n                                           Indonesia \n                                                  13 \n                          Iran (Islamic Republic of) \n                                                 214 \n                                                Iraq \n                                                  48 \n                                             Ireland \n                                                 108 \n                                              Israel \n                                                  94 \n                                               Italy \n                                                 103 \n                                             Jamaica \n                                                  56 \n                                               Japan \n                                                  60 \n                                              Jordan \n                                                 137 \n                                          Kazakhstan \n                                                  41 \n                                               Kenya \n                                                  22 \n                                            Kiribati \n                                                  56 \n                                              Kuwait \n                                                  13 \n                                          Kyrgyzstan \n                                                  51 \n                    Lao People's Democratic Republic \n                                                 117 \n                                              Latvia \n                                                  13 \n                                             Lebanon \n                                                  46 \n                                             Lesotho \n                                                  39 \n                                             Liberia \n                                                 116 \n                                               Libya \n                                                  37 \n                                           Lithuania \n                                                  24 \n                                          Madagascar \n                                                 218 \n                                              Malawi \n                                                  50 \n                                            Malaysia \n                                                  50 \n                                                Mali \n                                                  30 \n                                               Malta \n                                                  76 \n                                          Martinique \n                                                  65 \n                                          Mauritania \n                                                  53 \n                                           Mauritius \n                                                 193 \n                                           Melanesia \n                                                 174 \n                                              Mexico \n                                                  68 \n                                          Micronesia \n                                                 111 \n                    Micronesia (Federated States of) \n                                                  48 \n                                       Middle Africa \n                                                 174 \n                                            Mongolia \n                                                   2 \n                                          Montenegro \n                                                   3 \n                                          Montserrat \n                                                  53 \n                                             Morocco \n                                                  70 \n                                          Mozambique \n                                                  98 \n                                             Myanmar \n                                                 112 \n                                             Namibia \n                                                  70 \n                                               Nauru \n                                                  58 \n                                               Nepal \n                                                  48 \n                                         Netherlands \n                                                  10 \n                       Netherlands Antilles (former) \n                                                  58 \n                                       New Caledonia \n                                                  48 \n                                         New Zealand \n                                                 161 \n                                           Nicaragua \n                                                  45 \n                                               Niger \n                                                  17 \n                                             Nigeria \n                                                  13 \n                                                Niue \n                                                  47 \n                                     North Macedonia \n                                                   2 \n                                     Northern Africa \n                                                 290 \n                                    Northern America \n                                                 232 \n                                     Northern Europe \n                                                 232 \n                                              Norway \n                                                  17 \n                                             Oceania \n                                                 232 \n                                                Oman \n                                                  52 \n                     Pacific Islands Trust Territory \n                                                  28 \n                                            Pakistan \n                                                  94 \n                                           Palestine \n                                                  19 \n                                              Panama \n                                                 106 \n                                    Papua New Guinea \n                                                 140 \n                                            Paraguay \n                                                  87 \n                                         Philippines \n                                                 108 \n                                           Polynesia \n                                                 116 \n                                            Portugal \n                                                  92 \n                                         Puerto Rico \n                                                  15 \n                                               Qatar \n                                                  32 \n                                   Republic of Korea \n                                                  12 \n                                 Republic of Moldova \n                                                  52 \n                                             Réunion \n                                                 107 \n                                             Romania \n                                                 156 \n                                  Russian Federation \n                                                  45 \n                                              Rwanda \n                                                  73 \n        Saint Helena, Ascension and Tristan da Cunha \n                                                  30 \n                               Saint Kitts and Nevis \n                                                  55 \n                                         Saint Lucia \n                                                  40 \n                           Saint Pierre and Miquelon \n                                                  41 \n                    Saint Vincent and the Grenadines \n                                                  44 \n                                               Samoa \n                                                  50 \n                               Sao Tome and Principe \n                                                 168 \n                                        Saudi Arabia \n                                                  88 \n                                             Senegal \n                                                   5 \n                               Serbia and Montenegro \n                                                  53 \n                                          Seychelles \n                                                 100 \n                                        Sierra Leone \n                                                  78 \n                                           Singapore \n                                                  73 \n                                            Slovakia \n                                                   2 \n                                            Slovenia \n                                                  36 \n                                     Solomon Islands \n                                                  57 \n                                             Somalia \n                                                  58 \n                                        South Africa \n                                                 193 \n                                       South America \n                                                 232 \n                                         South Sudan \n                                                   7 \n                                  South-eastern Asia \n                                                 290 \n                                     Southern Africa \n                                                 261 \n                                       Southern Asia \n                                                 232 \n                                     Southern Europe \n                                                 290 \n                                               Spain \n                                                  91 \n                                           Sri Lanka \n                                                  10 \n                                      Sudan (former) \n                                                  30 \n                                            Suriname \n                                                  47 \n                                              Sweden \n                                                  49 \n                                         Switzerland \n                                                  20 \n                                Syrian Arab Republic \n                                                  32 \n                                            Thailand \n                                                  17 \n                                         Timor-Leste \n                                                  46 \n                                                Togo \n                                                  11 \n                                             Tokelau \n                                                  56 \n                                               Tonga \n                                                  32 \n                                 Trinidad and Tobago \n                                                  47 \n                                             Tunisia \n                                                  50 \n                                              Turkey \n                                                  56 \n                                        Turkmenistan \n                                                  49 \n                                              Tuvalu \n                                                  45 \n                                              Uganda \n                                                  24 \n                                United Arab Emirates \n                                                  49 \nUnited Kingdom of Great Britain and Northern Ireland \n                                                  28 \n                         United Republic of Tanzania \n                                                  97 \n                            United States of America \n                                                 116 \n                        United States Virgin Islands \n                                                  41 \n                                             Uruguay \n                                                 211 \n                                                USSR \n                                                  12 \n                                          Uzbekistan \n                                                  54 \n                                             Vanuatu \n                                                  51 \n                  Venezuela (Bolivarian Republic of) \n                                                  29 \n                                            Viet Nam \n                                                  79 \n                           Wallis and Futuna Islands \n                                                  54 \n                                      Western Africa \n                                                 116 \n                                        Western Asia \n                                                 290 \n                                      Western Europe \n                                                 290 \n                                               World \n                                                 290 \n                                               Yemen \n                                                  52 \n                                        Yugoslav SFR \n                                                   4 \n                                              Zambia \n                                                  49 \n                                            Zimbabwe \n                                                 158 \n\n\nCode\nitem<-select(birds,Item)\ntable(item)\n\n\nItem\n              Chickens                  Ducks Geese and guinea fowls \n                  7698                   4357                   2599 \n  Pigeons, other birds                Turkeys \n                   903                   3611 \n\n\nCode\ndomain<-select(birds,Domain)\ntable(domain)\n\n\nDomain\nLive Animals \n       19168 \n\n\nCode\nplot(value1~Year,birds)\n\n\n\n\n\nCode\nbirds_USA<-birds%>% filter(`Area`=='United States of America')\nplot(value1~Year,birds_USA)"
  },
  {
    "objectID": "posts/challenge2_jerinjacob.html",
    "href": "posts/challenge2_jerinjacob.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_jerinjacob.html#describe-the-data",
    "href": "posts/challenge2_jerinjacob.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\n# The dataset contains 2930 entries each showing the number of rail road employees in a US county. The data is categorized statewise. The data is taken from counties that have atleast one employee working in the rail road. Cook, IL has the most number of employees with a head count of 8207."
  },
  {
    "objectID": "posts/challenge2_jerinjacob.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_jerinjacob.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\ngroup_by(railroad, state)\n\n\n# A tibble: 2,930 × 3\n# Groups:   state [53]\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included."
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html#challenge-overview",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html#challenge-overview",
    "title": "Challenge 3",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html#read-in-data",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\nCode\n# Read the data regarding eggs\n\neggs <- read_csv(\"_data/eggs_tidy.csv\")\neggs\n\n\n# A tibble: 120 × 6\n   month      year large_half_dozen large_dozen extra_large_half_dozen extra_l…¹\n   <chr>     <dbl>            <dbl>       <dbl>                  <dbl>     <dbl>\n 1 January    2004             126         230                    132       230 \n 2 February   2004             128.        226.                   134.      230 \n 3 March      2004             131         225                    137       230 \n 4 April      2004             131         225                    137       234.\n 5 May        2004             131         225                    137       236 \n 6 June       2004             134.        231.                   137       241 \n 7 July       2004             134.        234.                   137       241 \n 8 August     2004             134.        234.                   137       241 \n 9 September  2004             130.        234.                   136.      241 \n10 October    2004             128.        234.                   136.      241 \n# … with 110 more rows, and abbreviated variable name ¹​extra_large_dozen\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nBriefly describe the data\nThis dataset shows that the price of eggs categorizied by their size from 2004 I think. Because this data has 6 columns, it cannot be easily recognized at once. So I will pivot it. The 4 columns about the types will be pivoted to “types” column."
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html#anticipate-the-end-result",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html#anticipate-the-end-result",
    "title": "Challenge 3",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA    1334.     752.\n2 USA      1990 NAFTA    1923.     649.\n3 France   1980 EU       1356.    1363.\n4 Mexico   1990 NAFTA    1123.    1074.\n5 USA      1980 NAFTA    1081.     936.\n6 France   1990 EU       -132.     897.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\nCode\n#existing rows/cases\nnrow(eggs)\n\n\n[1] 120\n\n\nCode\n#existing columns/cases\nncol(eggs)\n\n\n[1] 6\n\n\nCode\n#expected rows/cases\nnrow(eggs) * (ncol(eggs)-2)\n\n\n[1] 480\n\n\nCode\n# expected columns \n2 + 2\n\n\n[1] 4\n\n\nIn this ‘eggs’ dataset 2 of the variables are used to identify a case. So expected rows are 480 and columns are 4."
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html#pivot-the-data",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html#pivot-the-data",
    "title": "Challenge 3",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1334.\n 2 Mexico   1980 NAFTA incoming               752.\n 3 USA      1990 NAFTA outgoing              1923.\n 4 USA      1990 NAFTA incoming               649.\n 5 France   1980 EU    outgoing              1356.\n 6 France   1980 EU    incoming              1363.\n 7 Mexico   1990 NAFTA outgoing              1123.\n 8 Mexico   1990 NAFTA incoming              1074.\n 9 USA      1980 NAFTA outgoing              1081.\n10 USA      1980 NAFTA incoming               936.\n11 France   1990 EU    outgoing              -132.\n12 France   1990 EU    incoming               897.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\nCode\n#pivot data\n\npivot_eggs<-pivot_longer(eggs,col=c(large_half_dozen, large_dozen, extra_large_half_dozen, extra_large_dozen), names_to=\"types\", values_to=\"price\")\npivot_eggs\n\n\n# A tibble: 480 × 4\n   month     year types                  price\n   <chr>    <dbl> <chr>                  <dbl>\n 1 January   2004 large_half_dozen        126 \n 2 January   2004 large_dozen             230 \n 3 January   2004 extra_large_half_dozen  132 \n 4 January   2004 extra_large_dozen       230 \n 5 February  2004 large_half_dozen        128.\n 6 February  2004 large_dozen             226.\n 7 February  2004 extra_large_half_dozen  134.\n 8 February  2004 extra_large_dozen       230 \n 9 March     2004 large_half_dozen        131 \n10 March     2004 large_dozen             225 \n# … with 470 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n#change the order of columns\npivot_eggs<-pivot_eggs[c(2,1,3,4)]\npivot_eggs\n\n\n# A tibble: 480 × 4\n    year month    types                  price\n   <dbl> <chr>    <chr>                  <dbl>\n 1  2004 January  large_half_dozen        126 \n 2  2004 January  large_dozen             230 \n 3  2004 January  extra_large_half_dozen  132 \n 4  2004 January  extra_large_dozen       230 \n 5  2004 February large_half_dozen        128.\n 6  2004 February large_dozen             226.\n 7  2004 February extra_large_half_dozen  134.\n 8  2004 February extra_large_dozen       230 \n 9  2004 March    large_half_dozen        131 \n10  2004 March    large_dozen             225 \n# … with 470 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAny additional comments?\nIt has changed to 480 rows and 4 columns dataset."
  },
  {
    "objectID": "posts/challenge1_solutions.html",
    "href": "posts/challenge1_solutions.html",
    "title": "Challenge 1 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/challenge1_solutions.html#working-with-tabular-data",
    "href": "posts/challenge1_solutions.html#working-with-tabular-data",
    "title": "Challenge 1 Solution",
    "section": "Working with Tabular Data",
    "text": "Working with Tabular Data\nOur advanced datasets ( ⭐⭐⭐ and higher) are tabular data (i.e., tables) that are often published based on government sources or by other organizations. Tabular data is often made available in Excel format (.xls or .xlsx) and is formatted for ease of reading - but this can make it tricky to read into R and reshape into a usable dataset.\nReading in tabular data will follow the same general work flow or work process regardless of formatting differences. We will work through the steps in detail this week (and in future weeks as new datasets are introduced), but this is an outline of the basic process. Note that not every step is needed for every file.\n\nIdentify grouping variables and values to extract from the table\nIdentify formatting issues that need to be addressed or eliminated\nIdentify column issues to be addressed during data read-in\nChoose column names to allow pivoting or future analysis\nAddress issues in rows using filter (and stringr package)\nCreate or mutate new variables as required, using separate, pivot_longer, etc\n\n\nRailroad ⭐FAOSTAT ⭐⭐Wild Birds ⭐⭐⭐Railroad (xls) ⭐⭐⭐⭐\n\n\nIt is hard to get much information about the data source or contents from a .csv file - as compared to the formatted .xlsx version of the same data described below.\n\nRead the Data\n\n\nCode\nrailroad<-read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nRows: 2930 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, county\ndbl (1): total_employees\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\nFrom inspection, we can that the three variables are named state, county, and total employees. Combined with the name of the fail, this appears to be the aggregated data on the number of employees working for the railroad in each county 2012. We assume that the 2930 cases - which are counties embedded within states1 - consist only of counties where there are railroad employees?\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nWith a few simple commands, we can confirm that there are 53 “states” represented in the data. To identify the additional non-state areas (probably District of Columbia, plus some combination of Puerto Rico and/or overseas addresses), we can print out a list of unique state names.\n\n1: We can identify case variables because both are character variables, which in tidy lingo are grouping variables not values.\n\n\n\nOnce again, a .csv file lacks any of the additional information that might be present in a published Excel table. So, we know the data are likely to be about birds, but will we be looking at individual pet birds, prices of bird breeds sold in stores, the average flock size of wild birds - who knows!\nThe FAOSTAT*.csv files have some additional information - the FAO - which a Google search reveals to be the Food and Agriculture Association of the United Nations publishes country-level data regularly in a database called FAOSTAT. So my best guess at this point is that we are going to be looking at country-level estaimtes of the number of birds that are raised for eggs and poultry, but we will see if this is right by inspecting the data.\n\nRead the Data\n\n\nCode\nbirds<-read_csv(\"_data/birds.csv\")\n\n\nRows: 30977 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nbirds\n\n\n\n\n  \n\n\n\nCode\nchickens<-read_csv(\"_data/FAOSTAT_egg_chicken.csv\")\n\n\nRows: 38170 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nchickens\n\n\n\n\n  \n\n\n\nIt is pretty difficult to get a handle on what data are being captured by any of the FAOSTAT* (including the birds.csv) data sets simply from a quick scan of the tibble after read in. It was easy with the railroad data, but now we are going to have to work harder to describe exactly what comprises a case in these data and what values are present for each case. We can see that there are 30,970 rows in the birds data (and 38,170 rows in the chickens) - but this might not mean that there are 30,970 (or 38,170) cases because we aren’t sure what constitutes a case at this point.\n\n\nWhat is a case?\nOne approach to figuring out what constitutes a case is to identify the value variables and assume that what is leftover are the grouping variables. Unfortunately, there are six double variables (from the column descriptions that are automatically returned), and it appears that most of them are not grouping variables. For example, the variable “Area Code” is a double - but doesn’t appear to be a value that varies across rows. Thus, it is a grouping variable rather than a true value in tidy nomenclature. Similar issues can be found with Year and “Item Code” - both appear to be grouping variables. Ironically, it is the variable called Value which appears to the sole value in the data set - but what is it the value of?\nAnother approach to identifying a case is to look for variation (or lack of variation) in just the first few cases of the tibble. (Think of this as the basis for a minimal reproducible example.) In the first few cases, the variables of the first 10 cases appear to be identical until we get to Year and Year Code (which appear to be identical to each other.) So it appears that Value is varying by country-year - but perhaps also by information in one of the other variables. It also appears that many of the doubles are just numeric codes, so lets drop those variables to simplify (I’m going to drop down to just showing the birds data for now.)\n\n\nCode\nbirds.sm<-birds%>%\n  select(-contains(\"Code\"))\nbirds.sm\n\n\n\n\n  \n\n\n\nCode\nchickens.sm<-chickens%>%\n  select(-contains(\"Code\"))\n\n\n\n\nVisual Summary of Data Set\nBefore we go doing detailed cross-tabs to figure out where there is variation, lets do a high level summary of the dataset to see if - for example - there are multiple values in the Element variable - or if we only have a dataset with records containing estimates of Chicken Stocks (from Element + Item.)\nTo get a better grasp of the data, lets do a quick skim or summary of the dataset and see if we can find out more about our data at a glance. I am using the dfSummary function from the summarytools package -one of the more attractive ways to quickly summarise a dataset. I am using a few options to allow it to render directly to html.\n\n\nCode\nprint(summarytools::dfSummary(birds.sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nbirds.sm\nDimensions: 30977 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Live Animals\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Africa2. Asia3. Eastern Asia4. Egypt5. Europe6. France7. Greece8. Myanmar9. Northern Africa10. South-eastern Asia[ 238 others ]\n      290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)28077(90.6%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Stocks\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Chickens2. Ducks3. Geese and guinea fowls4. Pigeons, other birds5. Turkeys\n      13074(42.2%)6909(22.3%)4136(13.4%)1165(3.8%)5693(18.4%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.6 (16.7)min ≤ med ≤ max:1961 ≤ 1992 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 99410.6 (720611.4)min ≤ med ≤ max:0 ≤ 1800 ≤ 23707134IQR (CV) : 15233 (7.2)\n      11495 distinct values\n      \n      1036\n(3.3%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Im5. M\n      1494(7.4%)6488(32.1%)10007(49.5%)1213(6.0%)1002(5.0%)\n      \n      10773\n(34.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Data not available3. FAO data based on imputat4. FAO estimate5. Official data6. Unofficial figure\n      6488(20.9%)1002(3.2%)1213(3.9%)10007(32.3%)10773(34.8%)1494(4.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-18\n\n\n\nFinally - we have a much better grasp on what is going on. First, we know that all records in this data set are of the number of Live Animal Stocks (Domain + Element), with the value expressed as 1000 heads (Unit). These three variables are grouping variables but DO NOT vary in this particular data extract - but are probably used to create data extracts from the larger FAOSTAT database.. To see if we are correct, we will have to checkout the same fields in the chickens data below.\nSecond, we can now guess that a case consists of a country-year-animal record - as captured in the variables Area, Year and Item, respectively - estimate of the number of live animals (Value.) ALso, as a side note, it appears that the estimated number of animals may have a long right-hand tail - just looking at the mini-histogram. So we can now say that we have estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) in 248 areas (countries??) for 58 years between 1961-2018.\nThe only minor concern is that we are still not entirely sure what information is being captured in the Flag (and matching Flag Description) variable. It appears unlikely that there is more than one estimate per country-year-animal case (see the summary of Area where all countries have 290 observations.) An assumption of one type of estimate (the content of Flag Description) per year is also consistent with the histogram of Year, which is pretty consistent although more countries were clearly added later in the series and data collection is not complete for the most recent time period.\nWe can dig a bit more, and find the description of the Flag field on the FAOSTAT website.. Sure enough, this confirms that the flags correspond to what type of estimate is being used (e.g., official data vs an estimate by FAOSTAT or imputed data.)\nWe can also confirm that NOT all cases are countries, as there is a Flag value, A, described as aggregated data. A quick inspection of the areas using this flag confirm that all of the “countries” are actually regional aggregations, and should be filtered out of the dataset as they are not the same “type” of case as a country-level case. To fix these data into true tidy format, we would need to filter out the aggregates, then merge on the country group definitions from FAOSTAT to create new country-group or regional variables that could be used to recreate aggregated estimates with dplyr.\n\n\nCode\nbirds.sm%>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n\n\n  \n\n\n\n\n\nFAOstat*.csv\nLets take a quick look at our chickens data to see if it follows the same basic pattern as the birds data. Sure enough, it looks like we have a different domain (livestock products) but that the cases remain similar country-year-product, with three slightly different estimates related to egg-laying (instead of the five types of poultry.)\n\n\nCode\nprint(summarytools::dfSummary(chickens.sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nchickens.sm\nDimensions: 38170 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Livestock Primary\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Afghanistan2. Africa3. Albania4. Algeria5. American Samoa6. Americas7. Angola8. Antigua and Barbuda9. Argentina10. Asia[ 235 others ]\n      174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)36430(95.4%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Laying2. Production3. Yield\n      12679(33.2%)12840(33.6%)12651(33.1%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Eggs, hen, in shell\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.5 (16.7)min ≤ med ≤ max:1961 ≤ 1991 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head2. 100mg/An3. tonnes\n      12679(33.2%)12651(33.1%)12840(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 291341.2 (2232761)min ≤ med ≤ max:1 ≤ 31996 ≤ 76769955IQR (CV) : 91235.8 (7.7)\n      21325 distinct values\n      \n      40\n(0.1%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Fc5. Im6. M\n      1435(4.7%)3186(10.4%)10538(34.4%)13344(43.6%)2079(6.8%)40(0.1%)\n      \n      7548\n(19.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Calculated data3. Data not available4. FAO data based on imputat5. FAO estimate6. Official data7. Unofficial figure\n      3186(8.3%)13344(35.0%)40(0.1%)2079(5.4%)10538(27.6%)7548(19.8%)1435(3.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-18\n\n\n\n\n\n\nThe “wild_bird_data” sheet is in Excel format (.xlsx) instead of the .csv format of the earlier data sets. In theory, it should be no harder to read in an Excel worksheet (or even workbook) as compared to a .csv file - there is a package called read_xl that is part of the tidyverse that easily reads in excel files.\nHowever, in practice, most people use Excel sheets as a publication format - not a way to store data, so there is almost always a ton of “junk” in the file that is NOT part of the data table that we want to read in. Sometimes the additional “junk” is incredibly useful - it might include table notes or information about data sources. However, we still need a systematic way to identify this junk and get rid of it during the data reading step.\nFor example, lets see what happens here if we just read in the wild bird data straight from excel.\n\n\nCode\nwildbirds<-read_excel(\"_data/wild_bird_data.xlsx\")\nwildbirds\n\n\n\n\n  \n\n\n\nHm, this doesn’t seem quite right. It is clear that the first “case” has information in it that looks more like variable labels. Lets take a quick look at the raw data.\n\n\n\nWild Bird Excel File\n\n\nSure enough the Excel file first row does contain additional information, a pointer to the article that this data was drawn from, and a quick Google reveals the article is [Nee, S., Read, A., Greenwood, J. et al. The relationship between abundance and body size in British birds. Nature 351, 312–313 (1991)] (https://www.nature.com/articles/351312a0)\n\nSkipping a row\nWe could try to manually adjust things - remove the first row, change the column names, and then change the column types. But this is both a lot of work, and not really a best practice for data management. Lets instead re-read the data in with the skip option from read_excel, and see if it fixes all of our problems!\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 1)\nwildbirds\n\n\n\n\n  \n\n\n\nThis now looks great! Both variables are numeric, and now they correctly show up as double or (). The variable names might be a bit tough to work with, though, so it can be easier to assign new column names on the read in - and then manually adjust axis labels, etc once you are working on your publication-quality graphs.\nNote that I skip two rows this time, and apply my own column names.\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 2, \n                        col_names = c(\"weight\", \"pop_size\"))\nwildbirds\n\n\n\n\n  \n\n\n\n\n\n\nThe railroad data set is our most challenging data to read in this week, but is (by comparison) a fairly straightforward formatted table published by the Railroad Retirement Board. The value variable is a count of the number of employees in each county and state combination. \nLooking at the excel file, we can see that there are only a few issues: 1. There are three rows at the top of the sheet that are not needed 2. There are blank columns that are not needed. 3. There are Total rows for each state that are not needed\n\nSkipping title rows\nFor the first issue, we use the “skip” option on read_excel from the readxl package to skip the rows at the top.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 3)\n\n\nNew names:\n• `` -> `...2`\n• `` -> `...4`\n\n\n\n\n  \n\n\n\n\n\nRemoving empty columns\nFor the second issue, I name the blank columns “delete” to make is easy to remove the unwanted columns. I then use select (with the ! sign to designate the complement or NOT) to select columns we wish to keep in the dataset - the rest are removed. Note that I skip 4 rows this time as I do not need the original header row.\nThere are other approaches you could use for this task (e.g., remove all columns that have no valid volues), but hard coding of variable names and types during data read in is not considered a violation of best practices and - if used strategically - can often make later data cleaning much easier.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...4`\n\n\n\n\n  \n\n\n\n\n\nFiltering “total” rows\nFor the third issue, we are going to use filter to identify (and drop the rows that have the word “Total” in the State column). str_detect can be used to find specific rows within a column that have the designated “pattern”, while the “!” designates the complement of the selected rows (i.e., those without the “pattern” we are searching for.)\nThe str_detect command is from the stringr package, and is a powerful and easy to use implementation of grep and regex in the tidyverse - the base R functions (grep, gsub, etc) are classic but far more difficult to use, particularly for those not in practice. Be sure to explore the stringr package on your own.\n\n\nCode\nrailroad<-read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...4`\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\n\n\nRemove any table notes\nTables often have notes in the last few table rows. You can check table limits and use this information during data read-in to not read the notes by setting the n-max option at the total number of rows to read, or less commonly, the range option to specify the spreadsheet range in standard excel naming (e.g., “B4:R142”). If you didn’t handle this on read in, you can use the tail command to check for notes and either tail or head to keep only the rows that you need.\n\n\nCode\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\nCode\n#remove the last two observations\nrailroad <-head(railroad, -2)\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\n\n\nConfirm cases\nAnd that is all it takes! The data are now ready for analysis. Lets see if we get the same number of unique states that were in the cleaned data in exercise 1.\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  n_distinct(.)\n\n\n[1] 54\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nOh my goodness! It seems that we have an additional “State” - it looks like Canada is in the full excel data and not the tidy data. This is one example of why it is good practice to always work from the original data source!"
  },
  {
    "objectID": "posts/challenge2_WillMunson.html",
    "href": "posts/challenge2_WillMunson.html",
    "title": "Challenge 2 Will Munson",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_WillMunson.html#challenge-overview",
    "href": "posts/challenge2_WillMunson.html#challenge-overview",
    "title": "Challenge 2 Will Munson",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_WillMunson.html#read-in-the-data",
    "href": "posts/challenge2_WillMunson.html#read-in-the-data",
    "title": "Challenge 2 Will Munson",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\nCode\nFAOstat <- read_csv(\"_data/FAOSTAT_livestock.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_WillMunson.html#describe-the-data",
    "href": "posts/challenge2_WillMunson.html#describe-the-data",
    "title": "Challenge 2 Will Munson",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nSo, essentially, this data is interpreting the value of livestock around the world. The values are either official or unofficial data, or FAO estimates. \n\nThe data has a total of over 82k slots. Value is the only quantitative variable. Each set of data was recorded between the years of 1961 and 2018. \n\n\nCode\nsummary(FAOstat)\n\n\n Domain Code           Domain            Area Code          Area          \n Length:82116       Length:82116       Min.   :   1.0   Length:82116      \n Class :character   Class :character   1st Qu.:  73.0   Class :character  \n Mode  :character   Mode  :character   Median : 146.0   Mode  :character  \n                                       Mean   : 912.7                     \n                                       3rd Qu.: 221.0                     \n                                       Max.   :5504.0                     \n                                                                          \n  Element Code    Element            Item Code        Item          \n Min.   :5111   Length:82116       Min.   : 866   Length:82116      \n 1st Qu.:5111   Class :character   1st Qu.: 976   Class :character  \n Median :5111   Mode  :character   Median :1034   Mode  :character  \n Mean   :5111                      Mean   :1018                     \n 3rd Qu.:5111                      3rd Qu.:1096                     \n Max.   :5111                      Max.   :1126                     \n                                                                    \n   Year Code         Year          Unit               Value          \n Min.   :1961   Min.   :1961   Length:82116       Min.   :0.000e+00  \n 1st Qu.:1976   1st Qu.:1976   Class :character   1st Qu.:1.250e+04  \n Median :1991   Median :1991   Mode  :character   Median :2.247e+05  \n Mean   :1990   Mean   :1990                      Mean   :1.163e+07  \n 3rd Qu.:2005   3rd Qu.:2005                      3rd Qu.:2.377e+06  \n Max.   :2018   Max.   :2018                      Max.   :1.490e+09  \n                                                  NA's   :1301       \n     Flag           Flag Description  \n Length:82116       Length:82116      \n Class :character   Class :character  \n Mode  :character   Mode  :character"
  },
  {
    "objectID": "posts/challenge2_WillMunson.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_WillMunson.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Will Munson",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nFAOstat %>%\n  group_by(Year) %>%\n  filter(Item == 'Sheep' & `Flag Description` == 'Official data') %>%\n  summarize(mean = mean(Value, na.rm = TRUE), sd = sd(Value, na.rm = TRUE))\n\n\n# A tibble: 58 × 3\n    Year     mean        sd\n   <dbl>    <dbl>     <dbl>\n 1  1961 9172935. 22937872.\n 2  1962 8667943. 23653127.\n 3  1963 8787122. 24057267.\n 4  1964 8612111. 23824573.\n 5  1965 8492538. 23745915.\n 6  1966 9094943. 23326897.\n 7  1967 7984239. 22894441.\n 8  1968 8688126. 24101623.\n 9  1969 8518124. 24300499.\n10  1970 7955016. 24019630.\n# … with 48 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nI added a chart here to get a better understanding of how the mean official data has changed overtime. Just having the numbers here isn't as helpful as having a chart.\n\n\nCode\nFAO <- FAOstat %>%\n  group_by(Year) %>%\n  filter(Item == 'Sheep' & `Flag Description` == 'Official data') %>%\n  summarize(mean = mean(Value, na.rm = TRUE), sd = sd(Value, na.rm = TRUE))\n\nplot(x = FAO$Year, y = FAO$mean)\n\n\n\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included.\nThis specific group I chose involved sheep and official data. Since the value changes every year, I chose to take the average value and standard deviation of sheep for each year. What appears to be happening here is the mean is the average value of sheep appears to be relatively stagnant between the years 2000 and 2010. After 2010, the official data shows a major surplus in the value of sheep."
  },
  {
    "objectID": "posts/challenge2_EmmaRasmussen.html",
    "href": "posts/challenge2_EmmaRasmussen.html",
    "title": "Challenge 2 Attempt",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n##Reading in the Data/Tidying\nReading the data set into R and skipping the first 3 rows so the header includes state, county, and total\nSelecting for columns that contain variables:\nRemoving state employee total rows:\nRenaming the TOTAL column to more accurate describe the variable:\nThe code below returns the highest and lowest number of counties (with railroad employees) by state."
  },
  {
    "objectID": "posts/challenge2_EmmaRasmussen.html#describe-the-data",
    "href": "posts/challenge2_EmmaRasmussen.html#describe-the-data",
    "title": "Challenge 2 Attempt",
    "section": "Describe the data",
    "text": "Describe the data\nThis data was likely gathered from government employment records or employment records directly from the railroads or Department of Transportation. Cases- counties within states, variables- states, and the number of employees at railroads within each county.\nDimensions of the “tidied” dataset:\n\n\nCode\ndim(StateCounty2012)\n\n\n[1] 2930    3\n\n\nThe data set has 2930 rows (counties with railroads) and 3 columns (State, County, and number of employees).\nThe code below creates a table with a count of counties by state. For example, Florida (FL) has 67 counties with railroads.\n\n\nCode\ntable(StateCounty2012$STATE)\n\n\n\n AE  AK  AL  AP  AR  AZ  CA  CO  CT  DC  DE  FL  GA  HI  IA  ID  IL  IN  KS  KY \n  1   6  67   1  72  15  55  57   8   1   3  67 152   3  99  36 103  92  95 119 \n LA  MA  MD  ME  MI  MN  MO  MS  MT  NC  ND  NE  NH  NJ  NM  NV  NY  OH  OK  OR \n 63  12  24  16  78  86 115  78  53  94  49  89  10  21  29  12  61  88  73  33 \n PA  RI  SC  SD  TN  TX  UT  VA  VT  WA  WI  WV  WY \n 65   5  46  52  91 221  25  92  14  39  69  53  22 \n\n\nI was shocked to see that Texas has 221 counties with people who work for railroads, compared to MA with 12 counties. A quick Google Search says Texas actually has 254 counties. Since Texas has the greatest number of counties with railroad employees, I will look closer at the distribution of employees across Texas counties."
  },
  {
    "objectID": "posts/challenge2_EmmaRasmussen.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_EmmaRasmussen.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Attempt",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nFiltering the subgroup of New Hampshire counties: (I used to live on the NH border)\n\n\nCode\nfilter(StateCounty2012, STATE == \"NH\")\n\n\n# A tibble: 10 × 3\n   STATE COUNTY       Total_employees\n   <chr> <chr>                  <dbl>\n 1 NH    BELKNAP                    2\n 2 NH    CARROLL                   12\n 3 NH    CHESHIRE                  28\n 4 NH    COOS                      19\n 5 NH    GRAFTON                    7\n 6 NH    HILLSBOROUGH             136\n 7 NH    MERRIMACK                  9\n 8 NH    ROCKINGHAM               146\n 9 NH    STRAFFORD                 27\n10 NH    SULLIVAN                   7\n\n\nCalculating the Mode:\n\n\nCode\nNHTable<-filter(StateCounty2012, STATE == \"NH\")%>%\n  arrange(Total_employees)%>%\n  count(Total_employees)%>%\n  arrange(desc(n))\nNHTable$Total_employees[1]\n\n\n[1] 7\n\n\nWhile not super helpful, the most common number of employees at NH railroads by county is 7.\nOther summary statistics for the subgroup of NH counties:\n\n\nCode\nStateCounty2012\n\n\n# A tibble: 2,930 × 3\n   STATE COUNTY               Total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nStateCounty2012%>%\n  filter(STATE == \"NH\") %>%\n  summarize(\"NHmin\"= min(Total_employees), \"NHmax\"= max(Total_employees), \"NHmean\"= mean(Total_employees), \"NHmedian\"= median(Total_employees), \"NHmode\"= mode(Total_employees), \"NHsd\" = sd(Total_employees), \"NHIQR\"= IQR(Total_employees))\n\n\n# A tibble: 1 × 7\n  NHmin NHmax NHmean NHmedian NHmode   NHsd NHIQR\n  <dbl> <dbl>  <dbl>    <dbl> <chr>   <dbl> <dbl>\n1     2   146   39.3     15.5 numeric  54.3  20.2\n\n\nFinding the county in each state with the largest number of employees:\n\n\nCode\nStateCountyLarge<-group_by(StateCounty2012, STATE)%>%\n  arrange(StateCounty2012, (\"STATE\"), desc(\"Total_employees\"))%>%\n  slice(1)\nStateCountyLarge\n\n\n# A tibble: 53 × 3\n# Groups:   STATE [53]\n   STATE COUNTY        Total_employees\n   <chr> <chr>                   <dbl>\n 1 AE    APO                         2\n 2 AK    ANCHORAGE                   7\n 3 AL    AUTAUGA                   102\n 4 AP    APO                         1\n 5 AR    ARKANSAS                   11\n 6 AZ    APACHE                    270\n 7 CA    ALAMEDA                   346\n 8 CO    ADAMS                     553\n 9 CT    FAIRFIELD                 486\n10 DC    WASHINGTON DC             279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nFinding the mean of the largest total employee numbers in each state (this is a pretty useless stat but I was just trying to find some summary statistics from a grouped/sliced data set). I don’t love this code, there are too many variables but it’s not working when I edit it any more to try to calculate the median etc.\n\n\nCode\nStateMax<-summarize(StateCountyLarge, mt= mean(Total_employees))\nTotalStateMean<-summarize(StateMax, mt=mean(mt))\nprint(TotalStateMean)\n\n\n# A tibble: 1 × 1\n     mt\n  <dbl>\n1  93.0\n\n\nThe mean of the largest total employee numbers across the 53 included states/territories is 93.03774 employees.\n\nExplain and Interpret\nI chose NH because I was curious about the distribution of railroad employees by county in that area (I grew up on the NH border). There is lots of variability in NH railroad employment. Rockingham county has 146 employees, while Belknap county has only 2 railroad employees. I then tried to slice out the counties with the most employees by state to see how many employees there are on average at the “biggest” railroad counties in each state. I was surprised comparing the NH max to the mean of the largest railroads by county in each state(the second data analyzed that was sliced). I was surprise given the mean number of employees at the “largest” railroad/county in each state was only 93, but the max for NH is 146. I don’t think of NH has a big state or having a lot of railroads. One thing that might expplain this is that states like Texas (over 100) have more counties than NH (10) skewing the mean (93.0377). In comparison, NH would have larger employment numbers by county given it only has 10 counties."
  },
  {
    "objectID": "posts/challenge3_instructions.html",
    "href": "posts/challenge3_instructions.html",
    "title": "Challenge 3 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions.html#challenge-overview",
    "href": "posts/challenge3_instructions.html#challenge-overview",
    "title": "Challenge 3 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions.html#read-in-data",
    "href": "posts/challenge3_instructions.html#read-in-data",
    "title": "Challenge 3 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data\nDescribe the data, and be sure to comment on why you are planning to pivot it to make it “tidy”"
  },
  {
    "objectID": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "href": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "title": "Challenge 3 Instructions",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA    1170.    1356.\n2 USA      1990 NAFTA    -166.    2329.\n3 France   1980 EU        737.    1555.\n4 Mexico   1990 NAFTA    1420.    1313.\n5 USA      1980 NAFTA    1643.    2513.\n6 France   1990 EU        728.    -213.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge3_instructions.html#pivot-the-data",
    "href": "posts/challenge3_instructions.html#pivot-the-data",
    "title": "Challenge 3 Instructions",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1170.\n 2 Mexico   1980 NAFTA incoming              1356.\n 3 USA      1990 NAFTA outgoing              -166.\n 4 USA      1990 NAFTA incoming              2329.\n 5 France   1980 EU    outgoing               737.\n 6 France   1980 EU    incoming              1555.\n 7 Mexico   1990 NAFTA outgoing              1420.\n 8 Mexico   1990 NAFTA incoming              1313.\n 9 USA      1980 NAFTA outgoing              1643.\n10 USA      1980 NAFTA incoming              2513.\n11 France   1990 EU    outgoing               728.\n12 France   1990 EU    incoming              -213.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge2_ManiShankerKamarapu.html",
    "href": "posts/challenge2_ManiShankerKamarapu.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_ManiShankerKamarapu.html#read-in-the-data",
    "href": "posts/challenge2_ManiShankerKamarapu.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\nFAOSTAT_cattle_dairy <- read_csv(\"_data/FAOSTAT_cattle_dairy.csv\")\nView(FAOSTAT_cattle_dairy)\n\n\nAs we read the data, there are many interesting variables and a lot of observations based on yearly analysis of the various cattle products."
  },
  {
    "objectID": "posts/challenge2_ManiShankerKamarapu.html#describe-the-data",
    "href": "posts/challenge2_ManiShankerKamarapu.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\ndim(FAOSTAT_cattle_dairy)\n\n\n[1] 36449    14\n\n\nWe can see that there are 36449 rows and 14 columns in the data. We can see from observations from different variables that “Value” variable is pretty much the only real valuable data we are getting from this data set and remaining variables are either unchanged(Domain and Item) or grouping variables(Area, Element, Year and Flag) which can be used to summarize data and find the categories we are interested in and also can be used to find specific information we need and let’s try to get to some more useful info using some other functions.\n\n\nCode\ncolnames(FAOSTAT_cattle_dairy)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nThese are the variables available in the data set.\n\n\nCode\nsummary(filter(select(FAOSTAT_cattle_dairy, Value)))\n\n\n     Value          \n Min.   :        7  \n 1st Qu.:     7849  \n Median :    43266  \n Mean   :  4410235  \n 3rd Qu.:   700000  \n Max.   :683217055  \n NA's   :74         \n\n\nIt is the summary of the Value variable of the data set which is the only real data variable in data set.\n\n\nCode\nArea_num <- distinct(FAOSTAT_cattle_dairy, Area)\nElement_num <- distinct(FAOSTAT_cattle_dairy, Element)\nYear_num <- distinct(FAOSTAT_cattle_dairy, Year)\nFlag_num <- distinct(FAOSTAT_cattle_dairy, Flag)\nDistinct_count <- c(nrow(Area_num), nrow(Element_num), nrow(Year_num), nrow(Flag_num))\nDistinct_count\n\n\n[1] 232   3  58   7\n\n\nWe can observe that data set has the data from 1961-2018 on 3 elements at 232 different areas.\nI am interested in finding in more information about Flag and how it corresponds to the description they have given and can we group the values by using it.\n\n\nCode\nFLag_info <- FAOSTAT_cattle_dairy %>%\n  select(Flag,`Flag Description`)\nunique(FLag_info)\n\n\n# A tibble: 7 × 2\n  Flag  `Flag Description`                                                      \n  <chr> <chr>                                                                   \n1 F     FAO estimate                                                            \n2 Fc    Calculated data                                                         \n3 <NA>  Official data                                                           \n4 *     Unofficial figure                                                       \n5 Im    FAO data based on imputation methodology                                \n6 M     Data not available                                                      \n7 A     Aggregate, may include official, semi-official, estimated or calculated…\n\n\nFrom the above observation we got to know there are 7 types of flags and it is based on data provided and type of data and values they are and how is has been taken."
  },
  {
    "objectID": "posts/challenge2_ManiShankerKamarapu.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_ManiShankerKamarapu.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\n\nArea and Element wise analysis\n\n\nCode\nAE_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Area, Element) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE))\nAE_analysis\n\n\n# A tibble: 695 × 8\n# Groups:   Area [232]\n   Area        Element      mean_value median_…¹ sd_va…² min_v…³ max_v…⁴ IQR_v…⁵\n   <chr>       <chr>             <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Afghanistan Milk Animals   1790863.  1150000   1.08e6  7   e5  4.05e6  1.57e6\n 2 Afghanistan Production      914876.   600000   5.01e5  3   e5  1.87e6  8.86e5\n 3 Afghanistan Yield             5246.     5128   9.06e2  3.59e3  7.33e3  6.35e2\n 4 Africa      Milk Animals  37629052. 33173522.  1.75e7  1.70e7  7.13e7  3.00e7\n 5 Africa      Production    18949137. 15058869   1.02e7  7.65e6  3.68e7  1.88e7\n 6 Africa      Yield             4869.     4600   4.39e2  4.31e3  5.84e3  6.93e2\n 7 Albania     Milk Animals    288774.   284650   1.13e5  1.17e5  4.88e5  1.92e5\n 8 Albania     Production      521367.   392500   3.37e5  7.83e4  9.83e5  6.78e5\n 9 Albania     Yield            16197     13857   6.61e3  6.35e3  2.84e4  8.11e3\n10 Algeria     Milk Animals    659719.   640000   2.50e5  2.63e5  1.11e6  4.14e5\n# … with 685 more rows, and abbreviated variable names ¹​median_value,\n#   ²​sd_value, ³​min_value, ⁴​max_value, ⁵​IQR_value\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs per the analysis, in each area the we have three kinds of elements and there are a lot of variations and mean value is not dependent to element itself in any area but it differs in each area and we can get a deeper understanding when we analyse the area and element separately.\n\n\nYear and Element wise analysis\n\n\nCode\nYE_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Year, Element) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE))\nYE_analysis\n\n\n# A tibble: 174 × 8\n# Groups:   Year [58]\n    Year Element      mean_value median_value  sd_value min_va…¹ max_v…² IQR_v…³\n   <dbl> <chr>             <dbl>        <dbl>     <dbl>    <dbl>   <dbl>   <dbl>\n 1  1961 Milk Animals   3582516.       200550 15152245.        8  1.77e8  1.23e6\n 2  1961 Production     6335891.       100368 28456161.        7  3.14e8  1.74e6\n 3  1961 Yield            11949.         8812     9905.     1200  4.29e4  1.41e4\n 4  1962 Milk Animals   3567116.       200850 15117111.       17  1.77e8  1.21e6\n 5  1962 Production     6384721.       100000 28667301.       23  3.16e8  1.76e6\n 6  1962 Yield            12105.         8899    10173.     1200  4.43e4  1.41e4\n 7  1963 Milk Animals   3578137.       219550 15189434.       20  1.77e8  1.21e6\n 8  1963 Production     6319684.       102500 28305726.       20  3.13e8  1.78e6\n 9  1963 Yield            12147.         9051    10241.     1191  4.59e4  1.44e4\n10  1964 Milk Animals   3554094.       215650 15090377.       25  1.76e8  1.25e6\n# … with 164 more rows, and abbreviated variable names ¹​min_value, ²​max_value,\n#   ³​IQR_value\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs per the data, we can say that the elements has been increased by passing year, there are some fluctuations in between but there is an overall increase by the year.\n\n\nElement-wise analysis\n\n\nCode\nE_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Element) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE)) %>%\n  arrange(desc(mean_value))\nE_analysis\n\n\n# A tibble: 3 × 7\n  Element      mean_value median_value  sd_value min_value max_value IQR_value\n  <chr>             <dbl>        <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n1 Production     9001419.       295500 40268994.         7 683217055   2736860\n2 Milk Animals   4205410.       295000 18041595.         8 276573845   1525233\n3 Yield            19329.        13218    19361.       923    134121     21367\n\n\nAs per the above observation, the mean value of the production is maximum and yield is the minimum. It shows that the production value is more with the less yield and it leads to an overall profit.\n\n\nArea-wise analysis\n\n\nCode\nA_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Area) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE)) %>%\n  arrange(desc(mean_value))\nA_analysis\n\n\n# A tibble: 232 × 7\n   Area                     mean_value median_…¹ sd_va…² min_v…³ max_v…⁴ IQR_v…⁵\n   <chr>                         <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 World                    230162236.    2.20e8  2.02e8   17662  6.83e8  3.87e8\n 2 Europe                    99143192.    8.26e7  9.82e7   21977  2.80e8  2.09e8\n 3 Americas                  56060267.    4.51e7  5.50e7   22342  1.84e8  9.12e7\n 4 Asia                      48224349.    3.71e7  5.08e7    6034  2.13e8  7.49e7\n 5 Eastern Europe            45406646.    4.82e7  4.33e7   16644  1.40e8  7.46e7\n 6 USSR                      42870195.    4.18e7  3.68e7   15900  1.08e8  8.09e7\n 7 Western Europe            30380343.    1.67e7  3.27e7   30476  8.74e7  7.21e7\n 8 Northern America          29827044.    1.14e7  3.49e7   32302  1.06e8  6.43e7\n 9 Southern Asia             28165903.    2.50e7  2.75e7    3824  1.17e8  4.64e7\n10 United States of America  26633354.    1.00e7  3.15e7   33068  9.87e7  5.61e7\n# … with 222 more rows, and abbreviated variable names ¹​median_value,\n#   ²​sd_value, ³​min_value, ⁴​max_value, ⁵​IQR_value\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs per the analysis, Europe has the highest cattle dairy and British Virgin Islands has the minimum, we can say none.\n\n\nYear-wise analysis\n\n\nCode\nY_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Year) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE)) %>%\n  arrange(desc(mean_value))\nY_analysis\n\n\n# A tibble: 58 × 7\n    Year mean_value median_value  sd_value min_value max_value IQR_value\n   <dbl>      <dbl>        <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1  2017   5682309.       80522. 33349614.        20 677670685   958680 \n 2  2018   5654115.       79029  33488177.        21 683217055   938124 \n 3  2016   5617407.       77378. 32809823.        20 665596536   981852.\n 4  2015   5586308.       78062. 32577146.        20 661430554   964494 \n 5  2014   5534018.       77490  32246947.        20 655245580   947544.\n 6  2013   5411995.       75958. 31386003.        20 635379383   964509 \n 7  2012   5372784.       74057  31140061.        20 630244839   946680 \n 8  2011   5287165.       71750  30561395.        20 616177381   953420.\n 9  2010   5185483.       70586  29911569.        20 601868328   928111 \n10  2009   5088955.       70375  29357924.         9 590471016   895700 \n# … with 48 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs per the data, the cattle dairy production has been increased by the passing year, there are some fluctuations in middle but the overall it is increasing, we can see from the data that the increase is nearly 85 percentage from 1961 to 2018.\n\n\nExplain and Interpret\nI chose Element, Area and Year subgroups from data set. The reason for choosing them is there importance in analysis and also tried to analysis using two variables to know the dependency of values on different variables. So I have done Area and Element wise analysis, Year and Element wise analysis, Element-wise analysis, Area-wise analysis and Year-wise analysis. Conclusion to my analysis would be Europe has the highest of the cattle dairy and the production of the dairy is increasing gradually as by the year."
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html",
    "href": "posts/challenge1_TylerTewksbury.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html#challenge-overview",
    "href": "posts/challenge1_TylerTewksbury.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html#read-in-the-data",
    "href": "posts/challenge1_TylerTewksbury.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nrailroad_data <- read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html#describe-the-data",
    "href": "posts/challenge1_TylerTewksbury.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nrailroad_data %>%\nsummary()\n\n\n    state              county          total_employees  \n Length:2930        Length:2930        Min.   :   1.00  \n Class :character   Class :character   1st Qu.:   7.00  \n Mode  :character   Mode  :character   Median :  21.00  \n                                       Mean   :  87.18  \n                                       3rd Qu.:  65.00  \n                                       Max.   :8207.00  \n\n\nCode\nlength(unique(railroad_data$state))\n\n\n[1] 53\n\n\nThe dataset is a simple, pre cleaned spreadsheet consisting of three columns: State, County, and Total_Employees. These three columns are self explanatory, labeling the state, county, and the amount of employees in said railroad system. Using the summary function,we can see that there are 2930 reported counties within 53 states (including DC, Armed Forces Europe, Armed Forces Pacific). This data could be useful to determine the size of specific railroad systems based on employment. The dataset could be enhanced by adding in overall population data so one can answer questions such as the percent of people in a county who work on railroads."
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html",
    "href": "posts/challenge2_TylerTewksbury.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#challenge-overview",
    "href": "posts/challenge2_TylerTewksbury.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#read-in-the-data",
    "href": "posts/challenge2_TylerTewksbury.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\nCode\ndf <- read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#describe-the-data",
    "href": "posts/challenge2_TylerTewksbury.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nsummary(df)\n\n\n    state              county          total_employees  \n Length:2930        Length:2930        Min.   :   1.00  \n Class :character   Class :character   1st Qu.:   7.00  \n Mode  :character   Mode  :character   Median :  21.00  \n                                       Mean   :  87.18  \n                                       3rd Qu.:  65.00  \n                                       Max.   :8207.00  \n\n\nThis summary shows the amount of values in the three columns, as well as a high level overview of said values. State and county are both characters, which likely was taken from an existing survey data spreadsheet. Summary also gives us a brief insight into the total_employees, showing the max, min, median, etc. Just with this summary, questions can be asked about the data. How many counties correlate to each state? Do states with more counties have a lower average of employees? Many questions can be asked, but this correlation between county count and employee count average will be looked into."
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_TylerTewksbury.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nstates_mean <- df %>%\n  group_by(state) %>%\n  summarise(mean_employee = mean(total_employees)) %>%\n  arrange(desc(mean_employee), .by_group = TRUE)\n\nstate_county_count <- df %>%\n  count(state) %>%\n  arrange(n)\n\nstates_mean <- df %>%\n  group_by(state) %>%\n  summarise(mean_employee = mean(total_employees)) %>%\n  arrange(desc(mean_employee), .by_group = TRUE)\n\nstate_county_count <- df %>%\n  count(state) %>%\n  arrange(n)\n\nstate_info <- inner_join(state_county_count, states_mean)\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included.\nI performed three different analyses to get a deeper understanding of the data. First, I grouped by state and calculated the mean of total employees per state, and sorted by descending. This allowed me to see the states with, what can be presumed to be, the highest concentration of employees. Next, to get a better understanding at the state level, I calculated the amount of counties in each state, sorting by ascending. This would allow me to confirm if a state with a high mean would have few counties as well. The theory seemed to be true, as there were a few I recognized by manually looking. However, I could confirm this using R. To do this, I simply repeated the first two analayses, this time creating new data frames out of them. I then ran an inner join to join via state, creating a new table with all the data I calculated. Using this table, visualizations can be made to test/prove the hypothesis of low county count = higher number of employees in their counties."
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html",
    "href": "posts/challenge2_AnimeshSengupta.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#challenge-overview",
    "href": "posts/challenge2_AnimeshSengupta.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#read-in-the-data",
    "href": "posts/challenge2_AnimeshSengupta.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nFAOstat*.csv ⭐⭐⭐\n\n\n\nCode\n#! label: Read Data\nFAOstat_livestock <- read_csv(\"../posts/_data/FAOSTAT_livestock.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#describe-the-data",
    "href": "posts/challenge2_AnimeshSengupta.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\n#Data Dimensions\ndim(FAOstat_livestock)\n\n\n[1] 82116    14\n\n\nCode\n#Data Columns\ncolnames(FAOstat_livestock)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nCode\n#Data Preview\nhead(FAOstat_livestock,3)\n\n\n# A tibble: 3 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1961  1961 Head \n2 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1962  1962 Head \n3 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1963  1963 Head \n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names"
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#processed-data-for-analysis-and-some-information-about-it",
    "href": "posts/challenge2_AnimeshSengupta.html#processed-data-for-analysis-and-some-information-about-it",
    "title": "Challenge 2 Instructions",
    "section": "Processed Data for analysis and some information about it",
    "text": "Processed Data for analysis and some information about it\n\n\nCode\n#! label: Isolate data for analysis\nFAOstat_livestock_main <- select(FAOstat_livestock,\"Area\",\"Item\",\"Year\",\"Unit\",\"Value\")\nhead(FAOstat_livestock_main,5)\n\n\n# A tibble: 5 × 5\n  Area        Item   Year Unit    Value\n  <chr>       <chr> <dbl> <chr>   <dbl>\n1 Afghanistan Asses  1961 Head  1300000\n2 Afghanistan Asses  1962 Head   851850\n3 Afghanistan Asses  1963 Head  1001112\n4 Afghanistan Asses  1964 Head  1150000\n5 Afghanistan Asses  1965 Head  1300000\n\n\nCode\ncountries_num<-n_distinct(FAOstat_livestock_main$Area)\nyear_vector<-unique(FAOstat_livestock_main$Year)\nunique(FAOstat_livestock_main$Item)\n\n\n[1] \"Asses\"     \"Camels\"    \"Cattle\"    \"Goats\"     \"Horses\"    \"Mules\"    \n[7] \"Sheep\"     \"Buffaloes\" \"Pigs\"     \n\n\nThe data collected for FAO was between 1961 to 2018 for Areas in total.\nI am interested in finding the expenditure statistics on livestock for these countries. Lets find out below."
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_AnimeshSengupta.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nct_analysis <- FAOstat_livestock_main %>%\n                group_by(Area,Item)%>%\n                summarise(mean_val=mean(Value,na.rm = TRUE),\n                          median_val=median(Value,na.rm = TRUE),\n                          .groups = 'drop')\ndim(ct_analysis)\n\n\n[1] 1566    4\n\n\nCode\ncolnames(ct_analysis)\n\n\n[1] \"Area\"       \"Item\"       \"mean_val\"   \"median_val\"\n\n\n##Item wise by analysis\n\n\nCode\nct_item_group <- ct_analysis %>%\n  group_by(Item)%>%\n  summarise(mean_val=mean(mean_val,na.rm = TRUE))%>%\n  arrange(desc(mean_val))\n\nhead(ct_item_group,5)\n\n\n# A tibble: 5 × 2\n  Item       mean_val\n  <chr>         <dbl>\n1 Cattle    21058163.\n2 Sheep     20137879.\n3 Pigs      14207291.\n4 Goats     10801180.\n5 Buffaloes  8583673.\n\n\nAs per the data , Cattle generates the most value for any country , with the mean value being 2.1058163^{7}. We can speculatively attribute this high value to cattle due to its important role in society. For example, a cattle generates value not only be meat consumption but also dairy production. On the other hand, the Mule with lowest mean value of 4.4022093^{5}, has lesser societal value.\n##Area wise analysis\n\n\nCode\nct_area_group <- ct_analysis %>%\n  group_by(Area)%>%\n  summarise(mean_val=mean(mean_val,na.rm = TRUE))%>%\n  arrange(desc(mean_val))\n\nhead(ct_area_group,15)\n\n\n# A tibble: 15 × 2\n   Area                        mean_val\n   <chr>                          <dbl>\n 1 World                     449961866.\n 2 Asia                      190587493.\n 3 Americas                   95795716.\n 4 Eastern Asia               80092992.\n 5 Southern Asia              78704652.\n 6 Africa                     78159910.\n 7 China, mainland            73083831.\n 8 Europe                     71745016.\n 9 South America              56288859.\n10 India                      48618161.\n11 USSR                       36287113.\n12 Australia and New Zealand  36010992.\n13 Eastern Europe             34383679.\n14 Oceania                    31265613.\n15 Northern America           29793900.\n\n\nAmong the countries, Mainland China and India are the biggest mean producer of livestock value.\n##Time wise analysis of largest producer for most valuable livestock\n\n\nCode\nct_time_series <- FAOstat_livestock_main %>%\n  filter(Item == 'Cattle' & Area == 'China, mainland')%>%\n  group_by(Year)%>%\n  summarise(mean_val=mean(Value,na.rm = TRUE))%>%\n  arrange(Year)\n\nggplot(data = ct_time_series, aes(x = Year, y = mean_val)) +\n  geom_point()\n\n\n\n\n\nAs per the time chart plot, the cattle production started to increase substantially during the late 1900s and saw a peak during early 2000. Since then cattle production has a seen a downfall, probably attributing to advent of technology\n\nExplain and Interpret\nFor my analysis I chose the subgroup of Area, Item, Time and Value from the FAO Livestock dataset. The reason for choosing such a group of features was because of its high meaningfulness. This would also allow me to conduct Area wise, Item wise and time series Analysis. Conclusion to my analysis would be that Cattle is the MVP while China is the largest producer of livestock."
  },
  {
    "objectID": "posts/challenge1_AnimeshSengupta.html",
    "href": "posts/challenge1_AnimeshSengupta.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_AnimeshSengupta.html#challenge-overview",
    "href": "posts/challenge1_AnimeshSengupta.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_AnimeshSengupta.html#read-in-the-data",
    "href": "posts/challenge1_AnimeshSengupta.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\ndebt_data <- read_excel(\"../posts/_data/debt_in_trillions.xlsx\")\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_AnimeshSengupta.html#describe-the-data",
    "href": "posts/challenge1_AnimeshSengupta.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\ndim(debt_data)\n\n\n[1] 74  8\n\n\nCode\ncolnames(debt_data)\n\n\n[1] \"Year and Quarter\" \"Mortgage\"         \"HE Revolving\"     \"Auto Loan\"       \n[5] \"Credit Card\"      \"Student Loan\"     \"Other\"            \"Total\"           \n\n\nCode\nhead(debt_data,5)\n\n\n# A tibble: 5 × 8\n  `Year and Quarter` Mortgage `HE Revolving` Auto …¹ Credi…² Stude…³ Other Total\n  <chr>                 <dbl>          <dbl>   <dbl>   <dbl>   <dbl> <dbl> <dbl>\n1 03:Q1                  4.94          0.242   0.641   0.688   0.241 0.478  7.23\n2 03:Q2                  5.08          0.26    0.622   0.693   0.243 0.486  7.38\n3 03:Q3                  5.18          0.269   0.684   0.693   0.249 0.477  7.56\n4 03:Q4                  5.66          0.302   0.704   0.698   0.253 0.449  8.07\n5 04:Q1                  5.84          0.328   0.72    0.695   0.260 0.446  8.29\n# … with abbreviated variable names ¹​`Auto Loan`, ²​`Credit Card`,\n#   ³​`Student Loan`"
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html",
    "href": "posts/challenge3_EmmaRasmussen.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html#challenge-overview",
    "href": "posts/challenge3_EmmaRasmussen.html#challenge-overview",
    "title": "Challenge 3",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html#read-in-data",
    "href": "posts/challenge3_EmmaRasmussen.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\nanimal_weightOG<-animal_weight#saving a copy of the original data set\nanimal_weight\n\n\n# A tibble: 9 × 17\n  IPCC A…¹ Cattl…² Cattl…³ Buffa…⁴ Swine…⁵ Swine…⁶ Chick…⁷ Chick…⁸ Ducks Turkeys\n  <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl>\n1 Indian …     275     110     295      28      28     0.9     1.8   2.7     6.8\n2 Eastern…     550     391     380      50     180     0.9     1.8   2.7     6.8\n3 Africa       275     173     380      28      28     0.9     1.8   2.7     6.8\n4 Oceania      500     330     380      45     180     0.9     1.8   2.7     6.8\n5 Western…     600     420     380      50     198     0.9     1.8   2.7     6.8\n6 Latin A…     400     305     380      28      28     0.9     1.8   2.7     6.8\n7 Asia         350     391     380      50     180     0.9     1.8   2.7     6.8\n8 Middle …     275     173     380      28      28     0.9     1.8   2.7     6.8\n9 Norther…     604     389     380      46     198     0.9     1.8   2.7     6.8\n# … with 7 more variables: Sheep <dbl>, Goats <dbl>, Horses <dbl>, Asses <dbl>,\n#   Mules <dbl>, Camels <dbl>, Llamas <dbl>, and abbreviated variable names\n#   ¹​`IPCC Area`, ²​`Cattle - dairy`, ³​`Cattle - non-dairy`, ⁴​Buffaloes,\n#   ⁵​`Swine - market`, ⁶​`Swine - breeding`, ⁷​`Chicken - Broilers`,\n#   ⁸​`Chicken - Layers`\n# ℹ Use `colnames()` to see all variable names\n\n\n\nBriefly describe the data\nDescribe the data, and be sure to comment on why you are planning to pivot it to make it “tidy”\nThe data appears to illustrate (average?) animal weights by region. To tidy the data we will pivot the columns with animal names into a single column. Each “case” is an animal type within a region, and the values/dependent variable is the weight."
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html#anticipate-the-end-result",
    "href": "posts/challenge3_EmmaRasmussen.html#anticipate-the-end-result",
    "title": "Challenge 3",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA    1478.    1410.\n2 USA      1990 NAFTA    1346.     484.\n3 France   1980 EU       1272.    2470.\n4 Mexico   1990 NAFTA     474.    -233.\n5 USA      1980 NAFTA    1642.    1898.\n6 France   1990 EU       1464.     908.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\nOG Dataset has k=17 columns and n=9 rows. 17-1(1 existing variable to describe each case (country), the other 16 columns need to be pivoted) We wll now have three columns, one region, one animalm(new col) (together the IV), one weight(the DV) (new col) 9*16 rows expected in data frame= 144 3col byt 144 rows rows expected\n\n\nCode\nnrow(animal_weightOG)\n\n\n[1] 9\n\n\nCode\nncol(animal_weightOG)\n\n\n[1] 17\n\n\nCode\n#expected rows/cases\nnrow(animal_weightOG)*(ncol(animal_weightOG)-1)\n\n\n[1] 144\n\n\nCode\n#expected columns\n1+2\n\n\n[1] 3\n\n\nAny additional comments? See comment at bottom"
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html#pivot-the-data",
    "href": "posts/challenge3_EmmaRasmussen.html#pivot-the-data",
    "title": "Challenge 3",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1478.\n 2 Mexico   1980 NAFTA incoming              1410.\n 3 USA      1990 NAFTA outgoing              1346.\n 4 USA      1990 NAFTA incoming               484.\n 5 France   1980 EU    outgoing              1272.\n 6 France   1980 EU    incoming              2470.\n 7 Mexico   1990 NAFTA outgoing               474.\n 8 Mexico   1990 NAFTA incoming              -233.\n 9 USA      1980 NAFTA outgoing              1642.\n10 USA      1980 NAFTA incoming              1898.\n11 France   1990 EU    outgoing              1464.\n12 France   1990 EU    incoming               908.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nA case will be an animal from a particular region. It meets the requirements for tidy data because each case has its own row, and each variable has its own column.\n\n\nCode\npivot_longer(animal_weight, col = c(2:17),\n                 names_to=\"animal_type\",\n                 values_to = \"weight\")\n\n\n# A tibble: 144 × 3\n   `IPCC Area`         animal_type        weight\n   <chr>               <chr>               <dbl>\n 1 Indian Subcontinent Cattle - dairy      275  \n 2 Indian Subcontinent Cattle - non-dairy  110  \n 3 Indian Subcontinent Buffaloes           295  \n 4 Indian Subcontinent Swine - market       28  \n 5 Indian Subcontinent Swine - breeding     28  \n 6 Indian Subcontinent Chicken - Broilers    0.9\n 7 Indian Subcontinent Chicken - Layers      1.8\n 8 Indian Subcontinent Ducks                 2.7\n 9 Indian Subcontinent Turkeys               6.8\n10 Indian Subcontinent Sheep                28  \n# … with 134 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAny additional comments?\nI am a little confused by the calculation, I tried to work out everything above in a way that made sense. But we essentially start with: - how many columns will remain unpivoted (variables that start in the correct place/column) -how many columns are being pivoted (the rest or starting number of col minus number above) -The number of columns being pivoted*number of rows = new number of rows -the new number of columns is the unchanged columns plus 1 for the variables contained in the pivot plus one for the values?? So unchanged col+2"
  },
  {
    "objectID": "posts/challenge1_nickboonstra.html",
    "href": "posts/challenge1_nickboonstra.html",
    "title": "Nick Boonstra Challenge 1 Resubmit",
    "section": "",
    "text": "This challenge involves reading in and cleaning up data from the “birds” data set.\n\n\n\n\nCode\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_nickboonstra.html#reading-in-and-cleaning-up",
    "href": "posts/challenge1_nickboonstra.html#reading-in-and-cleaning-up",
    "title": "Nick Boonstra Challenge 1 Resubmit",
    "section": "Reading In and Cleaning Up",
    "text": "Reading In and Cleaning Up\n\n\nCode\nbirds<-read_csv(\"_data/birds.csv\")\nbirds\n\n\n# A tibble: 30,977 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1961  1961\n 2 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1962  1962\n 3 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1963  1963\n 4 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1964  1964\n 5 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1965  1965\n 6 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1966  1966\n 7 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1967  1967\n 8 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1968  1968\n 9 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1969  1969\n10 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1970  1970\n# … with 30,967 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nFortunately, all of the data came in tidy, meaning:\n\nEach column measured observations of only one variable;\nEach row provided values for only one observation; and,\nEach cell contained only one value.\n\n\nData Cleanup\nHowever, while every column was nominally tidy-compliant, a number of them were extraneous, in that they provided redundant or useless data. These columns, specifically, were “Domain,” “Domain Code,” “Element,” “Element Code,” “Unit,” and “Year Code.”\n“Year Code” was redundant, in that its values were equal to “Year” for every observation. Of course, attempting to verify this by hand would be borderline impossible, but luckily this was easily testable by some quick code:\n\n\nCode\nbirds_test <- read_csv(\"_data/birds.csv\")\nbirds_test <- birds_test %>%\n  mutate(year_test = case_when(\n    Year == `Year Code` ~ 1,\n    TRUE ~ 0\n  ))\ncount(birds_test,year_test)\n\n\n# A tibble: 1 × 2\n  year_test     n\n      <dbl> <int>\n1         1 30977\n\n\nCode\nrm(birds_test)\n\n\nBecause the value of value of the dummy “year_test” variable is equal to 1 for all observations, we can know that “Year Code” was equal to “Year” for all observations, and thus eliminate “Year Code” without losing any information.\nIn the case of the other five columns named above, all observations contained the same value, making the columns practically useless. Once again, this assertion was easily testable:\n\n\nCode\ncount(birds,Domain)\n\n\n# A tibble: 1 × 2\n  Domain           n\n  <chr>        <int>\n1 Live Animals 30977\n\n\nCode\ncount(birds,`Domain Code`)\n\n\n# A tibble: 1 × 2\n  `Domain Code`     n\n  <chr>         <int>\n1 QA            30977\n\n\nCode\ncount(birds,Element)\n\n\n# A tibble: 1 × 2\n  Element     n\n  <chr>   <int>\n1 Stocks  30977\n\n\nCode\ncount(birds,`Element Code`)\n\n\n# A tibble: 1 × 2\n  `Element Code`     n\n           <dbl> <int>\n1           5112 30977\n\n\nCode\ncount(birds,Unit)\n\n\n# A tibble: 1 × 2\n  Unit          n\n  <chr>     <int>\n1 1000 Head 30977\n\n\nThus, with each column only carrying one value throughout the data set, these columns could be deleted without pratically losing information. In this case, however, the values being removed from the dataframe are not already being kept somewhere else in the dataframe, as opposed to the duplication found in the Year/Year Code case. Thus, recording these values in a separate location may be desirable, depending upon the exact nature of the dataset and the desired analysis.\nBeyond removing these extraneous columns, the only other adjustments I found necessary were to rename the remaining columns to abide by “snake_case.” This was done for practicality (some of the column names had spaces), consistency, and personal preference.\nAll of these changes, then, are seen here:\n\n\nCode\nbirds<-birds %>%\n  select(-starts_with(\"Domain\")) %>% ## All values identical for all obs\n  rename(area_code = `Area Code`) %>%\n  rename(area = Area) %>%\n  select(-starts_with(\"Element\")) %>% ## All values identical for all obs\n  rename(item_code = `Item Code`) %>%\n  rename(item = Item) %>%\n  select(-`Year Code`) %>% ## Values identical to Year for all obs\n  rename(year = Year) %>%\n  select(-Unit) %>% ## All values identical for all obs\n  rename(value = Value) %>%\n  rename(flag = Flag) %>%\n  rename(flag_desc = `Flag Description`)\n\nbirds\n\n\n# A tibble: 30,977 × 8\n   area_code area        item_code item      year value flag  flag_desc    \n       <dbl> <chr>           <dbl> <chr>    <dbl> <dbl> <chr> <chr>        \n 1         2 Afghanistan      1057 Chickens  1961  4700 F     FAO estimate \n 2         2 Afghanistan      1057 Chickens  1962  4900 F     FAO estimate \n 3         2 Afghanistan      1057 Chickens  1963  5000 F     FAO estimate \n 4         2 Afghanistan      1057 Chickens  1964  5300 F     FAO estimate \n 5         2 Afghanistan      1057 Chickens  1965  5500 F     FAO estimate \n 6         2 Afghanistan      1057 Chickens  1966  5800 F     FAO estimate \n 7         2 Afghanistan      1057 Chickens  1967  6600 F     FAO estimate \n 8         2 Afghanistan      1057 Chickens  1968  6290 <NA>  Official data\n 9         2 Afghanistan      1057 Chickens  1969  6300 F     FAO estimate \n10         2 Afghanistan      1057 Chickens  1970  6000 F     FAO estimate \n# … with 30,967 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge1_nickboonstra.html#describing-the-data",
    "href": "posts/challenge1_nickboonstra.html#describing-the-data",
    "title": "Nick Boonstra Challenge 1 Resubmit",
    "section": "Describing the data",
    "text": "Describing the data\nThese data appear to be recording the populations of various types of birds across a number of countries and years.\n\n\nCode\nnames(birds)\n\n\n[1] \"area_code\" \"area\"      \"item_code\" \"item\"      \"year\"      \"value\"    \n[7] \"flag\"      \"flag_desc\"\n\n\nCode\nbirds %>%\n  group_by(item) %>%\n  summarise(\"Median Values by Type\" = median(value,na.rm=T))\n\n\n# A tibble: 5 × 2\n  item                   `Median Values by Type`\n  <chr>                                    <dbl>\n1 Chickens                                10784.\n2 Ducks                                     510 \n3 Geese and guinea fowls                    258 \n4 Pigeons, other birds                     2800 \n5 Turkeys                                   528 \n\n\nCode\nbirds %>%\n  group_by(year) %>%\n  summarise(\"Median Values by Year\" = median(value,na.rm=T))\n\n\n# A tibble: 58 × 2\n    year `Median Values by Year`\n   <dbl>                   <dbl>\n 1  1961                   1033 \n 2  1962                   1014 \n 3  1963                   1106 \n 4  1964                   1103 \n 5  1965                   1104 \n 6  1966                   1088.\n 7  1967                   1193 \n 8  1968                   1252.\n 9  1969                   1267 \n10  1970                   1259 \n# … with 48 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nbirds %>%\n  group_by(area) %>%\n  summarise(\"Median Values by Area\" = median(value,na.rm=T))\n\n\n# A tibble: 248 × 2\n   area                `Median Values by Area`\n   <chr>                                 <dbl>\n 1 Afghanistan                          6700  \n 2 Africa                              12910. \n 3 Albania                              1300  \n 4 Algeria                                42.5\n 5 American Samoa                         38  \n 6 Americas                            66924. \n 7 Angola                               6075  \n 8 Antigua and Barbuda                    85  \n 9 Argentina                            2355  \n10 Armenia                              1528. \n# … with 238 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html",
    "href": "posts/challenge1_QuinnHe.html",
    "title": "Challenge 1 Quinn He",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html#challenge-overview",
    "href": "posts/challenge1_QuinnHe.html#challenge-overview",
    "title": "Challenge 1 Quinn He",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html#read-in-the-data",
    "href": "posts/challenge1_QuinnHe.html#read-in-the-data",
    "title": "Challenge 1 Quinn He",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nbirds <- read_csv(\"_data/birds.csv\")\n\nview(birds)\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html#describe-the-data",
    "href": "posts/challenge1_QuinnHe.html#describe-the-data",
    "title": "Challenge 1 Quinn He",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nThe birds data set contains a wide range of range of entries. With the function below we can see all the column names listed. A few are hard to figure out what exactly the represent and just how important they are.\n\n\nCode\ncolnames(birds)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nIt appears the data set was taken from a farm organization. The data is definitely a little messy, but makes sense on the data entry side. Each country has descending rows of chickens, ducks, and fowls from 1961 to 2018. This is mostly a bit redundant. This whole data set keeps track of the value of these three types of birds in a 60 year window. There is also a possibility this data set came from a larger set with other types of animals because the “Domain” column lists ‘Livestock’ throughout the entire data set.\n\n\nCode\nsummary(birds)\n\n\n Domain Code           Domain            Area Code        Area          \n Length:30977       Length:30977       Min.   :   1   Length:30977      \n Class :character   Class :character   1st Qu.:  79   Class :character  \n Mode  :character   Mode  :character   Median : 156   Mode  :character  \n                                       Mean   :1202                     \n                                       3rd Qu.: 231                     \n                                       Max.   :5504                     \n                                                                        \n  Element Code    Element            Item Code        Item          \n Min.   :5112   Length:30977       Min.   :1057   Length:30977      \n 1st Qu.:5112   Class :character   1st Qu.:1057   Class :character  \n Median :5112   Mode  :character   Median :1068   Mode  :character  \n Mean   :5112                      Mean   :1066                     \n 3rd Qu.:5112                      3rd Qu.:1072                     \n Max.   :5112                      Max.   :1083                     \n                                                                    \n   Year Code         Year          Unit               Value         \n Min.   :1961   Min.   :1961   Length:30977       Min.   :       0  \n 1st Qu.:1976   1st Qu.:1976   Class :character   1st Qu.:     171  \n Median :1992   Median :1992   Mode  :character   Median :    1800  \n Mean   :1991   Mean   :1991                      Mean   :   99411  \n 3rd Qu.:2005   3rd Qu.:2005                      3rd Qu.:   15404  \n Max.   :2018   Max.   :2018                      Max.   :23707134  \n                                                  NA's   :1036      \n     Flag           Flag Description  \n Length:30977       Length:30977      \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n\nCode\ndim(birds)\n\n\n[1] 30977    14\n\n\nCode\nhead(birds)\n\n\n# A tibble: 6 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1961  1961 1000…\n2 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1962  1962 1000…\n3 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1963  1963 1000…\n4 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1964  1964 1000…\n5 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1965  1965 1000…\n6 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1966  1966 1000…\n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names\n\n\nCode\ntail(birds)\n\n\n# A tibble: 6 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2013  2013 1000…\n2 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2014  2014 1000…\n3 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2015  2015 1000…\n4 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2016  2016 1000…\n5 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2017  2017 1000…\n6 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2018  2018 1000…\n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names\n\n\nCode\n#table(birds)\n\nggplot(birds, mapping = aes(x = 'Year', y = 'Value'))\n\n\n\n\n\nI’m wondering if I should use the %>% function here. I’m also having an issue with my functions because they don’t run correctly. Is it from my the “delimiter” error above? It may also be an issue with my working directory.\nI commented out ‘table(birds)’ because it was giving me an error when I rendered it the function.\n\n\nCode\nbirds %>%\n  select(Item)\n\n\n# A tibble: 30,977 × 1\n   Item    \n   <chr>   \n 1 Chickens\n 2 Chickens\n 3 Chickens\n 4 Chickens\n 5 Chickens\n 6 Chickens\n 7 Chickens\n 8 Chickens\n 9 Chickens\n10 Chickens\n# … with 30,967 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge2_solutions.html",
    "href": "posts/challenge2_solutions.html",
    "title": "Challenge 2 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_solutions.html#challenge-overview",
    "href": "posts/challenge2_solutions.html#challenge-overview",
    "title": "Challenge 2 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics\n\n\nRailroad ⭐FAOstat* ⭐⭐⭐Hotel Bookings ⭐⭐⭐⭐\n\n\nThe railroad data contain 2931 county-level aggregated counts of the number of railroad employees in 2012. Counties are embedded within States, and all 50 states plus Canada, overseas addresses in Asia and Europe, and Washington, DC are represented.\n\n\n\n\n\n\nData Summaries\n\n\n\nThis is a concise summary of more extensive work we did in Challenge 1, and is an example of how you should describe data in public-facing work. A “public-facing” version of your work contains all critical details needed to replicate your work, but doesn’t contain a point-by-point rundown of the mental process you went through to get to that point. (Your internal analysis file should probably walk through that mental process, however!)\n\n\n\nRead the data\nHere, we are just reusing the code from Challenge 1. We are using the excel version, to ensure that we get Canada, and are renaming the missing data in county for Canada so that we don’t accidently filter that observation out.\n\nrailroad<-read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"state\", \"delete\",  \"county\",\n                                  \"delete\", \"employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(state, \"Total\"))\n\nrailroad<-head(railroad, -2)%>%\n  mutate(county = ifelse(state==\"CANADA\", \"CANADA\", county))\n\nrailroad\n\n\n\n  \n\n\n\n\n\nHow many values does X take on?\nNow, lets practice grouping our data and using other dplyr commands that make data wrangling super easy. First, lets take a closer look at how we counted the number of unique states last week. First, we selected the state column. Then we used the n_distinct command - which replicates the base R commands length(unique(var)).\n\n\n\n\n\n\nacross()\n\n\n\nInstead of counting the number of distinct values one at a time, I am doing an operation on two columns at the same time using across.\n\n\n\nrailroad%>%\n  summarise(across(c(state,county), n_distinct))\n\n\n\n  \n\n\n\nCheck this out - many counties have the same name! There are 2931 state-county cases, but only 1710 distinct county names. This is one reason it is so critical to understand “what is a case” when you are working with your data - otherwise you might accidently collapse or group information that isn’t intenced to be grouped.\n\n\nHow many total X are in group Y?\nSuppose we want to know the total number of railroad employees was in 2012, what is the best way to sum up all of the values in the data? The summarize function is useful for doing calculcations across some or all of a data set.\n\nrailroad%>%\n  summarise(total_employees = sum(employees))\n\n\n\n  \n\n\n\nAround a quarter of a million people were employed in the railroad industry in 2012. While this may seem like a lot, it was a significant decrease in employment from a few decades earlier, according to official Bureau of Labor Statistics (BLS) estimates.\nYou may notice that the BLS estimates are significantly lower than the ones we are using, provided by the Railroad Retirement Board. Given that the Railroad Retirement Board has “gold-standard” data on railroad employees, this discrepancy suggests that many people who work in the railroad industry are being classified in a different way by BLS statistics.\n\n\nWhich X have the most Y?\nSuppose we are interested in which county names are duplicated most often, or in which states have the most railroad employees. We can use the same basic approch to answer both “Which X have the most Y?” style questions\n\n\n\n\n\n\ndf-print: paged (YAML)\n\n\n\nWhen you are using df-print: paged in your yaml header, or are using tibbles, there is no need to rely on the head(data) commmand to limit your results to the top 10 of a list.\n\n\n\nrailroad%>%\n  group_by(state)%>%\n  summarise(total_employees = sum(employees),\n            num_counties = n())%>%\n  arrange(desc(total_employees))\n\n\n\n  \n\n\n\nLooking at the top 10 states in terms of total railroad employment, a few trends emerge. Several of the top 10 states with geographical activity are highly populous and geographically large. California, Texas, New York, Pennsylvania, Ohio, Illinois, and Georgia are all amonst the top-10 largest states - so it would make sense if there are more railroad employees in large states.\nBut railroads are spread out along geography, and thus we might also expect square mileage within a state to be related to state railroad employment - not just state population. For example, Texas is around 65% larger (in area) than California, and has around 50% more railroad employees.\nThere appear to be multiple exceptions to both rules, however. If geography plus population were the primary factors explaining railroad employment, then California would be ranked higher than New York and Illinois, and New York would likely rank higher than ILlinois. However, Illinois - Chicago in particular - is a hub of railroad activity, and thus Illinois’ higher ranking is likely reflecting hub activity and employment. New York is a hub for the East Coast in particular. While California may have hubs of train activity in Los Angeles or San Francisco, the Northeast has a higher density of train stations and almost certainly generates more passenger and freight miles than the larger and more populous California.\nThis final factor - the existence of heavily used train routes probably explains the high railroad employment in states like Nebraska, Indiana and Missouri - all of which lay along a major railway route between New York and Chicago, and then out to California. Anyway who has played Ticket to Ride probably recognizes many of these routes!\n\n\n\n\n\n\nGo further\n\n\n\nA fun exercise once you are comfortable with joins and map-based visualizatinos would be to join the railroad employment data to information about state population and geographic area, and also mapping information about railway routes, to get more insight into the factors driving railroad employment.\n\n\n\n\n\nThe FAOSTAT sheets are excerpts of the FAOSTAT database provided by the Food and Agriculture Association, an agency of the United Nations. We are using the file birds.csv that includes estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) for 248 areas for 58 years between 1961-2018. Estimated stocks are given in 1000 head.\nBecause we know (from challenge 1) that several of those areas include aggregated data (e.g., ) we are going to remove the aggregations, remove the unnecessary variables, and only work with the grouping variables available in the data. In a future challenge, we will join back on more data from the FAO to recreate regional groupings.\n\nbirds<-read_csv(\"_data/birds.csv\")%>%\n  select(-c(contains(\"Code\"), Element, Domain, Unit))%>%\n  filter(Flag!=\"A\")\nbirds\n\n\n\n  \n\n\n\n\nWhat is the average of Y for X groups?\nLets suppose we are starting off and know nothing about poultry stocks around the world, where could we start? Perhaps we could try to get a sense of the relative sizes of stocks of each of the five types of poultry, identified in the variable Item. Additionally, because some of the values may be missing, lets find out how many of the estimates are missing.\n\nbirds%>%\n  group_by(Item)%>%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE),\n            n_missing = sum(is.na(Value)))\n\n\n\n  \n\n\n\nOn average, we can see that countries have far more chickens as livestock (\\(\\bar{x}\\)=58.4million head) than other livestock birds (average stocks range between 2 and 10 million head). However, the information from the median stock counts suggest that there is significant variation across countries along with a strong right hand skew with regards to chicken stocks. The median number of chickens in a country is 3.8 million head - significantly less than the mean of almost 60 million. Overall, missing data doesn’t seem to be a huge issue, so we will just use na.rm=TRUE and not worry too much about the missingness for now.\nIt could be that stock head counts have changed over time, so lets try selecting two points in time and seeing whether or not average livestock counts are changing.\n\n\n\n\n\n\npivot-wider\n\n\n\nIt can be difficult to visually report data in tidy format. For example, it is tough to compare two values when they are on different rows. In this example, I use pivot-wider to swap a tidy grouping variable into multiple columns to be more “table-like.” I then do some manual formatting to make it easy to compare the grouped estimates.\n\n\n\nt1<-birds%>%\n  filter(Year %in% c(1966, 2016))%>%\n  group_by(Year, Item)%>%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE))%>%\n  pivot_wider(names_from = Year, values_from = c(avg_stocks, med_stocks))\n\nknitr::kable(t1,\n             digits=0,format.args = list(big.mark = \",\"),\n             col.names = c(\"Type\", \"1966\", \"2016\",\n                           \"1996\", \"2016\"))%>%\n  kableExtra::kable_styling(htmltable_class = \"lightable-minimal\")%>%\n  kableExtra::add_header_above(c(\" \" = 1, \"Mean Stock (1000)\" = 2,\n                                 \"Median Stock (1000)\" = 2))\n\n\n\n \n\n\nMean Stock (1000)\nMedian Stock (1000)\n\n  \n    Type \n    1966 \n    2016 \n    1996 \n    2016 \n  \n \n\n  \n    Chickens \n    25,264 \n    105,437 \n    2,315 \n    7,854 \n  \n  \n    Ducks \n    5,053 \n    14,842 \n    110 \n    236 \n  \n  \n    Geese and guinea fowls \n    2,468 \n    11,750 \n    97 \n    73 \n  \n  \n    Pigeons, other birds \n    3,314 \n    2,874 \n    3,000 \n    1,194 \n  \n  \n    Turkeys \n    843 \n    2,858 \n    74 \n    194 \n  \n\n\n\n\n\n\n\n\n\n\n\nkable and kableExtra\n\n\n\nI manually adjust table formatting (column names, setting significant digits, adding a comma) using kable and add a header row using kableExtra. Because df-print=paged option is set to make it easier to scroll through longer data frames, I need to directly specify that I want to produce an htmltable - not a default kable/rmarkdown table - when I use kable and kableExtra formatting directly.\n\n\nSure enough, it does look like stocks have changed significantly over time. The expansion of country-level chicken stocks over five decades between 1966 and 2016 are most noteworthy, with both average and median stock count going up by a factor of 4. Pigeons have never been very popular, and average stocks have actually decreased over the same time period while the other less popular bird - turkeys - saw significant incrases in stock count. Some countries increased specialization in goose and/or guinea fowl production, as the average stock count went up but the median went down over the same period.\n\n\n\n\n\n\nGo further\n\n\n\nIt could be really interesting to graph the rise and fall of poultry stocks overtime with these data, and match these changes to changes in population size and country GDP. Another option would be to match the countries back to regional groupings available from the UN FAO, a future “data join” challenge.\n\n\n\n\n\nThis data set contains 119,390 hotel bookings from two hotels (“City Hotel” and “Resort Hotel”) with an arrival date between July 2015 and August 2017 (more detail needed), including bookings that were later cancelled. Each row contains extensive information about a single booking:\n\nthe booking process (e.g., lead time, booking agent, deposit, changes made)\nbooking details (e.g., scheduled arrival date, length of stay)\nguest requests (e.g., type of room, meal(s) included, car parking)\nbooking channel (e.g., distribution, market segment, corporate affiliation for )\nguest information (e.g., child/adult, passport country)\nguest prior bookings (e.g., repeat customer, prior cancellations)\n\nThe data are a de-identified extract of real hotel demand data, made available by the authors.\n\nRead and make sense of the data\nThe hotel bookings data set is new to challenge 2, so we need to go through the same process we did during challenge 1 to find out more about the data. Lets read in the data and use the summmaryTools package to get an overview of the data set.\n\nbookings<-read_csv(\"_data/hotel_bookings.csv\")\n\nprint(summarytools::dfSummary(bookings,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE,\n                        style        = \"grid\",\n                        graph.magnif = 0.70,\n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nbookings\nDimensions: 119390 x 32\n  Duplicates: 31994\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      hotel\n[character]\n      1. City Hotel2. Resort Hotel\n      79330(66.4%)40060(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      is_canceled\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:75166(63.0%)1:44224(37.0%)\n      \n      0\n(0.0%)\n    \n    \n      lead_time\n[numeric]\n      Mean (sd) : 104 (106.9)min ≤ med ≤ max:0 ≤ 69 ≤ 737IQR (CV) : 142 (1)\n      479 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_year\n[numeric]\n      Mean (sd) : 2016.2 (0.7)min ≤ med ≤ max:2015 ≤ 2016 ≤ 2017IQR (CV) : 1 (0)\n      2015:21996(18.4%)2016:56707(47.5%)2017:40687(34.1%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_month\n[character]\n      1. August2. July3. May4. October5. April6. June7. September8. March9. February10. November[ 2 others ]\n      13877(11.6%)12661(10.6%)11791(9.9%)11160(9.3%)11089(9.3%)10939(9.2%)10508(8.8%)9794(8.2%)8068(6.8%)6794(5.7%)12709(10.6%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_week_number\n[numeric]\n      Mean (sd) : 27.2 (13.6)min ≤ med ≤ max:1 ≤ 28 ≤ 53IQR (CV) : 22 (0.5)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_day_of_month\n[numeric]\n      Mean (sd) : 15.8 (8.8)min ≤ med ≤ max:1 ≤ 16 ≤ 31IQR (CV) : 15 (0.6)\n      31 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_weekend_nights\n[numeric]\n      Mean (sd) : 0.9 (1)min ≤ med ≤ max:0 ≤ 1 ≤ 19IQR (CV) : 2 (1.1)\n      17 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_week_nights\n[numeric]\n      Mean (sd) : 2.5 (1.9)min ≤ med ≤ max:0 ≤ 2 ≤ 50IQR (CV) : 2 (0.8)\n      35 distinct values\n      \n      0\n(0.0%)\n    \n    \n      adults\n[numeric]\n      Mean (sd) : 1.9 (0.6)min ≤ med ≤ max:0 ≤ 2 ≤ 55IQR (CV) : 0 (0.3)\n      14 distinct values\n      \n      0\n(0.0%)\n    \n    \n      children\n[numeric]\n      Mean (sd) : 0.1 (0.4)min ≤ med ≤ max:0 ≤ 0 ≤ 10IQR (CV) : 0 (3.8)\n      0:110796(92.8%)1:4861(4.1%)2:3652(3.1%)3:76(0.1%)10:1(0.0%)\n      \n      4\n(0.0%)\n    \n    \n      babies\n[numeric]\n      Mean (sd) : 0 (0.1)min ≤ med ≤ max:0 ≤ 0 ≤ 10IQR (CV) : 0 (12.3)\n      0:118473(99.2%)1:900(0.8%)2:15(0.0%)9:1(0.0%)10:1(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      meal\n[character]\n      1. BB2. FB3. HB4. SC5. Undefined\n      92310(77.3%)798(0.7%)14463(12.1%)10650(8.9%)1169(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      country\n[character]\n      1. PRT2. GBR3. FRA4. ESP5. DEU6. ITA7. IRL8. BEL9. BRA10. NLD[ 168 others ]\n      48590(40.7%)12129(10.2%)10415(8.7%)8568(7.2%)7287(6.1%)3766(3.2%)3375(2.8%)2342(2.0%)2224(1.9%)2104(1.8%)18590(15.6%)\n      \n      0\n(0.0%)\n    \n    \n      market_segment\n[character]\n      1. Aviation2. Complementary3. Corporate4. Direct5. Groups6. Offline TA/TO7. Online TA8. Undefined\n      237(0.2%)743(0.6%)5295(4.4%)12606(10.6%)19811(16.6%)24219(20.3%)56477(47.3%)2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      distribution_channel\n[character]\n      1. Corporate2. Direct3. GDS4. TA/TO5. Undefined\n      6677(5.6%)14645(12.3%)193(0.2%)97870(82.0%)5(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      is_repeated_guest\n[numeric]\n      Min  : 0Mean : 0Max  : 1\n      0:115580(96.8%)1:3810(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      previous_cancellations\n[numeric]\n      Mean (sd) : 0.1 (0.8)min ≤ med ≤ max:0 ≤ 0 ≤ 26IQR (CV) : 0 (9.7)\n      15 distinct values\n      \n      0\n(0.0%)\n    \n    \n      previous_bookings_not_canceled\n[numeric]\n      Mean (sd) : 0.1 (1.5)min ≤ med ≤ max:0 ≤ 0 ≤ 72IQR (CV) : 0 (10.9)\n      73 distinct values\n      \n      0\n(0.0%)\n    \n    \n      reserved_room_type\n[character]\n      1. A2. B3. C4. D5. E6. F7. G8. H9. L10. P\n      85994(72.0%)1118(0.9%)932(0.8%)19201(16.1%)6535(5.5%)2897(2.4%)2094(1.8%)601(0.5%)6(0.0%)12(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      assigned_room_type\n[character]\n      1. A2. D3. E4. F5. G6. C7. B8. H9. I10. K[ 2 others ]\n      74053(62.0%)25322(21.2%)7806(6.5%)3751(3.1%)2553(2.1%)2375(2.0%)2163(1.8%)712(0.6%)363(0.3%)279(0.2%)13(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      booking_changes\n[numeric]\n      Mean (sd) : 0.2 (0.7)min ≤ med ≤ max:0 ≤ 0 ≤ 21IQR (CV) : 0 (2.9)\n      21 distinct values\n      \n      0\n(0.0%)\n    \n    \n      deposit_type\n[character]\n      1. No Deposit2. Non Refund3. Refundable\n      104641(87.6%)14587(12.2%)162(0.1%)\n      \n      0\n(0.0%)\n    \n    \n      agent\n[character]\n      1. 92. NULL3. 2404. 15. 146. 77. 68. 2509. 24110. 28[ 324 others ]\n      31961(26.8%)16340(13.7%)13922(11.7%)7191(6.0%)3640(3.0%)3539(3.0%)3290(2.8%)2870(2.4%)1721(1.4%)1666(1.4%)33250(27.8%)\n      \n      0\n(0.0%)\n    \n    \n      company\n[character]\n      1. NULL2. 403. 2234. 675. 456. 1537. 1748. 2199. 28110. 154[ 343 others ]\n      112593(94.3%)927(0.8%)784(0.7%)267(0.2%)250(0.2%)215(0.2%)149(0.1%)141(0.1%)138(0.1%)133(0.1%)3793(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      days_in_waiting_list\n[numeric]\n      Mean (sd) : 2.3 (17.6)min ≤ med ≤ max:0 ≤ 0 ≤ 391IQR (CV) : 0 (7.6)\n      128 distinct values\n      \n      0\n(0.0%)\n    \n    \n      customer_type\n[character]\n      1. Contract2. Group3. Transient4. Transient-Party\n      4076(3.4%)577(0.5%)89613(75.1%)25124(21.0%)\n      \n      0\n(0.0%)\n    \n    \n      adr\n[numeric]\n      Mean (sd) : 101.8 (50.5)min ≤ med ≤ max:-6.4 ≤ 94.6 ≤ 5400IQR (CV) : 56.7 (0.5)\n      8879 distinct values\n      \n      0\n(0.0%)\n    \n    \n      required_car_parking_spaces\n[numeric]\n      Mean (sd) : 0.1 (0.2)min ≤ med ≤ max:0 ≤ 0 ≤ 8IQR (CV) : 0 (3.9)\n      0:111974(93.8%)1:7383(6.2%)2:28(0.0%)3:3(0.0%)8:2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      total_of_special_requests\n[numeric]\n      Mean (sd) : 0.6 (0.8)min ≤ med ≤ max:0 ≤ 0 ≤ 5IQR (CV) : 1 (1.4)\n      0:70318(58.9%)1:33226(27.8%)2:12969(10.9%)3:2497(2.1%)4:340(0.3%)5:40(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status\n[character]\n      1. Canceled2. Check-Out3. No-Show\n      43017(36.0%)75166(63.0%)1207(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status_date\n[Date]\n      min : 2014-10-17med : 2016-08-07max : 2017-09-14range : 2y 10m 28d\n      926 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-18\n\n\n\nWow - there is a lot of information available here. Lets scan it and see what jumps out. First we can see that the summary function claims that there are almost 32,000 duplicates in the data. However, this is likely an artifact of the way that the bookings have been de-identified, and may reflect bookings with identical details but different individuals who made the bookings.\nWe can see that we are provided with limited information about the hotel. Hotels are identified only as “City” Hotel” or a “Resort Hotel”. Maybe we have bookings from only two hotels? Lets tentatively add that to our data description.\nThere is a flag for whether a booking is cancelled. This means that our universe of cases includes bookings where the guests showed up, as well as bookings that were later cancelled - we can add that to our data description.\nThere are multiple fields with the arrival date - year, month, etc. For now, we can tell that the arrival date of the bookings ranges between 2015 and 2017. More precise identification of the date range could be more easily done next challenge when we can recode the arrival date information using lubridate.But maybe it is possible to find out which values of month co-occur with specific years?\n\n\nWhich values of Y are nested within X?\nTo approach this question, we can narrow the dataset down to just the two variables of interest, and then use the distinct command.\n\nbookings%>%\n  select(arrival_date_year, arrival_date_month)%>%\n  distinct()\n\n\n\n  \n\n\n\nGreat - now we now that all bookings have arrival dates between June 2015 and August 2017, and can add that to the data description. Just for fun, lets see if we can confirm that the dates are the same for both hotels.\n\n\n\n\n\n\nslice()\n\n\n\nThis would be easier to investigate with proper date variables, but I am using slice to find the first and last row for each hotel, by position. This avoids printing out a long data list we have to scroll through, but would fail if the hotels had different sets of arrival month-year pairs.\n\n\n\nd<-bookings%>%\n  select(arrival_date_year, arrival_date_month)%>%\n  n_distinct\n\nbookings%>%\n  select(hotel, arrival_date_year, arrival_date_month)%>%\n  distinct()%>%\n  slice(c(1, d, d+1, d*2))\n\n\n\n  \n\n\n\nLets suppose we want to know whether or not the two hotels offer the same types of rooms? This is another query of the sort Which values of X are nested in y?\n\nbookings%>%\n  group_by(hotel)%>%\n  count(reserved_room_type)\n\n\n\n  \n\n\n\nIn this case, however, it is tough to directly compare - it appears that the hotel-roomtype pairs are not as consistent as the year-month pairs for the same hotels. A quick pivot-wider makes this comparison a little easier to visualize. Here we can see that the Resort Hotel has two additional room types: “H” and “L”.\n\nbookings%>%\n  group_by(hotel)%>%\n  count(reserved_room_type)%>%\n  pivot_wider(names_from= hotel, values_from = n)\n\n\n\n  \n\n\n\n\n\nWhat is the average of Y for group X?\nThe breakdown of rooms by hotel doesn’t shed much light on the room codes and what they might mean. Lets see if we can find average number of occupants and average price for each room type, and see if we can learn more about our data.\n\n\n\n\n\n\nmean(., na.rm=TRUE)\n\n\n\nI am using the mean function with the option na.rm=TRUE to deal with the four NA values in the children field, identified in the summary table above.\n\n\n\nt1<-bookings%>%\n  group_by(hotel, reserved_room_type)%>%\n  summarise(price = mean(adr),\n            adults = mean(adults),\n            children = mean(children+babies, na.rm=TRUE)\n            )%>%\n  pivot_wider(names_from= hotel,\n              values_from = c(price, adults, children))\n\nknitr::kable(t1,\n             digits=1,\n             col.names = c(\"Type\", \"City\", \"Resort\",\n                           \"City\", \"Resort\", \"City\", \"Resort\"))%>%\n  kableExtra::kable_styling(htmltable_class = \"lightable-minimal\")%>%\n  kableExtra::add_header_above(c(\"Room\" = 1, \"Price\" = 2,\n                                 \"Adults\" = 2, \"Children & Babies\" = 2))\n\n\nAverage Price and Occupancy, by hotel and room type\n \n\nRoom\nPrice\nAdults\nChildren & Babies\n\n  \n    Type \n    City \n    Resort \n    City \n    Resort \n    City \n    Resort \n  \n \n\n  \n    A \n    96.2 \n    76.2 \n    1.8 \n    1.8 \n    0.0 \n    0.0 \n  \n  \n    B \n    90.3 \n    104.7 \n    1.6 \n    2.0 \n    0.6 \n    0.0 \n  \n  \n    C \n    85.5 \n    161.4 \n    1.5 \n    2.0 \n    0.1 \n    1.4 \n  \n  \n    D \n    131.5 \n    103.6 \n    2.2 \n    2.0 \n    0.0 \n    0.1 \n  \n  \n    E \n    156.8 \n    114.5 \n    2.1 \n    2.0 \n    0.3 \n    0.0 \n  \n  \n    F \n    189.3 \n    132.8 \n    2.0 \n    2.0 \n    1.6 \n    0.1 \n  \n  \n    G \n    201.8 \n    168.2 \n    2.3 \n    2.0 \n    1.1 \n    1.4 \n  \n  \n    P \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    H \n    NA \n    188.2 \n    NA \n    2.7 \n    NA \n    1.0 \n  \n  \n    L \n    NA \n    124.7 \n    NA \n    2.2 \n    NA \n    0.0 \n  \n\n\n\n\n\n\n\n\n\n\n\nkable & kableExtra\n\n\n\nI manually adjust table formatting (column names, plus adding a header row) using kable and kableExtra package, respectively. Also, because df-print=paged is the option set in the YAML header, I need to directly specify that I want to produce an htmltable - not a default kable/rmarkdown table.\n\n\nBased on these descriptives broken down by hotel and room type, we can speculate that the “H” and “L” room types at the resort are likely some sort of multi-bedroom suite (because the average number of adults is over 2.) Similarly, we can speculate that the difference between ABC and DEF may be something related to room size or quality (e.g., number and size of beds) and/or related to meals included with the rooms - but this would require further investigation to pin down!\n\n\n\n\n\n\nGo further\n\n\n\nThere is lots more to explore in the hotel bookings dataset, but it will be a lot easier once we recode the date fields using lubridate."
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html",
    "href": "posts/challenge2_instructions-youngsoo choi.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html#challenge-overview",
    "href": "posts/challenge2_instructions-youngsoo choi.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html#read-in-the-data",
    "href": "posts/challenge2_instructions-youngsoo choi.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\nCode\nlibrary(readxl)\nstate2012 <- read_xls(\"_data/StateCounty2012.xls\", skip=3)\nstate2012\n\n\n# A tibble: 2,990 × 5\n   STATE     ...2  COUNTY               ...4  TOTAL\n   <chr>     <lgl> <chr>                <lgl> <dbl>\n 1 AE        NA    APO                  NA        2\n 2 AE Total1 NA    <NA>                 NA        2\n 3 AK        NA    ANCHORAGE            NA        7\n 4 AK        NA    FAIRBANKS NORTH STAR NA        2\n 5 AK        NA    JUNEAU               NA        3\n 6 AK        NA    MATANUSKA-SUSITNA    NA        2\n 7 AK        NA    SITKA                NA        1\n 8 AK        NA    SKAGWAY MUNICIPALITY NA       88\n 9 AK Total  NA    <NA>                 NA      103\n10 AL        NA    AUTAUGA              NA      102\n# … with 2,980 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\ncolnames(state2012)\n\n\n[1] \"STATE\"  \"...2\"   \"COUNTY\" \"...4\"   \"TOTAL\" \n\n\nThis dataset has 2990 rows and 5 columns. And each column name is “STATE”, “…2”, “COUNTY”, “…4”, “TOTAL”."
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html#describe-the-data",
    "href": "posts/challenge2_instructions-youngsoo choi.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nsummary(state2012)\n\n\n    STATE             ...2            COUNTY            ...4        \n Length:2990        Mode:logical   Length:2990        Mode:logical  \n Class :character   NA's:2990      Class :character   NA's:2990     \n Mode  :character                  Mode  :character                 \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n     TOTAL         \n Min.   :     1.0  \n 1st Qu.:     7.0  \n Median :    22.0  \n Mean   :   256.9  \n 3rd Qu.:    71.0  \n Max.   :255432.0  \n NA's   :5         \n\n\nThis dataset is about the number of workers related to railroad jobs in 2012 I think. And this dataset contains data of 2990 county."
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions-youngsoo choi.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nMA<-filter(state2012, STATE==\"MA\")\nsummary(MA)\n\n\n    STATE             ...2            COUNTY            ...4        \n Length:12          Mode:logical   Length:12          Mode:logical  \n Class :character   NA's:12        Class :character   NA's:12       \n Mode  :character                  Mode  :character                 \n                                                                    \n                                                                    \n                                                                    \n     TOTAL      \n Min.   : 44.0  \n 1st Qu.:101.8  \n Median :271.0  \n Mean   :281.6  \n 3rd Qu.:396.8  \n Max.   :673.0  \n\n\nCode\nCA<-filter(state2012, STATE==\"CA\")\nsummary(CA)\n\n\n    STATE             ...2            COUNTY            ...4        \n Length:55          Mode:logical   Length:55          Mode:logical  \n Class :character   NA's:55        Class :character   NA's:55       \n Mode  :character                  Mode  :character                 \n                                                                    \n                                                                    \n                                                                    \n     TOTAL       \n Min.   :   1.0  \n 1st Qu.:  12.5  \n Median :  61.0  \n Mean   : 238.9  \n 3rd Qu.: 200.5  \n Max.   :2888.0  \n\n\n\nExplain and Interpret\nMA is the state where I live in and CA is much bigger state I think. MA has 12 county and average total is 286.1 and CA has 55 county and average total is 238.9. So I figure CA has more county than MA but MA’s average total is larger than CA’s."
  },
  {
    "objectID": "posts/challenge1_jerinjacob.html",
    "href": "posts/challenge1_jerinjacob.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_jerinjacob.html#read-in-the-data",
    "href": "posts/challenge1_jerinjacob.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\n## Reading Dataset Railroad Employees\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")"
  },
  {
    "objectID": "posts/challenge1_jerinjacob.html#describe-the-data",
    "href": "posts/challenge1_jerinjacob.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\n## Describing railroad data\n## This is a data set of the rail road employees working in 2930 counties of the states in US in the year of 2012.\n## There are 3 variables in the dataset; state, county and total number of employees.\n\n## The minimum number of employees in a county is 1 and the maximum number is 8207\n\n## There are 145 counties where there is only 1 employee working in the rail road department whereas 27 counties have more than 1000 employees\nview(railroad)\nrailroad%>%\n  select(state)%>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\n## There are 53 distinct values in the variable column named state. This means that there are certain additional values other than the name of the states.\n\nrailroad%>%\n  select(state)%>%\n  distinct()\n\n\n# A tibble: 53 × 1\n   state\n   <chr>\n 1 AE   \n 2 AK   \n 3 AL   \n 4 AP   \n 5 AR   \n 6 AZ   \n 7 CA   \n 8 CO   \n 9 CT   \n10 DC   \n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n## The variable 'state' contains all the states along with armed forces, DC etc."
  },
  {
    "objectID": "posts/challenge2_instructions.html",
    "href": "posts/challenge2_instructions.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions.html#challenge-overview",
    "href": "posts/challenge2_instructions.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions.html#read-in-the-data",
    "href": "posts/challenge2_instructions.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_instructions.html#describe-the-data",
    "href": "posts/challenge2_instructions.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included."
  },
  {
    "objectID": "posts/challenge1_ShoshanaBuck.html",
    "href": "posts/challenge1_ShoshanaBuck.html",
    "title": "challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_ShoshanaBuck.html#challenge-overview",
    "href": "posts/challenge1_ShoshanaBuck.html#challenge-overview",
    "title": "challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview"
  },
  {
    "objectID": "posts/challenge1_ShoshanaBuck.html#read-in-the-data",
    "href": "posts/challenge1_ShoshanaBuck.html#read-in-the-data",
    "title": "challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")\nrailroad\n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThis data is separated into three columns: state, county, and total employees and 2,930 rows."
  },
  {
    "objectID": "posts/challenge1_ShoshanaBuck.html#describe-the-data",
    "href": "posts/challenge1_ShoshanaBuck.html#describe-the-data",
    "title": "challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nI imported the data set of railroad_2012_clean_county.csv and renamed it as Railroad. I then used the function colnames() to breakdown the three column names of “state” “county” and “total_employees.” From there I used the spec() function to extract the column names. I then used a pipe function in order to filter and select to see the total amount of employees in each state.\n\n\nCode\ncolnames(railroad)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nCode\nspec(railroad)\n\n\ncols(\n  state = col_character(),\n  county = col_character(),\n  total_employees = col_double()\n)\n\n\nCode\nhead(railroad)\n\n\n# A tibble: 6 × 3\n  state county               total_employees\n  <chr> <chr>                          <dbl>\n1 AE    APO                                2\n2 AK    ANCHORAGE                          7\n3 AK    FAIRBANKS NORTH STAR               2\n4 AK    JUNEAU                             3\n5 AK    MATANUSKA-SUSITNA                  2\n6 AK    SITKA                              1\n\n\nCode\nrailroad %>% \n  group_by(state) %>% \n  summarise(total_employees2=sum(total_employees))\n\n\n# A tibble: 53 × 2\n   state total_employees2\n   <chr>            <dbl>\n 1 AE                   2\n 2 AK                 103\n 3 AL                4257\n 4 AP                   1\n 5 AR                3871\n 6 AZ                3153\n 7 CA               13137\n 8 CO                3650\n 9 CT                2592\n10 DC                 279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html",
    "href": "posts/challenge2_AdithyaParupudi.html",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html#challenge-overview",
    "href": "posts/challenge2_AdithyaParupudi.html#challenge-overview",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html#read-in-the-data",
    "href": "posts/challenge2_AdithyaParupudi.html#read-in-the-data",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "Read in the Data",
    "text": "Read in the Data\nFocusing on FAOSTAT_cattle_dairy.csv\n\n\nCode\ncattle <- read_csv(\"_data/FAOSTAT_cattle_dairy.csv\")\ncattle\n\n\n\n\n  \n\n\n\nLooks like this is an extensive data gathered by faostat regarding the cattle’s products from different countries between 1961 and 2018. A quick glance at the last column tells whether this data is estimated or calculated by faostat. I am interested to see whether over the years, did faostat collect more data or did it unofficially made entries in this data set."
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html#describe-the-data",
    "href": "posts/challenge2_AdithyaParupudi.html#describe-the-data",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\ndim(cattle)\n\n\n[1] 36449    14\n\n\nCode\ncolnames(cattle)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\n\n\nCode\ncattle %>% \n  group_by(`Flag Description`) %>% \n  summarise()\n\n\n\n\n  \n\n\n\nCode\ncattle %>% \n  group_by(Area) %>% \n  summarise()\n\n\n\n\n  \n\n\n\nBy filtering the unique values of the Flag Description column, I notice there are some unofficial figures and “Data not available” entries. I want to see how many entries of these were since 1961 and did their count reduce. Maybe this will tell the data gathering techniques have improved over the years.\nThere are 232 unique countries from the data set with their domain = “Primary Livestock” common to all. (FAOSTAT focused on collecting livestock information from all the countries)"
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_AdithyaParupudi.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nall_countries <- cattle %>%\n  group_by(Area,`Flag Description`, Year) %>%\n  summarise(median_val=median(Value,na.rm = TRUE), .groups = 'drop') %>% \n  arrange(Year, `Flag Description`) %>% \n  select(Year, Area, `Flag Description`)\nall_countries\n\n\n\n\n  \n\n\n\nFor analysis purpose, I place my focus on these columns (Year, Area, Flag Description) and arranged the list in ascending order with respect to Year column\n\n\nCode\nall_grouped <- all_countries %>%\n  group_by(`Flag Description`, Year) %>%\n  summarise(count = n(), ) %>% \n  arrange(desc(Year)) %>% \n  select(Year, `Flag Description`, count)\nall_grouped\n\n\n\n\n  \n\n\n\nWe have grouped the data by year and flag-description to see the number of flag-description entries in each year. Now lets pick first and last years of the data set and compare the results.\n\n\nCode\nyr_2018 <- all_grouped %>% \n  filter(Year == '2018') %>% \n  arrange(desc(count))\nyr_2000 <- all_grouped %>% \n  filter(Year == '2000') %>% \n  arrange(desc(count))\nyr_2018\n\n\n\n\n  \n\n\n\nCode\nyr_2000\n\n\n\n\n  \n\n\n\nI wanted to compare the entries of “unofficial figure” of two different years. Looks like there were efficient data gathering methods in place, when we look at “Calculated Data” and “Official Data”. There is a rise in “FAO data based on imputation methodology” from 45 to 125 in 18 years.\n\n\nCode\nall_grouped %>% \n  filter(`Flag Description` == 'Unofficial figure') %>% \n  arrange(desc(count))\n\n\n\n\n  \n\n\n\nUnofficial figures were high in 1981. This observation didnt have a steady decline in number, but it varied was rather varied each year.\n\n\nCode\nindia <- cattle %>%\n  filter(Area == 'India') %>% \n  select(Area, Year, Item, Element, `Element Code` ,`Flag Description`) %>% \n  group_by(`Flag Description`) %>%\n  summarise(count = n())\nindia\n\n\n\n\n  \n\n\n\nCode\ncattle %>%\n  filter(Area == 'Afghanistan') %>% \n  select(Area, Year, Item, Element, `Element Code` ,`Flag Description`) %>% \n  group_by(`Flag Description`) %>%\n  summarise(count = n())\n\n\n\n\n  \n\n\n\nWanted to see India’s contribution to FAOSTAT since 1961. I’ve compared the details with Afghanistan and noticed India’s overall contribution to the faostat list is less, and unofficial figure numbers are higher. In India, there was no use of “FAO data based on imputation methodology” as well.\n\nExplain and Interpret\nI began this analysis, think to group Area, Flag Description and Year and probably tell that over the years (1961 to 2018) the numbers will tell a different story in terms of types of entries made. Though I didn’t uncover groundbreaking observations, it was interesting to see that not all countries have contributed to this FAOSTAT list equally in terms of how data was added."
  },
  {
    "objectID": "posts/challenge1_ManiShankerKamarapu.html",
    "href": "posts/challenge1_ManiShankerKamarapu.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_ManiShankerKamarapu.html#challenge-overview",
    "href": "posts/challenge1_ManiShankerKamarapu.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_ManiShankerKamarapu.html#read-in-the-data",
    "href": "posts/challenge1_ManiShankerKamarapu.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\n\nI will be working on the “wild_bird_data” dataset.\n\n\nCode\n# Loading `readxl` package\nlibrary(readxl)\nwild_bird <- read_xlsx(\"_data/wild_bird_data.xlsx\")\n\n# View the dataset\nwild_bird\n\n\n# A tibble: 147 × 2\n   Reference           `Taken from Figure 1 of Nee et al.`\n   <chr>               <chr>                              \n 1 Wet body weight [g] Population size                    \n 2 5.45887180052624    532194.395145161                   \n 3 7.76456810683605    3165107.44544653                   \n 4 8.63858738018464    2592996.86778979                   \n 5 10.6897349302105    3524193.2266336                    \n 6 7.41722577905587    389806.168891807                   \n 7 9.1169347252776     604765.97978904                    \n 8 8.03684333000353    192360.511579436                   \n 9 8.70473119796067    250452.449623033                   \n10 8.89032317828959    16997.4156415239                   \n# … with 137 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge1_ManiShankerKamarapu.html#describe-the-data",
    "href": "posts/challenge1_ManiShankerKamarapu.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\n# Use dim() to get dimensions of dataset\ndim(wild_bird)\n\n\n[1] 147   2\n\n\nThere are 147 cases in 2 columns(Reference and Taken from Figure 1 of Nee et al). Actually the second row has the real column names so we will now make second row as column names and remove the first row.\n\n\nCode\n#Rename the column names\ncolnames(wild_bird) <- wild_bird[1,]\n#Removing the first row\nwild_bird <- wild_bird[-1,]\n#New dimensions of dataset\ndim(wild_bird)\n\n\n[1] 146   2\n\n\nCode\n#View the dataset\nwild_bird\n\n\n# A tibble: 146 × 2\n   `Wet body weight [g]` `Population size`\n   <chr>                 <chr>            \n 1 5.45887180052624      532194.395145161 \n 2 7.76456810683605      3165107.44544653 \n 3 8.63858738018464      2592996.86778979 \n 4 10.6897349302105      3524193.2266336  \n 5 7.41722577905587      389806.168891807 \n 6 9.1169347252776       604765.97978904  \n 7 8.03684333000353      192360.511579436 \n 8 8.70473119796067      250452.449623033 \n 9 8.89032317828959      16997.4156415239 \n10 9.51590845877281      595.09393677964  \n# … with 136 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n#Summary of dataset\nsummary(wild_bird)\n\n\n Wet body weight [g] Population size   \n Length:146          Length:146        \n Class :character    Class :character  \n Mode  :character    Mode  :character  \n\n\nThe dataset is in character class so first we need to convert character class to numeric and then get the summary.\n\n\nCode\n#Converting datset to numeric\nwild_bird$`Wet body weight [g]` <- as.numeric(wild_bird$`Wet body weight [g]`)\nwild_bird$`Population size` <- as.numeric(wild_bird$`Population size`)\n#Summary of the converted dataset\nsummary(wild_bird)\n\n\n Wet body weight [g] Population size  \n Min.   :   5.459    Min.   :      5  \n 1st Qu.:  18.620    1st Qu.:   1821  \n Median :  69.232    Median :  24353  \n Mean   : 363.694    Mean   : 382874  \n 3rd Qu.: 309.826    3rd Qu.: 198515  \n Max.   :9639.845    Max.   :5093378  \n\n\nThis is the brief summary of the wild_bird dataset."
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html",
    "href": "posts/challenge2_MirandaManka.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html#challenge-overview",
    "href": "posts/challenge2_MirandaManka.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html#read-in-the-data",
    "href": "posts/challenge2_MirandaManka.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\nhotel_bookings = read_csv(\"_data/hotel_bookings.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html#describe-the-data",
    "href": "posts/challenge2_MirandaManka.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\n#Looking at the data\nview(hotel_bookings)\n\n#Dimensions of the data\ndim(hotel_bookings)\n\n\n[1] 119390     32\n\n\nCode\n#Summary of the variables in the dataset\nsummary(hotel_bookings)\n\n\n    hotel            is_canceled       lead_time   arrival_date_year\n Length:119390      Min.   :0.0000   Min.   :  0   Min.   :2015     \n Class :character   1st Qu.:0.0000   1st Qu.: 18   1st Qu.:2016     \n Mode  :character   Median :0.0000   Median : 69   Median :2016     \n                    Mean   :0.3704   Mean   :104   Mean   :2016     \n                    3rd Qu.:1.0000   3rd Qu.:160   3rd Qu.:2017     \n                    Max.   :1.0000   Max.   :737   Max.   :2017     \n                                                                    \n arrival_date_month arrival_date_week_number arrival_date_day_of_month\n Length:119390      Min.   : 1.00            Min.   : 1.0             \n Class :character   1st Qu.:16.00            1st Qu.: 8.0             \n Mode  :character   Median :28.00            Median :16.0             \n                    Mean   :27.17            Mean   :15.8             \n                    3rd Qu.:38.00            3rd Qu.:23.0             \n                    Max.   :53.00            Max.   :31.0             \n                                                                      \n stays_in_weekend_nights stays_in_week_nights     adults      \n Min.   : 0.0000         Min.   : 0.0         Min.   : 0.000  \n 1st Qu.: 0.0000         1st Qu.: 1.0         1st Qu.: 2.000  \n Median : 1.0000         Median : 2.0         Median : 2.000  \n Mean   : 0.9276         Mean   : 2.5         Mean   : 1.856  \n 3rd Qu.: 2.0000         3rd Qu.: 3.0         3rd Qu.: 2.000  \n Max.   :19.0000         Max.   :50.0         Max.   :55.000  \n                                                              \n    children           babies              meal             country         \n Min.   : 0.0000   Min.   : 0.000000   Length:119390      Length:119390     \n 1st Qu.: 0.0000   1st Qu.: 0.000000   Class :character   Class :character  \n Median : 0.0000   Median : 0.000000   Mode  :character   Mode  :character  \n Mean   : 0.1039   Mean   : 0.007949                                        \n 3rd Qu.: 0.0000   3rd Qu.: 0.000000                                        \n Max.   :10.0000   Max.   :10.000000                                        \n NA's   :4                                                                  \n market_segment     distribution_channel is_repeated_guest\n Length:119390      Length:119390        Min.   :0.00000  \n Class :character   Class :character     1st Qu.:0.00000  \n Mode  :character   Mode  :character     Median :0.00000  \n                                         Mean   :0.03191  \n                                         3rd Qu.:0.00000  \n                                         Max.   :1.00000  \n                                                          \n previous_cancellations previous_bookings_not_canceled reserved_room_type\n Min.   : 0.00000       Min.   : 0.0000                Length:119390     \n 1st Qu.: 0.00000       1st Qu.: 0.0000                Class :character  \n Median : 0.00000       Median : 0.0000                Mode  :character  \n Mean   : 0.08712       Mean   : 0.1371                                  \n 3rd Qu.: 0.00000       3rd Qu.: 0.0000                                  \n Max.   :26.00000       Max.   :72.0000                                  \n                                                                         \n assigned_room_type booking_changes   deposit_type          agent          \n Length:119390      Min.   : 0.0000   Length:119390      Length:119390     \n Class :character   1st Qu.: 0.0000   Class :character   Class :character  \n Mode  :character   Median : 0.0000   Mode  :character   Mode  :character  \n                    Mean   : 0.2211                                        \n                    3rd Qu.: 0.0000                                        \n                    Max.   :21.0000                                        \n                                                                           \n   company          days_in_waiting_list customer_type           adr         \n Length:119390      Min.   :  0.000      Length:119390      Min.   :  -6.38  \n Class :character   1st Qu.:  0.000      Class :character   1st Qu.:  69.29  \n Mode  :character   Median :  0.000      Mode  :character   Median :  94.58  \n                    Mean   :  2.321                         Mean   : 101.83  \n                    3rd Qu.:  0.000                         3rd Qu.: 126.00  \n                    Max.   :391.000                         Max.   :5400.00  \n                                                                             \n required_car_parking_spaces total_of_special_requests reservation_status\n Min.   :0.00000             Min.   :0.0000            Length:119390     \n 1st Qu.:0.00000             1st Qu.:0.0000            Class :character  \n Median :0.00000             Median :0.0000            Mode  :character  \n Mean   :0.06252             Mean   :0.5714                              \n 3rd Qu.:0.00000             3rd Qu.:1.0000                              \n Max.   :8.00000             Max.   :5.0000                              \n                                                                         \n reservation_status_date\n Min.   :2014-10-17     \n 1st Qu.:2016-02-01     \n Median :2016-08-07     \n Mean   :2016-07-30     \n 3rd Qu.:2017-02-08     \n Max.   :2017-09-14     \n                        \n\n\nThis dataset has 32 variables with 119,390 observations. The variables include information about hotel bookings, while each observation/case is a different hotel booking. Some variables include hotel type (city hotel vs resort hotel), if the booking was canceled, arrival date, number of nights stayed (week and weekend), number of people and kids and babies, the market segment, if the guest is a repeat guest, and room type (there are more, this just points out a few). Some of the variables have categories (city vs resort hotel, for the type of hotel), some are numeric and continuous (lead time, in days for example 14) and some are numerical but binary (is canceled, 0 or 1). This data likely came from a hotel chain with different locations and/or multiple hotels, as the country variable shows that these are hotels in different countries."
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html#provide-grouped-summary-statistics-explain-and-interpret",
    "href": "posts/challenge2_MirandaManka.html#provide-grouped-summary-statistics-explain-and-interpret",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics & Explain and Interpret",
    "text": "Provide Grouped Summary Statistics & Explain and Interpret\n\n\nCode\n#Find mean and sd for number of stays in week nights grouped by hotel type\nhotel_bookings %>%\n  group_by(hotel) %>%\n  summarise(mean = mean(stays_in_week_nights), sd = sd(stays_in_week_nights))\n\n\n# A tibble: 2 × 3\n  hotel         mean    sd\n  <chr>        <dbl> <dbl>\n1 City Hotel    2.18  1.46\n2 Resort Hotel  3.13  2.46\n\n\nCode\n#Find mean and sd for number of stays in weekend nights grouped by hotel type\nhotel_bookings %>%\n  group_by(hotel) %>%\n  summarise(mean = mean(stays_in_weekend_nights), sd = sd(stays_in_weekend_nights))\n\n\n# A tibble: 2 × 3\n  hotel         mean    sd\n  <chr>        <dbl> <dbl>\n1 City Hotel   0.795 0.885\n2 Resort Hotel 1.19  1.15 \n\n\nCode\n#Find mean and sd for number of stays in week nights grouped by whether the guest is a repeat guest\nhotel_bookings %>%\n  group_by(is_repeated_guest) %>%\n  summarise(mean = mean(stays_in_week_nights), sd = sd(stays_in_week_nights))\n\n\n# A tibble: 2 × 3\n  is_repeated_guest  mean    sd\n              <dbl> <dbl> <dbl>\n1                 0  2.53  1.91\n2                 1  1.48  1.62\n\n\nI started by picking out a few interesting variables and looking at them. First, I grouped by hotel and looked at number of night stayed during the week to see if there was any difference. The resort hotels had a higher mean (3.1 compared to 2.2 for the city hotels) which was interesting, I thought maybe people staying at resorts plan an extra day more of their trip during the week. I also looked the same hotel grouping for weekend nights and the mean for resort hotels was still higher (1.2 vs 0.8 for city), so maybe people staying at resort hotels simply stay longer. This could be explored more in the future. I also looked grouped by whether someone is a repeated guest (0 for no, 1 for yes), then examined the mean for how many week nights they stayed. Repeat guests tend to stay for shorter amount of nights (1.48 vs 2.53 for non repeat guests). I thought this was interesting because people who tend to stay again aren’t staying as long (maybe more business people for a night rather than a family vacation).\n\n\nCode\n#Find summary statistics for lead time for booking grouped by hotel type\nhotel_bookings %>%\n  group_by(hotel) %>%\n  select(lead_time, hotel) %>%\n  summarize_all(list(mean=mean, median = median, min = min, max = max, sd = sd, var = var, IQR = IQR), na.rm = TRUE)\n\n\n# A tibble: 2 × 8\n  hotel         mean median   min   max    sd    var   IQR\n  <chr>        <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>\n1 City Hotel   110.      74     0   629 111.  12310.   140\n2 Resort Hotel  92.7     57     0   737  97.3  9464.   145\n\n\nI wanted to look at the lead time for each type of hotel. Lead time is how many days ahead of their stay someone booked, for example 7 would mean they booked their hotel a week bfore they showed up. The mean lead time for city hotels is 109.7 days (about 3.5 months ahead of time), while the mean lead time for the resort hotels is 92.7 days (about 3 months ahead of time). That is interesting but is only different by a few weeks. The medians for both groups were much lower than the mean, which indicates the data are skewed (positively, or towards the right), meaning more of the lead times were lower values. The maximums were still high though, with 629 days for city hotels and 737 days for resort hotels, although they both had minimums of 0 (same day or walk-in). The standard deviation and other measures of dispersion were fairly large, indicating the data are spread out (looking at the maximums and minimums, this makes sense).\n\n\nCode\n#Creating variable to see if people got the room they booked\ndifferent_room = ifelse(hotel_bookings$reserved_room_type != hotel_bookings$assigned_room_type, 1, 0)\n\n#Looking at the results\nprop.table(table(different_room))\n\n\ndifferent_room\n        0         1 \n0.8750565 0.1249435 \n\n\nFinally, I thought it would be interesting to look at how many people got the room they booked. I made a binary indicator variable to do this. If someone got a different room they were assigned a 1 for different_room, otherwise a 0 indicating they got the room they booked. The proportion table shows that 87.5% of people got the room they wanted, and 12.5% of people did not."
  },
  {
    "objectID": "posts/challenge1_LindsayJones.html",
    "href": "posts/challenge1_LindsayJones.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_LindsayJones.html#describe-the-data",
    "href": "posts/challenge1_LindsayJones.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nData set contains the number of railroad employees in the United States in 2012, organized by county. Data was likely gathered reported to labor bureau either by counties or by each railroad station. Each row represents a county. Columns indicate state (or territory), county, and number of employees. There are 2930 counties as shown below:\n\n\nCode\ndim(Railroad)\n\n\n[1] 2930    3\n\n\nThe 10 counties with the most railroad employees are:\n\n\nCode\nRailroad %>%\n  arrange(desc(total_employees)) %>%\n  select(state,county,total_employees)%>%\n  group_by(total_employees) %>%\n  slice(1)%>%\n  ungroup()%>%\n  arrange(desc(total_employees))%>%\n  slice(1:10)\n\n\n\n\n  \n\n\n\nState_Railroad_Props illustrates the percentage of railroad workers located in each state or territory.\n\n\nCode\nState <- select(Railroad, state)\nState_Railroad_Props <- prop.table(table(State))*100\nState_Railroad_Props\n\n\nstate\n        AE         AK         AL         AP         AR         AZ         CA \n0.03412969 0.20477816 2.28668942 0.03412969 2.45733788 0.51194539 1.87713311 \n        CO         CT         DC         DE         FL         GA         HI \n1.94539249 0.27303754 0.03412969 0.10238908 2.28668942 5.18771331 0.10238908 \n        IA         ID         IL         IN         KS         KY         LA \n3.37883959 1.22866894 3.51535836 3.13993174 3.24232082 4.06143345 2.15017065 \n        MA         MD         ME         MI         MN         MO         MS \n0.40955631 0.81911263 0.54607509 2.66211604 2.93515358 3.92491468 2.66211604 \n        MT         NC         ND         NE         NH         NJ         NM \n1.80887372 3.20819113 1.67235495 3.03754266 0.34129693 0.71672355 0.98976109 \n        NV         NY         OH         OK         OR         PA         RI \n0.40955631 2.08191126 3.00341297 2.49146758 1.12627986 2.21843003 0.17064846 \n        SC         SD         TN         TX         UT         VA         VT \n1.56996587 1.77474403 3.10580205 7.54266212 0.85324232 3.13993174 0.47781570 \n        WA         WI         WV         WY \n1.33105802 2.35494881 1.80887372 0.75085324"
  },
  {
    "objectID": "posts/challenge3_Mekhala Kumar.html",
    "href": "posts/challenge3_Mekhala Kumar.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_Mekhala Kumar.html#cleaning-data-and-the-reasoning-for-using-pivot",
    "href": "posts/challenge3_Mekhala Kumar.html#cleaning-data-and-the-reasoning-for-using-pivot",
    "title": "Challenge 3",
    "section": "Cleaning data and the reasoning for using pivot",
    "text": "Cleaning data and the reasoning for using pivot\nThe dataset used was organicpoultry. It contains information about the quantity of different poultry types for all months across the years 2004 to 2013. Currently the month and year data all fall under a single column. In order to make the data easy to interpret, first, the column with the data of the month and year need to be separated into two columns.\nFollowing which, the data needs to be pivoted in such a manner that the years become columns and the types of poultry become rows. This format will make it easier to select a subgroup within the types of poultry and compare the changes across years.\n\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\neggpoul <- read_excel(\"_data/organiceggpoultry.xls\",skip=4)\nView(eggpoul)\ncolnames(eggpoul)\n\n\n [1] \"...1\"                            \"Extra Large \\nDozen\"            \n [3] \"Extra Large 1/2 Doz.\\n1/2 Dozen\" \"Large \\nDozen\"                  \n [5] \"Large \\n1/2 Doz.\"                \"...6\"                           \n [7] \"Whole\"                           \"B/S Breast\"                     \n [9] \"Bone-in Breast\"                  \"Whole Legs\"                     \n[11] \"Thighs\"                         \n\n\nCode\neggpoul=subset(eggpoul,select=-c(...6))\ntail(eggpoul, 10)\n\n\n# A tibble: 10 × 10\n   ...1     Extra…¹ Extra…² Large…³ Large…⁴ Whole B/S B…⁵ Bone-…⁶ Whole…⁷ Thighs\n   <chr>      <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl> <chr>     <dbl> <chr> \n 1 March        290    188.    268.     178  238.    704. 390.5      204. 216.25\n 2 April        290    188.    268.     178  238.    704. 390.5      204. 216.25\n 3 May          290    188.    268.     178  238.    704. 390.5      204. 216.25\n 4 June         290    188.    268.     178  238.    704. 390.5      204. 216.25\n 5 July         290    188.    268.     178  238.    704. 390.5      204. 216.25\n 6 August       290    188.    268.     178  238.    704. 390.5      204. 216.25\n 7 Septemb…     290    188.    268.     178  238.    704. 390.5      204. 216.25\n 8 October      290    188.    268.     178  238.    704. 390.5      204. 216.25\n 9 November     290    188.    268.     178  238.    704. 390.5      204. 216.25\n10 December     290    188.    268.     178  238.    704. 390.5      204. 216.25\n# … with abbreviated variable names ¹​`Extra Large \\nDozen`,\n#   ²​`Extra Large 1/2 Doz.\\n1/2 Dozen`, ³​`Large \\nDozen`, ⁴​`Large \\n1/2 Doz.`,\n#   ⁵​`B/S Breast`, ⁶​`Bone-in Breast`, ⁷​`Whole Legs`\n\n\nCode\nhead(eggpoul)\n\n\n# A tibble: 6 × 10\n  ...1     Extra …¹ Extra…² Large…³ Large…⁴ Whole B/S B…⁵ Bone-…⁶ Whole…⁷ Thighs\n  <chr>       <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl> <chr>     <dbl> <chr> \n1 Jan 2004     230     132     230     126   198.    646. too few    194. too f…\n2 February     230     134.    226.    128.  198.    642. too few    194. 203   \n3 March        230     137     225     131   209     642. too few    194. 203   \n4 April        234.    137     225     131   212     642. too few    194. 203   \n5 May          236     137     225     131   214.    642. too few    194. 203   \n6 June         241     137     231.    134.  216.    641  too few    202. 200.3…\n# … with abbreviated variable names ¹​`Extra Large \\nDozen`,\n#   ²​`Extra Large 1/2 Doz.\\n1/2 Dozen`, ³​`Large \\nDozen`, ⁴​`Large \\n1/2 Doz.`,\n#   ⁵​`B/S Breast`, ⁶​`Bone-in Breast`, ⁷​`Whole Legs`\n\n\nCode\nstr(eggpoul)\n\n\ntibble [120 × 10] (S3: tbl_df/tbl/data.frame)\n $ ...1                           : chr [1:120] \"Jan 2004\" \"February\" \"March\" \"April\" ...\n $ Extra Large \nDozen            : num [1:120] 230 230 230 234 236 ...\n $ Extra Large 1/2 Doz.\n1/2 Dozen: num [1:120] 132 134 137 137 137 ...\n $ Large \nDozen                  : num [1:120] 230 226 225 225 225 ...\n $ Large \n1/2 Doz.               : num [1:120] 126 128 131 131 131 ...\n $ Whole                          : num [1:120] 198 198 209 212 214 ...\n $ B/S Breast                     : num [1:120] 646 642 642 642 642 ...\n $ Bone-in Breast                 : chr [1:120] \"too few\" \"too few\" \"too few\" \"too few\" ...\n $ Whole Legs                     : num [1:120] 194 194 194 194 194 ...\n $ Thighs                         : chr [1:120] \"too few\" \"203\" \"203\" \"203\" ...\n\n\nCode\neggpoul<-eggpoul%>%\n   mutate(`Bone-in Breast` = parse_number(na_if(`Bone-in Breast`, \"too few\")),\n           Thighs = parse_number(na_if(Thighs, \"too few\")))\neggpoul<-eggpoul %>% separate(1, c(\"Month\", \"Year\"), extra = \"drop\", fill = \"right\")\nvec<-rep(c(1,2,3,4,5,6,7,8,9,10),each=12)\neggpoul$Year[vec==1] <- 2004\neggpoul$Year[vec==2] <- 2005\neggpoul$Year[vec==3] <- 2006\neggpoul$Year[vec==4] <- 2007\neggpoul$Year[vec==5] <- 2008\neggpoul$Year[vec==6] <- 2009\neggpoul$Year[vec==7] <- 2010\neggpoul$Year[vec==8] <- 2011\neggpoul$Year[vec==9] <- 2012\neggpoul$Year[vec==10] <- 2013\ndim(eggpoul)\n\n\n[1] 120  11\n\n\n\nChallenge: Describe the final dimensions\nThe original dataset has 120 rows and 11 columns. 2 of the variables are being used to identify a case. Hence,after pivoting, we expect to have 1080 rows and 4 columns. It is anticipated that the data will be long (taller).\n\n\nCode\n#existing rows/cases\nnrow(eggpoul)\n\n\n[1] 120\n\n\nCode\n#existing columns/cases\nncol(eggpoul)\n\n\n[1] 11\n\n\nCode\n#expected rows/cases\nnrow(eggpoul) * (ncol(eggpoul)-2)\n\n\n[1] 1080\n\n\nCode\n# expected columns \n(11-9)+2\n\n\n[1] 4\n\n\n\n\nChallenge: Pivot the Chosen Data\nAfter pivoting, the data has become taller. Pivoting has also ensured that all the variables of poultry types have been kept in a single column and the values corresponding to them are easy to access.\n\n\nCode\neggpoul<-pivot_longer(eggpoul, 3:11, names_to = \"Type of Poultry\", values_to = \"Amount\")\ndim(eggpoul)\n\n\n[1] 1080    4\n\n\nCode\nView(eggpoul)"
  },
  {
    "objectID": "posts/challenge1_AdithyaParupudi.html",
    "href": "posts/challenge1_AdithyaParupudi.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(dplyr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_AdithyaParupudi.html#challenge-overview",
    "href": "posts/challenge1_AdithyaParupudi.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_AdithyaParupudi.html#read-in-the-data",
    "href": "posts/challenge1_AdithyaParupudi.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nbirds_data <- read_csv(\"_data/birds.csv\",show_col_types = FALSE)\n#spec(birds_data) -> full column specification\n\n\nAfter importing the csv file, and I notice that out of the 14 columns, 8 of them are of character type and 6 columns are double. Total rows -> 30977!"
  },
  {
    "objectID": "posts/challenge1_AdithyaParupudi.html#describe-the-data",
    "href": "posts/challenge1_AdithyaParupudi.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nnames(birds_data)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nColumn names at a glance\n\n\nCode\nhead(birds_data)\n\n\n# A tibble: 6 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1961  1961 1000…\n2 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1962  1962 1000…\n3 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1963  1963 1000…\n4 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1964  1964 1000…\n5 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1965  1965 1000…\n6 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1966  1966 1000…\n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names\n\n\n\n\nCode\nstr(birds_data)\n\n\nspec_tbl_df [30,977 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Domain Code     : chr [1:30977] \"QA\" \"QA\" \"QA\" \"QA\" ...\n $ Domain          : chr [1:30977] \"Live Animals\" \"Live Animals\" \"Live Animals\" \"Live Animals\" ...\n $ Area Code       : num [1:30977] 2 2 2 2 2 2 2 2 2 2 ...\n $ Area            : chr [1:30977] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ Element Code    : num [1:30977] 5112 5112 5112 5112 5112 ...\n $ Element         : chr [1:30977] \"Stocks\" \"Stocks\" \"Stocks\" \"Stocks\" ...\n $ Item Code       : num [1:30977] 1057 1057 1057 1057 1057 ...\n $ Item            : chr [1:30977] \"Chickens\" \"Chickens\" \"Chickens\" \"Chickens\" ...\n $ Year Code       : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Year            : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Unit            : chr [1:30977] \"1000 Head\" \"1000 Head\" \"1000 Head\" \"1000 Head\" ...\n $ Value           : num [1:30977] 4700 4900 5000 5300 5500 5800 6600 6290 6300 6000 ...\n $ Flag            : chr [1:30977] \"F\" \"F\" \"F\" \"F\" ...\n $ Flag Description: chr [1:30977] \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Domain Code` = col_character(),\n  ..   Domain = col_character(),\n  ..   `Area Code` = col_double(),\n  ..   Area = col_character(),\n  ..   `Element Code` = col_double(),\n  ..   Element = col_character(),\n  ..   `Item Code` = col_double(),\n  ..   Item = col_character(),\n  ..   `Year Code` = col_double(),\n  ..   Year = col_double(),\n  ..   Unit = col_character(),\n  ..   Value = col_double(),\n  ..   Flag = col_character(),\n  ..   `Flag Description` = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nWe get to see a get a high level view of the column names and its entries.\n\n\nCode\nhist(birds_data$`Item Code`)\n\n\n\n\n\nCode\nhist(birds_data$`Area Code`)\n\n\n\n\n\nUsing the histogram functions, observed that the frequency for item code and area codes respectively."
  },
  {
    "objectID": "posts/challenge1_instructions.html",
    "href": "posts/challenge1_instructions.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#challenge-overview",
    "href": "posts/challenge1_instructions.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#read-in-the-data",
    "href": "posts/challenge1_instructions.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions.html#describe-the-data",
    "href": "posts/challenge1_instructions.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge1_instructions-Youngsoo Choi.html",
    "href": "posts/challenge1_instructions-Youngsoo Choi.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions-Youngsoo Choi.html#challenge-overview",
    "href": "posts/challenge1_instructions-Youngsoo Choi.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions-Youngsoo Choi.html#read-in-the-data",
    "href": "posts/challenge1_instructions-Youngsoo Choi.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\n# Read data about birds\nlibrary(readr)\nbirds <- read_csv(\"_data/birds.csv\")\nView(birds)\n\n\nThis dataset contains over 30000 birds’ characters include area, years, etc."
  },
  {
    "objectID": "posts/challenge1_instructions-Youngsoo Choi.html#describe-the-data",
    "href": "posts/challenge1_instructions-Youngsoo Choi.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\n# Count how many birds after 2000\na2000<-filter(birds, Year>=2000)\na2000\n\n\n# A tibble: 10,945 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2000  2000\n 2 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2001  2001\n 3 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2002  2002\n 4 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2003  2003\n 5 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2004  2004\n 6 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2005  2005\n 7 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2006  2006\n 8 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2007  2007\n 9 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2008  2008\n10 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2009  2009\n# … with 10,935 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nCode\ncount(a2000)\n\n\n# A tibble: 1 × 1\n      n\n  <int>\n1 10945\n\n\nAnd the number of birds with year 2000 after is 10945."
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html",
    "href": "posts/challenge2_LindsayJones.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#challenge-overview",
    "href": "posts/challenge2_LindsayJones.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#read-in-the-data",
    "href": "posts/challenge2_LindsayJones.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nhotel_bookings ⭐⭐⭐\nFAOstat*.csv ⭐⭐⭐⭐⭐ (join FAOSTAT_country_groups)\n\n\n\nCode\nlibrary(readxl)\nRailroad <- read_xls(\"_data/StateCounty2012.xls\", \n    skip =3)\nView(Railroad)"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#describe-the-data",
    "href": "posts/challenge2_LindsayJones.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nTotal railroad employment by state and county for calendar year 2012.\n\n\nCode\nlibrary(readxl)\nRailroad <- read_xls(\"_data/StateCounty2012.xls\", \n    skip =3)\nView(Railroad)"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_LindsayJones.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\nThe total number of railroad employees for each state/territory are shown below.\n\n\nCode\nstate_totals = Railroad %>% \n  filter(grepl(\"Total\", STATE)) %>%\n  filter(!grepl(\"Grand\", STATE))\nprint(state_totals)\n\n\n# A tibble: 53 × 5\n   STATE     ...2  COUNTY ...4  TOTAL\n   <chr>     <lgl> <chr>  <lgl> <dbl>\n 1 AE Total1 NA    <NA>   NA        2\n 2 AK Total  NA    <NA>   NA      103\n 3 AL Total  NA    <NA>   NA     4257\n 4 AP Total1 NA    <NA>   NA        1\n 5 AR Total  NA    <NA>   NA     3871\n 6 AZ Total  NA    <NA>   NA     3153\n 7 CA Total  NA    <NA>   NA    13137\n 8 CO Total  NA    <NA>   NA     3650\n 9 CT Total  NA    <NA>   NA     2592\n10 DC Total  NA    <NA>   NA      279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nHere is the median number of railroad employees per state:\n\n\nCode\nsummarize(state_totals,median(TOTAL))\n\n\n# A tibble: 1 × 1\n  `median(TOTAL)`\n            <dbl>\n1            3379\n\n\nThe mean number railroad employees in each state is:\n\n\nCode\nsummarize(state_totals,mean(TOTAL))\n\n\n# A tibble: 1 × 1\n  `mean(TOTAL)`\n          <dbl>\n1         4819.\n\n\nThe Armed Forces Pacific (AP) is the territory with the fewest employees:\n\n\nCode\nsummarize(state_totals,min(TOTAL))\n\n\n# A tibble: 1 × 1\n  `min(TOTAL)`\n         <dbl>\n1            1\n\n\nAnd Texas is the territory with the most employees:\n\n\nCode\nsummarize(state_totals,max(TOTAL))\n\n\n# A tibble: 1 × 1\n  `max(TOTAL)`\n         <dbl>\n1        19839\n\n\nStandard deviation of employees is:\n\n\nCode\nsd(state_totals$TOTAL)\n\n\n[1] 4781.829\n\n\n\nExplain and Interpret\nI chose the states because I found some of the totals for each state surprising, based on what I know about the size of those states’ populations."
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html",
    "href": "posts/challenge1_RoyYoon.html",
    "title": "Challenge 1 Roy Yoon",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#reading-data",
    "href": "posts/challenge1_RoyYoon.html#reading-data",
    "title": "Challenge 1 Roy Yoon",
    "section": "Reading Data",
    "text": "Reading Data\n\n\nCode\nlibrary(readr)\nbirds <- read_csv(\"_data/birds.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#a-quick-look-at-birds",
    "href": "posts/challenge1_RoyYoon.html#a-quick-look-at-birds",
    "title": "Challenge 1 Roy Yoon",
    "section": "A Quick Look at ‘birds’",
    "text": "A Quick Look at ‘birds’\n\n\nCode\nhead(birds)\n\n\n# A tibble: 6 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1961  1961 1000…\n2 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1962  1962 1000…\n3 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1963  1963 1000…\n4 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1964  1964 1000…\n5 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1965  1965 1000…\n6 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1966  1966 1000…\n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names"
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#dimensions",
    "href": "posts/challenge1_RoyYoon.html#dimensions",
    "title": "Challenge 1 Roy Yoon",
    "section": "Dimensions",
    "text": "Dimensions\n\n\nCode\n#understanding the dimensions of data set 'birds'\ndim(birds)\n\n\n[1] 30977    14"
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#column-names",
    "href": "posts/challenge1_RoyYoon.html#column-names",
    "title": "Challenge 1 Roy Yoon",
    "section": "Column Names",
    "text": "Column Names\nThere are 30977 rows and 14 column in the data set\n\n\nCode\n#column names in  data set 'birds'\n\ncolnames(birds)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\""
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#cases-when-birds-data-value-is-greater-then-10000",
    "href": "posts/challenge1_RoyYoon.html#cases-when-birds-data-value-is-greater-then-10000",
    "title": "Challenge 1 Roy Yoon",
    "section": "Cases when ‘birds’ data ‘Value’ is greater then 10000",
    "text": "Cases when ‘birds’ data ‘Value’ is greater then 10000\n\n\nCode\n#looking at 'birds' data set that has 'Value' column value greater than 10000\nmore_than_10000 <- filter(birds, Value > 10000)\n\nmore_than_10000 \n\n\n# A tibble: 8,991 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2002  2002\n 2 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2003  2003\n 3 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2004  2004\n 4 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2005  2005\n 5 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2006  2006\n 6 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2008  2008\n 7 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2009  2009\n 8 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2010  2010\n 9 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2011  2011\n10 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2012  2012\n# … with 8,981 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nCode\narrange(more_than_10000, Value)\n\n\n# A tibble: 8,991 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …     211 Swit…    5112 Stocks     1057 Chic…    2013  2013\n 2 QA           Live …     171 Phil…    5112 Stocks     1068 Ducks    2012  2012\n 3 QA           Live …     138 Mexi…    5112 Stocks     1079 Turk…    1982  1982\n 4 QA           Live …     115 Camb…    5112 Stocks     1057 Chic…    1994  1994\n 5 QA           Live …     222 Tuni…    5112 Stocks     1079 Turk…    2012  2012\n 6 QA           Live …      26 Brun…    5112 Stocks     1057 Chic…    2002  2002\n 7 QA           Live …      49 Cuba     5112 Stocks     1057 Chic…    1965  1965\n 8 QA           Live …     158 Niger    5112 Stocks     1057 Chic…    1990  1990\n 9 QA           Live …     133 Mali     5112 Stocks     1057 Chic…    1961  1961\n10 QA           Live …     225 Unit…    5112 Stocks     1057 Chic…    1994  1994\n# … with 8,981 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe data above shows ‘birds’ for if the Value column was greater than 10000. The data is arranged by the ‘Value’ column values."
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#data-for-algeria-in-bird",
    "href": "posts/challenge1_RoyYoon.html#data-for-algeria-in-bird",
    "title": "Challenge 1 Roy Yoon",
    "section": "Data for Algeria in ‘bird’",
    "text": "Data for Algeria in ‘bird’\n\n\nCode\n#looking at 'birds' data set that has 'Value' column value greater than 10000 specifically for Algeria \nmore_than_10000_ALG <- filter(more_than_10000, Area == \"Algeria\")\n\nmore_than_10000_ALG \n\n\n# A tibble: 54 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1965  1965\n 2 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1966  1966\n 3 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1967  1967\n 4 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1968  1968\n 5 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1969  1969\n 6 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1970  1970\n 7 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1971  1971\n 8 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1972  1972\n 9 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1973  1973\n10 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1974  1974\n# … with 44 more rows, 4 more variables: Unit <chr>, Value <dbl>, Flag <chr>,\n#   `Flag Description` <chr>, and abbreviated variable names ¹​`Domain Code`,\n#   ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe data above examines the data for Algeria by looking at ‘birds’ data set that haa values greater than 10000."
  },
  {
    "objectID": "posts/challenge3_ManiShankerKamarapu.html",
    "href": "posts/challenge3_ManiShankerKamarapu.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_ManiShankerKamarapu.html#read-in-data",
    "href": "posts/challenge3_ManiShankerKamarapu.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\nvote_response <- read_excel(\"_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\", sheet = \"Table 2\", skip = 7, col_names = c(\"Area\", \"Yes\", \"Yes_P\", \"No\", \"No_P\", \"Totalclear\", \"Totalclear_P\", \"Empty\", \"clear\", \"clear_P\", \"Not_Eligible\", \"Not_Eligible_P\", \"NoResponse\", \"NoResponse_P\", \"Total\", \"Total_P\")) %>%\n  select(\"Area\", \"Yes\", \"No\", \"Not_Eligible\", \"NoResponse\", \"Total\") %>%\n  drop_na(\"Area\") %>%\n  filter(!grepl(\"Total\", Area))\nR <- nrow(vote_response)-7\nvote_response <- slice(vote_response, 1:R)\nview(vote_response)\n\n\nThe data is on the postal survey of Australian Electoral Roll. It contains the data of the eligible participants and responses of the participants in federal election as at 24 August 2017. It has data based on different federal electoral divisions survey by the Australian Election Commission. It is basically the total number of response we got in each division during the federal election. The data set is untidy and variables are not defined correctly and there are a lot of extra variables which are not required and a bunch of NA values. So using different R functions to remove unwanted variables and drop some NA values and also remove extra rows."
  },
  {
    "objectID": "posts/challenge3_ManiShankerKamarapu.html#separating-different-divisions-and-areas",
    "href": "posts/challenge3_ManiShankerKamarapu.html#separating-different-divisions-and-areas",
    "title": "Challenge 3",
    "section": "Separating different divisions and areas",
    "text": "Separating different divisions and areas\n\n\nCode\nvote_response <- vote_response %>%\n  mutate(Division = case_when(\n    str_ends(Area, \"Divisions\") ~ Area,\n    TRUE ~ NA_character_ )) %>%\n  fill(Division) %>%\n  drop_na(\"Yes\")\nView(vote_response)\n\n\nAfter we read the data and did some hard coding along with read, now we have separated the divisions row from the area column and formed a new column so we can use it as a grouping variable."
  },
  {
    "objectID": "posts/challenge3_ManiShankerKamarapu.html#using-pivot-to-tidy-data",
    "href": "posts/challenge3_ManiShankerKamarapu.html#using-pivot-to-tidy-data",
    "title": "Challenge 3",
    "section": "Using Pivot to tidy data",
    "text": "Using Pivot to tidy data\n\n\nCode\nvote_response <- pivot_longer(vote_response, Yes:Total, names_to = \"Response\", values_to = \"Count\")\nView(vote_response)\n\n\nI have used pivot_longer to tidy the data set where I have collapse the responses(Yes, No, Not_Eligible, NoResponse and Total) into one column so we can vary them easily and really make sense. So now we can get the divisional level count or area level count easily and we can also plot it easily now in different ways."
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html",
    "href": "posts/challenge1_instructions_NJ.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html#challenge-overview",
    "href": "posts/challenge1_instructions_NJ.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html#read-in-the-data",
    "href": "posts/challenge1_instructions_NJ.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")\nrailroad\n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html#describe-the-data",
    "href": "posts/challenge1_instructions_NJ.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nspec(railroad)\n\n\ncols(\n  state = col_character(),\n  county = col_character(),\n  total_employees = col_double()\n)\n\n\nCode\nrailroad %>%\n  filter(state == \"AK\")\n\n\n# A tibble: 6 × 3\n  state county               total_employees\n  <chr> <chr>                          <dbl>\n1 AK    ANCHORAGE                          7\n2 AK    FAIRBANKS NORTH STAR               2\n3 AK    JUNEAU                             3\n4 AK    MATANUSKA-SUSITNA                  2\n5 AK    SITKA                              1\n6 AK    SKAGWAY MUNICIPALITY              88\n\n\nCode\nrailroad %>%\n  group_by(state) %>%\n  summarise(total_employees2 = sum(total_employees))\n\n\n# A tibble: 53 × 2\n   state total_employees2\n   <chr>            <dbl>\n 1 AE                   2\n 2 AK                 103\n 3 AL                4257\n 4 AP                   1\n 5 AR                3871\n 6 AZ                3153\n 7 CA               13137\n 8 CO                3650\n 9 CT                2592\n10 DC                 279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWhen I used the spec() function, it returned the variable types of the columns. We know that state is type col_character(), county is type col_character() and total_employees is type col_double(). When I filtered through just the state of AK it returns the list of the total employees for each county in that state. I created a pipe that groups all the states together an than is summarized by the total amount of employees in each state. This pipe is useful because I now know how many total employees are in each state. This data was likely gathered in 2012 from the most used railroad stations in the USA. This dataset is also long in the sense that the column length is much greater than the amount of columns present."
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html",
    "href": "posts/challenge2_instructions_NJ.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html#read-in-data",
    "href": "posts/challenge2_instructions_NJ.html#read-in-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in Data",
    "text": "Read in Data\n\n\nCode\nFAO <- read_csv(\"_data/FAOSTAT_livestock.csv\")\nFAO\n\n\n# A tibble: 82,116 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1961  1961\n 2 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1962  1962\n 3 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1963  1963\n 4 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1964  1964\n 5 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1965  1965\n 6 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1966  1966\n 7 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1967  1967\n 8 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1968  1968\n 9 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1969  1969\n10 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1970  1970\n# … with 82,106 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThis dataset comes from the Food and Agriculture Association of the United Nations. They publish country-level data regularly and I am going to be looking at country-level estimates of the number of animals that are raised for livestock. We can see that there are 82116 rows in the livestock data."
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html#describe-the-data",
    "href": "posts/challenge2_instructions_NJ.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nspec(FAO)\n\n\ncols(\n  `Domain Code` = col_character(),\n  Domain = col_character(),\n  `Area Code` = col_double(),\n  Area = col_character(),\n  `Element Code` = col_double(),\n  Element = col_character(),\n  `Item Code` = col_double(),\n  Item = col_character(),\n  `Year Code` = col_double(),\n  Year = col_double(),\n  Unit = col_character(),\n  Value = col_double(),\n  Flag = col_character(),\n  `Flag Description` = col_character()\n)\n\n\nCode\nFAO.sm <- FAO %>%\n  select(-contains(\"Code\"))\nFAO.sm\n\n\n# A tibble: 82,116 × 9\n   Domain       Area        Element Item   Year Unit    Value Flag  Flag Descr…¹\n   <chr>        <chr>       <chr>   <chr> <dbl> <chr>   <dbl> <chr> <chr>       \n 1 Live Animals Afghanistan Stocks  Asses  1961 Head  1300000 <NA>  Official da…\n 2 Live Animals Afghanistan Stocks  Asses  1962 Head   851850 <NA>  Official da…\n 3 Live Animals Afghanistan Stocks  Asses  1963 Head  1001112 <NA>  Official da…\n 4 Live Animals Afghanistan Stocks  Asses  1964 Head  1150000 F     FAO estimate\n 5 Live Animals Afghanistan Stocks  Asses  1965 Head  1300000 <NA>  Official da…\n 6 Live Animals Afghanistan Stocks  Asses  1966 Head  1200000 <NA>  Official da…\n 7 Live Animals Afghanistan Stocks  Asses  1967 Head  1200000 <NA>  Official da…\n 8 Live Animals Afghanistan Stocks  Asses  1968 Head  1328000 <NA>  Official da…\n 9 Live Animals Afghanistan Stocks  Asses  1969 Head  1250000 <NA>  Official da…\n10 Live Animals Afghanistan Stocks  Asses  1970 Head  1300000 <NA>  Official da…\n# … with 82,106 more rows, and abbreviated variable name ¹​`Flag Description`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nprint(dfSummary(FAO.sm, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nFAO.sm\nDimensions: 82116 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Live Animals\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Africa2. Asia3. China, mainland4. Eastern Africa5. Eastern Asia6. Eastern Europe7. Egypt8. Europe9. India10. Northern Africa[ 243 others ]\n      522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)76896(93.6%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Stocks\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Asses2. Buffaloes3. Camels4. Cattle5. Goats6. Horses7. Mules8. Pigs9. Sheep\n      8571(10.4%)3505(4.3%)3265(4.0%)13086(15.9%)12498(15.2%)11104(13.5%)6153(7.5%)12015(14.6%)11919(14.5%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.4 (16.8)min ≤ med ≤ max:1961 ≤ 1991 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. Head\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 11625569 (64779790)min ≤ med ≤ max:0 ≤ 224667 ≤ 1489744504IQR (CV) : 2364200 (5.6)\n      43667 distinct values\n      \n      1301\n(1.6%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Im5. M\n      2667(6.1%)12567(28.7%)24550(56.0%)2877(6.6%)1185(2.7%)\n      \n      38270\n(46.6%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Data not available3. FAO data based on imputat4. FAO estimate5. Official data6. Unofficial figure\n      12567(15.3%)1185(1.4%)2877(3.5%)24550(29.9%)38270(46.6%)2667(3.2%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-18\n\n\n\nBased on the results of the spec() function, I can see that there are six variables that are type double and eight that are type character. Out of the six double() variables, Area Code, Year and Item Code are all good grouping variables because they do not have values that vary across rows. I dropped the double() variables that contain code because they are just numeric codes for database management purposes. Using summarytools(), I can say that the records in this dataset are the number of Live Animal Stocks and the units of the values is Head. Each case in this dataset consists of an animal record based on the country and year that tries to estimate the number of live animals which is represented by Value. In total, I have estimates of the stock of nine different types of livestock (Asses, Buffaloes, Camels, Cattle, Goats, Horses, Mules, Pigs, Sheep ) in 253 areas for 58 years. The flags correspond to what type of estimate is being used."
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions_NJ.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\n\n\nCode\nFAO.sm %>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n# A tibble: 28 × 2\n   Area                          n\n   <chr>                     <int>\n 1 Africa                      522\n 2 Americas                    464\n 3 Asia                        522\n 4 Australia and New Zealand   376\n 5 Caribbean                   464\n 6 Central America             406\n 7 Central Asia                243\n 8 Eastern Africa              522\n 9 Eastern Asia                522\n10 Eastern Europe              522\n# … with 18 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nFAO_clc <- FAO.sm %>%\n  filter(Flag!=\"A\")\n\nFAO_clc  \n\n\n# A tibble: 31,279 × 9\n   Domain       Area        Element Item   Year Unit    Value Flag  Flag Descr…¹\n   <chr>        <chr>       <chr>   <chr> <dbl> <chr>   <dbl> <chr> <chr>       \n 1 Live Animals Afghanistan Stocks  Asses  1964 Head  1150000 F     FAO estimate\n 2 Live Animals Afghanistan Stocks  Asses  1973 Head  1250000 F     FAO estimate\n 3 Live Animals Afghanistan Stocks  Asses  1974 Head  1250000 F     FAO estimate\n 4 Live Animals Afghanistan Stocks  Asses  1975 Head  1250000 F     FAO estimate\n 5 Live Animals Afghanistan Stocks  Asses  1976 Head  1250000 F     FAO estimate\n 6 Live Animals Afghanistan Stocks  Asses  1978 Head  1300000 *     Unofficial …\n 7 Live Animals Afghanistan Stocks  Asses  1979 Head  1300000 *     Unofficial …\n 8 Live Animals Afghanistan Stocks  Asses  1980 Head  1295000 *     Unofficial …\n 9 Live Animals Afghanistan Stocks  Asses  1981 Head  1315000 *     Unofficial …\n10 Live Animals Afghanistan Stocks  Asses  1982 Head  1315000 *     Unofficial …\n# … with 31,269 more rows, and abbreviated variable name ¹​`Flag Description`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nFAO_clc %>%\n  group_by(Item) %>%\n  summarize(avg=mean(Value, na.rm = TRUE),\n            mode = n(),\n            median = median(Value, na.rm = TRUE),\n            stdev= sd(Value, na.rm = TRUE),\n            min = min(Value, na.rm = TRUE),\n            max = max(Value, na.rm = TRUE))\n\n\n# A tibble: 9 × 7\n  Item           avg  mode median     stdev   min       max\n  <chr>        <dbl> <int>  <dbl>     <dbl> <dbl>     <dbl>\n1 Asses      196051.  4899  14300   615866.     0   8793747\n2 Buffaloes 5901247.   756   6550 19546207.    20 114151770\n3 Camels     499737.  1075  85350  1311815.    45   7762545\n4 Cattle    4380953.  3554  47650 21361967.    15 203634000\n5 Goats     2577844.  4711  57625 11790005.     0 139467008\n6 Horses     276368.  5046  11500   999297.     0  10479246\n7 Mules       80414.  3357   4400   357196.     0   3287449\n8 Pigs       746710.  4107  29000  9429158.     0 345754816\n9 Sheep     2463044.  3774  40000  8206951.     0 111238000\n\n\n\nExplain and Interpret\nHere we can confirm that not all cases are countries. Flag Value A corresponds to Areas that are actually regional aggregations. These should be filtered out if I want to keep the same type of case as a country-level case. The second filter statement removes all cases with Flag Value A so that our dataset is at a country-level case. It seems like the distribution of cases for regional aggregations is even except for Areas Melanesia and Micronesia. FAO_clc is more specific version of the dataset that only includes the cases that are type country-level. I have conducted exploartory analysis on FAO_clc on the group Item and my first impression was how vastly different the mean and median were for each Item. This implies that our data is skewed in one direction. I also see that each Item stdev is really high which indicates that the data observed is quite spread out. The min and max values tell little about the dataset"
  },
  {
    "objectID": "posts/challenge1_MirandaManka.html",
    "href": "posts/challenge1_MirandaManka.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_MirandaManka.html#challenge-overview",
    "href": "posts/challenge1_MirandaManka.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_MirandaManka.html#read-in-the-data",
    "href": "posts/challenge1_MirandaManka.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\n#Read in the excel file\nwild_bird_data = read_excel(\"_data/wild_bird_data.xlsx\")"
  },
  {
    "objectID": "posts/challenge1_MirandaManka.html#describe-the-data",
    "href": "posts/challenge1_MirandaManka.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, provide a high level description of the data.\n\n\nCode\n#View the data\nview(wild_bird_data)\n\n#Find the dimensions of the data\ndim(wild_bird_data)\n\n\n[1] 147   2\n\n\nCode\n#Look at the first few observations\nhead(wild_bird_data)\n\n\n# A tibble: 6 × 2\n  Reference           `Taken from Figure 1 of Nee et al.`\n  <chr>               <chr>                              \n1 Wet body weight [g] Population size                    \n2 5.45887180052624    532194.395145161                   \n3 7.76456810683605    3165107.44544653                   \n4 8.63858738018464    2592996.86778979                   \n5 10.6897349302105    3524193.2266336                    \n6 7.41722577905587    389806.168891807                   \n\n\nCode\n#Get column names\ncolnames(wild_bird_data)\n\n\n[1] \"Reference\"                         \"Taken from Figure 1 of Nee et al.\"\n\n\nThe data appear to be about 146 wild birds, detailing two pieces of information for each - their wet body weight (in grams) and the size of the population they are in. It is hard to tell more about this data without other information, such as the types of birds or how/when/where the data were collected, although if I had to guess, I would say that it was probably collected in forests or other outdoor areas with many birds because the title of the dataset indicates they are wild birds.\nInterestingly, the actual variable names seem to be in the next row instead of the labels the data currently has (the variables should be “Wet body weight [g]” for the first variable/column instead of “Reference”, and “Population size” for the second instead of “Taken from Figure 1 of Nee et al.”)\n\n\nCode\n#Work to correct variable/column names\n#Remove first row\nwild_bird_data <- wild_bird_data[-c(1), ]\n#Rename columns/variables\nwild_bird_data = rename(wild_bird_data, wet_body_weight_g = Reference)\nwild_bird_data = rename(wild_bird_data, population_size = `Taken from Figure 1 of Nee et al.`)\n#Create value to use from each column\nwet_body_weight_g = wild_bird_data[, 1]\npopulation_size = wild_bird_data[, 2]\n\n#Change type\nwet_body_weight_g = as.numeric(unlist(wet_body_weight_g))\npopulation_size = as.numeric(unlist(population_size))\n\n#Summary of variables\nsummary(wet_body_weight_g)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   5.459   18.620   69.232  363.694  309.826 9639.845 \n\n\nCode\nsummary(population_size)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      5    1821   24353  382874  198515 5093378 \n\n\nThe data are very spread out, with a lot of variation in values. This indicates that there is likely a large variety in the types of birds and/or the geographical location of the birds in these measurements."
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html",
    "href": "posts/challenge3_instructions_NJ.html",
    "title": "Challenge 3 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html#challenge-overview",
    "href": "posts/challenge3_instructions_NJ.html#challenge-overview",
    "title": "Challenge 3 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html#read-in-data",
    "href": "posts/challenge3_instructions_NJ.html#read-in-data",
    "title": "Challenge 3 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\nCode\neggs <-read_csv(\"_data/eggs_tidy.csv\",\n                        show_col_types = FALSE)\neggs\n\n\n# A tibble: 120 × 6\n   month      year large_half_dozen large_dozen extra_large_half_dozen extra_l…¹\n   <chr>     <dbl>            <dbl>       <dbl>                  <dbl>     <dbl>\n 1 January    2004             126         230                    132       230 \n 2 February   2004             128.        226.                   134.      230 \n 3 March      2004             131         225                    137       230 \n 4 April      2004             131         225                    137       234.\n 5 May        2004             131         225                    137       236 \n 6 June       2004             134.        231.                   137       241 \n 7 July       2004             134.        234.                   137       241 \n 8 August     2004             134.        234.                   137       241 \n 9 September  2004             130.        234.                   136.      241 \n10 October    2004             128.        234.                   136.      241 \n# … with 110 more rows, and abbreviated variable name ¹​extra_large_dozen\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nprint(dfSummary(eggs, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\neggs\nDimensions: 120 x 6\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      month\n[character]\n      1. April2. August3. December4. February5. January6. July7. June8. March9. May10. November[ 2 others ]\n      10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)20(16.7%)\n      \n      0\n(0.0%)\n    \n    \n      year\n[numeric]\n      Mean (sd) : 2008.5 (2.9)min ≤ med ≤ max:2004 ≤ 2008.5 ≤ 2013IQR (CV) : 5 (0)\n      2004:12(10.0%)2005:12(10.0%)2006:12(10.0%)2007:12(10.0%)2008:12(10.0%)2009:12(10.0%)2010:12(10.0%)2011:12(10.0%)2012:12(10.0%)2013:12(10.0%)\n      \n      0\n(0.0%)\n    \n    \n      large_half_dozen\n[numeric]\n      Mean (sd) : 155.2 (22.6)min ≤ med ≤ max:126 ≤ 174.5 ≤ 178IQR (CV) : 45.1 (0.1)\n      126.00  :1(0.8%)128.50  :29(24.2%)129.75  :1(0.8%)131.00  :3(2.5%)131.12 !:1(0.8%)132.00  :15(12.5%)133.50  :3(2.5%)173.25  :6(5.0%)174.50  :47(39.2%)178.00  :14(11.7%)! rounded\n      \n\n\n      0\n(0.0%)\n    \n    \n      large_dozen\n[numeric]\n      Mean (sd) : 254.2 (18.5)min ≤ med ≤ max:225 ≤ 267.5 ≤ 277.5IQR (CV) : 34.5 (0.1)\n      12 distinct values\n      \n      0\n(0.0%)\n    \n    \n      extra_large_half_dozen\n[numeric]\n      Mean (sd) : 164.2 (24.7)min ≤ med ≤ max:132 ≤ 185.5 ≤ 188.1IQR (CV) : 49.7 (0.2)\n      132.00  :1(0.8%)134.50  :1(0.8%)135.50  :28(23.3%)135.88 !:1(0.8%)137.00  :6(5.0%)138.12 !:1(0.8%)139.00  :15(12.5%)185.50  :53(44.2%)188.13  :14(11.7%)! rounded\n      \n\n\n      0\n(0.0%)\n    \n    \n      extra_large_dozen\n[numeric]\n      Mean (sd) : 266.8 (22.8)min ≤ med ≤ max:230 ≤ 285.5 ≤ 290IQR (CV) : 44 (0.1)\n      11 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-18\n\n\n\n\nBriefly describe the data\nThis dataset is comprised of 120 rows with 6 variables. One variable is a character (month) and the rest are doubles. Based off the summary statistics of the egg variables, I can see that the variance of sales for each dozen of eggs are all high meaning that most cases are not near there average. ## Anticipate the End Result\nThe column names large_half_dozen, large_dozen, extra_large_half_dozen and extra_large_dozen represent values of the type variable, the values in the columns represent values of the sales variable, and each row represents multiple observations, not one.\n\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA    1375.    1319.\n2 USA      1990 NAFTA    1030.     961.\n3 France   1980 EU        891.     258.\n4 Mexico   1990 NAFTA    1960.     983.\n5 USA      1980 NAFTA    1223.    -203.\n6 France   1990 EU       1031.    1250.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\nCode\nnrow(eggs)\n\n\n[1] 120\n\n\nCode\nncol(eggs)\n\n\n[1] 6\n\n\nWhat needs to changed is that the column names that include the values for dozens needs to become its own columns called type and the corresponding values for that type will be stored in a column names sales"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html#pivot-the-data",
    "href": "posts/challenge3_instructions_NJ.html#pivot-the-data",
    "title": "Challenge 3 Instructions",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1375.\n 2 Mexico   1980 NAFTA incoming              1319.\n 3 USA      1990 NAFTA outgoing              1030.\n 4 USA      1990 NAFTA incoming               961.\n 5 France   1980 EU    outgoing               891.\n 6 France   1980 EU    incoming               258.\n 7 Mexico   1990 NAFTA outgoing              1960.\n 8 Mexico   1990 NAFTA incoming               983.\n 9 USA      1980 NAFTA outgoing              1223.\n10 USA      1980 NAFTA incoming              -203.\n11 France   1990 EU    outgoing              1031.\n12 France   1990 EU    incoming              1250.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\nCode\neggs <- pivot_longer(eggs, col = c(large_half_dozen, large_dozen, extra_large_half_dozen, extra_large_dozen),\n                 names_to=\"type\",\n                 values_to = \"sales\")\neggs\n\n\n# A tibble: 480 × 4\n   month     year type                   sales\n   <chr>    <dbl> <chr>                  <dbl>\n 1 January   2004 large_half_dozen        126 \n 2 January   2004 large_dozen             230 \n 3 January   2004 extra_large_half_dozen  132 \n 4 January   2004 extra_large_dozen       230 \n 5 February  2004 large_half_dozen        128.\n 6 February  2004 large_dozen             226.\n 7 February  2004 extra_large_half_dozen  134.\n 8 February  2004 extra_large_dozen       230 \n 9 March     2004 large_half_dozen        131 \n10 March     2004 large_dozen             225 \n# … with 470 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow this dataset is in tidy form because each row is now one observation. I used pivot_longer because we needed more rows so that each observation could be individual in the dataset. When you go to read the dataset U can see each case only has one value attached to it."
  },
  {
    "objectID": "posts/challenge2_MekhalaKumar.html",
    "href": "posts/challenge2_MekhalaKumar.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_MekhalaKumar.html#reading-in-data",
    "href": "posts/challenge2_MekhalaKumar.html#reading-in-data",
    "title": "Challenge 2",
    "section": "Reading in Data",
    "text": "Reading in Data\nThe dataset used is State County (the excel file). While reading in the data, it was also cleaned by removing the first few rows and last few rows which had unnecessary text. Since the country Canada was also present in the last row of the dataset but is a different country, it has been removed as well.\n\n\nCode\nlibrary(readr)\nlibrary(readxl)\nlibrary(stringr)\nrailroad_df <- read_excel(\"_data/StateCounty2012.xls\",skip=3,col_names= c(\"state\", \"delete\", \"county\", \"delete\", \"total_employees\"))%>%\nselect(!contains(\"delete\"))%>%\n  filter(!str_detect(state, \"Total\"))\ntail(railroad_df, 10)\n\n\n# A tibble: 10 × 3\n   state                                               county     total_employ…¹\n   <chr>                                               <chr>      <chr>         \n 1 WY                                                  PLATTE     129           \n 2 WY                                                  SHERIDAN   252           \n 3 WY                                                  SUBLETTE   3             \n 4 WY                                                  SWEETWATER 196           \n 5 WY                                                  UINTA      49            \n 6 WY                                                  WASHAKIE   10            \n 7 WY                                                  WESTON     37            \n 8 CANADA                                              <NA>       662           \n 9 1  Military designation.                            <NA>       <NA>          \n10 NOTE:  Excludes 2,896 employees without an address. <NA>       <NA>          \n# … with abbreviated variable name ¹​total_employees\n\n\nCode\nrailroad_df<-head(railroad_df, -2)\ntail(railroad_df, 10)\n\n\n# A tibble: 10 × 3\n   state  county     total_employees\n   <chr>  <chr>      <chr>          \n 1 WY     NIOBRARA   51             \n 2 WY     PARK       29             \n 3 WY     PLATTE     129            \n 4 WY     SHERIDAN   252            \n 5 WY     SUBLETTE   3              \n 6 WY     SWEETWATER 196            \n 7 WY     UINTA      49             \n 8 WY     WASHAKIE   10             \n 9 WY     WESTON     37             \n10 CANADA <NA>       662            \n\n\nCode\nrailroad_df<-head(railroad_df, -1)\ntail(railroad_df, 10)\n\n\n# A tibble: 10 × 3\n   state county     total_employees\n   <chr> <chr>      <chr>          \n 1 WY    NATRONA    92             \n 2 WY    NIOBRARA   51             \n 3 WY    PARK       29             \n 4 WY    PLATTE     129            \n 5 WY    SHERIDAN   252            \n 6 WY    SUBLETTE   3              \n 7 WY    SWEETWATER 196            \n 8 WY    UINTA      49             \n 9 WY    WASHAKIE   10             \n10 WY    WESTON     37             \n\n\nCode\nrailroad_df = railroad_df[-1,] \nrailroad_df <- transform(railroad_df,employees = as.numeric(total_employees))\n View(railroad_df)"
  },
  {
    "objectID": "posts/challenge2_MekhalaKumar.html#data-description",
    "href": "posts/challenge2_MekhalaKumar.html#data-description",
    "title": "Challenge 2",
    "section": "Data description",
    "text": "Data description\nThe data consists of 3 columns- state, county and employees. Since the employees column contained character datatype, total_employees was created with the double type of variable. There are 53 states included in the data and 1709 counties. The number of employees by state can also be seen in the third table.\n\n\nCode\nhead(railroad_df)\n\n\n  state               county total_employees employees\n1    AE                  APO               2         2\n2    AK            ANCHORAGE               7         7\n3    AK FAIRBANKS NORTH STAR               2         2\n4    AK               JUNEAU               3         3\n5    AK    MATANUSKA-SUSITNA               2         2\n6    AK                SITKA               1         1\n\n\nCode\nrailroad_df%>%\n  select(state)%>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad_df%>%\n  select(county)%>%\n  n_distinct(.)\n\n\n[1] 1709\n\n\nCode\nrailroad_df %>%\n  group_by(state) %>%\n  summarise(employees2 = sum(employees))\n\n\n# A tibble: 53 × 2\n   state employees2\n   <chr>      <dbl>\n 1 AE             2\n 2 AK           103\n 3 AL          4257\n 4 AP             1\n 5 AR          3871\n 6 AZ          3153\n 7 CA         13137\n 8 CO          3650\n 9 CT          2592\n10 DC           279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge2_MekhalaKumar.html#grouped-summary-statistics-and-interpretation",
    "href": "posts/challenge2_MekhalaKumar.html#grouped-summary-statistics-and-interpretation",
    "title": "Challenge 2",
    "section": "Grouped Summary Statistics and Interpretation",
    "text": "Grouped Summary Statistics and Interpretation\nFirst, the summary statistics were checked for a country wide comparison. Then a few states were selected in such a way that a few states had many employees (around 13000) and a few states selected had a small number of employees (100-200). This was done in order to check how the central tendency and dispersion varied across counties in a particular state.\nBoth California and Nebraska, which had a larger number of employees, had a higher mean for the total employees among the counties compared to the mean number of employees across states. This is possibly due to the fact that they had more employees. Similarly, it was observed that in the case where there were a lower number of employees, the mean for the total employees across counties were lower and the standard deviation across counties was lower (in Vermont and Alaska). As can be seen below, the District of Columbia cannot be analysed in this way as it consists of a single county.\nFinally, the total number of employees in each county of a state were arranged in descending order because a county with a higher number of employees indicates that there are more job opportunities or perhaps a higher population in the area.\n\n\nCode\nlibrary(dplyr)\nlibrary(summarytools)\n\nrailroad_df%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1       87.17816               21             1          8207     283.6359\n  var.employees IQR.employees\n1      80449.32            58\n\n\nCode\nrailroad_df%>% filter(state == \"CA\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1       238.8545               61             1          2888     549.4692\n  var.employees IQR.employees\n1      301916.3           188\n\n\nCode\nrailroad_df%>% filter(state == \"CA\")%>%group_by(county)%>%arrange(county, desc(employees))\n\n\n# A tibble: 55 × 4\n# Groups:   county [55]\n   state county       total_employees employees\n   <chr> <chr>        <chr>               <dbl>\n 1 CA    ALAMEDA      346                   346\n 2 CA    AMADOR       9                       9\n 3 CA    BUTTE        69                     69\n 4 CA    CALAVERAS    30                     30\n 5 CA    COLUSA       2                       2\n 6 CA    CONTRA COSTA 348                   348\n 7 CA    EL DORADO    103                   103\n 8 CA    FRESNO       341                   341\n 9 CA    GLENN        4                       4\n10 CA    HUMBOLDT     2                       2\n# … with 45 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nrailroad_df%>% filter(state == \"NE\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1       148.0449               15             1          3797     511.5816\n  var.employees IQR.employees\n1      261715.7            66\n\n\nCode\nrailroad_df%>% filter(state == \"NE\")%>%group_by(county)%>%arrange(county, desc(employees))\n\n\n# A tibble: 89 × 4\n# Groups:   county [89]\n   state county    total_employees employees\n   <chr> <chr>     <chr>               <dbl>\n 1 NE    ADAMS     77                     77\n 2 NE    ANTELOPE  2                       2\n 3 NE    ARTHUR    2                       2\n 4 NE    BANNER    8                       8\n 5 NE    BLAINE    2                       2\n 6 NE    BOONE     2                       2\n 7 NE    BOX BUTTE 1168                 1168\n 8 NE    BROWN     1                       1\n 9 NE    BUFFALO   107                   107\n10 NE    BURT      9                       9\n# … with 79 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nrailroad_df%>% filter(state == \"DC\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1            279              279           279           279           NA\n  var.employees IQR.employees\n1            NA             0\n\n\nCode\nrailroad_df%>% filter(state == \"VT\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1           18.5              8.5             3            83     24.54431\n  var.employees IQR.employees\n1      602.4231             8\n\n\nCode\nrailroad_df%>% filter(state == \"VT\")%>%group_by(county)%>%arrange(county, desc(employees))\n\n\n# A tibble: 14 × 4\n# Groups:   county [14]\n   state county     total_employees employees\n   <chr> <chr>      <chr>               <dbl>\n 1 VT    ADDISON    8                       8\n 2 VT    BENNINGTON 8                       8\n 3 VT    CALEDONIA  3                       3\n 4 VT    CHITTENDEN 40                     40\n 5 VT    ESSEX      4                       4\n 6 VT    FRANKLIN   83                     83\n 7 VT    GRAND ISLE 5                       5\n 8 VT    LAMOILLE   4                       4\n 9 VT    ORANGE     9                       9\n10 VT    ORLEANS    13                     13\n11 VT    RUTLAND    59                     59\n12 VT    WASHINGTON 3                       3\n13 VT    WINDHAM    10                     10\n14 VT    WINDSOR    10                     10\n\n\nCode\nrailroad_df%>% filter(state == \"AK\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1       17.16667              2.5             1            88     34.76445\n  var.employees IQR.employees\n1      1208.567             4\n\n\nCode\nrailroad_df%>% filter(state == \"AK\")%>%group_by(county)%>%arrange(county, desc(employees))\n\n\n# A tibble: 6 × 4\n# Groups:   county [6]\n  state county               total_employees employees\n  <chr> <chr>                <chr>               <dbl>\n1 AK    ANCHORAGE            7                       7\n2 AK    FAIRBANKS NORTH STAR 2                       2\n3 AK    JUNEAU               3                       3\n4 AK    MATANUSKA-SUSITNA    2                       2\n5 AK    SITKA                1                       1\n6 AK    SKAGWAY MUNICIPALITY 88                     88"
  },
  {
    "objectID": "posts/challenge3_solutions.html",
    "href": "posts/challenge3_solutions.html",
    "title": "Challenge 3 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE, cache=TRUE)"
  },
  {
    "objectID": "posts/challenge3_solutions.html#challenge-overview",
    "href": "posts/challenge3_solutions.html#challenge-overview",
    "title": "Challenge 3 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nanticipate the shape of pivoted data, and\npivot the data into tidy format using pivot_longer\n\n\nExampleAnimal Weights ⭐Eggs ⭐⭐/⭐⭐⭐Australian Marriage Ballot ⭐⭐⭐USA Households ⭐⭐⭐⭐\n\n\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nFind current and future data dimensions\nLets see if this works with a simple example.\n\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n\n  \n\n\n#existing rows/cases\nnrow(df)\n\n[1] 6\n\n#existing columns/cases\nncol(df)\n\n[1] 5\n\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n[1] 12\n\n# expected columns \n3 + 2\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nPivot the data\n\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n\n  \n\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\n\nThe animal weights dataset contains tabular-style data, with cells representing the average live animal weight (in kg) of 16 types of livestock for each of 9 geographic areas as defined by the Intergovernmental Panel on Climate Change (IPCC. Livestock weights are a critical part of the Global Livestock Envrionmental Assessment Model used by the FAO.\n\nanimal_weight<-read_csv(\"_data/animal_weight.csv\")\nanimal_weight\n\n\n\n  \n\n\n\nBecause the animal weights data is in tabular format, it is easy to see that \\(n=9\\) regions (categories or cases) in the original data, and that there are \\(k=16\\) types of livestock (categories or columns). Therefore, we expect the pivoted dataset to have \\(9 * 16\\) = 144 rows and 3 columns (region, animal type, and animal weight.)\n\n\n\n\n\n\ninline R code\n\n\n\nIf you check out the code above, you will see that I didn’t use a calculator to figure out \\(9*16=144\\), but used inline r code like this: `r 9*16`.\n\n\n\nPivot the data\n\nanimal_weight_longer<-pivot_longer(animal_weight, \n                                    col=-`IPCC Area`,\n                                    names_to = \"Livestock\",\n                                    values_to = \"Weight\")\nanimal_weight_longer\n\n\n\n  \n\n\n\nYes, it looks like we ended up with 144 rows and 3 columns, exactly as expected!\n\n\n\n\n\n\nNote\n\n\n\n#Go further\nstringr functions, and separate from tidyr, would be useful in helping split out additional infromation from the Livestock column.\n\n\n\n\n\nThis section covers pivoting for the organic eggs data, available in both excel and (partially cleaned) .csv format. The data reports the average price per carton paid to the farmer or producer for organic eggs (and organic chicken), reported monthly from 2004 to 20013. Average price is reported by carton type, which can vary in both size (x-large or large) and quantity (half-dozen or dozen.)\nIf you are using the eggs_tidy.csv, you can skip the first section as your data is in .csv format and already partially cleaned. The first section reviews data read-in and cleaning for the organicpoultry.xls file.\n\nRead and Clean the dataPivot Type OnlyPivot Size and Quantity\n\n\nThere are three sheets in the organicpoultry.xls workbook: one titled Data, one titled “Organic egg prices, 2004-13” and one with a similar title for chicken prices. While I can tell all of this from inspection, I can also use a ask R to return the sheet names for me.\n\n\n\n\n\n\nGet sheet names with excel_sheets()\n\n\n\nBoth readxl and googlesheets4 have a function that can return sheet names as a vector. This is really useful if you need to parse and read multiple sheets in the same workbook.\n\n\n\nexcel_sheets(\"_data/organiceggpoultry.xls\")\n\n[1] \"Data\"                            \"Organic egg prices, 2004-13\"    \n[3] \"Organic poultry prices, 2004-13\"\n\n\nWhile it may seem like it would be easier to read in the individual egg prices and chicken prices, the amount of formatting introduced into the second and third sheets is pretty intimidating (see the screenshot below.) There are repeated headers to remove, a year column to shift, and other formatting issues. Ironically, it may be easier to read in the egg data from the Data sheet, with a skip of 5 (to skip the table title, etc), custom column names designed for pivoting to two categories (final section) and only reading in columns B to F.\n\n\n\nOrganic Poultry Data\n\n\n\n\n\nOrganic Poultry Egg Prices\n\n\n\n\n\n\n\n\nHard-coding Table Formats\n\n\n\nFormatted excel tables are a horrible data source, but may be the only way to get some data. If table formatting is consistent from year to year, hard-coding can be an acceptable approach. If table format is inconsistent, then more powerful tools are needed.\n\n\n\neggs_orig<-read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dozen\",\n                               \"xlarge_halfdozen\", \"large_dozen\",\n                               \"large_halfdozen\")\n                 )\neggs_orig\n\n\n\n  \n\n\n\nSometimes there are notes in the first column of tables, so lets make sure that isn’t an issue.\n\neggs_orig%>%\n  count(date)\n\n\n\n  \n\n\n\nWe need to remove the “note” indicators in two of the rows. Some characters require an escape to be included in regular expressions, but this time it is straightforward to find ” /1”.\n\neggs<-eggs_orig%>%\n  mutate(date = str_remove(date, \" /1\"))\n\nOne final step is needed to split the year variable away from the month. You will often need to separate out two variables from a single column when working with published tables, and also need to use the equivalent of dragging to fill in a normal spreadsheet. Lets look at the easiest way to fix both of these issues.\n\n\n\n\n\n\ntidyr::separate()\n\n\n\nSeparate is a fantastic function for working with strings. It will break a string column into multiple new (named) columns, at the indicated separator character (e.g., “,” or ” “). The old variable is automatically removed, but can be left.\n\n\n\n\n\n\n\n\ntidyr::fill()\n\n\n\nFill works like dragging to fill functionality in a spreadsheet. You can choose the direction to fill.\n\n\n\neggs<-eggs%>%\n  separate(date, into=c(\"month\", \"year\"), sep=\" \")%>%\n  fill(year)\neggs\n\n\n\n  \n\n\n\n\n\nLooking at the data, we can see that each of the original 120 cases consist of a year-month combination (e.g., January 2004), while the values are the average price (in cents) of four different types of eggs (e.g., large_half_dozen, large_dozen, etc) So to tidy our data, we should create a matrix with a year-month-eggType combination, with a single price value for each case.\nTo do this (and make our data easier to graph and analyze), we can pivot longer - changing our data from 120 rows with 6 variables (2 grouping and 4 values) to 480 rows of 4 variables (with 3 grouping variables and a single price value).\n\neggs_long<-eggs%>%\n  pivot_longer(cols=contains(\"large\"), \n               names_to = \"eggType\",\n               values_to = \"avgPrice\"\n  )\neggs_long\n\n\n\n  \n\n\n\nWell, that was super easy. But wait, what if you are interested in egg size - you want to know how much more expensive extra-large eggs are compared to large eggs. Right now, that will be annoying, as you will have to keep sorting out the egg quantity - whether the price is for a half_dozen or a dozen eggs.\n\n\nWouldn’t it be nice if we had two new columns - size and quantity - in place of the existing eggType categorical variable? In other words, to have fully tidy data, we would need 4 grouping variables (year, month, size, and quantity) and the same value (price). So, we want to use pivot longer, but we will be adding two new category variables (for a total of 4) and this will cut the number of rows in half (to 240).\nHow can we let R know what we want it to do?? Thankfully, we created pretty systematic column names for egg types in our original data, following the general pattern: size-quantity. Maybe we can use this to our advantage? Working with patterns in the names_sep option of the pivot functions makes it easier than you would think to pivot four existing columns into two new columns.\n\neggs_long<- eggs%>%\n  pivot_longer(cols=contains(\"large\"),\n               names_to = c(\"size\", \"quantity\"),\n               names_sep=\"_\",\n               values_to = \"price\"\n  )\neggs_long\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is another tabular data source published by the Australian Bureau of Statistics that requires a decent amount of cleaning. In 2017, Australia conducted a postal survey to gauge citizens’ opinions towards same sex marriage: “Should the law be changed to allow same-sex couples to marry?” All Australian citizens are required to vote in elections, so citizens could respond in one of four ways: vote yes, vote no, vote in an unclear way, or fail to vote. (See the “Explanatory Notes” sheet for more details.)\nThe provided table includes estimates of the proportion of citizens choosing each of the four options, aggregated by Federal Electoral District, which are nested within one of 8 overarching Electoral Divisions. Here is a quick image showing the original table format.\n ### Identify desired data structure\nInspection reveals several critical issues to address: - Typical long header (skip = 7) - No single row with variable names - Two redundant values (count and percentage - percentage is easy to recover from complete count data) - Total columns that are redundant (remove) - The sum of “Yes” and “No” votes appears to be redundant with Response Clear in columns I and J - District and Division are in the same column\nIn this example, we are going to identify the desired structure early in the process, because clever naming of variables makes it much easier to use pivot functions. We will skip reading in redundant data (proportions and “totals” columns), and then can identify four potentially distinct pieces of information. Three grouping variables: Division (in column 1), District (also in column 1), and citizen Response (yes, no, unclear, and non-response), plus one value: aggregated response Count.\nOur basic data reading and cleaning process should therefore follow these steps:\n\nRead in data, skipping unneeded columns and renaming variables\nCreate Division and District variables using separate() and fill()\npivot_longer() four response variables into 2 new Response and Count variables (double the number of rows)\n\n\nRead DataSeparate District and DivisionPivot_longer to tidy format\n\n\nIt is best to confine serious hard-coding to the initial data read in step, to make it easy to locate and make changes or replicate in the future. So, we will use a combination of tools introduced earlier to read and reformat the data: skip and col_names to read in the data, select to get rid of unneeded columns, and filter to get rid of unneeded rows. We also use the drop_na function to filter unwanted rows.\n\nvote_orig <- read_excel(\"_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\",\n           sheet=\"Table 2\",\n           skip=7,\n           col_names = c(\"District\", \"Yes\", \"del\", \"No\", rep(\"del\", 6), \"Illegible\", \"del\", \"No Response\", rep(\"del\", 3)))%>%\n  select(!starts_with(\"del\"))%>%\n  drop_na(District)%>%\n  filter(!str_detect(District, \"(Total)\"))%>%\n  filter(!str_starts(District, \"\\\\(\"))\nvote_orig\n\n\n\n  \n\n\n\n\n\nThe most glaring remaining issue is that the administrative Division is not in its own column, but is on its own row within the District column. The following code uses case_when to make a new Division variable with an entry (e.g., New South Wales Division) where there is a Division name in the District column, and otherwise it create just an empty space. After that, fill can be used to fill in empty spaces with the most recent Division name. We then filter out rows with only the title information.\n\nvote<- vote_orig%>%\n  mutate(Division = case_when(\n    str_ends(District, \"Divisions\") ~ District,\n    TRUE ~ NA_character_ ))%>%\n  fill(Division, .direction = \"down\")\nvote<- filter(vote,!str_detect(District, \"Division|Australia\"))\nvote\n\n\n\n  \n\n\n\n\n\nSupposed we wanted to create a stacked bar chart to compare the % who votes Yes to the people who either said No or didn’t vote. Or if we wanted to use division level characteristics to predict the proortion of people voting in a specific way? In both cases, we would need tidy data, which requires us to pivot longer into the original (aggregated) data format: Division, District, Response, Count. We should end up with 600 rows and 4 columns.\n\nvote_long<- vote%>%\n  pivot_longer(\n    cols = Yes:`No Response`,\n    names_to = \"Response\",\n    values_to = \"Count\"\n  )\nvote\n\n\n\n  \n\n\n\n\n\n\n\n\nThe excel workbook “USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019” is clearly a table of census-type household data (e.g., Current Population Study or CPS, American Community Study or ACS, etc.) Row 3 of the workbook provides a link to more details about the origin of the data used to produce the table.\nThe cases in this example are essentially year-identity groups, where I use the term identity to refer to the wide range of ways that the census can cluster racial and identity identity. While there are 12 categories in the data, many of these overlap and/or are not available in specific years. For example, one category is “All Races”, and it overlaps with all other categories but cannot be easily eliminated because it isn’t clear how\n\n\n\nExcel Workbook Screenshot\n\n\n\nIdentify desired data structure\nInspection of the excel workbook reveals several critical features of the data. - column names (of a sort) are in rows 4 and 5 (skip=5 and rename) - first column includes year and race/hispanic origin households - first column appears to have notes of some sort (remove notes) - there are end notes starting in row 358 (n_max = 352) - “Total” column appears to be redundant proportion info\nThe data appears to have two grouping variables (year and identityity), plus several values:\n\na count of number of households\nmedian and mean income (and associated margin of error)\nproportion of households with hhold income in one of 9 designated ranges or brackets\n\nThe final data should probably\n\nRead and clean the dataClean and separate *year” columnSanity check for identity\n\n\n\nincome_brackets <- c(i1 = \"Under $15,000\",\n                     i2 = \"$15,000 to $24,999\",\n                     i3 = \"$25,000 to $34,999\",\n                     i4= \"$35,000 to $49,999\",\n                     i5 = \"$50,000 to $74,999\",\n                     i6 = \"$75,000 to $99,999\",\n                     i7 = \"$100,000 to $149,999\",\n                     i8 = \"$150,000 to $199,999\",\n                     i9 = \"$200,000 and over\")\n\nushh_orig <- read_excel(\"_data/USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\",\n                        skip=5,\n                        n_max = 352,\n                        col_names = c(\"year\", \"hholds\", \"del\", \n                                str_c(\"income\",1:9,sep=\"_i\"),\n                               \"median_inc\", \"median_se\", \"mean_inc\",\"mean_se\"))%>%\n  select(-del)\n\n\n\nThe current year column still has identityity information on the hholds, as well as notes that need to be removed. Because identityity labels have spaces, we will need to remove those first before our typical approach to removing notes using separate is going to work.\n\n\n\n\n\n\nRegex and Regexr\n\n\n\nRegular expressions are a critical tool for messy, real world data where you will need to search, replace, and extract information from string variables. Learning regex is tough, but Regexer makes it much easier!\n\n\n\nushh_orig%>%\n  filter(str_detect(year, \"[[:alpha:]]\"))\n\n\n\n  \n\n\n\nNow that we know how to use regular expressions to find the household identityity information, we can quickly separate out the identityity information from the years, then do the standard fill prior to removing the unneeded category rows.\nOnce that is done, we can use separate to remove the notes from the year column. Removing notes from the identityity column is a bit trickier, and requires regex to find cases where there is a space then at least one numeric digit\n\nushh_id<-ushh_orig%>%\n  mutate(identity = case_when(\n    str_detect(year, \"[[:alpha:]]\") ~ year,\n    TRUE ~ NA_character_\n  ))%>%\n  fill(identity)%>%\n  filter(!str_detect(year, \"[[:alpha:]]\"))\n\nushh_id<-ushh_id%>%\n  separate(year, into=c(\"year\", \"delete\"), sep=\" \")%>%\n  mutate(identity = str_remove(identity, \" [0-9]+\"),\n         year = parse_number(year))%>%\n  select(-delete)\n\n\n\nEven from the detailed notes, it is difficult to fully understand what is going on with the identity variable, and whether all of the values are available in every year. A simple sanity check is to pick out several years mentioned in the notes and see if the number of households are available for all categories, and also check to see if there are specific categories that add up to the “all races” category.\n\nushh_id%>%\n  filter(year%in%c(1970, 1972, 1980, 2001, 2002))%>%\n  select(identity, hholds, year)%>%\n  pivot_wider(values_from=hholds, names_from=year)\n\n\n\n  \n\n\n\nBased on these examples, we can now confirm that the survey did not include a question about Hispanic background prior to 109228, that only “White” and “Black” (and not “Asian”) were systematically recorded prior to 2002, and that other mentioned dates of changes are not relevant to the categories represented in the data. Additionally, we can see from the example years that it would be reasonable to create a consistent time series that collapses the “White” and “White Alone” and “Black” and “Black A labels.\nBased on this exploratory data, one reasonable option that will streamline future analysis is to create two new variables “race” and “hispanic” as follows. :::{.callout-tip} ## Keep your original data\nOriginal data that has been carefully documented can be overly detailed and broken into categories that make systematic analysis difficult. When you simplify data categories for exploratory work, keep the original data so that you can reintroduce it at the appropriate point.\n\n\n\n\nushh <-ushh_id%>%\n  mutate(id = case_when(\n    identity %in% c(\"White\", \"White Alone\") ~ \"race_white\"\n  ))"
  },
  {
    "objectID": "posts/KaushikaPotluri_Challenge1.html",
    "href": "posts/KaushikaPotluri_Challenge1.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(dplyr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/KaushikaPotluri_Challenge1.html#challenge-overview",
    "href": "posts/KaushikaPotluri_Challenge1.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/KaushikaPotluri_Challenge1.html#read-in-the-data",
    "href": "posts/KaushikaPotluri_Challenge1.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\ndata <- read_csv(\"_data/birds.csv\", col_types = \"ccncncncnncncc\")\nspec(data)\n\n\ncols(\n  `Domain Code` = col_character(),\n  Domain = col_character(),\n  `Area Code` = col_number(),\n  Area = col_character(),\n  `Element Code` = col_number(),\n  Element = col_character(),\n  `Item Code` = col_number(),\n  Item = col_character(),\n  `Year Code` = col_number(),\n  Year = col_number(),\n  Unit = col_character(),\n  Value = col_number(),\n  Flag = col_character(),\n  `Flag Description` = col_character()\n)\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/KaushikaPotluri_Challenge1.html#describe-the-data",
    "href": "posts/KaushikaPotluri_Challenge1.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nnames(data)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nCode\ndim(data)\n\n\n[1] 30977    14\n\n\nCode\nstr(data)\n\n\nspec_tbl_df [30,977 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Domain Code     : chr [1:30977] \"QA\" \"QA\" \"QA\" \"QA\" ...\n $ Domain          : chr [1:30977] \"Live Animals\" \"Live Animals\" \"Live Animals\" \"Live Animals\" ...\n $ Area Code       : num [1:30977] 2 2 2 2 2 2 2 2 2 2 ...\n $ Area            : chr [1:30977] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ Element Code    : num [1:30977] 5112 5112 5112 5112 5112 ...\n $ Element         : chr [1:30977] \"Stocks\" \"Stocks\" \"Stocks\" \"Stocks\" ...\n $ Item Code       : num [1:30977] 1057 1057 1057 1057 1057 ...\n $ Item            : chr [1:30977] \"Chickens\" \"Chickens\" \"Chickens\" \"Chickens\" ...\n $ Year Code       : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Year            : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Unit            : chr [1:30977] \"1000 Head\" \"1000 Head\" \"1000 Head\" \"1000 Head\" ...\n $ Value           : num [1:30977] 4700 4900 5000 5300 5500 5800 6600 6290 6300 6000 ...\n $ Flag            : chr [1:30977] \"F\" \"F\" \"F\" \"F\" ...\n $ Flag Description: chr [1:30977] \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Domain Code` = col_character(),\n  ..   Domain = col_character(),\n  ..   `Area Code` = col_number(),\n  ..   Area = col_character(),\n  ..   `Element Code` = col_number(),\n  ..   Element = col_character(),\n  ..   `Item Code` = col_number(),\n  ..   Item = col_character(),\n  ..   `Year Code` = col_number(),\n  ..   Year = col_number(),\n  ..   Unit = col_character(),\n  ..   Value = col_number(),\n  ..   Flag = col_character(),\n  ..   `Flag Description` = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nCode\nsummary(data)\n\n\n Domain Code           Domain            Area Code        Area          \n Length:30977       Length:30977       Min.   :   1   Length:30977      \n Class :character   Class :character   1st Qu.:  79   Class :character  \n Mode  :character   Mode  :character   Median : 156   Mode  :character  \n                                       Mean   :1202                     \n                                       3rd Qu.: 231                     \n                                       Max.   :5504                     \n                                                                        \n  Element Code    Element            Item Code        Item          \n Min.   :5112   Length:30977       Min.   :1057   Length:30977      \n 1st Qu.:5112   Class :character   1st Qu.:1057   Class :character  \n Median :5112   Mode  :character   Median :1068   Mode  :character  \n Mean   :5112                      Mean   :1066                     \n 3rd Qu.:5112                      3rd Qu.:1072                     \n Max.   :5112                      Max.   :1083                     \n                                                                    \n   Year Code         Year          Unit               Value         \n Min.   :1961   Min.   :1961   Length:30977       Min.   :       0  \n 1st Qu.:1976   1st Qu.:1976   Class :character   1st Qu.:     171  \n Median :1992   Median :1992   Mode  :character   Median :    1800  \n Mean   :1991   Mean   :1991                      Mean   :   99411  \n 3rd Qu.:2005   3rd Qu.:2005                      3rd Qu.:   15404  \n Max.   :2018   Max.   :2018                      Max.   :23707134  \n                                                  NA's   :1036      \n     Flag           Flag Description  \n Length:30977       Length:30977      \n Class :character   Class :character  \n Mode  :character   Mode  :character"
  },
  {
    "objectID": "posts/challenge2_RoyYoon.html",
    "href": "posts/challenge2_RoyYoon.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n#library(readr)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_RoyYoon.html#read-in-the-dataset-railroad_2012_clean_county.csv",
    "href": "posts/challenge2_RoyYoon.html#read-in-the-dataset-railroad_2012_clean_county.csv",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Dataset “railroad_2012_clean_county.csv”",
    "text": "Read in the Dataset “railroad_2012_clean_county.csv”\n\n\nCode\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")\n\nrailroad\n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\ndim(railroad)\n\n\n[1] 2930    3\n\n\nCode\ncolnames(railroad)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nThere are three variable names: ‘state’, ‘county’, and ‘total_employees’."
  },
  {
    "objectID": "posts/challenge2_RoyYoon.html#describe-the-data",
    "href": "posts/challenge2_RoyYoon.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\ncolnames(railroad)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nCode\ndim(railroad)\n\n\n[1] 2930    3\n\n\nCode\nrailroad \n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge2_RoyYoon.html#summary-statistics",
    "href": "posts/challenge2_RoyYoon.html#summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nFirst I tried to attempt at making the data grouped by states with a single total employee count(regardless of the counties)\n\n\nCode\nstate_grouped_railroad <- railroad %>%\n    select(state, total_employees) %>%\n    group_by(state) %>%\n    tally(total_employees)\n\nstate_grouped_railroad <-rename(state_grouped_railroad, total_employees = n)\n\nstate_grouped_railroad\n\n\n# A tibble: 53 × 2\n   state total_employees\n   <chr>           <dbl>\n 1 AE                  2\n 2 AK                103\n 3 AL               4257\n 4 AP                  1\n 5 AR               3871\n 6 AZ               3153\n 7 CA              13137\n 8 CO               3650\n 9 CT               2592\n10 DC                279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nExplain and Interpret\nWork is still in progress.\nThought process:\n\nexamine which state has the most/least ‘total_employees’\nidentify which county has the most/least ‘total_employees’ in the state with the most/least ‘total_employees’\nlook at overall, which county has the most/least ‘total_employees’, and how does that compare to the state values\nexamine the average, min, max across states/counties"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge3.html",
    "href": "posts/SaaradhaaM_Challenge3.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge3.html#reading-in-data",
    "href": "posts/SaaradhaaM_Challenge3.html#reading-in-data",
    "title": "Challenge 3",
    "section": "Reading in data",
    "text": "Reading in data\nI will be working with the households dataset.\n\n\nCode\n# Reading in data.\nhouseholds <-read_excel(\"_data/USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\", skip=4)\nhouseholds\n\n\n# A tibble: 383 × 16\n   ...1      ...2  Total Under…¹ $15,0…² $25,0…³ $35,0…⁴ $50,0…⁵ $75,0…⁶ $100,…⁷\n   <chr>     <chr> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 ALL RACES <NA>     NA    NA      NA      NA      NA      NA      NA      NA  \n 2 2019      1284…   100     9.1     8       8.3    11.7    16.5    12.3    15.5\n 3 2018      1285…   100    10.1     8.8     8.7    12      17      12.5    15  \n 4 2017 2    1276…   100    10       9.1     9.2    12      16.4    12.4    14.7\n 5 2017      1275…   100    10.1     9.1     9.2    11.9    16.3    12.6    14.8\n 6 2016      1262…   100    10.4     9       9.2    12.3    16.7    12.2    15  \n 7 2015      1258…   100    10.6    10       9.6    12.1    16.1    12.4    14.9\n 8 2014      1245…   100    11.4    10.5     9.6    12.6    16.4    12.1    14  \n 9 2013 3    1239…   100    11.4    10.3     9.5    12.5    16.8    12      13.9\n10 2013 4    1229…   100    11.3    10.4     9.7    13.1    17      12.5    13.6\n# … with 373 more rows, 6 more variables: `$150,000\\r\\nto\\r\\n$199,999` <dbl>,\n#   `$200,000 and over` <dbl>, Estimate...13 <dbl>,\n#   `Margin of error1 (±)...14` <dbl>, Estimate...15 <chr>,\n#   `Margin of error1 (±)...16` <chr>, and abbreviated variable names\n#   ¹​`Under $15,000`, ²​`$15,000\\r\\nto\\r\\n$24,999`, ³​`$25,000\\r\\nto\\r\\n$34,999`,\n#   ⁴​`$35,000\\r\\nto\\r\\n$49,999`, ⁵​`$50,000\\r\\nto\\r\\n$74,999`,\n#   ⁶​`$75,000\\r\\nto\\r\\n$99,999`, ⁷​`$100,000\\r\\nto\\r\\n$149,999`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\nBrief description of data\nWhen reading in the data, I skipped the first four rows (they just describe the dataset). The dataset segments households by their income levels, race and Hispanic origin of householder from 1967 to 2019. It also has an external link to https://www2.census.gov/programs-surveys/cps/techdocs/cpsmar20.pdf, which shows that the data is part of the Annual Social and Economic Supplement in the Census. I need to re-name some headers, remove “Total” (redundant column) and remove rows 353 to 383 (they’re just notes).\n\n\nCode\n# Rename column headers.\ncolnames(households)\n\n\n [1] \"...1\"                       \"...2\"                      \n [3] \"Total\"                      \"Under $15,000\"             \n [5] \"$15,000\\r\\nto\\r\\n$24,999\"   \"$25,000\\r\\nto\\r\\n$34,999\"  \n [7] \"$35,000\\r\\nto\\r\\n$49,999\"   \"$50,000\\r\\nto\\r\\n$74,999\"  \n [9] \"$75,000\\r\\nto\\r\\n$99,999\"   \"$100,000\\r\\nto\\r\\n$149,999\"\n[11] \"$150,000\\r\\nto\\r\\n$199,999\" \"$200,000 and over\"         \n[13] \"Estimate...13\"              \"Margin of error1 (±)...14\" \n[15] \"Estimate...15\"              \"Margin of error1 (±)...16\" \n\n\nCode\nhouseholds <- rename(households, \"year\" = \"...1\", \"num_thousands\" = \"...2\", \"estimated_median_income\" = \"Estimate...13\", \"median_moe\" = \"Margin of error1 (±)...14\", \"estimated_mean_income\" = \"Estimate...15\", \"mean_moe\" = \"Margin of error1 (±)...16\")\n# Remove \"Total\" column.\nhouseholds <- households[,-3]\n# Remove rows 353-383.\nhouseholds <- households[-c(353:383),]\n# Which rows have missing values? This tells me how many rows of races there are.\nwhich(rowSums(is.na(households))>0)\n\n\n [1]   1  57  78 114 135 166 187 208 244 265 286 302"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge3.html#anticipate-end-result-and-find-current-and-future-data-dimensions.",
    "href": "posts/SaaradhaaM_Challenge3.html#anticipate-end-result-and-find-current-and-future-data-dimensions.",
    "title": "Challenge 3",
    "section": "Anticipate end result and find current and future data dimensions.",
    "text": "Anticipate end result and find current and future data dimensions.\nNow the dataset is a lot cleaner. We can see that in the “year” column, there are rows of races (N = 12). Race should actually be entered as a separate column, but I don’t know how to select specific rows in the “year” column to create a new column. I’ll remove those rows just for the purposes of working through this exercise.\n\n\nCode\n# Remove race rows.\nhouseholds_new <- households[-c(1,57,78,114,135,166,187,208,244,265,286,302),]\ndim(households_new)\n\n\n[1] 340  15\n\n\nThe current dimensions are 340 rows and 15 columns. I would like to shift all the income categories into an “income” column, so this should give me a lot more rows and 8 columns.\n\nPivoting dataset\n\n\nCode\n# Attempt pivotlonger().\nhouseholds_new <- pivot_longer(households_new, cols = contains(\"$\"), names_to = \"income\", values_to = \"proportion\")\nhouseholds_new\n\n\n# A tibble: 3,060 × 8\n   year  num_thousands estimated_median…¹ media…² estim…³ mean_…⁴ income propo…⁵\n   <chr> <chr>                      <dbl>   <dbl> <chr>   <chr>   <chr>    <dbl>\n 1 2019  128451                     68703     904 98088   1042    \"Unde…     9.1\n 2 2019  128451                     68703     904 98088   1042    \"$15,…     8  \n 3 2019  128451                     68703     904 98088   1042    \"$25,…     8.3\n 4 2019  128451                     68703     904 98088   1042    \"$35,…    11.7\n 5 2019  128451                     68703     904 98088   1042    \"$50,…    16.5\n 6 2019  128451                     68703     904 98088   1042    \"$75,…    12.3\n 7 2019  128451                     68703     904 98088   1042    \"$100…    15.5\n 8 2019  128451                     68703     904 98088   1042    \"$150…     8.3\n 9 2019  128451                     68703     904 98088   1042    \"$200…    10.3\n10 2018  128579                     64324     704 91652   914     \"Unde…    10.1\n# … with 3,050 more rows, and abbreviated variable names\n#   ¹​estimated_median_income, ²​median_moe, ³​estimated_mean_income, ⁴​mean_moe,\n#   ⁵​proportion\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow we suddenly have >3000 rows. This is because the columns estimated_median_income, median_moe, estimated_mean_income and mean_moe are the same for each year (regardless of income bracket, which we’ve just pivoted into a new column). So I’m going to split the data into two tables to make it easier to understand.\n\n\nCode\n# Creating table 1 by removing appropriate columns.\nhouseholds_1 <- households_new[,-c(3:6)]\n\n# Changing num_thousands to numeric so that the next argument runs properly.\nhouseholds_1$num_thousands <- as.numeric(households_1$num_thousands)\n\n# Merging 2 columns into 1.\nhouseholds_1 <- households_1 %>% mutate(count_thousands = `num_thousands`*(`proportion`/100))\n\n# Removing the 2 old columns.\nhouseholds_1 <- households_1[,-c(2,4)]\n\n\n\n\nCode\n# Creating table 2 by removing appropriate columns.\nhouseholds_2 <- households_new[,-c(2, 7:8)]\n\n# Remove duplicate rows in table 2.\nhouseholds_2 %>% distinct()\n\n\n# A tibble: 340 × 5\n   year   estimated_median_income median_moe estimated_mean_income mean_moe\n   <chr>                    <dbl>      <dbl> <chr>                 <chr>   \n 1 2019                     68703        904 98088                 1042    \n 2 2018                     64324        704 91652                 914     \n 3 2017 2                   63761        552 91406                 979     \n 4 2017                     64007        575 89922                 892     \n 5 2016                     62898        764 88578                 822     \n 6 2015                     60987        570 85533                 715     \n 7 2014                     58001        697 81870                 793     \n 8 2013 3                   58904       1183 82660                 1201    \n 9 2013 4                   57095        499 79852                 902     \n10 2012                     56912        384 79510                 773     \n# … with 330 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow we have 2 tables that are relatively easier to comprehend than what we had at the start. This is a work in progress - I want to figure out how to add the race column, and also to interpret the tables I’ve created."
  },
  {
    "objectID": "posts/challenge2_nickboonstra.html",
    "href": "posts/challenge2_nickboonstra.html",
    "title": "Nick Boonstra Challenge 2",
    "section": "",
    "text": "For today’s challenge, I will be reading in and wrangling data from the “hotel_bookings” dataset.\n\n\nFirst, my R setup chunk:\n\n\nCode\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_nickboonstra.html#reading-in-the-data",
    "href": "posts/challenge2_nickboonstra.html#reading-in-the-data",
    "title": "Nick Boonstra Challenge 2",
    "section": "Reading in the data",
    "text": "Reading in the data\nReading in the data was a fairly straightforward process:\n\n\nCode\nhotels<-read_csv(\"_data/hotel_bookings.csv\")\nhotels\n\n\n# A tibble: 119,390 × 32\n   hotel  is_ca…¹ lead_…² arriv…³ arriv…⁴ arriv…⁵ arriv…⁶ stays…⁷ stays…⁸ adults\n   <chr>    <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 Resor…       0     342    2015 July         27       1       0       0      2\n 2 Resor…       0     737    2015 July         27       1       0       0      2\n 3 Resor…       0       7    2015 July         27       1       0       1      1\n 4 Resor…       0      13    2015 July         27       1       0       1      1\n 5 Resor…       0      14    2015 July         27       1       0       2      2\n 6 Resor…       0      14    2015 July         27       1       0       2      2\n 7 Resor…       0       0    2015 July         27       1       0       2      2\n 8 Resor…       0       9    2015 July         27       1       0       2      2\n 9 Resor…       1      85    2015 July         27       1       0       3      2\n10 Resor…       1      75    2015 July         27       1       0       3      2\n# … with 119,380 more rows, 22 more variables: children <dbl>, babies <dbl>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, booking_changes <dbl>,\n#   deposit_type <chr>, agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\nTransformations\nAfter reading in the data, I made a few transformations:\n\n\nCode\nhotels<-hotels %>% \n  rename(is_cancelled = is_canceled) %>%  ## i'm petty\n  mutate(booking_dummy = case_when( ## dummy var for whether or not changes were made\n    booking_changes == 0 ~ 0,\n    T ~ 1\n  )) %>% \n  mutate(arrival_date_month_num = case_when( ## numerical variable for months\n    arrival_date_month == \"January\" ~ 1,\n    arrival_date_month == \"February\" ~ 2,\n    arrival_date_month == \"March\" ~ 3,\n    arrival_date_month == \"April\" ~ 4,\n    arrival_date_month == \"May\" ~ 5,\n    arrival_date_month == \"June\" ~ 6,\n    arrival_date_month == \"July\" ~ 7,\n    arrival_date_month == \"August\" ~ 8,\n    arrival_date_month == \"September\" ~ 9,\n    arrival_date_month == \"October\" ~ 10,\n    arrival_date_month == \"November\" ~ 11,\n    arrival_date_month == \"December\" ~ 12\n  ))\n\n\nI found that this data set only required fairly minimal/minor transformations. Firstly, I renamed the “is_canceled” variable to “is_cancelled” primarily because I’m petty, and I knew I would want to spell it with the double “L” the whole time. Next, I created a dummy variable called “booking_dummy” for whether or not any changes were made to a booking, regardless of how many such changes there were. Lastly, I created a variable named “arrival_date_month_num” to assign the corresponding number to each month as named in the “arrival_date_month” column."
  },
  {
    "objectID": "posts/challenge2_nickboonstra.html#describing-the-data",
    "href": "posts/challenge2_nickboonstra.html#describing-the-data",
    "title": "Nick Boonstra Challenge 2",
    "section": "Describing the data",
    "text": "Describing the data\nBefore I started summarising, I wanted to get a sense of what the data “looked like,” so to speak:\n\n\nCode\nnames(hotels)\n\n\n [1] \"hotel\"                          \"is_cancelled\"                  \n [3] \"lead_time\"                      \"arrival_date_year\"             \n [5] \"arrival_date_month\"             \"arrival_date_week_number\"      \n [7] \"arrival_date_day_of_month\"      \"stays_in_weekend_nights\"       \n [9] \"stays_in_week_nights\"           \"adults\"                        \n[11] \"children\"                       \"babies\"                        \n[13] \"meal\"                           \"country\"                       \n[15] \"market_segment\"                 \"distribution_channel\"          \n[17] \"is_repeated_guest\"              \"previous_cancellations\"        \n[19] \"previous_bookings_not_canceled\" \"reserved_room_type\"            \n[21] \"assigned_room_type\"             \"booking_changes\"               \n[23] \"deposit_type\"                   \"agent\"                         \n[25] \"company\"                        \"days_in_waiting_list\"          \n[27] \"customer_type\"                  \"adr\"                           \n[29] \"required_car_parking_spaces\"    \"total_of_special_requests\"     \n[31] \"reservation_status\"             \"reservation_status_date\"       \n[33] \"booking_dummy\"                  \"arrival_date_month_num\"        \n\n\nCode\ncount(hotels)\n\n\n# A tibble: 1 × 1\n       n\n   <int>\n1 119390\n\n\nCode\ncount(hotels,hotel)\n\n\n# A tibble: 2 × 2\n  hotel            n\n  <chr>        <int>\n1 City Hotel   79330\n2 Resort Hotel 40060\n\n\nCode\ncount(hotels,country)\n\n\n# A tibble: 178 × 2\n   country     n\n   <chr>   <int>\n 1 ABW         2\n 2 AGO       362\n 3 AIA         1\n 4 ALB        12\n 5 AND         7\n 6 ARE        51\n 7 ARG       214\n 8 ARM         8\n 9 ASM         1\n10 ATA         2\n# … with 168 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\narrivals<-xtabs(~arrival_date_year+arrival_date_month_num,hotels)\narrivals\n\n\n                 arrival_date_month_num\narrival_date_year    1    2    3    4    5    6    7    8    9   10   11   12\n             2015    0    0    0    0    0    0 2776 3889 5114 4957 2340 2920\n             2016 2248 3891 4824 5428 5478 5292 4572 5063 5394 6203 4454 3860\n             2017 3681 4177 4970 5661 6313 5647 5313 4925    0    0    0    0\n\n\nThis data set appears to describe hotel bookings from across a number of different countries and kinds of hotels, in the time range between July 2015 and August 2018. Each observation appears to be a single booking, with a range of information about each booking tracked in each column."
  },
  {
    "objectID": "posts/challenge2_nickboonstra.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_nickboonstra.html#provide-grouped-summary-statistics",
    "title": "Nick Boonstra Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nAs I looked through the data, I found myself gravitating toward the information about booking changes and cancellations.\n\n\nCode\nhotels %>% \n  group_by(is_cancelled) %>% \n  summarise(mean(booking_changes,na.rm=T))\n\n\n# A tibble: 2 × 2\n  is_cancelled `mean(booking_changes, na.rm = T)`\n         <dbl>                              <dbl>\n1            0                             0.293 \n2            1                             0.0983\n\n\nCode\nhotels %>% \n  group_by(is_cancelled) %>%\n  summarise(median(booking_changes))\n\n\n# A tibble: 2 × 2\n  is_cancelled `median(booking_changes)`\n         <dbl>                     <dbl>\n1            0                         0\n2            1                         0\n\n\nClearly, most bookings did not have any changes to their booking, demonstrated by the fact that the median number of changes for both cancelled and non-cancelled bookings was 0. However, it is interesting to observe that non-cancelled bookings tended to have more booking changes performed, suggesting that making changes to a booking may have increased the likelihood of that booking not having to be cancelled – a boon for hotels and travel agencies, if that extra bit of work is all it takes to retain a customer.\n\nVisualizing and Interpreting\nThese observations can be seen much more clearly in a graphic visualization:\n\n\nCode\nggplot(hotels,aes(x=factor(booking_dummy),fill=factor(is_cancelled))) +\n  geom_bar() +\n  theme_bw() +\n  labs(title = \"Cancellations by changes in booking\", x = \"Change in booking?\", y= \"Number of bookings\",\n       fill = \"Booking cancelled?\")\n\n\n\n\n\nThis bar graph utilizes the “booking_dummy” variable, easily dividing all bookings into those that had changes performed and those that didn’t. As can be seen, there were more bookings cancelled without changes being made than there were total bookings with changes! Additionally, a much smaller proportion of bookings with changes ended up being cancelled when compared to bookings without any changes made (though cancellations accounted for less than half of each group in the end). Of course, it is hard to make broad generalizations without knowing more of the story told by this data. However, on the face of things it looks as though a case could be made for flexibility with regards to changing bookings as a strong protection against customers cancelling reservations."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html",
    "href": "posts/SaaradhaaM_Challenge2.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html#reading-in-the-data",
    "href": "posts/SaaradhaaM_Challenge2.html#reading-in-the-data",
    "title": "Challenge 2",
    "section": "Reading in the data",
    "text": "Reading in the data\n\n\nCode\n# Read in and view the dataset.\nhotel <- read.csv(\"_data/hotel_bookings.csv\")\nhotel"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html#description-of-data",
    "href": "posts/SaaradhaaM_Challenge2.html#description-of-data",
    "title": "Challenge 2",
    "section": "Description of data",
    "text": "Description of data\n\n\nCode\n# Get rows and columns.\ndim(hotel)\n\n\n[1] 119390     32\n\n\nCode\n# Find which columns have missing data.\nwhich(colSums(is.na(hotel))>0)\n\n\nchildren \n      11 \n\n\nIn the hotel bookings dataset, there are 119390 cases and 32 columns. Only the children column has missing data (N = 11). Interesting columns include assigned room type, previous cancellations, days in waiting list and is_canceled. There are also columns for country and hotel, indicating that the data was likely gathered by surveying different hotels around the world."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html#grouped-summary-statistics-1",
    "href": "posts/SaaradhaaM_Challenge2.html#grouped-summary-statistics-1",
    "title": "Challenge 2",
    "section": "Grouped summary statistics #1",
    "text": "Grouped summary statistics #1\nI first want to examine the relationship between number of days in the waiting list and whether the booking was cancelled.\n\n\nCode\n# Check if is_canceled is binary.\napply(hotel,2,function(x) { all(x %in% 0:1) })\n\n\n                         hotel                    is_canceled \n                         FALSE                           TRUE \n                     lead_time              arrival_date_year \n                         FALSE                          FALSE \n            arrival_date_month       arrival_date_week_number \n                         FALSE                          FALSE \n     arrival_date_day_of_month        stays_in_weekend_nights \n                         FALSE                          FALSE \n          stays_in_week_nights                         adults \n                         FALSE                          FALSE \n                      children                         babies \n                         FALSE                          FALSE \n                          meal                        country \n                         FALSE                          FALSE \n                market_segment           distribution_channel \n                         FALSE                          FALSE \n             is_repeated_guest         previous_cancellations \n                          TRUE                          FALSE \nprevious_bookings_not_canceled             reserved_room_type \n                         FALSE                          FALSE \n            assigned_room_type                booking_changes \n                         FALSE                          FALSE \n                  deposit_type                          agent \n                         FALSE                          FALSE \n                       company           days_in_waiting_list \n                         FALSE                          FALSE \n                 customer_type                            adr \n                         FALSE                          FALSE \n   required_car_parking_spaces      total_of_special_requests \n                         FALSE                          FALSE \n            reservation_status        reservation_status_date \n                         FALSE                          FALSE \n\n\nCode\n# Check mean and median for days in waiting list.\nsummarise(hotel, diwl_mean = mean(days_in_waiting_list), diwl_median = median(days_in_waiting_list))\n\n\n\n\n  \n\n\n\nCode\n# Check mean of is_canceled, grouped by days in waiting list.\nhotel %>%\n  group_by(days_in_waiting_list) %>%\n  select(`is_canceled`) %>%\nsummarise(is_canceled_mean = mean(is_canceled))\n\n\n\n\n  \n\n\n\nCommon sense tells me that those who wait longer are more likely to cancel their reservations, but I want to check if this can actually be observed in our data. The code chunk above demonstrates that is_canceled is a binary variable (with values 0 and 1). The tables generated also show that on average, people spend about 2 days on the waiting list. However, some people were on the waiting list for over a year!\nThe mean cancellation rate is 1 for those who waited 391 days (understandable), and 0.36 for those who didn’t wait at all."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html#grouped-summary-statistics-1-1",
    "href": "posts/SaaradhaaM_Challenge2.html#grouped-summary-statistics-1-1",
    "title": "Challenge 2",
    "section": "Grouped summary statistics #1",
    "text": "Grouped summary statistics #1\nI also want to examine the relationship between assigned room type and previous cancellations.\n\n\nCode\n# Check median previous cancellations, grouped by assigned room type.\nhotel %>%\n  group_by(assigned_room_type) %>%\n  select(previous_cancellations) %>%\nsummarise(previous_cancellations_median = median(previous_cancellations))\n\n\n\n\n  \n\n\n\nMost people assigned types A to K and type P were not likely to have previously cancelled their bookings. However, most people assigned type L were like to have previously cancelled their bookings on one occasion - if we can find the data source, it would be interesting to uncover how type L differs from the other room types (were these people given smaller rooms?)."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge1.html",
    "href": "posts/SaaradhaaM_Challenge1.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge1.html#challenge-overview",
    "href": "posts/SaaradhaaM_Challenge1.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to read in a dataset and describe the dataset using both words and any supporting information."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge1.html#read-in-the-data",
    "href": "posts/SaaradhaaM_Challenge1.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nI will be working with the wild bird dataset.\n\n\nCode\n# Load readxl package.\nlibrary(readxl)\n#Read in and view the dataset.\nwildbird <- read_excel(\"_data/wild_bird_data.xlsx\")\nview(wildbird)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge1.html#describe-the-data",
    "href": "posts/SaaradhaaM_Challenge1.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, our task is to provide a high level description of the data.\n\n\nCode\n# Run dim() to get the number of cases.\ndim(wildbird)\n\n\n[1] 147   2\n\n\nCode\n# There are 147 cases and 2 columns in this dataset.\n# Run view() to see what these 2 columns are.\nview(wildbird)\n\n\nThere are 147 cases in 2 columns, which are Wet Body Weight (g) and Population Size (but these are in the rows and need to be renamed). Additionally, viewing the dataset shows that there are no missing cases.\nFrom one of the columns, I can see that the data was taken from Figure 1 of a paper written by Nee and colleagues (finding this paper will probably tell me which country this data is from). The column names also show that the data was probably collected via field research with wild birds.\n\n\nCode\n#Rename columns.\nlibrary(dplyr)\nwildbird_new <- rename(wildbird, \"wet_body_weight\" = \"Reference\", \"pop_size\" = \"Taken from Figure 1 of Nee et al.\")\n#Remove the first row of data.\nwildbird_new <- wildbird_new[-1,]\n#Check that the cleaning was done correctly.\nview(wildbird_new)\n#Check the number of cases again.\ndim(wildbird_new)\n\n\n[1] 146   2\n\n\nNow that the columns are renamed and the first row is removed, we see that the true number of cases is 146.\n\n\nCode\n# Let's check the descriptive statistics.\nsummary(wildbird_new)\n\n\n wet_body_weight      pop_size        \n Length:146         Length:146        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\nCode\n# The data is in characters, so we need to convert it to numbers.\nwildbird_new$wet_body_weight <- as.numeric(wildbird_new$wet_body_weight)\nwildbird_new$pop_size <- as.numeric(wildbird_new$pop_size)\n\n\n\n\nCode\n# Now let's check the descriptive statistics again.\nlibrary(dplyr)\nsummary(wildbird_new)\n\n\n wet_body_weight       pop_size      \n Min.   :   5.459   Min.   :      5  \n 1st Qu.:  18.620   1st Qu.:   1821  \n Median :  69.232   Median :  24353  \n Mean   : 363.694   Mean   : 382874  \n 3rd Qu.: 309.826   3rd Qu.: 198515  \n Max.   :9639.845   Max.   :5093378  \n\n\nThe mean wet body weight of the wild birds analysed was about 364g, and the mean population size was close to 383000. There was also a wide range of entries in both variables. Now, let’s check if they’re correlated.\n\n\nCode\n# Running correlation.\ncor(wildbird_new$wet_body_weight,wildbird_new$pop_size)\n\n\n[1] -0.1162993\n\n\nCode\nsummary(lm(wildbird_new$wet_body_weight~wildbird_new$pop_size))\n\n\n\nCall:\nlm(formula = wildbird_new$wet_body_weight ~ wildbird_new$pop_size)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-400.1 -369.5 -275.6   -0.4 9230.6 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            4.097e+02  8.748e+01   4.683 6.47e-06 ***\nwildbird_new$pop_size -1.202e-04  8.552e-05  -1.405    0.162    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 980.3 on 144 degrees of freedom\nMultiple R-squared:  0.01353,   Adjusted R-squared:  0.006675 \nF-statistic: 1.974 on 1 and 144 DF,  p-value: 0.1621\n\n\nThey are quite weakly correlated."
  },
  {
    "objectID": "posts/challenge4_instructions.html",
    "href": "posts/challenge4_instructions.html",
    "title": "Challenge 4 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_instructions.html#challenge-overview",
    "href": "posts/challenge4_instructions.html#challenge-overview",
    "title": "Challenge 4 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_instructions.html#read-in-data",
    "href": "posts/challenge4_instructions.html#read-in-data",
    "title": "Challenge 4 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ⭐\npoultry_tidy.csv⭐⭐\nFedFundsRate.csv⭐⭐⭐\nhotel_bookings.csv⭐⭐⭐⭐\ndebt_in_trillions ⭐⭐⭐⭐⭐\n\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "title": "Challenge 4 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Instructions",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "about/Rosemary.html",
    "href": "about/Rosemary.html",
    "title": "Rosemary",
    "section": "",
    "text": "PSU Political Science and Social Data Analytics"
  },
  {
    "objectID": "about/Rosemary.html#r-experience",
    "href": "about/Rosemary.html#r-experience",
    "title": "Rosemary",
    "section": "R experience",
    "text": "R experience\nUse R in research and teach R programming"
  },
  {
    "objectID": "about/Rosemary.html#research-interests",
    "href": "about/Rosemary.html#research-interests",
    "title": "Rosemary",
    "section": "Research interests",
    "text": "Research interests\nAuthoritarian politics, Survey, Text as data"
  },
  {
    "objectID": "about/Rosemary.html#hometown",
    "href": "about/Rosemary.html#hometown",
    "title": "Rosemary",
    "section": "Hometown",
    "text": "Hometown\nTianjin, China"
  },
  {
    "objectID": "about/Rosemary.html#hobbies",
    "href": "about/Rosemary.html#hobbies",
    "title": "Rosemary",
    "section": "Hobbies",
    "text": "Hobbies\nHorseback riding, journaling, cooking"
  },
  {
    "objectID": "about/Rosemary.html#fun-fact",
    "href": "about/Rosemary.html#fun-fact",
    "title": "Rosemary",
    "section": "Fun fact",
    "text": "Fun fact\nI’m a zoo parent of an African lion."
  },
  {
    "objectID": "about/WillMunson.html",
    "href": "about/WillMunson.html",
    "title": "Will Munson",
    "section": "",
    "text": "UMass Amherst - BS in Math (Concentration in statistics) Montgomery College Rockville - AS in Math Cogentiv Solutions Llc. - Database Management World Data Science Institute - Data Science Researcher"
  },
  {
    "objectID": "about/WillMunson.html#r-experience",
    "href": "about/WillMunson.html#r-experience",
    "title": "Will Munson",
    "section": "R experience",
    "text": "R experience\nR is pretty much my favorite language. I was introduced to R before I graduated high school, and started actively pursuing it by the time I started coming to UMass Amherst."
  },
  {
    "objectID": "about/WillMunson.html#research-interests",
    "href": "about/WillMunson.html#research-interests",
    "title": "Will Munson",
    "section": "Research interests",
    "text": "Research interests\nMy research interests are actually very broad, but lately, my interests are leaning more towards environmental and agricultural research. I also have interests in what affects a city’s median household income, or the wellbeing of people who live in those cities."
  },
  {
    "objectID": "about/WillMunson.html#hometown",
    "href": "about/WillMunson.html#hometown",
    "title": "Will Munson",
    "section": "Hometown",
    "text": "Hometown\nHaydenville, MA"
  },
  {
    "objectID": "about/WillMunson.html#hobbies",
    "href": "about/WillMunson.html#hobbies",
    "title": "Will Munson",
    "section": "Hobbies",
    "text": "Hobbies\n\nPainting\nDrawing\nMagnet fishing\nVideo games"
  },
  {
    "objectID": "about/WillMunson.html#fun-fact",
    "href": "about/WillMunson.html#fun-fact",
    "title": "Will Munson",
    "section": "Fun fact",
    "text": "Fun fact\nI used to be Mormon, but I left before graduating high school."
  },
  {
    "objectID": "about/JerinJacob.html",
    "href": "about/JerinJacob.html",
    "title": "Jerin Jacob",
    "section": "",
    "text": "Work: Business Consultant, Freelance (India) - March 2020 to July 2022 Managing Director, Cresoul Trading & Logistics Pvt Ltd (India) - November 2014 to December 2019 Marketing Specialist, Eastern Hazar (Saudi Arabia) - October 2012 to October 2014 Marketing Manager, Sookshamtech Integral Techno Solutions Pvt Ltd (India) - May 2011 to April 2012 Assistant Store Manager, PJ Electronics & Home Appliance (India) - June 2008 to May 2009\nEducation: MBA with Marketing & Finance Specialization, University of Kerala - 2009 to 2011 Bachelor’s degree in Physics, Mahatma Gandhi University - 2005 to 2008"
  },
  {
    "objectID": "about/JerinJacob.html#r-experience",
    "href": "about/JerinJacob.html#r-experience",
    "title": "Jerin Jacob",
    "section": "R experience",
    "text": "R experience\nBeginner"
  },
  {
    "objectID": "about/JerinJacob.html#research-interests",
    "href": "about/JerinJacob.html#research-interests",
    "title": "Jerin Jacob",
    "section": "Research interests",
    "text": "Research interests\nUse of economic, political, geographical and meteorological data to predict the demand and supply of products and services, Effectiveness of marketing and PR in business and politics, based on analyzing patterns of social media usage and other media exposure of a population, Effect of different media and public responses of leaders/ famous personalities in the decision making of common people, Business decision-making based on data"
  },
  {
    "objectID": "about/JerinJacob.html#hometown",
    "href": "about/JerinJacob.html#hometown",
    "title": "Jerin Jacob",
    "section": "Hometown",
    "text": "Hometown\nKottayam, Kerala"
  },
  {
    "objectID": "about/JerinJacob.html#hobbies",
    "href": "about/JerinJacob.html#hobbies",
    "title": "Jerin Jacob",
    "section": "Hobbies",
    "text": "Hobbies\nmusic, traveling, farming"
  },
  {
    "objectID": "about/JerinJacob.html#fun-fact",
    "href": "about/JerinJacob.html#fun-fact",
    "title": "Jerin Jacob",
    "section": "Fun fact",
    "text": "Fun fact\nWhen I am in stress, I love to sleep ;)"
  },
  {
    "objectID": "about/MaddiHertz.html",
    "href": "about/MaddiHertz.html",
    "title": "Maddi Hertz",
    "section": "",
    "text": "I am a lecturer, academic advisor, and general do-it-all person for the DACSS program. I was a part of the first cohort of DACSS students who started Fall 2020. I graduated this May and am thrilled to be able to continue working with everyone on the DACSS team.\nM.S. DACSS - May 2022\nB.A. in Economics & American Studies from UMass Lowell"
  },
  {
    "objectID": "about/MaddiHertz.html#r-experience",
    "href": "about/MaddiHertz.html#r-experience",
    "title": "Maddi Hertz",
    "section": "R experience",
    "text": "R experience\nI learned a lot during my two years in the program, but there is so much more to learn. The RStudio Conference has really thrown me for a loop; things change faster than I can keep up.\nThe next big goal is to get comfortable using tidymodels and associated packages."
  },
  {
    "objectID": "about/MaddiHertz.html#research-interests",
    "href": "about/MaddiHertz.html#research-interests",
    "title": "Maddi Hertz",
    "section": "Research interests",
    "text": "Research interests\nToo indecisive to identify my own research interests, but I’d love to give you my two cents about yours."
  },
  {
    "objectID": "about/MaddiHertz.html#hometown",
    "href": "about/MaddiHertz.html#hometown",
    "title": "Maddi Hertz",
    "section": "Hometown",
    "text": "Hometown\nAndover, MA"
  },
  {
    "objectID": "about/MaddiHertz.html#hobbies",
    "href": "about/MaddiHertz.html#hobbies",
    "title": "Maddi Hertz",
    "section": "Hobbies",
    "text": "Hobbies\nbeing stressed"
  },
  {
    "objectID": "about/MaddiHertz.html#fun-fact",
    "href": "about/MaddiHertz.html#fun-fact",
    "title": "Maddi Hertz",
    "section": "Fun fact",
    "text": "Fun fact\nThe TV show The Sopranos was named after my family (supposedly—at least according to some distant cousin)."
  },
  {
    "objectID": "about/KaushikaPotluri.html",
    "href": "about/KaushikaPotluri.html",
    "title": "KaushikaPotluri",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/KaushikaPotluri.html#educationwork-background",
    "href": "about/KaushikaPotluri.html#educationwork-background",
    "title": "KaushikaPotluri",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nI completed my undergraduate in Computer Science and Engineering at Bennett University, Delhi, India. I have work experience of 2 years now"
  },
  {
    "objectID": "about/KaushikaPotluri.html#r-experience",
    "href": "about/KaushikaPotluri.html#r-experience",
    "title": "KaushikaPotluri",
    "section": "R experience",
    "text": "R experience\nI am kind of new to R but I am trying to get there"
  },
  {
    "objectID": "about/KaushikaPotluri.html#research-interests",
    "href": "about/KaushikaPotluri.html#research-interests",
    "title": "KaushikaPotluri",
    "section": "Research interests",
    "text": "Research interests\nMy research interests are statistical analysis on data"
  },
  {
    "objectID": "about/KaushikaPotluri.html#hometown",
    "href": "about/KaushikaPotluri.html#hometown",
    "title": "KaushikaPotluri",
    "section": "Hometown",
    "text": "Hometown\nMy hometown is Hyderabad, India."
  },
  {
    "objectID": "about/KaushikaPotluri.html#hobbies",
    "href": "about/KaushikaPotluri.html#hobbies",
    "title": "KaushikaPotluri",
    "section": "Hobbies",
    "text": "Hobbies\nI love playing tennis and watching basically all sports (mostly basketball). I am into photography."
  },
  {
    "objectID": "about/KaushikaPotluri.html#fun-fact",
    "href": "about/KaushikaPotluri.html#fun-fact",
    "title": "KaushikaPotluri",
    "section": "Fun fact",
    "text": "Fun fact\nI am really short for someone who loves basketball!"
  },
  {
    "objectID": "about/TylerTewksbury.html",
    "href": "about/TylerTewksbury.html",
    "title": "Tyler Tewksbury",
    "section": "",
    "text": "2022 graduate with a B.A. in Economics from UMass Amherst. Worked as a Business Systems Analyst Intern at Epsilon in the Summer of 2022, and have been a DACSS Student Employee for over a year (Social Media currently, previously Instructional Design)."
  },
  {
    "objectID": "about/TylerTewksbury.html#r-experience",
    "href": "about/TylerTewksbury.html#r-experience",
    "title": "Tyler Tewksbury",
    "section": "R experience",
    "text": "R experience\nI have experience in R from classwork in Econometrics and Quantitative Research Methods, as well as working on 601 as an Instructional Designer."
  },
  {
    "objectID": "about/TylerTewksbury.html#research-interests",
    "href": "about/TylerTewksbury.html#research-interests",
    "title": "Tyler Tewksbury",
    "section": "Research interests",
    "text": "Research interests\nData Visualization, Autonomous Transportation"
  },
  {
    "objectID": "about/TylerTewksbury.html#hometown",
    "href": "about/TylerTewksbury.html#hometown",
    "title": "Tyler Tewksbury",
    "section": "Hometown",
    "text": "Hometown\nOriginally from Milton Massachusetts, but moved around a lot. Lived in New York, Maine, and now Amherst!"
  },
  {
    "objectID": "about/TylerTewksbury.html#hobbies",
    "href": "about/TylerTewksbury.html#hobbies",
    "title": "Tyler Tewksbury",
    "section": "Hobbies",
    "text": "Hobbies\nWeightlifting, Reading, Video Games, Crossword Puzzles, Geography."
  },
  {
    "objectID": "about/TylerTewksbury.html#fun-fact",
    "href": "about/TylerTewksbury.html#fun-fact",
    "title": "Tyler Tewksbury",
    "section": "Fun fact",
    "text": "Fun fact\nI met the creator of Pokémon in 2015! He sat at the table next to me in a restaurant in Boston."
  },
  {
    "objectID": "about/MirandaManka.html",
    "href": "about/MirandaManka.html",
    "title": "Miranda Manka",
    "section": "",
    "text": "I graduated with a degree in statistics and a minor in sociology from Virginia Tech in May 2022. I have had multiple summer internships including working for the Washington Metropolitan Area Transit Authority (Metro in DC) and for the Alliance for Academic Internal Medicine."
  },
  {
    "objectID": "about/MirandaManka.html#r-experience",
    "href": "about/MirandaManka.html#r-experience",
    "title": "Miranda Manka",
    "section": "R experience",
    "text": "R experience\nI have worked with R for around 4 years. Many of my statistics classes used R throughout my undergraduate degree."
  },
  {
    "objectID": "about/MirandaManka.html#research-interests",
    "href": "about/MirandaManka.html#research-interests",
    "title": "Miranda Manka",
    "section": "Research interests",
    "text": "Research interests\nData Analytics & Visualization, Sociology."
  },
  {
    "objectID": "about/MirandaManka.html#hometown",
    "href": "about/MirandaManka.html#hometown",
    "title": "Miranda Manka",
    "section": "Hometown",
    "text": "Hometown\nAlexandria, VA."
  },
  {
    "objectID": "about/MirandaManka.html#hobbies",
    "href": "about/MirandaManka.html#hobbies",
    "title": "Miranda Manka",
    "section": "Hobbies",
    "text": "Hobbies\nI like bowling, bike riding, traveling, and playing video games."
  },
  {
    "objectID": "about/MirandaManka.html#fun-fact",
    "href": "about/MirandaManka.html#fun-fact",
    "title": "Miranda Manka",
    "section": "Fun fact",
    "text": "Fun fact\nI have been skydiving twice."
  },
  {
    "objectID": "about/RoyYoon.html",
    "href": "about/RoyYoon.html",
    "title": "Roy Yoon",
    "section": "",
    "text": "UMass Amherst BA Linguistics and Political Science Minor in Spanish MA National Guard"
  },
  {
    "objectID": "about/RoyYoon.html#r-experience",
    "href": "about/RoyYoon.html#r-experience",
    "title": "Roy Yoon",
    "section": "R experience",
    "text": "R experience\nBeginner 2.0"
  },
  {
    "objectID": "about/RoyYoon.html#research-interests",
    "href": "about/RoyYoon.html#research-interests",
    "title": "Roy Yoon",
    "section": "Research interests",
    "text": "Research interests\nharm reduction, international human rights"
  },
  {
    "objectID": "about/RoyYoon.html#hometown",
    "href": "about/RoyYoon.html#hometown",
    "title": "Roy Yoon",
    "section": "Hometown",
    "text": "Hometown\nWorcester, MA"
  },
  {
    "objectID": "about/RoyYoon.html#hobbies",
    "href": "about/RoyYoon.html#hobbies",
    "title": "Roy Yoon",
    "section": "Hobbies",
    "text": "Hobbies\nMusic, running, hair"
  },
  {
    "objectID": "about/RoyYoon.html#fun-fact",
    "href": "about/RoyYoon.html#fun-fact",
    "title": "Roy Yoon",
    "section": "Fun fact",
    "text": "Fun fact\nI get the no.25 from Miss Saigon with extra noodles, sate broth, and the occasional extra meat!"
  },
  {
    "objectID": "about/AnimeshSengupta.html",
    "href": "about/AnimeshSengupta.html",
    "title": "AnimeshSengupta",
    "section": "",
    "text": "High school : DLF Public School : 92.6%\nB.Tech CSE: Vellore institute of technology: 9.14 CGPA\nIntern->Sr. Software developer : Societe Generale GSC"
  },
  {
    "objectID": "about/AnimeshSengupta.html#r-experience",
    "href": "about/AnimeshSengupta.html#r-experience",
    "title": "AnimeshSengupta",
    "section": "R experience",
    "text": "R experience\nMynR journey began during my undergraduate days. I had learnt intermediary R syntax during that time. Since, I have had no industry experience. I can be considered beginner."
  },
  {
    "objectID": "about/AnimeshSengupta.html#research-interests",
    "href": "about/AnimeshSengupta.html#research-interests",
    "title": "AnimeshSengupta",
    "section": "Research interests",
    "text": "Research interests\nMy research interests include: 1. X-shot deep learning techniques to reduce learning in dearth of dataset 2. Meta agnostic learning and prototype learning and MAML to perform meta data learning 3. Generative adversarial networks to create new data."
  },
  {
    "objectID": "about/AnimeshSengupta.html#hometown",
    "href": "about/AnimeshSengupta.html#hometown",
    "title": "AnimeshSengupta",
    "section": "Hometown",
    "text": "Hometown\nI hail from New Delhi, India"
  },
  {
    "objectID": "about/AnimeshSengupta.html#hobbies",
    "href": "about/AnimeshSengupta.html#hobbies",
    "title": "AnimeshSengupta",
    "section": "Hobbies",
    "text": "Hobbies\nMy hobbies are: 1. Soccer fan - Chelsea for life 2. Numismatics 3. Bibliophile"
  },
  {
    "objectID": "about/AnimeshSengupta.html#fun-fact",
    "href": "about/AnimeshSengupta.html#fun-fact",
    "title": "AnimeshSengupta",
    "section": "Fun fact",
    "text": "Fun fact\nI am great with bonfires"
  },
  {
    "objectID": "about/ProfRolfe.html",
    "href": "about/ProfRolfe.html",
    "title": "Meredith Rolfe",
    "section": "",
    "text": "Associate Professor, Political Science | UMass Amherst Lecturer in Public Management | London School of Economics Nuffield Posdoctoral Prize Fellow | Oxford University\nPhD, Political Science | University of Chicago BA, Comparative Area Studies | Duke University"
  },
  {
    "objectID": "about/ProfRolfe.html#r-experience",
    "href": "about/ProfRolfe.html#r-experience",
    "title": "Meredith Rolfe",
    "section": "R experience",
    "text": "R experience\nI was using R before it was cool (and when it was still called S…)"
  },
  {
    "objectID": "about/ProfRolfe.html#research-interests",
    "href": "about/ProfRolfe.html#research-interests",
    "title": "Meredith Rolfe",
    "section": "Research interests",
    "text": "Research interests\nTheories that combine social interaction and cognitive micro-foundations; innovative applications of methods of any sort (networks, text, simulations, experiments, surveys…)"
  },
  {
    "objectID": "about/ProfRolfe.html#hometown",
    "href": "about/ProfRolfe.html#hometown",
    "title": "Meredith Rolfe",
    "section": "Hometown",
    "text": "Hometown\nCharlotte, NC"
  },
  {
    "objectID": "about/ProfRolfe.html#hobbies",
    "href": "about/ProfRolfe.html#hobbies",
    "title": "Meredith Rolfe",
    "section": "Hobbies",
    "text": "Hobbies\num, DACSS???"
  },
  {
    "objectID": "about/ProfRolfe.html#fun-fact",
    "href": "about/ProfRolfe.html#fun-fact",
    "title": "Meredith Rolfe",
    "section": "Fun fact",
    "text": "Fun fact\ncoffee is my favorite food"
  },
  {
    "objectID": "about/LindsayJones.html",
    "href": "about/LindsayJones.html",
    "title": "Lindsay Jones",
    "section": "",
    "text": "B.A. - Sociology, UC San Diego\nMinor in Communication\nExecutive Assistant of a retirement community from 2019-2022."
  },
  {
    "objectID": "about/LindsayJones.html#r-experience",
    "href": "about/LindsayJones.html#r-experience",
    "title": "Lindsay Jones",
    "section": "R experience",
    "text": "R experience\nYou’re looking at it!"
  },
  {
    "objectID": "about/LindsayJones.html#research-interests",
    "href": "about/LindsayJones.html#research-interests",
    "title": "Lindsay Jones",
    "section": "Research interests",
    "text": "Research interests\nSocial Network Analysis, Digital Behaviors, Social Demography"
  },
  {
    "objectID": "about/LindsayJones.html#hometown",
    "href": "about/LindsayJones.html#hometown",
    "title": "Lindsay Jones",
    "section": "Hometown",
    "text": "Hometown\nSacramento, California"
  },
  {
    "objectID": "about/LindsayJones.html#hobbies",
    "href": "about/LindsayJones.html#hobbies",
    "title": "Lindsay Jones",
    "section": "Hobbies",
    "text": "Hobbies\nLately I’ve enjoyed playing trumpet, playing Redactle, and making ice cream."
  },
  {
    "objectID": "about/LindsayJones.html#fun-fact",
    "href": "about/LindsayJones.html#fun-fact",
    "title": "Lindsay Jones",
    "section": "Fun fact",
    "text": "Fun fact\nI once made the mistake of climbing a waterfall in running shoes."
  },
  {
    "objectID": "about/EmmaRasmussen.html",
    "href": "about/EmmaRasmussen.html",
    "title": "Emma Rasmussen",
    "section": "",
    "text": "I graduated in 2021 with a B.S. in Public Health and Anthropology/Sociology and a minor in Biology from Roger Williams University. Since graduating, I completed a research internship with the Framingham Heart Study for their Brain Aging Program. In between, I have spent a lot of time working in restaurants."
  },
  {
    "objectID": "about/EmmaRasmussen.html#r-experience",
    "href": "about/EmmaRasmussen.html#r-experience",
    "title": "Emma Rasmussen",
    "section": "R experience",
    "text": "R experience\nI have no background in R"
  },
  {
    "objectID": "about/EmmaRasmussen.html#research-interests",
    "href": "about/EmmaRasmussen.html#research-interests",
    "title": "Emma Rasmussen",
    "section": "Research interests",
    "text": "Research interests\nPopulation Health, Demography, Social Determinants of Health, Environmental Health, Disaster Research, Social Movements"
  },
  {
    "objectID": "about/EmmaRasmussen.html#hometown",
    "href": "about/EmmaRasmussen.html#hometown",
    "title": "Emma Rasmussen",
    "section": "Hometown",
    "text": "Hometown\nGroton, MA"
  },
  {
    "objectID": "about/EmmaRasmussen.html#hobbies",
    "href": "about/EmmaRasmussen.html#hobbies",
    "title": "Emma Rasmussen",
    "section": "Hobbies",
    "text": "Hobbies\n\nBaking and cooking: I just got a sourdough starter which I have been experimenting with\nListening to podcasts: My favorite is Iliza Schlesinger’s podcast “Ask Iliza Anything”\nTraveling and exploring new places: I have been to Europe a few times, but since Covid I have been exploring more locally. And I’m a foodie, so I love to eat my way around new places."
  },
  {
    "objectID": "about/EmmaRasmussen.html#fun-fact",
    "href": "about/EmmaRasmussen.html#fun-fact",
    "title": "Emma Rasmussen",
    "section": "Fun fact",
    "text": "Fun fact\nThis past February, I donated stem cells through Be The Match"
  },
  {
    "objectID": "about/nickboonstra_v2.html",
    "href": "about/nickboonstra_v2.html",
    "title": "Nick Boonstra",
    "section": "",
    "text": "I started college at Boston University studying physics. Financial issues kept me from continuing, so I spent a few years in Boston working part-time and taking classes at Bunker Hill Community College. I eventually transferred to UMass Lowell, where I finished with a Bachelor’s in Liberal Arts with concentrations in Political Science and Writing. Professionally, I have experience in a few areas, including banking and journalism."
  },
  {
    "objectID": "about/nickboonstra_v2.html#r-experience",
    "href": "about/nickboonstra_v2.html#r-experience",
    "title": "Nick Boonstra",
    "section": "R experience",
    "text": "R experience\nNone before this summer! In fact, I have very little programming experience in general, beyond a rusty grasp of LaTeX and a bit of dabbling in Python years ago. However, I have been fortunate to have started working for DACSS remotely a couple before arriving in Western Mass, which provided me the opportunity to get a jump start on R."
  },
  {
    "objectID": "about/nickboonstra_v2.html#research-and-career-interests",
    "href": "about/nickboonstra_v2.html#research-and-career-interests",
    "title": "Nick Boonstra",
    "section": "Research and career interests",
    "text": "Research and career interests\nI have a wide range of career interests, from political science professor to broadcast journalist to investment banker. When it comes to political science, my research interest tend to revolve around partisanship and polarization, and particularly around understanding how to bridge the perception gap between opposite partisans in the United States and abroad."
  },
  {
    "objectID": "about/nickboonstra_v2.html#hometown",
    "href": "about/nickboonstra_v2.html#hometown",
    "title": "Nick Boonstra",
    "section": "Hometown",
    "text": "Hometown\nI grew up in Leominster, MA. After living in Boston for a few years, and in Lowell for a few more, I have just moved to Chicopee."
  },
  {
    "objectID": "about/nickboonstra_v2.html#hobbies",
    "href": "about/nickboonstra_v2.html#hobbies",
    "title": "Nick Boonstra",
    "section": "Hobbies",
    "text": "Hobbies\nI am a massive fan of West Ham United Football Club, and follow the English Premier League far more closely than any American league, though I will always be a devoted fan of the Celtics and Red Sox. I also have passing interests in rugby and cricket, and have recently been getting into Formula 1.\nBeyond sports, I’m a big indie music junkie; some of my favorite artists right now include Beirut, Fleet Foxes, and Day Wave, though I’m equally at home with Foo Fighters, Coldplay, and Indigo Girls. When I’m looking to kill time, my go-to is usually my lichess app, or maybe a crossword."
  },
  {
    "objectID": "about/nickboonstra_v2.html#fun-fact",
    "href": "about/nickboonstra_v2.html#fun-fact",
    "title": "Nick Boonstra",
    "section": "Fun fact",
    "text": "Fun fact\nI once took a one-night trip from Boston to London to see a football match! January 29, 2020, WHU 0 - 2 LIV – not a surprising scoreline but still a disappointing one. The Reds dominated the game; the closest we came to scoring was an errant cross from Trent Alexander-Arnold clattering off of his own post. In other words: Yes, we were so bad that not even Liverpool could have scored for us. But it was an incredible experience nonetheless, and I’m so glad I got to go when I did."
  },
  {
    "objectID": "about/NJani.html",
    "href": "about/NJani.html",
    "title": "Nayan Jani",
    "section": "",
    "text": "I received my B.S. in Data Science from the University of Rhode Island this past year. Over the summer of 2021 I worked for a startup company called Bora. At Bora, the product they are trying to bring to market is an app based, self service beach chair rental company stationed at the most popular locations across the country. My job was to collect data on certain locations and then run statistical analysis on that collected data to find out which locations are the busiest based on the characteristics of beaches. While working for Bora I was in constant contact with potential clients and the CEO of the company. Working for a startup showed me how a business functions from the ground up and how to interact with clients in a professional manner."
  },
  {
    "objectID": "about/NJani.html#r-experience",
    "href": "about/NJani.html#r-experience",
    "title": "Nayan Jani",
    "section": "R experience",
    "text": "R experience\nI have been working in R and R markdown since my sophomore year of college. I have mostly used it for statistical reports and model building for data sets. I only was introduced to the tidyverse for a short period of time so I will need to practice using its functions more."
  },
  {
    "objectID": "about/NJani.html#research-interests",
    "href": "about/NJani.html#research-interests",
    "title": "Nayan Jani",
    "section": "Research interests",
    "text": "Research interests\nThe topic I explored for my Senior recitation is the use of Machine Learning in estimating the heterogeneous treatment effect using datasets that involve AIDS and breast cancer treatments. More specifically, I looked into certain meta learners that can estimate the conditional average treatment effect so that we can personalize treatment regimes. Here at Umass I hope to learn more about how to apply data science to medical treatments for people with illnesses."
  },
  {
    "objectID": "about/NJani.html#hometown",
    "href": "about/NJani.html#hometown",
    "title": "Nayan Jani",
    "section": "Hometown",
    "text": "Hometown\nWestford, MA"
  },
  {
    "objectID": "about/NJani.html#hobbies",
    "href": "about/NJani.html#hobbies",
    "title": "Nayan Jani",
    "section": "Hobbies",
    "text": "Hobbies\n\nVideo Games\nFantasy Football\nWatching Sports\nPlaying Cards and Board Games"
  },
  {
    "objectID": "about/NJani.html#fun-fact",
    "href": "about/NJani.html#fun-fact",
    "title": "Nayan Jani",
    "section": "Fun fact",
    "text": "Fun fact\nI have over 50 cousins that live in the UK!"
  },
  {
    "objectID": "about/ShoshanaBuck.html",
    "href": "about/ShoshanaBuck.html",
    "title": "Shoshana Buck",
    "section": "",
    "text": "I spent my freshman year of undergrad at Simmons University in Boston and then in 2019 I transferred to UMass Amherst for my sophomore year. I graduated this past May with a B.A in Communication and am continuing my education at UMass with the DACSS program."
  },
  {
    "objectID": "about/ShoshanaBuck.html#r-experience",
    "href": "about/ShoshanaBuck.html#r-experience",
    "title": "Shoshana Buck",
    "section": "R experience",
    "text": "R experience\nI have very limited amount of experience with R. I took a sociology course about data collection and analysis, which introduced me to research design and how data is obtained and analyzed. I used R for my final research which made me interested to learn and use R more."
  },
  {
    "objectID": "about/ShoshanaBuck.html#research-interests",
    "href": "about/ShoshanaBuck.html#research-interests",
    "title": "Shoshana Buck",
    "section": "Research interests",
    "text": "Research interests\nSome of my research interests include digital inequalities, media effects especially in children, and disinformation."
  },
  {
    "objectID": "about/ShoshanaBuck.html#hometown",
    "href": "about/ShoshanaBuck.html#hometown",
    "title": "Shoshana Buck",
    "section": "Hometown",
    "text": "Hometown\nI am originally from Framingham Massachusetts."
  },
  {
    "objectID": "about/ShoshanaBuck.html#hobbies",
    "href": "about/ShoshanaBuck.html#hobbies",
    "title": "Shoshana Buck",
    "section": "Hobbies",
    "text": "Hobbies\nI like to go to the beach, hang out with my family and friends, take my dog on walks, and listen to live music."
  },
  {
    "objectID": "about/ShoshanaBuck.html#fun-fact",
    "href": "about/ShoshanaBuck.html#fun-fact",
    "title": "Shoshana Buck",
    "section": "Fun fact",
    "text": "Fun fact\nA fun fat about me is that my first ever concert that I went to was the Cheetah Girls and Vanessa Hudgens."
  },
  {
    "objectID": "about/ManiShankerKamarapu.html",
    "href": "about/ManiShankerKamarapu.html",
    "title": "ManiShankerKamarapu",
    "section": "",
    "text": "Work: Senior Systems Engineer,Infosys Ltd - December 2019 to July 2022\nEducation: B.E with Electronics and Communication Engineering Specialization, Osmania University - 2015 to 2019"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#r-experience",
    "href": "about/ManiShankerKamarapu.html#r-experience",
    "title": "ManiShankerKamarapu",
    "section": "R experience",
    "text": "R experience\nBeginner"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#research-interests",
    "href": "about/ManiShankerKamarapu.html#research-interests",
    "title": "ManiShankerKamarapu",
    "section": "Research interests",
    "text": "Research interests\nData analytics, data communication & visualization, machine learning, and networks"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#hometown",
    "href": "about/ManiShankerKamarapu.html#hometown",
    "title": "ManiShankerKamarapu",
    "section": "Hometown",
    "text": "Hometown\nKarimnagar, Telangana, India - 505001"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#hobbies",
    "href": "about/ManiShankerKamarapu.html#hobbies",
    "title": "ManiShankerKamarapu",
    "section": "Hobbies",
    "text": "Hobbies\nMusic, playing games and long walks"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#fun-fact",
    "href": "about/ManiShankerKamarapu.html#fun-fact",
    "title": "ManiShankerKamarapu",
    "section": "Fun fact",
    "text": "Fun fact\nI love to listen music."
  },
  {
    "objectID": "about/AdithyaParupudi.html",
    "href": "about/AdithyaParupudi.html",
    "title": "Adithya Parupudi",
    "section": "",
    "text": "save a copy of this document as FirstLast.qmd in the about folder\nreplace Your Name in yaml header with your first and last name\nsave an image (jpeg, png) to use in the about/images folder\nreplace image names in yaml header\nreplace github user name\nfill in information below, as appropriate\ncommit to github and submit pull request"
  },
  {
    "objectID": "about/AdithyaParupudi.html#educationwork-background",
    "href": "about/AdithyaParupudi.html#educationwork-background",
    "title": "Adithya Parupudi",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nI’ve graduated in 2019 with a B.Tech in Computer Science & Engineering (CSE) from JNTU, Hyderabad\nI’ve worked in Tech Mahindra for 2.8 yrs as a Oracle SOA 12c developer and production support engineer."
  },
  {
    "objectID": "about/AdithyaParupudi.html#r-experience",
    "href": "about/AdithyaParupudi.html#r-experience",
    "title": "Adithya Parupudi",
    "section": "R experience",
    "text": "R experience\nI have foundational knowledge in R, gathered from youtube and datacamp."
  },
  {
    "objectID": "about/AdithyaParupudi.html#research-interests",
    "href": "about/AdithyaParupudi.html#research-interests",
    "title": "Adithya Parupudi",
    "section": "Research interests",
    "text": "Research interests\nInterested about Psychology and Social Behaviour"
  },
  {
    "objectID": "about/AdithyaParupudi.html#hometown",
    "href": "about/AdithyaParupudi.html#hometown",
    "title": "Adithya Parupudi",
    "section": "Hometown",
    "text": "Hometown\nI’m from Hyderabad, India"
  },
  {
    "objectID": "about/AdithyaParupudi.html#hobbies",
    "href": "about/AdithyaParupudi.html#hobbies",
    "title": "Adithya Parupudi",
    "section": "Hobbies",
    "text": "Hobbies\nCooking, Anime, Workout, Photography"
  },
  {
    "objectID": "about/AdithyaParupudi.html#fun-fact",
    "href": "about/AdithyaParupudi.html#fun-fact",
    "title": "Adithya Parupudi",
    "section": "Fun fact",
    "text": "Fun fact\nI’m scared of heights and I avoid horror movies. :)"
  },
  {
    "objectID": "about/YoungsooChoi.html",
    "href": "about/YoungsooChoi.html",
    "title": "Young Soo Choi",
    "section": "",
    "text": "I had majored in Sociology and then worked for Korean Government over 10 years."
  },
  {
    "objectID": "about/YoungsooChoi.html#r-experience",
    "href": "about/YoungsooChoi.html#r-experience",
    "title": "Young Soo Choi",
    "section": "R experience",
    "text": "R experience\nNone. I have no idea about R at all."
  },
  {
    "objectID": "about/YoungsooChoi.html#research-interests",
    "href": "about/YoungsooChoi.html#research-interests",
    "title": "Young Soo Choi",
    "section": "Research interests",
    "text": "Research interests\nPublic Safety and disaster management."
  },
  {
    "objectID": "about/YoungsooChoi.html#hometown",
    "href": "about/YoungsooChoi.html#hometown",
    "title": "Young Soo Choi",
    "section": "Hometown",
    "text": "Hometown\nvarious cities in the Korea"
  },
  {
    "objectID": "about/YoungsooChoi.html#hobbies",
    "href": "about/YoungsooChoi.html#hobbies",
    "title": "Young Soo Choi",
    "section": "Hobbies",
    "text": "Hobbies\nMusic"
  },
  {
    "objectID": "about/YoungsooChoi.html#fun-fact",
    "href": "about/YoungsooChoi.html#fun-fact",
    "title": "Young Soo Choi",
    "section": "Fun fact",
    "text": "Fun fact\nI have two sons."
  },
  {
    "objectID": "about/QuinnHe.html",
    "href": "about/QuinnHe.html",
    "title": "Quinn He",
    "section": "",
    "text": "I recently graduated from the University of Massachusetts Amherst in 2021 with a BA in English and a minor in Psychology. Afterwards, I worked for one year in retail while taking courses for the DACSS certificate. I wanted to further my education and applied to the DACSS MS program, which I am starting this Fall 2022."
  },
  {
    "objectID": "about/QuinnHe.html#r-experience",
    "href": "about/QuinnHe.html#r-experience",
    "title": "Quinn He",
    "section": "R experience",
    "text": "R experience\nI have beginner/intermediate experience in R. For the year year I was working, I would study R in my spare time to keep my skills sharp. I still have much to learn when it comes to R/RStudio"
  },
  {
    "objectID": "about/QuinnHe.html#research-interests",
    "href": "about/QuinnHe.html#research-interests",
    "title": "Quinn He",
    "section": "Research interests",
    "text": "Research interests\nI have not fully honed in my research interests, but I hope to come out of the program with extensive knowledge in the technical skills of data analysis and research."
  },
  {
    "objectID": "about/QuinnHe.html#hometown",
    "href": "about/QuinnHe.html#hometown",
    "title": "Quinn He",
    "section": "Hometown",
    "text": "Hometown\nWayne, NJ"
  },
  {
    "objectID": "about/QuinnHe.html#hobbies",
    "href": "about/QuinnHe.html#hobbies",
    "title": "Quinn He",
    "section": "Hobbies",
    "text": "Hobbies\nI enjoy playing guitar, biking, reading, and cooking."
  },
  {
    "objectID": "about/QuinnHe.html#fun-fact",
    "href": "about/QuinnHe.html#fun-fact",
    "title": "Quinn He",
    "section": "Fun fact",
    "text": "Fun fact\nI met Jesse Eisenberg at an empty movie theater in NYC."
  },
  {
    "objectID": "about/MekhalaKumar.html",
    "href": "about/MekhalaKumar.html",
    "title": "Mekhala Kumar",
    "section": "",
    "text": "I studied at FLAME University where I did an integrated program, earning a Bachelor’s degree and Postgraduate Diploma. My major was Applied Mathematics and minor was Economics. Through my courses, I also had programming experience in Java, Python, R and STATA and visualisation experience in Google Data Studio and Tableau.\nMoreover, I interned at the WageIndicator Foundation for two years, as an intern in Data Visualisation and Analysis team for a year and then as a co-manager of the same team in the following year."
  },
  {
    "objectID": "about/MekhalaKumar.html#r-experience",
    "href": "about/MekhalaKumar.html#r-experience",
    "title": "Mekhala Kumar",
    "section": "R experience",
    "text": "R experience\nI attended a two week R Workshop at FLAME University where we learnt about operations in R and how to handle datasets. Additionally, I utilised R for statistical analysis in a few courses at FLAME University."
  },
  {
    "objectID": "about/MekhalaKumar.html#research-interests",
    "href": "about/MekhalaKumar.html#research-interests",
    "title": "Mekhala Kumar",
    "section": "Research interests",
    "text": "Research interests\nMigration, Gender Economics, Social Justice"
  },
  {
    "objectID": "about/MekhalaKumar.html#hometown",
    "href": "about/MekhalaKumar.html#hometown",
    "title": "Mekhala Kumar",
    "section": "Hometown",
    "text": "Hometown\nBengaluru"
  },
  {
    "objectID": "about/MekhalaKumar.html#hobbies",
    "href": "about/MekhalaKumar.html#hobbies",
    "title": "Mekhala Kumar",
    "section": "Hobbies",
    "text": "Hobbies\nI enjoy practising Karate, singing, swimming and reading books."
  },
  {
    "objectID": "about/MekhalaKumar.html#fun-fact",
    "href": "about/MekhalaKumar.html#fun-fact",
    "title": "Mekhala Kumar",
    "section": "Fun fact",
    "text": "Fun fact\nI have made a few parodies, they’re on YouTube."
  },
  {
    "objectID": "about/SaaradhaaM.html",
    "href": "about/SaaradhaaM.html",
    "title": "Saaradhaa M",
    "section": "",
    "text": "I received an Honours degree in Psychology from the National University of Singapore, then worked in government and industry. I’m currently a graduate student in the DACSS program."
  },
  {
    "objectID": "about/SaaradhaaM.html#r-experience",
    "href": "about/SaaradhaaM.html#r-experience",
    "title": "Saaradhaa M",
    "section": "R Experience 💡",
    "text": "R Experience 💡\nI took Microsoft’s Introduction to R course in 2020. However, the bulk of my R experience will be coming from DACSS 601. I’m excited to learn!"
  },
  {
    "objectID": "about/SaaradhaaM.html#research-interests",
    "href": "about/SaaradhaaM.html#research-interests",
    "title": "Saaradhaa M",
    "section": "Research Interests 🔍",
    "text": "Research Interests 🔍\nGroup dynamics, implicit social cognition, inclusivity and networks in STEM"
  },
  {
    "objectID": "about/SaaradhaaM.html#hometown",
    "href": "about/SaaradhaaM.html#hometown",
    "title": "Saaradhaa M",
    "section": "Hometown 🇸🇬",
    "text": "Hometown 🇸🇬\nI was born and raised in Singapore, a small city in Asia."
  },
  {
    "objectID": "about/SaaradhaaM.html#hobbies",
    "href": "about/SaaradhaaM.html#hobbies",
    "title": "Saaradhaa M",
    "section": "Hobbies 🐾",
    "text": "Hobbies 🐾\n\nI picked up yoga during the pandemic.\nI do a cappella with my college friends.\nOther things I enjoy include travel, hiking and trying new food."
  },
  {
    "objectID": "about/SaaradhaaM.html#fun-fact",
    "href": "about/SaaradhaaM.html#fun-fact",
    "title": "Saaradhaa M",
    "section": "Fun Fact ⭐️",
    "text": "Fun Fact ⭐️\nI am left-handed, but I play string instruments right-handed because those who taught me were right-handed."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Adithya Parupudi\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimeshSengupta\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmma Rasmussen\n\n\n\n\n\n\n\n\n\n\n\n\n\nJerin Jacob\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaushikaPotluri\n\n\n\n\n\n\n\n\n\n\n\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaddi Hertz\n\n\n\n\n\n\n\n\n\n\n\n\n\nManiShankerKamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\nMekhala Kumar\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiranda Manka\n\n\n\n\n\n\n\n\n\n\n\n\n\nNayan Jani\n\n\n\n\n\n\n\n\n\n\n\n\n\nNick Boonstra\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\nRosemary\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoy Yoon\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaaradhaa M\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoshana Buck\n\n\n\n\n\n\n\n\n\n\n\n\n\nTyler Tewksbury\n\n\n\n\n\n\n\n\n\n\n\n\n\nWill Munson\n\n\n\n\n\n\n\n\n\n\n\n\n\nYoung Soo Choi\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601 August 2022",
    "section": "",
    "text": "Challenge 3 Solutions\n\n\n\n\n\n\n\nchallenge_3\n\n\nsolution\n\n\n\n\nTidy Data: Pivoting\n\n\n\n\n\n\nAug 18, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Instructions\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\nMore data wrangling: mutate\n\n\n\n\n\n\nAug 18, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nYoung Soo Choi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3 Instructions\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\nTidy Data: Pivoting\n\n\n\n\n\n\nAug 17, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nTyler Tewksbury\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\nanimal_weights\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Solutions\n\n\n\n\n\n\n\nchallenge_2\n\n\nsolution\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\norganicpoultry\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nMekhala Kumar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\nAustralian_marriage_law\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nMani Shanker Kamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3 Instructions\n\n\n\n\n\n\n\nchallenge_3\n\n\nPivot\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nNayan Jani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\ntidyverse\n\n\nreadxl\n\n\ndplyr\n\n\ntidyr\n\n\nhouseholds\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nSaaradhaa M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nJerin jacob\n\n\n\n\n\n\n  \n\n\n\n\nChallenge 1 Solution\n\n\n\n\n\n\n\nchallenge_1\n\n\nsolution\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Will Munson\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nWill Munson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Attempt\n\n\n\n\n\n\n\nchallenge_2\n\n\nrailroads\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nFAOSTAT_cattle_dairy.csv\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMani Shanker Kamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nTyler Tewksbury\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nAnimesh Sengupta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNick Boonstra Challenge 1 Resubmit\n\n\n\n\n\n\n\nchallenge_1\n\n\nboonstra\n\n\nweek_1\n\n\nbirds\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nNick Boonstra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nYoung Soo Choi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nJerin Jacob\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nrailroads_2012_clean_county.csv\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nShoshana Buck\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 - Adithya Parupudi\n\n\n\n\n\n\n\nchallenge_2\n\n\nhw3\n\n\nfaostat_cattle_diary.csv\n\n\ndplyr\n\n\ntidyverse\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nAdithya Parupudi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nhotel_bookings\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMiranda Manka\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\ntidyverse\n\n\nbirds.csv\n\n\nhw2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nAdithya Parupudi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nStateCounty\n\n\n\n\nData wrangling: using group() and summarise(\n\n\n\n\n\n\nAug 16, 2022\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\nFAO\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nState County dataset\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nMekhala Kumar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\nrailroad\n\n\nquestion\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nRoy Yoon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNick Boonstra Challenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nboonstra\n\n\nweek_1\n\n\nhotels\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nNick Boonstra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nhotel_bookings.csv\n\n\ntidyverse\n\n\nreadr\n\n\n\n\nData Wrangling\n\n\n\n\n\n\nAug 16, 2022\n\n\nSaaradhaa M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMekhala Kumar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nAnimesh Sengupta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Quinn He\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMani Shanker Kamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nrailroads\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 15, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nYoung Soo Choi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Roy Yoon\n\n\n\n\n\n\n\nchallenge_1\n\n\nbirds.csv\n\n\nsubmission 2\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nRoy Yoon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nwild_bird_data\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMiranda Manka\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nKaushika Potluri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\ntidyverse\n\n\nreadxl\n\n\ndplyr\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 15, 2022\n\n\nSaaradhaa M\n\n\n\n\n\n\nNo matching items"
  }
]