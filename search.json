[
  {
    "objectID": "posts/challenge1_solutions.html",
    "href": "posts/challenge1_solutions.html",
    "title": "Challenge 1 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/challenge1_solutions.html#working-with-tabular-data",
    "href": "posts/challenge1_solutions.html#working-with-tabular-data",
    "title": "Challenge 1 Solution",
    "section": "Working with Tabular Data",
    "text": "Working with Tabular Data\nOur advanced datasets ( ‚≠ê‚≠ê‚≠ê and higher) are tabular data (i.e., tables) that are often published based on government sources or by other organizations. Tabular data is often made available in Excel format (.xls or .xlsx) and is formatted for ease of reading - but this can make it tricky to read into R and reshape into a usable dataset.\nReading in tabular data will follow the same general work flow or work process regardless of formatting differences. We will work through the steps in detail this week (and in future weeks as new datasets are introduced), but this is an outline of the basic process. Note that not every step is needed for every file.\n\nIdentify grouping variables and values to extract from the table\nIdentify formatting issues that need to be addressed or eliminated\nIdentify column issues to be addressed during data read-in\nChoose column names to allow pivoting or future analysis\nAddress issues in rows using filter (and stringr package)\nCreate or mutate new variables as required, using separate, pivot_longer, etc\n\n\nRailroad ‚≠êFAOSTAT ‚≠ê‚≠êWild Birds ‚≠ê‚≠ê‚≠êRailroad (xls) ‚≠ê‚≠ê‚≠ê‚≠ê\n\n\nIt is hard to get much information about the data source or contents from a .csv file - as compared to the formatted .xlsx version of the same data described below.\n\nRead the Data\n\n\nCode\nrailroad<-read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nRows: 2930 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): state, county\ndbl (1): total_employees\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\nFrom inspection, we can that the three variables are named state, county, and total employees. Combined with the name of the fail, this appears to be the aggregated data on the number of employees working for the railroad in each county 2012. We assume that the 2930 cases - which are counties embedded within states1 - consist only of counties where there are railroad employees?\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nWith a few simple commands, we can confirm that there are 53 ‚Äústates‚Äù represented in the data. To identify the additional non-state areas (probably District of Columbia, plus some combination of Puerto Rico and/or overseas addresses), we can print out a list of unique state names.\n\n1: We can identify case variables because both are character variables, which in tidy lingo are grouping variables not values.\n\n\n\nOnce again, a .csv file lacks any of the additional information that might be present in a published Excel table. So, we know the data are likely to be about birds, but will we be looking at individual pet birds, prices of bird breeds sold in stores, the average flock size of wild birds - who knows!\nThe FAOSTAT*.csv files have some additional information - the FAO - which a Google search reveals to be the Food and Agriculture Association of the United Nations publishes country-level data regularly in a database called FAOSTAT. So my best guess at this point is that we are going to be looking at country-level estaimtes of the number of birds that are raised for eggs and poultry, but we will see if this is right by inspecting the data.\n\nRead the Data\n\n\nCode\nbirds<-read_csv(\"_data/birds.csv\")\n\n\nRows: 30977 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nbirds\n\n\n\n\n  \n\n\n\nCode\nchickens<-read_csv(\"_data/FAOSTAT_egg_chicken.csv\")\n\n\nRows: 38170 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nchickens\n\n\n\n\n  \n\n\n\nIt is pretty difficult to get a handle on what data are being captured by any of the FAOSTAT* (including the birds.csv) data sets simply from a quick scan of the tibble after read in. It was easy with the railroad data, but now we are going to have to work harder to describe exactly what comprises a case in these data and what values are present for each case. We can see that there are 30,970 rows in the birds data (and 38,170 rows in the chickens) - but this might not mean that there are 30,970 (or 38,170) cases because we aren‚Äôt sure what constitutes a case at this point.\n\n\nWhat is a case?\nOne approach to figuring out what constitutes a case is to identify the value variables and assume that what is leftover are the grouping variables. Unfortunately, there are six double variables (from the column descriptions that are automatically returned), and it appears that most of them are not grouping variables. For example, the variable ‚ÄúArea Code‚Äù is a double - but doesn‚Äôt appear to be a value that varies across rows. Thus, it is a grouping variable rather than a true value in tidy nomenclature. Similar issues can be found with Year and ‚ÄúItem Code‚Äù - both appear to be grouping variables. Ironically, it is the variable called Value which appears to the sole value in the data set - but what is it the value of?\nAnother approach to identifying a case is to look for variation (or lack of variation) in just the first few cases of the tibble. (Think of this as the basis for a minimal reproducible example.) In the first few cases, the variables of the first 10 cases appear to be identical until we get to Year and Year Code (which appear to be identical to each other.) So it appears that Value is varying by country-year - but perhaps also by information in one of the other variables. It also appears that many of the doubles are just numeric codes, so lets drop those variables to simplify (I‚Äôm going to drop down to just showing the birds data for now.)\n\n\nCode\nbirds.sm<-birds%>%\n  select(-contains(\"Code\"))\nbirds.sm\n\n\n\n\n  \n\n\n\nCode\nchickens.sm<-chickens%>%\n  select(-contains(\"Code\"))\n\n\n\n\nVisual Summary of Data Set\nBefore we go doing detailed cross-tabs to figure out where there is variation, lets do a high level summary of the dataset to see if - for example - there are multiple values in the Element variable - or if we only have a dataset with records containing estimates of Chicken Stocks (from Element + Item.)\nTo get a better grasp of the data, lets do a quick skim or summary of the dataset and see if we can find out more about our data at a glance. I am using the dfSummary function from the summarytools package -one of the more attractive ways to quickly summarise a dataset. I am using a few options to allow it to render directly to html.\n\n\nCode\nprint(summarytools::dfSummary(birds.sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nbirds.sm\nDimensions: 30977 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Live Animals\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Africa2. Asia3. Eastern Asia4. Egypt5. Europe6. France7. Greece8. Myanmar9. Northern Africa10. South-eastern Asia[ 238 others ]\n      290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)28077(90.6%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Stocks\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Chickens2. Ducks3. Geese and guinea fowls4. Pigeons, other birds5. Turkeys\n      13074(42.2%)6909(22.3%)4136(13.4%)1165(3.8%)5693(18.4%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.6 (16.7)min ‚â§ med ‚â§ max:1961 ‚â§ 1992 ‚â§ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 99410.6 (720611.4)min ‚â§ med ‚â§ max:0 ‚â§ 1800 ‚â§ 23707134IQR (CV) : 15233 (7.2)\n      11495 distinct values\n      \n      1036\n(3.3%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Im5. M\n      1494(7.4%)6488(32.1%)10007(49.5%)1213(6.0%)1002(5.0%)\n      \n      10773\n(34.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Data not available3. FAO data based on imputat4. FAO estimate5. Official data6. Unofficial figure\n      6488(20.9%)1002(3.2%)1213(3.9%)10007(32.3%)10773(34.8%)1494(4.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-17\n\n\n\nFinally - we have a much better grasp on what is going on. First, we know that all records in this data set are of the number of Live Animal Stocks (Domain + Element), with the value expressed as 1000 heads (Unit). These three variables are grouping variables but DO NOT vary in this particular data extract - but are probably used to create data extracts from the larger FAOSTAT database.. To see if we are correct, we will have to checkout the same fields in the chickens data below.\nSecond, we can now guess that a case consists of a country-year-animal record - as captured in the variables Area, Year and Item, respectively - estimate of the number of live animals (Value.) ALso, as a side note, it appears that the estimated number of animals may have a long right-hand tail - just looking at the mini-histogram. So we can now say that we have estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) in 248 areas (countries??) for 58 years between 1961-2018.\nThe only minor concern is that we are still not entirely sure what information is being captured in the Flag (and matching Flag Description) variable. It appears unlikely that there is more than one estimate per country-year-animal case (see the summary of Area where all countries have 290 observations.) An assumption of one type of estimate (the content of Flag Description) per year is also consistent with the histogram of Year, which is pretty consistent although more countries were clearly added later in the series and data collection is not complete for the most recent time period.\nWe can dig a bit more, and find the description of the Flag field on the FAOSTAT website.. Sure enough, this confirms that the flags correspond to what type of estimate is being used (e.g., official data vs an estimate by FAOSTAT or imputed data.)\nWe can also confirm that NOT all cases are countries, as there is a Flag value, A, described as aggregated data. A quick inspection of the areas using this flag confirm that all of the ‚Äúcountries‚Äù are actually regional aggregations, and should be filtered out of the dataset as they are not the same ‚Äútype‚Äù of case as a country-level case. To fix these data into true tidy format, we would need to filter out the aggregates, then merge on the country group definitions from FAOSTAT to create new country-group or regional variables that could be used to recreate aggregated estimates with dplyr.\n\n\nCode\nbirds.sm%>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n\n\n  \n\n\n\n\n\nFAOstat*.csv\nLets take a quick look at our chickens data to see if it follows the same basic pattern as the birds data. Sure enough, it looks like we have a different domain (livestock products) but that the cases remain similar country-year-product, with three slightly different estimates related to egg-laying (instead of the five types of poultry.)\n\n\nCode\nprint(summarytools::dfSummary(chickens.sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nchickens.sm\nDimensions: 38170 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Livestock Primary\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Afghanistan2. Africa3. Albania4. Algeria5. American Samoa6. Americas7. Angola8. Antigua and Barbuda9. Argentina10. Asia[ 235 others ]\n      174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)36430(95.4%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Laying2. Production3. Yield\n      12679(33.2%)12840(33.6%)12651(33.1%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Eggs, hen, in shell\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.5 (16.7)min ‚â§ med ‚â§ max:1961 ‚â§ 1991 ‚â§ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head2. 100mg/An3. tonnes\n      12679(33.2%)12651(33.1%)12840(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 291341.2 (2232761)min ‚â§ med ‚â§ max:1 ‚â§ 31996 ‚â§ 76769955IQR (CV) : 91235.8 (7.7)\n      21325 distinct values\n      \n      40\n(0.1%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Fc5. Im6. M\n      1435(4.7%)3186(10.4%)10538(34.4%)13344(43.6%)2079(6.8%)40(0.1%)\n      \n      7548\n(19.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Calculated data3. Data not available4. FAO data based on imputat5. FAO estimate6. Official data7. Unofficial figure\n      3186(8.3%)13344(35.0%)40(0.1%)2079(5.4%)10538(27.6%)7548(19.8%)1435(3.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-17\n\n\n\n\n\n\nThe ‚Äúwild_bird_data‚Äù sheet is in Excel format (.xlsx) instead of the .csv format of the earlier data sets. In theory, it should be no harder to read in an Excel worksheet (or even workbook) as compared to a .csv file - there is a package called read_xl that is part of the tidyverse that easily reads in excel files.\nHowever, in practice, most people use Excel sheets as a publication format - not a way to store data, so there is almost always a ton of ‚Äújunk‚Äù in the file that is NOT part of the data table that we want to read in. Sometimes the additional ‚Äújunk‚Äù is incredibly useful - it might include table notes or information about data sources. However, we still need a systematic way to identify this junk and get rid of it during the data reading step.\nFor example, lets see what happens here if we just read in the wild bird data straight from excel.\n\n\nCode\nwildbirds<-read_excel(\"_data/wild_bird_data.xlsx\")\nwildbirds\n\n\n\n\n  \n\n\n\nHm, this doesn‚Äôt seem quite right. It is clear that the first ‚Äúcase‚Äù has information in it that looks more like variable labels. Lets take a quick look at the raw data.\n\n\n\nWild Bird Excel File\n\n\nSure enough the Excel file first row does contain additional information, a pointer to the article that this data was drawn from, and a quick Google reveals the article is [Nee, S., Read, A., Greenwood, J. et al.¬†The relationship between abundance and body size in British birds. Nature 351, 312‚Äì313 (1991)] (https://www.nature.com/articles/351312a0)\n\nSkipping a row\nWe could try to manually adjust things - remove the first row, change the column names, and then change the column types. But this is both a lot of work, and not really a best practice for data management. Lets instead re-read the data in with the skip option from read_excel, and see if it fixes all of our problems!\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 1)\nwildbirds\n\n\n\n\n  \n\n\n\nThis now looks great! Both variables are numeric, and now they correctly show up as double or (). The variable names might be a bit tough to work with, though, so it can be easier to assign new column names on the read in - and then manually adjust axis labels, etc once you are working on your publication-quality graphs.\nNote that I skip two rows this time, and apply my own column names.\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 2, \n                        col_names = c(\"weight\", \"pop_size\"))\nwildbirds\n\n\n\n\n  \n\n\n\n\n\n\nThe railroad data set is our most challenging data to read in this week, but is (by comparison) a fairly straightforward formatted table published by the Railroad Retirement Board. The value variable is a count of the number of employees in each county and state combination. \nLooking at the excel file, we can see that there are only a few issues: 1. There are three rows at the top of the sheet that are not needed 2. There are blank columns that are not needed. 3. There are Total rows for each state that are not needed\n\nSkipping title rows\nFor the first issue, we use the ‚Äúskip‚Äù option on read_excel from the readxl package to skip the rows at the top.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 3)\n\n\nNew names:\n‚Ä¢ `` -> `...2`\n‚Ä¢ `` -> `...4`\n\n\n\n\n  \n\n\n\n\n\nRemoving empty columns\nFor the second issue, I name the blank columns ‚Äúdelete‚Äù to make is easy to remove the unwanted columns. I then use select (with the ! sign to designate the complement or NOT) to select columns we wish to keep in the dataset - the rest are removed. Note that I skip 4 rows this time as I do not need the original header row.\nThere are other approaches you could use for this task (e.g., remove all columns that have no valid volues), but hard coding of variable names and types during data read in is not considered a violation of best practices and - if used strategically - can often make later data cleaning much easier.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))\n\n\nNew names:\n‚Ä¢ `delete` -> `delete...2`\n‚Ä¢ `delete` -> `delete...4`\n\n\n\n\n  \n\n\n\n\n\nFiltering ‚Äútotal‚Äù rows\nFor the third issue, we are going to use filter to identify (and drop the rows that have the word ‚ÄúTotal‚Äù in the State column). str_detect can be used to find specific rows within a column that have the designated ‚Äúpattern‚Äù, while the ‚Äú!‚Äù designates the complement of the selected rows (i.e., those without the ‚Äúpattern‚Äù we are searching for.)\nThe str_detect command is from the stringr package, and is a powerful and easy to use implementation of grep and regex in the tidyverse - the base R functions (grep, gsub, etc) are classic but far more difficult to use, particularly for those not in practice. Be sure to explore the stringr package on your own.\n\n\nCode\nrailroad<-read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n‚Ä¢ `delete` -> `delete...2`\n‚Ä¢ `delete` -> `delete...4`\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\n\n\nRemove any table notes\nTables often have notes in the last few table rows. You can check table limits and use this information during data read-in to not read the notes by setting the n-max option at the total number of rows to read, or less commonly, the range option to specify the spreadsheet range in standard excel naming (e.g., ‚ÄúB4:R142‚Äù). If you didn‚Äôt handle this on read in, you can use the tail command to check for notes and either tail or head to keep only the rows that you need.\n\n\nCode\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\nCode\n#remove the last two observations\nrailroad <-head(railroad, -2)\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\n\n\nConfirm cases\nAnd that is all it takes! The data are now ready for analysis. Lets see if we get the same number of unique states that were in the cleaned data in exercise 1.\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  n_distinct(.)\n\n\n[1] 54\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nOh my goodness! It seems that we have an additional ‚ÄúState‚Äù - it looks like Canada is in the full excel data and not the tidy data. This is one example of why it is good practice to always work from the original data source!"
  },
  {
    "objectID": "posts/challenge3_instructions.html",
    "href": "posts/challenge3_instructions.html",
    "title": "Challenge 3 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions.html#challenge-overview",
    "href": "posts/challenge3_instructions.html#challenge-overview",
    "title": "Challenge 3 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions.html#read-in-data",
    "href": "posts/challenge3_instructions.html#read-in-data",
    "title": "Challenge 3 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ‚≠ê\neggs_tidy.csv ‚≠ê‚≠ê or organicpoultry.xls ‚≠ê‚≠ê‚≠ê\naustralian_marriage*.xlsx ‚≠ê‚≠ê‚≠ê\nUSA Households*.xlsx ‚≠ê‚≠ê‚≠ê‚≠ê\nsce_labor_chart_data_public.csv üåüüåüüåüüåüüåü\n\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data\nDescribe the data, and be sure to comment on why you are planning to pivot it to make it ‚Äútidy‚Äù"
  },
  {
    "objectID": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "href": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "title": "Challenge 3 Instructions",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 √ó 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA    1098.    1045.\n2 USA      1990 NAFTA    1493.    2369.\n3 France   1980 EU        160.    1207.\n4 Mexico   1990 NAFTA     623.    -612.\n5 USA      1980 NAFTA    1106.     992.\n6 France   1990 EU       1327.    1418.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge3_instructions.html#pivot-the-data",
    "href": "posts/challenge3_instructions.html#pivot-the-data",
    "title": "Challenge 3 Instructions",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a ‚Äúsanity‚Äù check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 √ó 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1098.\n 2 Mexico   1980 NAFTA incoming              1045.\n 3 USA      1990 NAFTA outgoing              1493.\n 4 USA      1990 NAFTA incoming              2369.\n 5 France   1980 EU    outgoing               160.\n 6 France   1980 EU    incoming              1207.\n 7 Mexico   1990 NAFTA outgoing               623.\n 8 Mexico   1990 NAFTA incoming              -612.\n 9 USA      1980 NAFTA outgoing              1106.\n10 USA      1980 NAFTA incoming               992.\n11 France   1990 EU    outgoing              1327.\n12 France   1990 EU    incoming              1418.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new ‚Äúcase‚Äù be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html",
    "href": "posts/challenge1_TylerTewksbury.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html#challenge-overview",
    "href": "posts/challenge1_TylerTewksbury.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html#read-in-the-data",
    "href": "posts/challenge1_TylerTewksbury.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ‚≠ê\nbirds.csv ‚≠ê‚≠ê\nFAOstat*.csv ‚≠ê‚≠ê\nwild_bird_data.xlsx ‚≠ê‚≠ê‚≠ê\nStateCounty2012.xlsx ‚≠ê‚≠ê‚≠ê‚≠ê\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nrailroad_data <- read.csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html#describe-the-data",
    "href": "posts/challenge1_TylerTewksbury.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nrailroad_data %>%\nsummary()\n\n\n    state              county          total_employees  \n Length:2930        Length:2930        Min.   :   1.00  \n Class :character   Class :character   1st Qu.:   7.00  \n Mode  :character   Mode  :character   Median :  21.00  \n                                       Mean   :  87.18  \n                                       3rd Qu.:  65.00  \n                                       Max.   :8207.00  \n\n\nCode\nlength(unique(railroad_data$state))\n\n\n[1] 53\n\n\nThe dataset is a simple, pre cleaned spreadsheet consisting of three columns: State, County, and Total_Employees. These three columns are self explanatory, labeling the state, county, and the amount of employees in said railroad system. Using the summary function,we can see that there are 2930 reported counties within 53 states (including DC, Armed Forces Europe, Armed Forces Pacific). This data could be useful to determine the size of specific railroad systems based on employment. The dataset could be enhanced by adding in overall population data so one can answer questions such as the percent of people in a county who work on railroads."
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html",
    "href": "posts/challenge2_TylerTewksbury.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#challenge-overview",
    "href": "posts/challenge2_TylerTewksbury.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#read-in-the-data",
    "href": "posts/challenge2_TylerTewksbury.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ‚≠ê\nFAOstat*.csv ‚≠ê‚≠ê‚≠ê\nhotel_bookings ‚≠ê‚≠ê‚≠ê‚≠ê\n\n\n\nCode\ndf <- read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#describe-the-data",
    "href": "posts/challenge2_TylerTewksbury.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nsummary(df)\n\n\n    state              county          total_employees  \n Length:2930        Length:2930        Min.   :   1.00  \n Class :character   Class :character   1st Qu.:   7.00  \n Mode  :character   Mode  :character   Median :  21.00  \n                                       Mean   :  87.18  \n                                       3rd Qu.:  65.00  \n                                       Max.   :8207.00  \n\n\nThis summary shows the amount of values in the three columns, as well as a high level overview of said values. State and county are both characters, which likely was taken from an existing survey data spreadsheet. Summary also gives us a brief insight into the total_employees, showing the max, min, median, etc. Just with this summary, questions can be asked about the data. How many counties correlate to each state? Do states with more counties have a lower average of employees? Many questions can be asked, but this correlation between county count and employee count average will be looked into."
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_TylerTewksbury.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nstates_mean <- df %>%\n  group_by(state) %>%\n  summarise(mean_employee = mean(total_employees)) %>%\n  arrange(desc(mean_employee), .by_group = TRUE)\n\nstate_county_count <- df %>%\n  count(state) %>%\n  arrange(n)\n\nstates_mean <- df %>%\n  group_by(state) %>%\n  summarise(mean_employee = mean(total_employees)) %>%\n  arrange(desc(mean_employee), .by_group = TRUE)\n\nstate_county_count <- df %>%\n  count(state) %>%\n  arrange(n)\n\nstate_info <- inner_join(state_county_count, states_mean)\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included.\nI performed three different analyses to get a deeper understanding of the data. First, I grouped by state and calculated the mean of total employees per state, and sorted by descending. This allowed me to see the states with, what can be presumed to be, the highest concentration of employees. Next, to get a better understanding at the state level, I calculated the amount of counties in each state, sorting by ascending. This would allow me to confirm if a state with a high mean would have few counties as well. The theory seemed to be true, as there were a few I recognized by manually looking. However, I could confirm this using R. To do this, I simply repeated the first two analayses, this time creating new data frames out of them. I then ran an inner join to join via state, creating a new table with all the data I calculated. Using this table, visualizations can be made to test/prove the hypothesis of low county count = higher number of employees in their counties."
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html",
    "href": "posts/challenge1_QuinnHe.html",
    "title": "Challenge 1 Quinn He",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html#challenge-overview",
    "href": "posts/challenge1_QuinnHe.html#challenge-overview",
    "title": "Challenge 1 Quinn He",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html#read-in-the-data",
    "href": "posts/challenge1_QuinnHe.html#read-in-the-data",
    "title": "Challenge 1 Quinn He",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ‚≠ê\nbirds.csv ‚≠ê‚≠ê\nFAOstat*.csv ‚≠ê‚≠ê\nwild_bird_data.xlsx ‚≠ê‚≠ê‚≠ê\nStateCounty2012.xlsx ‚≠ê‚≠ê‚≠ê‚≠ê\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nbirds <- read_csv(\"_data/birds.csv\")\n\nview(birds)\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html#describe-the-data",
    "href": "posts/challenge1_QuinnHe.html#describe-the-data",
    "title": "Challenge 1 Quinn He",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nThe birds data set contains a wide range of range of entries. With the function below we can see all the column names listed. A few are hard to figure out what exactly the represent and just how important they are.\n\n\nCode\ncolnames(birds)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nIt appears the data set was taken from a farm organization. The data is definitely a little messy, but makes sense on the data entry side. Each country has descending rows of chickens, ducks, and fowls from 1961 to 2018. This is mostly a bit redundant. This whole data set keeps track of the value of these three types of birds in a 60 year window. There is also a possibility this data set came from a larger set with other types of animals because the ‚ÄúDomain‚Äù column lists ‚ÄòLivestock‚Äô throughout the entire data set.\n\n\nCode\nsummary(birds)\n\n\n Domain Code           Domain            Area Code        Area          \n Length:30977       Length:30977       Min.   :   1   Length:30977      \n Class :character   Class :character   1st Qu.:  79   Class :character  \n Mode  :character   Mode  :character   Median : 156   Mode  :character  \n                                       Mean   :1202                     \n                                       3rd Qu.: 231                     \n                                       Max.   :5504                     \n                                                                        \n  Element Code    Element            Item Code        Item          \n Min.   :5112   Length:30977       Min.   :1057   Length:30977      \n 1st Qu.:5112   Class :character   1st Qu.:1057   Class :character  \n Median :5112   Mode  :character   Median :1068   Mode  :character  \n Mean   :5112                      Mean   :1066                     \n 3rd Qu.:5112                      3rd Qu.:1072                     \n Max.   :5112                      Max.   :1083                     \n                                                                    \n   Year Code         Year          Unit               Value         \n Min.   :1961   Min.   :1961   Length:30977       Min.   :       0  \n 1st Qu.:1976   1st Qu.:1976   Class :character   1st Qu.:     171  \n Median :1992   Median :1992   Mode  :character   Median :    1800  \n Mean   :1991   Mean   :1991                      Mean   :   99411  \n 3rd Qu.:2005   3rd Qu.:2005                      3rd Qu.:   15404  \n Max.   :2018   Max.   :2018                      Max.   :23707134  \n                                                  NA's   :1036      \n     Flag           Flag Description  \n Length:30977       Length:30977      \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n\nCode\ndim(birds)\n\n\n[1] 30977    14\n\n\nCode\nhead(birds)\n\n\n# A tibble: 6 √ó 14\n  Domai‚Ä¶¬π Domain Area ‚Ä¶¬≤ Area  Eleme‚Ä¶¬≥ Element Item ‚Ä¶‚Å¥ Item  Year ‚Ä¶‚Åµ  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live ‚Ä¶       2 Afgh‚Ä¶    5112 Stocks     1057 Chic‚Ä¶    1961  1961 1000‚Ä¶\n2 QA      Live ‚Ä¶       2 Afgh‚Ä¶    5112 Stocks     1057 Chic‚Ä¶    1962  1962 1000‚Ä¶\n3 QA      Live ‚Ä¶       2 Afgh‚Ä¶    5112 Stocks     1057 Chic‚Ä¶    1963  1963 1000‚Ä¶\n4 QA      Live ‚Ä¶       2 Afgh‚Ä¶    5112 Stocks     1057 Chic‚Ä¶    1964  1964 1000‚Ä¶\n5 QA      Live ‚Ä¶       2 Afgh‚Ä¶    5112 Stocks     1057 Chic‚Ä¶    1965  1965 1000‚Ä¶\n6 QA      Live ‚Ä¶       2 Afgh‚Ä¶    5112 Stocks     1057 Chic‚Ä¶    1966  1966 1000‚Ä¶\n# ‚Ä¶ with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¬π‚Äã`Domain Code`, ¬≤‚Äã`Area Code`,\n#   ¬≥‚Äã`Element Code`, ‚Å¥‚Äã`Item Code`, ‚Åµ‚Äã`Year Code`\n# ‚Ñπ Use `colnames()` to see all variable names\n\n\nCode\ntail(birds)\n\n\n# A tibble: 6 √ó 14\n  Domai‚Ä¶¬π Domain Area ‚Ä¶¬≤ Area  Eleme‚Ä¶¬≥ Element Item ‚Ä¶‚Å¥ Item  Year ‚Ä¶‚Åµ  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live ‚Ä¶    5504 Poly‚Ä¶    5112 Stocks     1068 Ducks    2013  2013 1000‚Ä¶\n2 QA      Live ‚Ä¶    5504 Poly‚Ä¶    5112 Stocks     1068 Ducks    2014  2014 1000‚Ä¶\n3 QA      Live ‚Ä¶    5504 Poly‚Ä¶    5112 Stocks     1068 Ducks    2015  2015 1000‚Ä¶\n4 QA      Live ‚Ä¶    5504 Poly‚Ä¶    5112 Stocks     1068 Ducks    2016  2016 1000‚Ä¶\n5 QA      Live ‚Ä¶    5504 Poly‚Ä¶    5112 Stocks     1068 Ducks    2017  2017 1000‚Ä¶\n6 QA      Live ‚Ä¶    5504 Poly‚Ä¶    5112 Stocks     1068 Ducks    2018  2018 1000‚Ä¶\n# ‚Ä¶ with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¬π‚Äã`Domain Code`, ¬≤‚Äã`Area Code`,\n#   ¬≥‚Äã`Element Code`, ‚Å¥‚Äã`Item Code`, ‚Åµ‚Äã`Year Code`\n# ‚Ñπ Use `colnames()` to see all variable names\n\n\nCode\n#table(birds)\n\nggplot(birds, mapping = aes(x = 'Year', y = 'Value'))\n\n\n\n\n\nI‚Äôm wondering if I should use the %>% function here. I‚Äôm also having an issue with my functions because they don‚Äôt run correctly. Is it from my the ‚Äúdelimiter‚Äù error above? It may also be an issue with my working directory.\nI commented out ‚Äòtable(birds)‚Äô because it was giving me an error when I rendered it the function.\n\n\nCode\nbirds %>%\n  select(Item)\n\n\n# A tibble: 30,977 √ó 1\n   Item    \n   <chr>   \n 1 Chickens\n 2 Chickens\n 3 Chickens\n 4 Chickens\n 5 Chickens\n 6 Chickens\n 7 Chickens\n 8 Chickens\n 9 Chickens\n10 Chickens\n# ‚Ä¶ with 30,967 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge2_solutions.html",
    "href": "posts/challenge2_solutions.html",
    "title": "Challenge 2 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_solutions.html#challenge-overview",
    "href": "posts/challenge2_solutions.html#challenge-overview",
    "title": "Challenge 2 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics\n\n\nRailroad ‚≠êFAOstat* ‚≠ê‚≠ê‚≠êHotel Bookings ‚≠ê‚≠ê‚≠ê‚≠ê\n\n\nThe railroad data contain 2931 county-level aggregated counts of the number of railroad employees in 2012. Counties are embedded within States, and all 50 states plus Canada, overseas addresses in Asia and Europe, and Washington, DC are represented.\n\n\n\n\n\n\nData Summaries\n\n\n\nThis is a concise summary of more extensive work we did in Challenge 1, and is an example of how you should describe data in public-facing work. A ‚Äúpublic-facing‚Äù version of your work contains all critical details needed to replicate your work, but doesn‚Äôt contain a point-by-point rundown of the mental process you went through to get to that point. (Your internal analysis file should probably walk through that mental process, however!)\n\n\n\nRead the data\nHere, we are just reusing the code from Challenge 1. We are using the excel version, to ensure that we get Canada, and are renaming the missing data in county for Canada so that we don‚Äôt accidently filter that observation out.\n\nrailroad<-read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"state\", \"delete\",  \"county\",\n                                  \"delete\", \"employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(state, \"Total\"))\n\nrailroad<-head(railroad, -2)%>%\n  mutate(county = ifelse(state==\"CANADA\", \"CANADA\", county))\n\nrailroad\n\n\n\n  \n\n\n\n\n\nHow many values does X take on?\nNow, lets practice grouping our data and using other dplyr commands that make data wrangling super easy. First, lets take a closer look at how we counted the number of unique states last week. First, we selected the state column. Then we used the n_distinct command - which replicates the base R commands length(unique(var)).\n\n\n\n\n\n\nacross()\n\n\n\nInstead of counting the number of distinct values one at a time, I am doing an operation on two columns at the same time using across.\n\n\n\nrailroad%>%\n  summarise(across(c(state,county), n_distinct))\n\n\n\n  \n\n\n\nCheck this out - many counties have the same name! There are 2931 state-county cases, but only 1710 distinct county names. This is one reason it is so critical to understand ‚Äúwhat is a case‚Äù when you are working with your data - otherwise you might accidently collapse or group information that isn‚Äôt intenced to be grouped.\n\n\nHow many total X are in group Y?\nSuppose we want to know the total number of railroad employees was in 2012, what is the best way to sum up all of the values in the data? The summarize function is useful for doing calculcations across some or all of a data set.\n\nrailroad%>%\n  summarise(total_employees = sum(employees))\n\n\n\n  \n\n\n\nAround a quarter of a million people were employed in the railroad industry in 2012. While this may seem like a lot, it was a significant decrease in employment from a few decades earlier, according to official Bureau of Labor Statistics (BLS) estimates.\nYou may notice that the BLS estimates are significantly lower than the ones we are using, provided by the Railroad Retirement Board. Given that the Railroad Retirement Board has ‚Äúgold-standard‚Äù data on railroad employees, this discrepancy suggests that many people who work in the railroad industry are being classified in a different way by BLS statistics.\n\n\nWhich X have the most Y?\nSuppose we are interested in which county names are duplicated most often, or in which states have the most railroad employees. We can use the same basic approch to answer both ‚ÄúWhich X have the most Y?‚Äù style questions\n\n\n\n\n\n\ndf-print: paged (YAML)\n\n\n\nWhen you are using df-print: paged in your yaml header, or are using tibbles, there is no need to rely on the head(data) commmand to limit your results to the top 10 of a list.\n\n\n\nrailroad%>%\n  group_by(state)%>%\n  summarise(total_employees = sum(employees),\n            num_counties = n())%>%\n  arrange(desc(total_employees))\n\n\n\n  \n\n\n\nLooking at the top 10 states in terms of total railroad employment, a few trends emerge. Several of the top 10 states with geographical activity are highly populous and geographically large. California, Texas, New York, Pennsylvania, Ohio, Illinois, and Georgia are all amonst the top-10 largest states - so it would make sense if there are more railroad employees in large states.\nBut railroads are spread out along geography, and thus we might also expect square mileage within a state to be related to state railroad employment - not just state population. For example, Texas is around 65% larger (in area) than California, and has around 50% more railroad employees.\nThere appear to be multiple exceptions to both rules, however. If geography plus population were the primary factors explaining railroad employment, then California would be ranked higher than New York and Illinois, and New York would likely rank higher than ILlinois. However, Illinois - Chicago in particular - is a hub of railroad activity, and thus Illinois‚Äô higher ranking is likely reflecting hub activity and employment. New York is a hub for the East Coast in particular. While California may have hubs of train activity in Los Angeles or San Francisco, the Northeast has a higher density of train stations and almost certainly generates more passenger and freight miles than the larger and more populous California.\nThis final factor - the existence of heavily used train routes probably explains the high railroad employment in states like Nebraska, Indiana and Missouri - all of which lay along a major railway route between New York and Chicago, and then out to California. Anyway who has played Ticket to Ride probably recognizes many of these routes!\n\n\n\n\n\n\nGo further\n\n\n\nA fun exercise once you are comfortable with joins and map-based visualizatinos would be to join the railroad employment data to information about state population and geographic area, and also mapping information about railway routes, to get more insight into the factors driving railroad employment.\n\n\n\n\n\nThe FAOSTAT sheets are excerpts of the FAOSTAT database provided by the Food and Agriculture Association, an agency of the United Nations. We are using the file birds.csv that includes estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) for 248 areas for 58 years between 1961-2018. Estimated stocks are given in 1000 head.\nBecause we know (from challenge 1) that several of those areas include aggregated data (e.g., ) we are going to remove the aggregations, remove the unnecessary variables, and only work with the grouping variables available in the data. In a future challenge, we will join back on more data from the FAO to recreate regional groupings.\n\nbirds<-read_csv(\"_data/birds.csv\")%>%\n  select(-c(contains(\"Code\"), Element, Domain, Unit))%>%\n  filter(Flag!=\"A\")\nbirds\n\n\n\n  \n\n\n\n\nWhat is the average of Y for X groups?\nLets suppose we are starting off and know nothing about poultry stocks around the world, where could we start? Perhaps we could try to get a sense of the relative sizes of stocks of each of the five types of poultry, identified in the variable Item. Additionally, because some of the values may be missing, lets find out how many of the estimates are missing.\n\nbirds%>%\n  group_by(Item)%>%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE),\n            n_missing = sum(is.na(Value)))\n\n\n\n  \n\n\n\nOn average, we can see that countries have far more chickens as livestock (\\(\\bar{x}\\)=58.4million head) than other livestock birds (average stocks range between 2 and 10 million head). However, the information from the median stock counts suggest that there is significant variation across countries along with a strong right hand skew with regards to chicken stocks. The median number of chickens in a country is 3.8 million head - significantly less than the mean of almost 60 million. Overall, missing data doesn‚Äôt seem to be a huge issue, so we will just use na.rm=TRUE and not worry too much about the missingness for now.\nIt could be that stock head counts have changed over time, so lets try selecting two points in time and seeing whether or not average livestock counts are changing.\n\n\n\n\n\n\npivot-wider\n\n\n\nIt can be difficult to visually report data in tidy format. For example, it is tough to compare two values when they are on different rows. In this example, I use pivot-wider to swap a tidy grouping variable into multiple columns to be more ‚Äútable-like.‚Äù I then do some manual formatting to make it easy to compare the grouped estimates.\n\n\n\nt1<-birds%>%\n  filter(Year %in% c(1966, 2016))%>%\n  group_by(Year, Item)%>%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE))%>%\n  pivot_wider(names_from = Year, values_from = c(avg_stocks, med_stocks))\n\nknitr::kable(t1,\n             digits=0,format.args = list(big.mark = \",\"),\n             col.names = c(\"Type\", \"1966\", \"2016\",\n                           \"1996\", \"2016\"))%>%\n  kableExtra::kable_styling(htmltable_class = \"lightable-minimal\")%>%\n  kableExtra::add_header_above(c(\" \" = 1, \"Mean Stock (1000)\" = 2,\n                                 \"Median Stock (1000)\" = 2))\n\n\n\n \n\n\nMean Stock (1000)\nMedian Stock (1000)\n\n  \n    Type \n    1966 \n    2016 \n    1996 \n    2016 \n  \n \n\n  \n    Chickens \n    25,264 \n    105,437 \n    2,315 \n    7,854 \n  \n  \n    Ducks \n    5,053 \n    14,842 \n    110 \n    236 \n  \n  \n    Geese and guinea fowls \n    2,468 \n    11,750 \n    97 \n    73 \n  \n  \n    Pigeons, other birds \n    3,314 \n    2,874 \n    3,000 \n    1,194 \n  \n  \n    Turkeys \n    843 \n    2,858 \n    74 \n    194 \n  \n\n\n\n\n\n\n\n\n\n\n\nkable and kableExtra\n\n\n\nI manually adjust table formatting (column names, setting significant digits, adding a comma) using kable and add a header row using kableExtra. Because df-print=paged option is set to make it easier to scroll through longer data frames, I need to directly specify that I want to produce an htmltable - not a default kable/rmarkdown table - when I use kable and kableExtra formatting directly.\n\n\nSure enough, it does look like stocks have changed significantly over time. The expansion of country-level chicken stocks over five decades between 1966 and 2016 are most noteworthy, with both average and median stock count going up by a factor of 4. Pigeons have never been very popular, and average stocks have actually decreased over the same time period while the other less popular bird - turkeys - saw significant incrases in stock count. Some countries increased specialization in goose and/or guinea fowl production, as the average stock count went up but the median went down over the same period.\n\n\n\n\n\n\nGo further\n\n\n\nIt could be really interesting to graph the rise and fall of poultry stocks overtime with these data, and match these changes to changes in population size and country GDP. Another option would be to match the countries back to regional groupings available from the UN FAO, a future ‚Äúdata join‚Äù challenge.\n\n\n\n\n\nThis data set contains 119,390 hotel bookings from two hotels (‚ÄúCity Hotel‚Äù and ‚ÄúResort Hotel‚Äù) with an arrival date between July 2015 and August 2017 (more detail needed), including bookings that were later cancelled. Each row contains extensive information about a single booking:\n\nthe booking process (e.g., lead time, booking agent, deposit, changes made)\nbooking details (e.g., scheduled arrival date, length of stay)\nguest requests (e.g., type of room, meal(s) included, car parking)\nbooking channel (e.g., distribution, market segment, corporate affiliation for )\nguest information (e.g., child/adult, passport country)\nguest prior bookings (e.g., repeat customer, prior cancellations)\n\nThe data are a de-identified extract of real hotel demand data, made available by the authors.\n\nRead and make sense of the data\nThe hotel bookings data set is new to challenge 2, so we need to go through the same process we did during challenge 1 to find out more about the data. Lets read in the data and use the summmaryTools package to get an overview of the data set.\n\nbookings<-read_csv(\"_data/hotel_bookings.csv\")\n\nprint(summarytools::dfSummary(bookings,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nbookings\nDimensions: 119390 x 32\n  Duplicates: 31994\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      hotel\n[character]\n      1. City Hotel2. Resort Hotel\n      79330(66.4%)40060(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      is_canceled\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:75166(63.0%)1:44224(37.0%)\n      \n      0\n(0.0%)\n    \n    \n      lead_time\n[numeric]\n      Mean (sd) : 104 (106.9)min ‚â§ med ‚â§ max:0 ‚â§ 69 ‚â§ 737IQR (CV) : 142 (1)\n      479 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_year\n[numeric]\n      Mean (sd) : 2016.2 (0.7)min ‚â§ med ‚â§ max:2015 ‚â§ 2016 ‚â§ 2017IQR (CV) : 1 (0)\n      2015:21996(18.4%)2016:56707(47.5%)2017:40687(34.1%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_month\n[character]\n      1. August2. July3. May4. October5. April6. June7. September8. March9. February10. November[ 2 others ]\n      13877(11.6%)12661(10.6%)11791(9.9%)11160(9.3%)11089(9.3%)10939(9.2%)10508(8.8%)9794(8.2%)8068(6.8%)6794(5.7%)12709(10.6%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_week_number\n[numeric]\n      Mean (sd) : 27.2 (13.6)min ‚â§ med ‚â§ max:1 ‚â§ 28 ‚â§ 53IQR (CV) : 22 (0.5)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_day_of_month\n[numeric]\n      Mean (sd) : 15.8 (8.8)min ‚â§ med ‚â§ max:1 ‚â§ 16 ‚â§ 31IQR (CV) : 15 (0.6)\n      31 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_weekend_nights\n[numeric]\n      Mean (sd) : 0.9 (1)min ‚â§ med ‚â§ max:0 ‚â§ 1 ‚â§ 19IQR (CV) : 2 (1.1)\n      17 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_week_nights\n[numeric]\n      Mean (sd) : 2.5 (1.9)min ‚â§ med ‚â§ max:0 ‚â§ 2 ‚â§ 50IQR (CV) : 2 (0.8)\n      35 distinct values\n      \n      0\n(0.0%)\n    \n    \n      adults\n[numeric]\n      Mean (sd) : 1.9 (0.6)min ‚â§ med ‚â§ max:0 ‚â§ 2 ‚â§ 55IQR (CV) : 0 (0.3)\n      14 distinct values\n      \n      0\n(0.0%)\n    \n    \n      children\n[numeric]\n      Mean (sd) : 0.1 (0.4)min ‚â§ med ‚â§ max:0 ‚â§ 0 ‚â§ 10IQR (CV) : 0 (3.8)\n      0:110796(92.8%)1:4861(4.1%)2:3652(3.1%)3:76(0.1%)10:1(0.0%)\n      \n      4\n(0.0%)\n    \n    \n      babies\n[numeric]\n      Mean (sd) : 0 (0.1)min ‚â§ med ‚â§ max:0 ‚â§ 0 ‚â§ 10IQR (CV) : 0 (12.3)\n      0:118473(99.2%)1:900(0.8%)2:15(0.0%)9:1(0.0%)10:1(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      meal\n[character]\n      1. BB2. FB3. HB4. SC5. Undefined\n      92310(77.3%)798(0.7%)14463(12.1%)10650(8.9%)1169(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      country\n[character]\n      1. PRT2. GBR3. FRA4. ESP5. DEU6. ITA7. IRL8. BEL9. BRA10. NLD[ 168 others ]\n      48590(40.7%)12129(10.2%)10415(8.7%)8568(7.2%)7287(6.1%)3766(3.2%)3375(2.8%)2342(2.0%)2224(1.9%)2104(1.8%)18590(15.6%)\n      \n      0\n(0.0%)\n    \n    \n      market_segment\n[character]\n      1. Aviation2. Complementary3. Corporate4. Direct5. Groups6. Offline TA/TO7. Online TA8. Undefined\n      237(0.2%)743(0.6%)5295(4.4%)12606(10.6%)19811(16.6%)24219(20.3%)56477(47.3%)2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      distribution_channel\n[character]\n      1. Corporate2. Direct3. GDS4. TA/TO5. Undefined\n      6677(5.6%)14645(12.3%)193(0.2%)97870(82.0%)5(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      is_repeated_guest\n[numeric]\n      Min  : 0Mean : 0Max  : 1\n      0:115580(96.8%)1:3810(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      previous_cancellations\n[numeric]\n      Mean (sd) : 0.1 (0.8)min ‚â§ med ‚â§ max:0 ‚â§ 0 ‚â§ 26IQR (CV) : 0 (9.7)\n      15 distinct values\n      \n      0\n(0.0%)\n    \n    \n      previous_bookings_not_canceled\n[numeric]\n      Mean (sd) : 0.1 (1.5)min ‚â§ med ‚â§ max:0 ‚â§ 0 ‚â§ 72IQR (CV) : 0 (10.9)\n      73 distinct values\n      \n      0\n(0.0%)\n    \n    \n      reserved_room_type\n[character]\n      1. A2. B3. C4. D5. E6. F7. G8. H9. L10. P\n      85994(72.0%)1118(0.9%)932(0.8%)19201(16.1%)6535(5.5%)2897(2.4%)2094(1.8%)601(0.5%)6(0.0%)12(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      assigned_room_type\n[character]\n      1. A2. D3. E4. F5. G6. C7. B8. H9. I10. K[ 2 others ]\n      74053(62.0%)25322(21.2%)7806(6.5%)3751(3.1%)2553(2.1%)2375(2.0%)2163(1.8%)712(0.6%)363(0.3%)279(0.2%)13(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      booking_changes\n[numeric]\n      Mean (sd) : 0.2 (0.7)min ‚â§ med ‚â§ max:0 ‚â§ 0 ‚â§ 21IQR (CV) : 0 (2.9)\n      21 distinct values\n      \n      0\n(0.0%)\n    \n    \n      deposit_type\n[character]\n      1. No Deposit2. Non Refund3. Refundable\n      104641(87.6%)14587(12.2%)162(0.1%)\n      \n      0\n(0.0%)\n    \n    \n      agent\n[character]\n      1. 92. NULL3. 2404. 15. 146. 77. 68. 2509. 24110. 28[ 324 others ]\n      31961(26.8%)16340(13.7%)13922(11.7%)7191(6.0%)3640(3.0%)3539(3.0%)3290(2.8%)2870(2.4%)1721(1.4%)1666(1.4%)33250(27.8%)\n      \n      0\n(0.0%)\n    \n    \n      company\n[character]\n      1. NULL2. 403. 2234. 675. 456. 1537. 1748. 2199. 28110. 154[ 343 others ]\n      112593(94.3%)927(0.8%)784(0.7%)267(0.2%)250(0.2%)215(0.2%)149(0.1%)141(0.1%)138(0.1%)133(0.1%)3793(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      days_in_waiting_list\n[numeric]\n      Mean (sd) : 2.3 (17.6)min ‚â§ med ‚â§ max:0 ‚â§ 0 ‚â§ 391IQR (CV) : 0 (7.6)\n      128 distinct values\n      \n      0\n(0.0%)\n    \n    \n      customer_type\n[character]\n      1. Contract2. Group3. Transient4. Transient-Party\n      4076(3.4%)577(0.5%)89613(75.1%)25124(21.0%)\n      \n      0\n(0.0%)\n    \n    \n      adr\n[numeric]\n      Mean (sd) : 101.8 (50.5)min ‚â§ med ‚â§ max:-6.4 ‚â§ 94.6 ‚â§ 5400IQR (CV) : 56.7 (0.5)\n      8879 distinct values\n      \n      0\n(0.0%)\n    \n    \n      required_car_parking_spaces\n[numeric]\n      Mean (sd) : 0.1 (0.2)min ‚â§ med ‚â§ max:0 ‚â§ 0 ‚â§ 8IQR (CV) : 0 (3.9)\n      0:111974(93.8%)1:7383(6.2%)2:28(0.0%)3:3(0.0%)8:2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      total_of_special_requests\n[numeric]\n      Mean (sd) : 0.6 (0.8)min ‚â§ med ‚â§ max:0 ‚â§ 0 ‚â§ 5IQR (CV) : 1 (1.4)\n      0:70318(58.9%)1:33226(27.8%)2:12969(10.9%)3:2497(2.1%)4:340(0.3%)5:40(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status\n[character]\n      1. Canceled2. Check-Out3. No-Show\n      43017(36.0%)75166(63.0%)1207(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status_date\n[Date]\n      min : 2014-10-17med : 2016-08-07max : 2017-09-14range : 2y 10m 28d\n      926 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-17\n\n\n\nWow - there is a lot of information available here. Lets scan it and see what jumps out. First we can see that the summary function claims that there are almost 32,000 duplicates in the data. However, this is likely an artifact of the way that the bookings have been de-identified, and may reflect bookings with identical details but different individuals who made the bookings.\nWe can see that we are provided with limited information about the hotel. Hotels are identified only as ‚ÄúCity‚Äù Hotel‚Äù or a ‚ÄúResort Hotel‚Äù. Maybe we have bookings from only two hotels? Lets tentatively add that to our data description.\nThere is a flag for whether a booking is cancelled. This means that our universe of cases includes bookings where the guests showed up, as well as bookings that were later cancelled - we can add that to our data description.\nThere are multiple fields with the arrival date - year, month, etc. For now, we can tell that the arrival date of the bookings ranges between 2015 and 2017. More precise identification of the date range could be more easily done next challenge when we can recode the arrival date information using lubridate.But maybe it is possible to find out which values of month co-occur with specific years?\n\n\nWhich values of Y are nested within X?\nTo approach this question, we can narrow the dataset down to just the two variables of interest, and then use the distinct command.\n\nbookings%>%\n  select(arrival_date_year, arrival_date_month)%>%\n  distinct()\n\n\n\n  \n\n\n\nGreat - now we now that all bookings have arrival dates between June 2015 and August 2017, and can add that to the data description. Just for fun, lets see if we can confirm that the dates are the same for both hotels.\n\n\n\n\n\n\nslice()\n\n\n\nThis would be easier to investigate with proper date variables, but I am using slice to find the first and last row for each hotel, by position. This avoids printing out a long data list we have to scroll through, but would fail if the hotels had different sets of arrival month-year pairs.\n\n\n\nd<-bookings%>%\n  select(arrival_date_year, arrival_date_month)%>%\n  n_distinct\n\nbookings%>%\n  select(hotel, arrival_date_year, arrival_date_month)%>%\n  distinct()%>%\n  slice(c(1, d, d+1, d*2))\n\n\n\n  \n\n\n\nLets suppose we want to know whether or not the two hotels offer the same types of rooms? This is another query of the sort Which values of X are nested in y?\n\nbookings%>%\n  group_by(hotel)%>%\n  count(reserved_room_type)\n\n\n\n  \n\n\n\nIn this case, however, it is tough to directly compare - it appears that the hotel-roomtype pairs are not as consistent as the year-month pairs for the same hotels. A quick pivot-wider makes this comparison a little easier to visualize. Here we can see that the Resort Hotel has two additional room types: ‚ÄúH‚Äù and ‚ÄúL‚Äù.\n\nbookings%>%\n  group_by(hotel)%>%\n  count(reserved_room_type)%>%\n  pivot_wider(names_from= hotel, values_from = n)\n\n\n\n  \n\n\n\n\n\nWhat is the average of Y for group X?\nThe breakdown of rooms by hotel doesn‚Äôt shed much light on the room codes and what they might mean. Lets see if we can find average number of occupants and average price for each room type, and see if we can learn more about our data.\n\n\n\n\n\n\nmean(., na.rm=TRUE)\n\n\n\nI am using the mean function with the option na.rm=TRUE to deal with the four NA values in the children field, identified in the summary table above.\n\n\n\nt1<-bookings%>%\n  group_by(hotel, reserved_room_type)%>%\n  summarise(price = mean(adr),\n            adults = mean(adults),\n            children = mean(children+babies, na.rm=TRUE)\n            )%>%\n  pivot_wider(names_from= hotel, \n              values_from = c(price, adults, children))\n\nknitr::kable(t1,\n             digits=1,\n             col.names = c(\"Type\", \"City\", \"Resort\",\n                           \"City\", \"Resort\", \"City\", \"Resort\"))%>%\n  kableExtra::kable_styling(htmltable_class = \"lightable-minimal\")%>%\n  kableExtra::add_header_above(c(\"Room\" = 1, \"Price\" = 2,\n                                 \"Adults\" = 2, \"Children & Babies\" = 2))\n\n\nAverage Price and Occupancy, by hotel and room type\n \n\nRoom\nPrice\nAdults\nChildren & Babies\n\n  \n    Type \n    City \n    Resort \n    City \n    Resort \n    City \n    Resort \n  \n \n\n  \n    A \n    96.2 \n    76.2 \n    1.8 \n    1.8 \n    0.0 \n    0.0 \n  \n  \n    B \n    90.3 \n    104.7 \n    1.6 \n    2.0 \n    0.6 \n    0.0 \n  \n  \n    C \n    85.5 \n    161.4 \n    1.5 \n    2.0 \n    0.1 \n    1.4 \n  \n  \n    D \n    131.5 \n    103.6 \n    2.2 \n    2.0 \n    0.0 \n    0.1 \n  \n  \n    E \n    156.8 \n    114.5 \n    2.1 \n    2.0 \n    0.3 \n    0.0 \n  \n  \n    F \n    189.3 \n    132.8 \n    2.0 \n    2.0 \n    1.6 \n    0.1 \n  \n  \n    G \n    201.8 \n    168.2 \n    2.3 \n    2.0 \n    1.1 \n    1.4 \n  \n  \n    P \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    H \n    NA \n    188.2 \n    NA \n    2.7 \n    NA \n    1.0 \n  \n  \n    L \n    NA \n    124.7 \n    NA \n    2.2 \n    NA \n    0.0 \n  \n\n\n\n\n\n\n\n\n\n\n\nkable & kableExtra\n\n\n\nI manually adjust table formatting (column names, plus adding a header row) using kable and kableExtra package, respectively. Also, because df-print=paged is the option set in the YAML header, I need to directly specify that I want to produce an htmltable - not a default kable/rmarkdown table.\n\n\nBased on these descriptives broken down by hotel and room type, we can speculate that the ‚ÄúH‚Äù and ‚ÄúL‚Äù room types at the resort are likely some sort of multi-bedroom suite (because the average number of adults is over 2.) Similarly, we can speculate that the difference between ABC and DEF may be something related to room size or quality (e.g., number and size of beds) and/or related to meals included with the rooms - but this would require further investigation to pin down!\n\n\n\n\n\n\nGo further\n\n\n\nThere is lots more to explore in the hotel bookings dataset, but it will be a lot easier once we recode the date fields using lubridate."
  },
  {
    "objectID": "posts/challenge2_instructions.html",
    "href": "posts/challenge2_instructions.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions.html#challenge-overview",
    "href": "posts/challenge2_instructions.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions.html#read-in-the-data",
    "href": "posts/challenge2_instructions.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ‚≠ê\nFAOstat*.csv ‚≠ê‚≠ê‚≠ê\nhotel_bookings ‚≠ê‚≠ê‚≠ê‚≠ê\n\n\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_instructions.html#describe-the-data",
    "href": "posts/challenge2_instructions.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included."
  },
  {
    "objectID": "posts/challenge1_LindsayJones.html",
    "href": "posts/challenge1_LindsayJones.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_LindsayJones.html#describe-the-data",
    "href": "posts/challenge1_LindsayJones.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nData set contains the number of railroad employees in the United States in 2012, organized by county. Data was likely gathered reported to labor bureau either by counties or by each railroad station. Each row represents a county. Columns indicate state (or territory), county, and number of employees. There are 2930 counties as shown below:\n\n\nCode\ndim(Railroad)\n\n\n[1] 2930    3\n\n\nThe 10 counties with the most railroad employees are:\n\n\nCode\nRailroad %>%\n  arrange(desc(total_employees)) %>%\n  select(state,county,total_employees)%>%\n  group_by(total_employees) %>%\n  slice(1)%>%\n  ungroup()%>%\n  arrange(desc(total_employees))%>%\n  slice(1:10)\n\n\n\n\n  \n\n\n\nState_Railroad_Props illustrates the percentage of railroad workers located in each state or territory.\n\n\nCode\nState <- select(Railroad, state)\nState_Railroad_Props <- prop.table(table(State))*100\nState_Railroad_Props\n\n\nstate\n        AE         AK         AL         AP         AR         AZ         CA \n0.03412969 0.20477816 2.28668942 0.03412969 2.45733788 0.51194539 1.87713311 \n        CO         CT         DC         DE         FL         GA         HI \n1.94539249 0.27303754 0.03412969 0.10238908 2.28668942 5.18771331 0.10238908 \n        IA         ID         IL         IN         KS         KY         LA \n3.37883959 1.22866894 3.51535836 3.13993174 3.24232082 4.06143345 2.15017065 \n        MA         MD         ME         MI         MN         MO         MS \n0.40955631 0.81911263 0.54607509 2.66211604 2.93515358 3.92491468 2.66211604 \n        MT         NC         ND         NE         NH         NJ         NM \n1.80887372 3.20819113 1.67235495 3.03754266 0.34129693 0.71672355 0.98976109 \n        NV         NY         OH         OK         OR         PA         RI \n0.40955631 2.08191126 3.00341297 2.49146758 1.12627986 2.21843003 0.17064846 \n        SC         SD         TN         TX         UT         VA         VT \n1.56996587 1.77474403 3.10580205 7.54266212 0.85324232 3.13993174 0.47781570 \n        WA         WI         WV         WY \n1.33105802 2.35494881 1.80887372 0.75085324"
  },
  {
    "objectID": "posts/challenge1_instructions.html",
    "href": "posts/challenge1_instructions.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#challenge-overview",
    "href": "posts/challenge1_instructions.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#read-in-the-data",
    "href": "posts/challenge1_instructions.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ‚≠ê\nbirds.csv ‚≠ê‚≠ê\nFAOstat*.csv ‚≠ê‚≠ê\nwild_bird_data.xlsx ‚≠ê‚≠ê‚≠ê\nStateCounty2012.xlsx ‚≠ê‚≠ê‚≠ê‚≠ê\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions.html#describe-the-data",
    "href": "posts/challenge1_instructions.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html",
    "href": "posts/challenge2_LindsayJones.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#challenge-overview",
    "href": "posts/challenge2_LindsayJones.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#read-in-the-data",
    "href": "posts/challenge2_LindsayJones.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ‚≠ê\nhotel_bookings ‚≠ê‚≠ê‚≠ê\nFAOstat*.csv ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (join FAOSTAT_country_groups)\n\n\n\nCode\nlibrary(readxl)\nRailroad <- read_xls(\"_data/StateCounty2012.xls\", \n    skip =3)\nView(Railroad)"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#describe-the-data",
    "href": "posts/challenge2_LindsayJones.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nTotal railroad employment by state and county for calendar year 2012.\n\n\nCode\nlibrary(readxl)\nRailroad <- read_xls(\"_data/StateCounty2012.xls\", \n    skip =3)\nView(Railroad)"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_LindsayJones.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\nThe total number of railroad employees for each state/territory are shown below.\n\n\nCode\nstate_totals = Railroad %>% \n  filter(grepl(\"Total\", STATE)) %>%\n  filter(!grepl(\"Grand\", STATE))\nprint(state_totals)\n\n\n# A tibble: 53 √ó 5\n   STATE     ...2  COUNTY ...4  TOTAL\n   <chr>     <lgl> <chr>  <lgl> <dbl>\n 1 AE Total1 NA    <NA>   NA        2\n 2 AK Total  NA    <NA>   NA      103\n 3 AL Total  NA    <NA>   NA     4257\n 4 AP Total1 NA    <NA>   NA        1\n 5 AR Total  NA    <NA>   NA     3871\n 6 AZ Total  NA    <NA>   NA     3153\n 7 CA Total  NA    <NA>   NA    13137\n 8 CO Total  NA    <NA>   NA     3650\n 9 CT Total  NA    <NA>   NA     2592\n10 DC Total  NA    <NA>   NA      279\n# ‚Ä¶ with 43 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nHere is the median number of railroad employees per state:\n\n\nCode\nsummarize(state_totals,median(TOTAL))\n\n\n# A tibble: 1 √ó 1\n  `median(TOTAL)`\n            <dbl>\n1            3379\n\n\nThe mean number railroad employees in each state is:\n\n\nCode\nsummarize(state_totals,mean(TOTAL))\n\n\n# A tibble: 1 √ó 1\n  `mean(TOTAL)`\n          <dbl>\n1         4819.\n\n\nThe Armed Forces Pacific (AP) is the territory with the fewest employees:\n\n\nCode\nsummarize(state_totals,min(TOTAL))\n\n\n# A tibble: 1 √ó 1\n  `min(TOTAL)`\n         <dbl>\n1            1\n\n\nAnd Texas is the territory with the most employees:\n\n\nCode\nsummarize(state_totals,max(TOTAL))\n\n\n# A tibble: 1 √ó 1\n  `max(TOTAL)`\n         <dbl>\n1        19839\n\n\nStandard deviation of employees is:\n\n\nCode\nsd(state_totals$TOTAL)\n\n\n[1] 4781.829\n\n\n\nExplain and Interpret\nI chose the states because I found some of the totals for each state surprising, based on what I know about the size of those states‚Äô populations."
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html",
    "href": "posts/challenge1_instructions_NJ.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html#challenge-overview",
    "href": "posts/challenge1_instructions_NJ.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html#read-in-the-data",
    "href": "posts/challenge1_instructions_NJ.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ‚≠ê\nbirds.csv ‚≠ê‚≠ê\nFAOstat*.csv ‚≠ê‚≠ê\nwild_bird_data.xlsx ‚≠ê‚≠ê‚≠ê\nStateCounty2012.xlsx ‚≠ê‚≠ê‚≠ê‚≠ê\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")\nrailroad\n\n\n# A tibble: 2,930 √ó 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# ‚Ä¶ with 2,920 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html#describe-the-data",
    "href": "posts/challenge1_instructions_NJ.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nspec(railroad)\n\n\ncols(\n  state = col_character(),\n  county = col_character(),\n  total_employees = col_double()\n)\n\n\nCode\nrailroad %>%\n  filter(state == \"AK\")\n\n\n# A tibble: 6 √ó 3\n  state county               total_employees\n  <chr> <chr>                          <dbl>\n1 AK    ANCHORAGE                          7\n2 AK    FAIRBANKS NORTH STAR               2\n3 AK    JUNEAU                             3\n4 AK    MATANUSKA-SUSITNA                  2\n5 AK    SITKA                              1\n6 AK    SKAGWAY MUNICIPALITY              88\n\n\nCode\nrailroad %>%\n  group_by(state) %>%\n  summarise(total_employees2 = sum(total_employees))\n\n\n# A tibble: 53 √ó 2\n   state total_employees2\n   <chr>            <dbl>\n 1 AE                   2\n 2 AK                 103\n 3 AL                4257\n 4 AP                   1\n 5 AR                3871\n 6 AZ                3153\n 7 CA               13137\n 8 CO                3650\n 9 CT                2592\n10 DC                 279\n# ‚Ä¶ with 43 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nWhen I used the spec() function, it returned the variable types of the columns. We know that state is type col_character(), county is type col_character() and total_employees is type col_double(). When I filtered through just the state of AK it returns the list of the total employees for each county in that state. I created a pipe that groups all the states together an than is summarized by the total amount of employees in each state. This pipe is useful because I now know how many total employees are in each state. This data was likely gathered in 2012 from the most used railroad stations in the USA. This dataset is also long in the sense that the column length is much greater than the amount of columns present."
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html",
    "href": "posts/challenge2_instructions_NJ.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html#read-in-data",
    "href": "posts/challenge2_instructions_NJ.html#read-in-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in Data",
    "text": "Read in Data\n\n\nCode\nFAO <- read_csv(\"_data/FAOSTAT_livestock.csv\")\nFAO\n\n\n# A tibble: 82,116 √ó 14\n   Domain Cod‚Ä¶¬π Domain Area ‚Ä¶¬≤ Area  Eleme‚Ä¶¬≥ Element Item ‚Ä¶‚Å¥ Item  Year ‚Ä¶‚Åµ  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1961  1961\n 2 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1962  1962\n 3 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1963  1963\n 4 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1964  1964\n 5 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1965  1965\n 6 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1966  1966\n 7 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1967  1967\n 8 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1968  1968\n 9 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1969  1969\n10 QA           Live ‚Ä¶       2 Afgh‚Ä¶    5111 Stocks     1107 Asses    1970  1970\n# ‚Ä¶ with 82,106 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¬π‚Äã`Domain Code`, ¬≤‚Äã`Area Code`, ¬≥‚Äã`Element Code`, ‚Å¥‚Äã`Item Code`, ‚Åµ‚Äã`Year Code`\n# ‚Ñπ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThis dataset comes from the Food and Agriculture Association of the United Nations. They publish country-level data regularly and I am going to be looking at country-level estimates of the number of animals that are raised for livestock. We can see that there are 82116 rows in the livestock data."
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html#describe-the-data",
    "href": "posts/challenge2_instructions_NJ.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nspec(FAO)\n\n\ncols(\n  `Domain Code` = col_character(),\n  Domain = col_character(),\n  `Area Code` = col_double(),\n  Area = col_character(),\n  `Element Code` = col_double(),\n  Element = col_character(),\n  `Item Code` = col_double(),\n  Item = col_character(),\n  `Year Code` = col_double(),\n  Year = col_double(),\n  Unit = col_character(),\n  Value = col_double(),\n  Flag = col_character(),\n  `Flag Description` = col_character()\n)\n\n\nCode\nFAO.sm <- FAO %>%\n  select(-contains(\"Code\"))\nFAO.sm\n\n\n# A tibble: 82,116 √ó 9\n   Domain       Area        Element Item   Year Unit    Value Flag  Flag Descr‚Ä¶¬π\n   <chr>        <chr>       <chr>   <chr> <dbl> <chr>   <dbl> <chr> <chr>       \n 1 Live Animals Afghanistan Stocks  Asses  1961 Head  1300000 <NA>  Official da‚Ä¶\n 2 Live Animals Afghanistan Stocks  Asses  1962 Head   851850 <NA>  Official da‚Ä¶\n 3 Live Animals Afghanistan Stocks  Asses  1963 Head  1001112 <NA>  Official da‚Ä¶\n 4 Live Animals Afghanistan Stocks  Asses  1964 Head  1150000 F     FAO estimate\n 5 Live Animals Afghanistan Stocks  Asses  1965 Head  1300000 <NA>  Official da‚Ä¶\n 6 Live Animals Afghanistan Stocks  Asses  1966 Head  1200000 <NA>  Official da‚Ä¶\n 7 Live Animals Afghanistan Stocks  Asses  1967 Head  1200000 <NA>  Official da‚Ä¶\n 8 Live Animals Afghanistan Stocks  Asses  1968 Head  1328000 <NA>  Official da‚Ä¶\n 9 Live Animals Afghanistan Stocks  Asses  1969 Head  1250000 <NA>  Official da‚Ä¶\n10 Live Animals Afghanistan Stocks  Asses  1970 Head  1300000 <NA>  Official da‚Ä¶\n# ‚Ä¶ with 82,106 more rows, and abbreviated variable name ¬π‚Äã`Flag Description`\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nCode\nprint(dfSummary(FAO.sm, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nFAO.sm\nDimensions: 82116 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Live Animals\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Africa2. Asia3. China, mainland4. Eastern Africa5. Eastern Asia6. Eastern Europe7. Egypt8. Europe9. India10. Northern Africa[ 243 others ]\n      522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)76896(93.6%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Stocks\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Asses2. Buffaloes3. Camels4. Cattle5. Goats6. Horses7. Mules8. Pigs9. Sheep\n      8571(10.4%)3505(4.3%)3265(4.0%)13086(15.9%)12498(15.2%)11104(13.5%)6153(7.5%)12015(14.6%)11919(14.5%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.4 (16.8)min ‚â§ med ‚â§ max:1961 ‚â§ 1991 ‚â§ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. Head\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 11625569 (64779790)min ‚â§ med ‚â§ max:0 ‚â§ 224667 ‚â§ 1489744504IQR (CV) : 2364200 (5.6)\n      43667 distinct values\n      \n      1301\n(1.6%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Im5. M\n      2667(6.1%)12567(28.7%)24550(56.0%)2877(6.6%)1185(2.7%)\n      \n      38270\n(46.6%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Data not available3. FAO data based on imputat4. FAO estimate5. Official data6. Unofficial figure\n      12567(15.3%)1185(1.4%)2877(3.5%)24550(29.9%)38270(46.6%)2667(3.2%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-17\n\n\n\nBased on the results of the spec() function, I can see that there are six variables that are type double and eight that are type character. Out of the six double() variables, Area Code, Year and Item Code are all good grouping variables because they do not have values that vary across rows. I dropped the double() variables that contain code because they are just numeric codes for database management purposes. Using summarytools(), I can say that the records in this dataset are the number of Live Animal Stocks and the units of the values is Head. Each case in this dataset consists of an animal record based on the country and year that tries to estimate the number of live animals which is represented by Value. In total, I have estimates of the stock of nine different types of livestock (Asses, Buffaloes, Camels, Cattle, Goats, Horses, Mules, Pigs, Sheep ) in 253 areas for 58 years. The flags correspond to what type of estimate is being used."
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions_NJ.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\n\n\nCode\nFAO.sm %>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n# A tibble: 28 √ó 2\n   Area                          n\n   <chr>                     <int>\n 1 Africa                      522\n 2 Americas                    464\n 3 Asia                        522\n 4 Australia and New Zealand   376\n 5 Caribbean                   464\n 6 Central America             406\n 7 Central Asia                243\n 8 Eastern Africa              522\n 9 Eastern Asia                522\n10 Eastern Europe              522\n# ‚Ä¶ with 18 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nCode\nFAO_clc <- FAO.sm %>%\n  filter(Flag!=\"A\")\n\nFAO_clc  \n\n\n# A tibble: 31,279 √ó 9\n   Domain       Area        Element Item   Year Unit    Value Flag  Flag Descr‚Ä¶¬π\n   <chr>        <chr>       <chr>   <chr> <dbl> <chr>   <dbl> <chr> <chr>       \n 1 Live Animals Afghanistan Stocks  Asses  1964 Head  1150000 F     FAO estimate\n 2 Live Animals Afghanistan Stocks  Asses  1973 Head  1250000 F     FAO estimate\n 3 Live Animals Afghanistan Stocks  Asses  1974 Head  1250000 F     FAO estimate\n 4 Live Animals Afghanistan Stocks  Asses  1975 Head  1250000 F     FAO estimate\n 5 Live Animals Afghanistan Stocks  Asses  1976 Head  1250000 F     FAO estimate\n 6 Live Animals Afghanistan Stocks  Asses  1978 Head  1300000 *     Unofficial ‚Ä¶\n 7 Live Animals Afghanistan Stocks  Asses  1979 Head  1300000 *     Unofficial ‚Ä¶\n 8 Live Animals Afghanistan Stocks  Asses  1980 Head  1295000 *     Unofficial ‚Ä¶\n 9 Live Animals Afghanistan Stocks  Asses  1981 Head  1315000 *     Unofficial ‚Ä¶\n10 Live Animals Afghanistan Stocks  Asses  1982 Head  1315000 *     Unofficial ‚Ä¶\n# ‚Ä¶ with 31,269 more rows, and abbreviated variable name ¬π‚Äã`Flag Description`\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nCode\nFAO_clc %>%\n  group_by(Item) %>%\n  summarize(avg=mean(Value, na.rm = TRUE),\n            mode = n(),\n            median = median(Value, na.rm = TRUE),\n            stdev= sd(Value, na.rm = TRUE),\n            min = min(Value, na.rm = TRUE),\n            max = max(Value, na.rm = TRUE))\n\n\n# A tibble: 9 √ó 7\n  Item           avg  mode median     stdev   min       max\n  <chr>        <dbl> <int>  <dbl>     <dbl> <dbl>     <dbl>\n1 Asses      196051.  4899  14300   615866.     0   8793747\n2 Buffaloes 5901247.   756   6550 19546207.    20 114151770\n3 Camels     499737.  1075  85350  1311815.    45   7762545\n4 Cattle    4380953.  3554  47650 21361967.    15 203634000\n5 Goats     2577844.  4711  57625 11790005.     0 139467008\n6 Horses     276368.  5046  11500   999297.     0  10479246\n7 Mules       80414.  3357   4400   357196.     0   3287449\n8 Pigs       746710.  4107  29000  9429158.     0 345754816\n9 Sheep     2463044.  3774  40000  8206951.     0 111238000\n\n\n\nExplain and Interpret\nHere we can confirm that not all cases are countries. Flag Value A corresponds to Areas that are actually regional aggregations. These should be filtered out if I want to keep the same type of case as a country-level case. The second filter statement removes all cases with Flag Value A so that our dataset is at a country-level case. It seems like the distribution of cases for regional aggregations is even except for Areas Melanesia and Micronesia. FAO_clc is more specific version of the dataset that only includes the cases that are type country-level. I have conducted exploartory analysis on FAO_clc on the group Item and my first impression was how vastly different the mean and median were for each Item. This implies that our data is skewed in one direction. I also see that each Item stdev is really high which indicates that the data observed is quite spread out. The min and max values tell little about the dataset"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html",
    "href": "posts/challenge3_instructions_NJ.html",
    "title": "Challenge 3 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html#challenge-overview",
    "href": "posts/challenge3_instructions_NJ.html#challenge-overview",
    "title": "Challenge 3 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html#read-in-data",
    "href": "posts/challenge3_instructions_NJ.html#read-in-data",
    "title": "Challenge 3 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ‚≠ê\neggs_tidy.csv ‚≠ê‚≠ê or organicpoultry.xls ‚≠ê‚≠ê‚≠ê\naustralian_marriage*.xlsx ‚≠ê‚≠ê‚≠ê\nUSA Households*.xlsx ‚≠ê‚≠ê‚≠ê‚≠ê\nsce_labor_chart_data_public.csv üåüüåüüåüüåüüåü\n\n\n\nCode\neggs <-read_csv(\"_data/eggs_tidy.csv\",\n                        show_col_types = FALSE)\neggs\n\n\n# A tibble: 120 √ó 6\n   month      year large_half_dozen large_dozen extra_large_half_dozen extra_l‚Ä¶¬π\n   <chr>     <dbl>            <dbl>       <dbl>                  <dbl>     <dbl>\n 1 January    2004             126         230                    132       230 \n 2 February   2004             128.        226.                   134.      230 \n 3 March      2004             131         225                    137       230 \n 4 April      2004             131         225                    137       234.\n 5 May        2004             131         225                    137       236 \n 6 June       2004             134.        231.                   137       241 \n 7 July       2004             134.        234.                   137       241 \n 8 August     2004             134.        234.                   137       241 \n 9 September  2004             130.        234.                   136.      241 \n10 October    2004             128.        234.                   136.      241 \n# ‚Ä¶ with 110 more rows, and abbreviated variable name ¬π‚Äãextra_large_dozen\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nCode\nprint(dfSummary(eggs, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\neggs\nDimensions: 120 x 6\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      month\n[character]\n      1. April2. August3. December4. February5. January6. July7. June8. March9. May10. November[ 2 others ]\n      10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)20(16.7%)\n      \n      0\n(0.0%)\n    \n    \n      year\n[numeric]\n      Mean (sd) : 2008.5 (2.9)min ‚â§ med ‚â§ max:2004 ‚â§ 2008.5 ‚â§ 2013IQR (CV) : 5 (0)\n      2004:12(10.0%)2005:12(10.0%)2006:12(10.0%)2007:12(10.0%)2008:12(10.0%)2009:12(10.0%)2010:12(10.0%)2011:12(10.0%)2012:12(10.0%)2013:12(10.0%)\n      \n      0\n(0.0%)\n    \n    \n      large_half_dozen\n[numeric]\n      Mean (sd) : 155.2 (22.6)min ‚â§ med ‚â§ max:126 ‚â§ 174.5 ‚â§ 178IQR (CV) : 45.1 (0.1)\n      126.00‚Äâ‚Äâ:1(0.8%)128.50‚Äâ‚Äâ:29(24.2%)129.75‚Äâ‚Äâ:1(0.8%)131.00‚Äâ‚Äâ:3(2.5%)131.12‚Äâ!:1(0.8%)132.00‚Äâ‚Äâ:15(12.5%)133.50‚Äâ‚Äâ:3(2.5%)173.25‚Äâ‚Äâ:6(5.0%)174.50‚Äâ‚Äâ:47(39.2%)178.00‚Äâ‚Äâ:14(11.7%)!‚Äârounded\n      \n\n\n      0\n(0.0%)\n    \n    \n      large_dozen\n[numeric]\n      Mean (sd) : 254.2 (18.5)min ‚â§ med ‚â§ max:225 ‚â§ 267.5 ‚â§ 277.5IQR (CV) : 34.5 (0.1)\n      12 distinct values\n      \n      0\n(0.0%)\n    \n    \n      extra_large_half_dozen\n[numeric]\n      Mean (sd) : 164.2 (24.7)min ‚â§ med ‚â§ max:132 ‚â§ 185.5 ‚â§ 188.1IQR (CV) : 49.7 (0.2)\n      132.00‚Äâ‚Äâ:1(0.8%)134.50‚Äâ‚Äâ:1(0.8%)135.50‚Äâ‚Äâ:28(23.3%)135.88‚Äâ!:1(0.8%)137.00‚Äâ‚Äâ:6(5.0%)138.12‚Äâ!:1(0.8%)139.00‚Äâ‚Äâ:15(12.5%)185.50‚Äâ‚Äâ:53(44.2%)188.13‚Äâ‚Äâ:14(11.7%)!‚Äârounded\n      \n\n\n      0\n(0.0%)\n    \n    \n      extra_large_dozen\n[numeric]\n      Mean (sd) : 266.8 (22.8)min ‚â§ med ‚â§ max:230 ‚â§ 285.5 ‚â§ 290IQR (CV) : 44 (0.1)\n      11 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-17\n\n\n\n\nBriefly describe the data\nThis dataset is comprised of 120 rows with 6 variables. One variable is a character (month) and the rest are doubles. Based off the summary statistics of the egg variables, I can see that the variance of sales for each dozen of eggs are all high meaning that most cases are not near there average. ## Anticipate the End Result\nThe column names large_half_dozen, large_dozen, extra_large_half_dozen and extra_large_dozen represent values of the type variable, the values in the columns represent values of the sales variable, and each row represents multiple observations, not one.\n\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 √ó 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA     762.    1547.\n2 USA      1990 NAFTA    1100.     978.\n3 France   1980 EU       1295.     766.\n4 Mexico   1990 NAFTA    1688.     194.\n5 USA      1980 NAFTA    1135.     756.\n6 France   1990 EU        746.    1270.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\nCode\nnrow(eggs)\n\n\n[1] 120\n\n\nCode\nncol(eggs)\n\n\n[1] 6\n\n\nWhat needs to changed is that the column names that include the values for dozens needs to become its own columns called type and the corresponding values for that type will be stored in a column names sales"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html#pivot-the-data",
    "href": "posts/challenge3_instructions_NJ.html#pivot-the-data",
    "title": "Challenge 3 Instructions",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a ‚Äúsanity‚Äù check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 √ó 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing               762.\n 2 Mexico   1980 NAFTA incoming              1547.\n 3 USA      1990 NAFTA outgoing              1100.\n 4 USA      1990 NAFTA incoming               978.\n 5 France   1980 EU    outgoing              1295.\n 6 France   1980 EU    incoming               766.\n 7 Mexico   1990 NAFTA outgoing              1688.\n 8 Mexico   1990 NAFTA incoming               194.\n 9 USA      1980 NAFTA outgoing              1135.\n10 USA      1980 NAFTA incoming               756.\n11 France   1990 EU    outgoing               746.\n12 France   1990 EU    incoming              1270.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new ‚Äúcase‚Äù be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\nCode\neggs <- pivot_longer(eggs, col = c(large_half_dozen, large_dozen, extra_large_half_dozen, extra_large_dozen),\n                 names_to=\"type\",\n                 values_to = \"sales\")\neggs\n\n\n# A tibble: 480 √ó 4\n   month     year type                   sales\n   <chr>    <dbl> <chr>                  <dbl>\n 1 January   2004 large_half_dozen        126 \n 2 January   2004 large_dozen             230 \n 3 January   2004 extra_large_half_dozen  132 \n 4 January   2004 extra_large_dozen       230 \n 5 February  2004 large_half_dozen        128.\n 6 February  2004 large_dozen             226.\n 7 February  2004 extra_large_half_dozen  134.\n 8 February  2004 extra_large_dozen       230 \n 9 March     2004 large_half_dozen        131 \n10 March     2004 large_dozen             225 \n# ‚Ä¶ with 470 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nNow this dataset is in tidy form because each row is now one observation. I used pivot_longer because we needed more rows so that each observation could be individual in the dataset. When you go to read the dataset U can see each case only has one value attached to it."
  },
  {
    "objectID": "posts/challenge3_solutions.html",
    "href": "posts/challenge3_solutions.html",
    "title": "Challenge 3 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE, cache=TRUE)"
  },
  {
    "objectID": "posts/challenge3_solutions.html#challenge-overview",
    "href": "posts/challenge3_solutions.html#challenge-overview",
    "title": "Challenge 3 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nanticipate the shape of pivoted data, and\npivot the data into tidy format using pivot_longer\n\n\nExampleAnimal Weights ‚≠êEggs ‚≠ê‚≠ê/‚≠ê‚≠ê‚≠êAustralian Marriage Ballot ‚≠ê‚≠ê‚≠êUSA Households ‚≠ê‚≠ê‚≠ê‚≠ê\n\n\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nFind current and future data dimensions\nLets see if this works with a simple example.\n\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n\n  \n\n\n#existing rows/cases\nnrow(df)\n\n[1] 6\n\n#existing columns/cases\nncol(df)\n\n[1] 5\n\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n[1] 12\n\n# expected columns \n3 + 2\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nPivot the data\n\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n\n  \n\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\n\nThe animal weights dataset contains tabular-style data, with cells representing the average live animal weight (in kg) of 16 types of livestock for each of 9 geographic areas as defined by the Intergovernmental Panel on Climate Change (IPCC. Livestock weights are a critical part of the Global Livestock Envrionmental Assessment Model used by the FAO.\n\nanimal_weight<-read_csv(\"_data/animal_weight.csv\")\nanimal_weight\n\n\n\n  \n\n\n\nBecause the animal weights data is in tabular format, it is easy to see that \\(n=9\\) regions (categories or cases) in the original data, and that there are \\(k=16\\) types of livestock (categories or columns). Therefore, we expect the pivoted dataset to have \\(9 * 16\\) = 144 rows and 3 columns (region, animal type, and animal weight.)\n\n\n\n\n\n\ninline R code\n\n\n\nIf you check out the code above, you will see that I didn‚Äôt use a calculator to figure out \\(9*16=144\\), but used inline r code like this: `r 9*16`.\n\n\n\nPivot the data\n\nanimal_weight_longer<-pivot_longer(animal_weight, \n                                    col=-`IPCC Area`,\n                                    names_to = \"Livestock\",\n                                    values_to = \"Weight\")\nanimal_weight_longer\n\n\n\n  \n\n\n\nYes, it looks like we ended up with 144 rows and 3 columns, exactly as expected!\n\n\n\n\n\n\nNote\n\n\n\n#Go further\nstringr functions, and separate from tidyr, would be useful in helping split out additional infromation from the Livestock column.\n\n\n\n\n\nThis section covers pivoting for the organic eggs data, available in both excel and (partially cleaned) .csv format. The data reports the average price per carton paid to the farmer or producer for organic eggs (and organic chicken), reported monthly from 2004 to 20013. Average price is reported by carton type, which can vary in both size (x-large or large) and quantity (half-dozen or dozen.)\nIf you are using the eggs_tidy.csv, you can skip the first section as your data is in .csv format and already partially cleaned. The first section reviews data read-in and cleaning for the organicpoultry.xls file.\n\nRead and Clean the dataPivot Type OnlyPivot Size and Quantity\n\n\nThere are three sheets in the organicpoultry.xls workbook: one titled Data, one titled ‚ÄúOrganic egg prices, 2004-13‚Äù and one with a similar title for chicken prices. While I can tell all of this from inspection, I can also use a ask R to return the sheet names for me.\n\n\n\n\n\n\nGet sheet names with excel_sheets()\n\n\n\nBoth readxl and googlesheets4 have a function that can return sheet names as a vector. This is really useful if you need to parse and read multiple sheets in the same workbook.\n\n\n\nexcel_sheets(\"_data/organiceggpoultry.xls\")\n\n[1] \"Data\"                            \"Organic egg prices, 2004-13\"    \n[3] \"Organic poultry prices, 2004-13\"\n\n\nWhile it may seem like it would be easier to read in the individual egg prices and chicken prices, the amount of formatting introduced into the second and third sheets is pretty intimidating (see the screenshot below.) There are repeated headers to remove, a year column to shift, and other formatting issues. Ironically, it may be easier to read in the egg data from the Data sheet, with a skip of 5 (to skip the table title, etc), custom column names designed for pivoting to two categories (final section) and only reading in columns B to F.\n\n\n\nOrganic Poultry Data\n\n\n\n\n\nOrganic Poultry Egg Prices\n\n\n\n\n\n\n\n\nHard-coding Table Formats\n\n\n\nFormatted excel tables are a horrible data source, but may be the only way to get some data. If table formatting is consistent from year to year, hard-coding can be an acceptable approach. If table format is inconsistent, then more powerful tools are needed.\n\n\n\neggs_orig<-read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dozen\",\n                               \"xlarge_halfdozen\", \"large_dozen\",\n                               \"large_halfdozen\")\n                 )\neggs_orig\n\n\n\n  \n\n\n\nSometimes there are notes in the first column of tables, so lets make sure that isn‚Äôt an issue.\n\neggs_orig%>%\n  count(date)\n\n\n\n  \n\n\n\nWe need to remove the ‚Äúnote‚Äù indicators in two of the rows. Some characters require an escape to be included in regular expressions, but this time it is straightforward to find ‚Äù /1‚Äù.\n\neggs<-eggs_orig%>%\n  mutate(date = str_remove(date, \" /1\"))\n\nOne final step is needed to split the year variable away from the month. You will often need to separate out two variables from a single column when working with published tables, and also need to use the equivalent of dragging to fill in a normal spreadsheet. Lets look at the easiest way to fix both of these issues.\n\n\n\n\n\n\ntidyr::separate()\n\n\n\nSeparate is a fantastic function for working with strings. It will break a string column into multiple new (named) columns, at the indicated separator character (e.g., ‚Äú,‚Äù or ‚Äù ‚Äú). The old variable is automatically removed, but can be left.\n\n\n\n\n\n\n\n\ntidyr::fill()\n\n\n\nFill works like dragging to fill functionality in a spreadsheet. You can choose the direction to fill.\n\n\n\neggs<-eggs%>%\n  separate(date, into=c(\"month\", \"year\"), sep=\" \")%>%\n  fill(year)\neggs\n\n\n\n  \n\n\n\n\n\nLooking at the data, we can see that each of the original 120 cases consist of a year-month combination (e.g., January 2004), while the values are the average price (in cents) of four different types of eggs (e.g., large_half_dozen, large_dozen, etc) So to tidy our data, we should create a matrix with a year-month-eggType combination, with a single price value for each case.\nTo do this (and make our data easier to graph and analyze), we can pivot longer - changing our data from 120 rows with 6 variables (2 grouping and 4 values) to 480 rows of 4 variables (with 3 grouping variables and a single price value).\n\neggs_long<-eggs%>%\n  pivot_longer(cols=contains(\"large\"), \n               names_to = \"eggType\",\n               values_to = \"avgPrice\"\n  )\neggs_long\n\n\n\n  \n\n\n\nWell, that was super easy. But wait, what if you are interested in egg size - you want to know how much more expensive extra-large eggs are compared to large eggs. Right now, that will be annoying, as you will have to keep sorting out the egg quantity - whether the price is for a half_dozen or a dozen eggs.\n\n\nWouldn‚Äôt it be nice if we had two new columns - size and quantity - in place of the existing eggType categorical variable? In other words, to have fully tidy data, we would need 4 grouping variables (year, month, size, and quantity) and the same value (price). So, we want to use pivot longer, but we will be adding two new category variables (for a total of 4) and this will cut the number of rows in half (to 240).\nHow can we let R know what we want it to do?? Thankfully, we created pretty systematic column names for egg types in our original data, following the general pattern: size-quantity. Maybe we can use this to our advantage? Working with patterns in the names_sep option of the pivot functions makes it easier than you would think to pivot four existing columns into two new columns.\n\neggs_long<- eggs%>%\n  pivot_longer(cols=contains(\"large\"),\n               names_to = c(\"size\", \"quantity\"),\n               names_sep=\"_\",\n               values_to = \"price\"\n  )\neggs_long\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is another tabular data source published by the Australian Bureau of Statistics that requires a decent amount of cleaning. In 2017, Australia conducted a postal survey to gauge citizens‚Äô opinions towards same sex marriage: ‚ÄúShould the law be changed to allow same-sex couples to marry?‚Äù All Australian citizens are required to vote in elections, so citizens could respond in one of four ways: vote yes, vote no, vote in an unclear way, or fail to vote. (See the ‚ÄúExplanatory Notes‚Äù sheet for more details.)\nThe provided table includes estimates of the proportion of citizens choosing each of the four options, aggregated by Federal Electoral District, which are nested within one of 8 overarching Electoral Divisions. Here is a quick image showing the original table format.\n ### Identify desired data structure\nInspection reveals several critical issues to address: - Typical long header (skip = 7) - No single row with variable names - Two redundant values (count and percentage - percentage is easy to recover from complete count data) - Total columns that are redundant (remove) - The sum of ‚ÄúYes‚Äù and ‚ÄúNo‚Äù votes appears to be redundant with Response Clear in columns I and J - District and Division are in the same column\nIn this example, we are going to identify the desired structure early in the process, because clever naming of variables makes it much easier to use pivot functions. We will skip reading in redundant data (proportions and ‚Äútotals‚Äù columns), and then can identify four potentially distinct pieces of information. Three grouping variables: Division (in column 1), District (also in column 1), and citizen Response (yes, no, unclear, and non-response), plus one value: aggregated response Count.\nOur basic data reading and cleaning process should therefore follow these steps:\n\nRead in data, skipping unneeded columns and renaming variables\nCreate Division and District variables using separate() and fill()\npivot_longer() four response variables into 2 new Response and Count variables (double the number of rows)\n\n\nRead DataSeparate District and DivisionPivot_longer to tidy format\n\n\nIt is best to confine serious hard-coding to the initial data read in step, to make it easy to locate and make changes or replicate in the future. So, we will use a combination of tools introduced earlier to read and reformat the data: skip and col_names to read in the data, select to get rid of unneeded columns, and filter to get rid of unneeded rows. We also use the drop_na function to filter unwanted rows.\n\nvote_orig <- read_excel(\"_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\",\n           sheet=\"Table 2\",\n           skip=7,\n           col_names = c(\"District\", \"Yes\", \"del\", \"No\", rep(\"del\", 6), \"Illegible\", \"del\", \"No Response\", rep(\"del\", 3)))%>%\n  select(!starts_with(\"del\"))%>%\n  drop_na(District)%>%\n  filter(!str_detect(District, \"(Total)\"))%>%\n  filter(!str_starts(District, \"\\\\(\"))\nvote_orig\n\n\n\n  \n\n\n\n\n\nThe most glaring remaining issue is that the administrative Division is not in its own column, but is on its own row within the District column. The following code uses case_when to make a new Division variable with an entry (e.g., New South Wales Division) where there is a Division name in the District column, and otherwise it create just an empty space. After that, fill can be used to fill in empty spaces with the most recent Division name. We then filter out rows with only the title information.\n\nvote<- vote_orig%>%\n  mutate(Division = case_when(\n    str_ends(District, \"Divisions\") ~ District,\n    TRUE ~ NA_character_ ))%>%\n  fill(Division, .direction = \"down\")\nvote<- filter(vote,!str_detect(District, \"Division|Australia\"))\nvote\n\n\n\n  \n\n\n\n\n\nSupposed we wanted to create a stacked bar chart to compare the % who votes Yes to the people who either said No or didn‚Äôt vote. Or if we wanted to use division level characteristics to predict the proortion of people voting in a specific way? In both cases, we would need tidy data, which requires us to pivot longer into the original (aggregated) data format: Division, District, Response, Count. We should end up with 600 rows and 4 columns.\n\nvote_long<- vote%>%\n  pivot_longer(\n    cols = Yes:`No Response`,\n    names_to = \"Response\",\n    values_to = \"Count\"\n  )\nvote\n\n\n\n  \n\n\n\n\n\n\n\n\nThe excel workbook ‚ÄúUSA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019‚Äù is clearly a table of census-type household data (e.g., Current Population Study or CPS, American Community Study or ACS, etc.) Row 3 of the workbook provides a link to more details about the origin of the data used to produce the table.\nThe cases in this example are essentially year-identity groups, where I use the term identity to refer to the wide range of ways that the census can cluster racial and identity identity. While there are 12 categories in the data, many of these overlap and/or are not available in specific years. For example, one category is ‚ÄúAll Races‚Äù, and it overlaps with all other categories but cannot be easily eliminated because it isn‚Äôt clear how\n\n\n\nExcel Workbook Screenshot\n\n\n\nIdentify desired data structure\nInspection of the excel workbook reveals several critical features of the data. - column names (of a sort) are in rows 4 and 5 (skip=5 and rename) - first column includes year and race/hispanic origin households - first column appears to have notes of some sort (remove notes) - there are end notes starting in row 358 (n_max = 352) - ‚ÄúTotal‚Äù column appears to be redundant proportion info\nThe data appears to have two grouping variables (year and identityity), plus several values:\n\na count of number of households\nmedian and mean income (and associated margin of error)\nproportion of households with hhold income in one of 9 designated ranges or brackets\n\nThe final data should probably\n\nRead and clean the dataClean and separate *year‚Äù columnSanity check for identity\n\n\n\nincome_brackets <- c(i1 = \"Under $15,000\",\n                     i2 = \"$15,000 to $24,999\",\n                     i3 = \"$25,000 to $34,999\",\n                     i4= \"$35,000 to $49,999\",\n                     i5 = \"$50,000 to $74,999\",\n                     i6 = \"$75,000 to $99,999\",\n                     i7 = \"$100,000 to $149,999\",\n                     i8 = \"$150,000 to $199,999\",\n                     i9 = \"$200,000 and over\")\n\nushh_orig <- read_excel(\"_data/USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\",\n                        skip=5,\n                        n_max = 352,\n                        col_names = c(\"year\", \"hholds\", \"del\", \n                                str_c(\"income\",1:9,sep=\"_i\"),\n                               \"median_inc\", \"median_se\", \"mean_inc\",\"mean_se\"))%>%\n  select(-del)\n\n\n\nThe current year column still has identityity information on the hholds, as well as notes that need to be removed. Because identityity labels have spaces, we will need to remove those first before our typical approach to removing notes using separate is going to work.\n\n\n\n\n\n\nRegex and Regexr\n\n\n\nRegular expressions are a critical tool for messy, real world data where you will need to search, replace, and extract information from string variables. Learning regex is tough, but Regexer makes it much easier!\n\n\n\nushh_orig%>%\n  filter(str_detect(year, \"[[:alpha:]]\"))\n\n\n\n  \n\n\n\nNow that we know how to use regular expressions to find the household identityity information, we can quickly separate out the identityity information from the years, then do the standard fill prior to removing the unneeded category rows.\nOnce that is done, we can use separate to remove the notes from the year column. Removing notes from the identityity column is a bit trickier, and requires regex to find cases where there is a space then at least one numeric digit\n\nushh_id<-ushh_orig%>%\n  mutate(identity = case_when(\n    str_detect(year, \"[[:alpha:]]\") ~ year,\n    TRUE ~ NA_character_\n  ))%>%\n  fill(identity)%>%\n  filter(!str_detect(year, \"[[:alpha:]]\"))\n\nushh_id<-ushh_id%>%\n  separate(year, into=c(\"year\", \"delete\"), sep=\" \")%>%\n  mutate(identity = str_remove(identity, \" [0-9]+\"),\n         year = parse_number(year))%>%\n  select(-delete)\n\n\n\nEven from the detailed notes, it is difficult to fully understand what is going on with the identity variable, and whether all of the values are available in every year. A simple sanity check is to pick out several years mentioned in the notes and see if the number of households are available for all categories, and also check to see if there are specific categories that add up to the ‚Äúall races‚Äù category.\n\nushh_id%>%\n  filter(year%in%c(1970, 1972, 1980, 2001, 2002))%>%\n  select(identity, hholds, year)%>%\n  pivot_wider(values_from=hholds, names_from=year)\n\n\n\n  \n\n\n\nBased on these examples, we can now confirm that the survey did not include a question about Hispanic background prior to 109228, that only ‚ÄúWhite‚Äù and ‚ÄúBlack‚Äù (and not ‚ÄúAsian‚Äù) were systematically recorded prior to 2002, and that other mentioned dates of changes are not relevant to the categories represented in the data. Additionally, we can see from the example years that it would be reasonable to create a consistent time series that collapses the ‚ÄúWhite‚Äù and ‚ÄúWhite Alone‚Äù and ‚ÄúBlack‚Äù and ‚ÄúBlack A labels.\nBased on this exploratory data, one reasonable option that will streamline future analysis is to create two new variables ‚Äúrace‚Äù and ‚Äúhispanic‚Äù as follows. :::{.callout-tip} ## Keep your original data\nOriginal data that has been carefully documented can be overly detailed and broken into categories that make systematic analysis difficult. When you simplify data categories for exploratory work, keep the original data so that you can reintroduce it at the appropriate point.\n\n\n\n\nushh <-ushh_id%>%\n  mutate(id = case_when(\n    identity %in% c(\"White\", \"White Alone\") ~ \"race_white\"\n  ))"
  },
  {
    "objectID": "posts/challenge4_instructions.html",
    "href": "posts/challenge4_instructions.html",
    "title": "Challenge 4 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_instructions.html#challenge-overview",
    "href": "posts/challenge4_instructions.html#challenge-overview",
    "title": "Challenge 4 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday‚Äôs challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_instructions.html#read-in-data",
    "href": "posts/challenge4_instructions.html#read-in-data",
    "title": "Challenge 4 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ‚≠ê\npoultry_tidy.csv‚≠ê‚≠ê\nFedFundsRate.csv‚≠ê‚≠ê‚≠ê\nhotel_bookings.csv‚≠ê‚≠ê‚≠ê‚≠ê\ndebt_in_trillions ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "title": "Challenge 4 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Instructions",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "about/Rosemary.html",
    "href": "about/Rosemary.html",
    "title": "Rosemary",
    "section": "",
    "text": "PSU Political Science and Social Data Analytics"
  },
  {
    "objectID": "about/Rosemary.html#r-experience",
    "href": "about/Rosemary.html#r-experience",
    "title": "Rosemary",
    "section": "R experience",
    "text": "R experience\nUse R in research and teach R programming"
  },
  {
    "objectID": "about/Rosemary.html#research-interests",
    "href": "about/Rosemary.html#research-interests",
    "title": "Rosemary",
    "section": "Research interests",
    "text": "Research interests\nAuthoritarian politics, Survey, Text as data"
  },
  {
    "objectID": "about/Rosemary.html#hometown",
    "href": "about/Rosemary.html#hometown",
    "title": "Rosemary",
    "section": "Hometown",
    "text": "Hometown\nTianjin, China"
  },
  {
    "objectID": "about/Rosemary.html#hobbies",
    "href": "about/Rosemary.html#hobbies",
    "title": "Rosemary",
    "section": "Hobbies",
    "text": "Hobbies\nHorseback riding, journaling, cooking"
  },
  {
    "objectID": "about/Rosemary.html#fun-fact",
    "href": "about/Rosemary.html#fun-fact",
    "title": "Rosemary",
    "section": "Fun fact",
    "text": "Fun fact\nI‚Äôm a zoo parent of an African lion."
  },
  {
    "objectID": "about/TylerTewksbury.html",
    "href": "about/TylerTewksbury.html",
    "title": "Tyler Tewksbury",
    "section": "",
    "text": "2022 graduate with a B.A. in Economics from UMass Amherst. Worked as a Business Systems Analyst Intern at Epsilon in the Summer of 2022, and have been a DACSS Student Employee for over a year (Social Media currently, previously Instructional Design)."
  },
  {
    "objectID": "about/TylerTewksbury.html#r-experience",
    "href": "about/TylerTewksbury.html#r-experience",
    "title": "Tyler Tewksbury",
    "section": "R experience",
    "text": "R experience\nI have experience in R from classwork in Econometrics and Quantitative Research Methods, as well as working on 601 as an Instructional Designer."
  },
  {
    "objectID": "about/TylerTewksbury.html#research-interests",
    "href": "about/TylerTewksbury.html#research-interests",
    "title": "Tyler Tewksbury",
    "section": "Research interests",
    "text": "Research interests\nData Visualization, Autonomous Transportation"
  },
  {
    "objectID": "about/TylerTewksbury.html#hometown",
    "href": "about/TylerTewksbury.html#hometown",
    "title": "Tyler Tewksbury",
    "section": "Hometown",
    "text": "Hometown\nOriginally from Milton Massachusetts, but moved around a lot. Lived in New York, Maine, and now Amherst!"
  },
  {
    "objectID": "about/TylerTewksbury.html#hobbies",
    "href": "about/TylerTewksbury.html#hobbies",
    "title": "Tyler Tewksbury",
    "section": "Hobbies",
    "text": "Hobbies\nWeightlifting, Reading, Video Games, Crossword Puzzles, Geography."
  },
  {
    "objectID": "about/TylerTewksbury.html#fun-fact",
    "href": "about/TylerTewksbury.html#fun-fact",
    "title": "Tyler Tewksbury",
    "section": "Fun fact",
    "text": "Fun fact\nI met the creator of Pok√©mon in 2015! He sat at the table next to me in a restaurant in Boston."
  },
  {
    "objectID": "about/ProfRolfe.html",
    "href": "about/ProfRolfe.html",
    "title": "Meredith Rolfe",
    "section": "",
    "text": "Associate Professor, Political Science | UMass Amherst Lecturer in Public Management | London School of Economics Nuffield Posdoctoral Prize Fellow | Oxford University\nPhD, Political Science | University of Chicago BA, Comparative Area Studies | Duke University"
  },
  {
    "objectID": "about/ProfRolfe.html#r-experience",
    "href": "about/ProfRolfe.html#r-experience",
    "title": "Meredith Rolfe",
    "section": "R experience",
    "text": "R experience\nI was using R before it was cool (and when it was still called S‚Ä¶)"
  },
  {
    "objectID": "about/ProfRolfe.html#research-interests",
    "href": "about/ProfRolfe.html#research-interests",
    "title": "Meredith Rolfe",
    "section": "Research interests",
    "text": "Research interests\nTheories that combine social interaction and cognitive micro-foundations; innovative applications of methods of any sort (networks, text, simulations, experiments, surveys‚Ä¶)"
  },
  {
    "objectID": "about/ProfRolfe.html#hometown",
    "href": "about/ProfRolfe.html#hometown",
    "title": "Meredith Rolfe",
    "section": "Hometown",
    "text": "Hometown\nCharlotte, NC"
  },
  {
    "objectID": "about/ProfRolfe.html#hobbies",
    "href": "about/ProfRolfe.html#hobbies",
    "title": "Meredith Rolfe",
    "section": "Hobbies",
    "text": "Hobbies\num, DACSS???"
  },
  {
    "objectID": "about/ProfRolfe.html#fun-fact",
    "href": "about/ProfRolfe.html#fun-fact",
    "title": "Meredith Rolfe",
    "section": "Fun fact",
    "text": "Fun fact\ncoffee is my favorite food"
  },
  {
    "objectID": "about/LindsayJones.html",
    "href": "about/LindsayJones.html",
    "title": "Lindsay Jones",
    "section": "",
    "text": "B.A. - Sociology, UC San Diego\nMinor in Communication\nExecutive Assistant of a retirement community from 2019-2022."
  },
  {
    "objectID": "about/LindsayJones.html#r-experience",
    "href": "about/LindsayJones.html#r-experience",
    "title": "Lindsay Jones",
    "section": "R experience",
    "text": "R experience\nYou‚Äôre looking at it!"
  },
  {
    "objectID": "about/LindsayJones.html#research-interests",
    "href": "about/LindsayJones.html#research-interests",
    "title": "Lindsay Jones",
    "section": "Research interests",
    "text": "Research interests\nSocial Network Analysis, Digital Behaviors, Social Demography"
  },
  {
    "objectID": "about/LindsayJones.html#hometown",
    "href": "about/LindsayJones.html#hometown",
    "title": "Lindsay Jones",
    "section": "Hometown",
    "text": "Hometown\nSacramento, California"
  },
  {
    "objectID": "about/LindsayJones.html#hobbies",
    "href": "about/LindsayJones.html#hobbies",
    "title": "Lindsay Jones",
    "section": "Hobbies",
    "text": "Hobbies\nLately I‚Äôve enjoyed playing trumpet, playing Redactle, and making ice cream."
  },
  {
    "objectID": "about/LindsayJones.html#fun-fact",
    "href": "about/LindsayJones.html#fun-fact",
    "title": "Lindsay Jones",
    "section": "Fun fact",
    "text": "Fun fact\nI once made the mistake of climbing a waterfall in running shoes."
  },
  {
    "objectID": "about/NJani.html",
    "href": "about/NJani.html",
    "title": "Nayan Jani",
    "section": "",
    "text": "I received my B.S. in Data Science from the University of Rhode Island this past year. Over the summer of 2021 I worked for a startup company called Bora. At Bora, the product they are trying to bring to market is an app based, self service beach chair rental company stationed at the most popular locations across the country. My job was to collect data on certain locations and then run statistical analysis on that collected data to find out which locations are the busiest based on the characteristics of beaches. While working for Bora I was in constant contact with potential clients and the CEO of the company. Working for a startup showed me how a business functions from the ground up and how to interact with clients in a professional manner."
  },
  {
    "objectID": "about/NJani.html#r-experience",
    "href": "about/NJani.html#r-experience",
    "title": "Nayan Jani",
    "section": "R experience",
    "text": "R experience\nI have been working in R and R markdown since my sophomore year of college. I have mostly used it for statistical reports and model building for data sets. I only was introduced to the tidyverse for a short period of time so I will need to practice using its functions more."
  },
  {
    "objectID": "about/NJani.html#research-interests",
    "href": "about/NJani.html#research-interests",
    "title": "Nayan Jani",
    "section": "Research interests",
    "text": "Research interests\nThe topic I explored for my Senior recitation is the use of Machine Learning in estimating the heterogeneous treatment effect using datasets that involve AIDS and breast cancer treatments. More specifically, I looked into certain meta learners that can estimate the conditional average treatment effect so that we can personalize treatment regimes. Here at Umass I hope to learn more about how to apply data science to medical treatments for people with illnesses."
  },
  {
    "objectID": "about/NJani.html#hometown",
    "href": "about/NJani.html#hometown",
    "title": "Nayan Jani",
    "section": "Hometown",
    "text": "Hometown\nWestford, MA"
  },
  {
    "objectID": "about/NJani.html#hobbies",
    "href": "about/NJani.html#hobbies",
    "title": "Nayan Jani",
    "section": "Hobbies",
    "text": "Hobbies\n\nVideo Games\nFantasy Football\nWatching Sports\nPlaying Cards and Board Games"
  },
  {
    "objectID": "about/NJani.html#fun-fact",
    "href": "about/NJani.html#fun-fact",
    "title": "Nayan Jani",
    "section": "Fun fact",
    "text": "Fun fact\nI have over 50 cousins that live in the UK!"
  },
  {
    "objectID": "about/QuinnHe.html",
    "href": "about/QuinnHe.html",
    "title": "Quinn He",
    "section": "",
    "text": "I recently graduated from the University of Massachusetts Amherst in 2021 with a BA in English and a minor in Psychology. Afterwards, I worked for one year in retail while taking courses for the DACSS certificate. I wanted to further my education and applied to the DACSS MS program, which I am starting this Fall 2022."
  },
  {
    "objectID": "about/QuinnHe.html#r-experience",
    "href": "about/QuinnHe.html#r-experience",
    "title": "Quinn He",
    "section": "R experience",
    "text": "R experience\nI have beginner/intermediate experience in R. For the year year I was working, I would study R in my spare time to keep my skills sharp. I still have much to learn when it comes to R/RStudio"
  },
  {
    "objectID": "about/QuinnHe.html#research-interests",
    "href": "about/QuinnHe.html#research-interests",
    "title": "Quinn He",
    "section": "Research interests",
    "text": "Research interests\nI have not fully honed in my research interests, but I hope to come out of the program with extensive knowledge in the technical skills of data analysis and research."
  },
  {
    "objectID": "about/QuinnHe.html#hometown",
    "href": "about/QuinnHe.html#hometown",
    "title": "Quinn He",
    "section": "Hometown",
    "text": "Hometown\nWayne, NJ"
  },
  {
    "objectID": "about/QuinnHe.html#hobbies",
    "href": "about/QuinnHe.html#hobbies",
    "title": "Quinn He",
    "section": "Hobbies",
    "text": "Hobbies\nI enjoy playing guitar, biking, reading, and cooking."
  },
  {
    "objectID": "about/QuinnHe.html#fun-fact",
    "href": "about/QuinnHe.html#fun-fact",
    "title": "Quinn He",
    "section": "Fun fact",
    "text": "Fun fact\nI met Jesse Eisenberg at an empty movie theater in NYC."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601 August 2022",
    "section": "",
    "text": "Challenge 3 Solutions\n\n\n\n\n\n\n\nchallenge_3\n\n\nsolution\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Instructions\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3 Instructions\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nTyler Tewksbury\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Solutions\n\n\n\n\n\n\n\nchallenge_2\n\n\nsolution\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3 Instructions\n\n\n\n\n\n\n\nchallenge_3\n\n\nPivot\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nNayan Jani\n\n\n\n\n\n\n  \n\n\n\n\nChallenge 1 Solution\n\n\n\n\n\n\n\nchallenge_1\n\n\nsolution\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nStateCounty\n\n\n\n\nData wrangling: using group() and summarise(\n\n\n\n\n\n\nAug 16, 2022\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\nFAO\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Quinn He\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nrailroads\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Lindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\nNayan Jani\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\nRosemary\n\n\n\n\n\n\n\n\n\n\n\n\n\nTyler Tewksbury\n\n\n\n\n\n\n\nNo matching items"
  }
]