[
  {
    "objectID": "posts/challenge5_nickboonstra.html",
    "href": "posts/challenge5_nickboonstra.html",
    "title": "Challenge 5",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_nickboonstra.html#read-in-data",
    "href": "posts/challenge5_nickboonstra.html#read-in-data",
    "title": "Challenge 5",
    "section": "Read in data",
    "text": "Read in data\n\nrr_orig<-read_csv(\"_data/railroad_2012_clean_county.csv\")\n\nrr_orig\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nBriefly describe the data\nThis data set records railroad employment numbers in the U.S. (and certain overseas locations)\n\nprint(dfSummary(rr_orig, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nrr_orig\nDimensions: 2930 x 3\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      state\n[character]\n      1. TX2. GA3. KY4. MO5. IL6. IA7. KS8. NC9. IN10. VA[ 43 others ]\n      221(7.5%)152(5.2%)119(4.1%)115(3.9%)103(3.5%)99(3.4%)95(3.2%)94(3.2%)92(3.1%)92(3.1%)1748(59.7%)\n      \n      0\n(0.0%)\n    \n    \n      county\n[character]\n      1. WASHINGTON2. JEFFERSON3. FRANKLIN4. LINCOLN5. JACKSON6. MADISON7. MONTGOMERY8. CLAY9. MARION10. MONROE[ 1699 others ]\n      31(1.1%)26(0.9%)24(0.8%)24(0.8%)22(0.8%)19(0.6%)18(0.6%)17(0.6%)17(0.6%)17(0.6%)2715(92.7%)\n      \n      0\n(0.0%)\n    \n    \n      total_employees\n[numeric]\n      Mean (sd) : 87.2 (283.6)min ≤ med ≤ max:1 ≤ 21 ≤ 8207IQR (CV) : 58 (3.3)\n      404 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\nOne piece of information I was particularly curious about is how average employees per county compared across states:\n\nrr_orig %>% \n  group_by(state) %>% \n  summarise(mean_emp=mean(total_employees,na.rm=T)) %>% \n  arrange(desc(mean_emp)) %>% \n  slice(1:10)\n\n# A tibble: 10 × 2\n   state mean_emp\n   <chr>    <dbl>\n 1 DE        498.\n 2 NJ        397.\n 3 CT        324 \n 4 MA        282.\n 5 NY        280.\n 6 DC        279 \n 7 CA        239.\n 8 AZ        210.\n 9 PA        196.\n10 MD        196.\n\n\nThe data show that Delaware has the highest number of average employees per county. This finding becomes even more interesting when investigating how many (or few) counties Delaware has:\n\nrr_orig %>% \n  filter(state==\"DE\")\n\n# A tibble: 3 × 3\n  state county     total_employees\n  <chr> <chr>                <dbl>\n1 DE    KENT                   158\n2 DE    NEW CASTLE            1275\n3 DE    SUSSEX                  62\n\n\nClearly, New Castle county does a lot to offset the mean, especially given that the state of Delaware only has three counties. However, this is not the highest employment in the country:\n\nrr_orig %>% \n  arrange(desc(total_employees)) %>% \n  slice(1:10)\n\n# A tibble: 10 × 3\n   state county           total_employees\n   <chr> <chr>                      <dbl>\n 1 IL    COOK                        8207\n 2 TX    TARRANT                     4235\n 3 NE    DOUGLAS                     3797\n 4 NY    SUFFOLK                     3685\n 5 VA    INDEPENDENT CITY            3249\n 6 FL    DUVAL                       3073\n 7 CA    SAN BERNARDINO              2888\n 8 CA    LOS ANGELES                 2545\n 9 TX    HARRIS                      2535\n10 NE    LINCOLN                     2289\n\n\nPerhaps unsurprisingly, Cook County, IL – home of a major transit center in Chicago – employs the most railroad workers of any county in the country. A bit more surprisingly, New Castle County’s 1,000+ employees are not actually enough for it to register in the top ten counties!"
  },
  {
    "objectID": "posts/challenge5_nickboonstra.html#tidy-data-as-needed",
    "href": "posts/challenge5_nickboonstra.html#tidy-data-as-needed",
    "title": "Challenge 5",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nThese data are already tidy!"
  },
  {
    "objectID": "posts/challenge5_nickboonstra.html#visualization",
    "href": "posts/challenge5_nickboonstra.html#visualization",
    "title": "Challenge 5",
    "section": "Visualization",
    "text": "Visualization\nUsing ggplot2, I was able to create a visualization overlaying a density function on top of a histogram of average number of employees per county, when grouped by state.\n\nrr_orig %>% \n  group_by(state) %>% \n  summarise(mean_emp=mean(total_employees)) %>% \n  ggplot(aes(x=mean_emp)) +\n  geom_histogram(aes(y=..density..),bins=50,alpha=0.5,fill=\"red\") +\n  geom_density(fill=\"blue\",alpha=0.2) +\n  theme_bw() +\n  labs(title=\"Average Number of Employees per County, by State\",\n       x=\"Number of Employees\",\n       y=\"Density\")\n\n\n\n\nBecause this data set only contains one value, I was not sure how I would create a bivariate visualization."
  },
  {
    "objectID": "posts/challenge1_MekhalaKumar.html",
    "href": "posts/challenge1_MekhalaKumar.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_MekhalaKumar.html#datasets-used",
    "href": "posts/challenge1_MekhalaKumar.html#datasets-used",
    "title": "Challenge 1",
    "section": "Datasets used",
    "text": "Datasets used\nTwo datasets have been used: Railroad_2012_clean_county and birds.\n\n\nCode\n#1st Dataset Railroad_2012_clean_county\nlibrary(readr)\nrailroad_2012_clean_county <- read_csv(\"_data/railroad_2012_clean_county.csv\")\nview(railroad_2012_clean_county)\n#2nd Dataset birds\nbirds <- read_csv(\"_data/birds.csv\")\nView(birds)"
  },
  {
    "objectID": "posts/challenge1_MekhalaKumar.html#description-of-datasets",
    "href": "posts/challenge1_MekhalaKumar.html#description-of-datasets",
    "title": "Challenge 1",
    "section": "Description of Datasets",
    "text": "Description of Datasets\nThe first dataset is about the number of employees in each company. There are 3 variables as can be seen using the colnames command- state, county, number of employees. The number of employees is a continuous variable. The data was gathered from several states as seen in the table.\nThe second dataset has 14 columns and 30977 observations. From colnames, we get to know that the dataset gives us the values of the dietary energy intake for different countries across different years. Data types of the columns, value could actually be converted into double type. There were around 11000 missing values found and removed from the data. Many countries were included in this dataset and there are 6 types of birds but only one domain of animals present.\nA plot was created to visualise the changes in the Value across the years.It can be seen that the values have increased over time. A plot was also created to visualise the changes in a specific country, in this case, the USA.\n\n\nCode\n#1st dataset Railroad_2012_clean_county\n\ncolnames(railroad_2012_clean_county)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nCode\nstates<-select(railroad_2012_clean_county,state)\ntable(states)\n\n\nstate\n AE  AK  AL  AP  AR  AZ  CA  CO  CT  DC  DE  FL  GA  HI  IA  ID  IL  IN  KS  KY \n  1   6  67   1  72  15  55  57   8   1   3  67 152   3  99  36 103  92  95 119 \n LA  MA  MD  ME  MI  MN  MO  MS  MT  NC  ND  NE  NH  NJ  NM  NV  NY  OH  OK  OR \n 63  12  24  16  78  86 115  78  53  94  49  89  10  21  29  12  61  88  73  33 \n PA  RI  SC  SD  TN  TX  UT  VA  VT  WA  WI  WV  WY \n 65   5  46  52  91 221  25  92  14  39  69  53  22 \n\n\nCode\nprop.table(table(states))\n\n\nstate\n          AE           AK           AL           AP           AR           AZ \n0.0003412969 0.0020477816 0.0228668942 0.0003412969 0.0245733788 0.0051194539 \n          CA           CO           CT           DC           DE           FL \n0.0187713311 0.0194539249 0.0027303754 0.0003412969 0.0010238908 0.0228668942 \n          GA           HI           IA           ID           IL           IN \n0.0518771331 0.0010238908 0.0337883959 0.0122866894 0.0351535836 0.0313993174 \n          KS           KY           LA           MA           MD           ME \n0.0324232082 0.0406143345 0.0215017065 0.0040955631 0.0081911263 0.0054607509 \n          MI           MN           MO           MS           MT           NC \n0.0266211604 0.0293515358 0.0392491468 0.0266211604 0.0180887372 0.0320819113 \n          ND           NE           NH           NJ           NM           NV \n0.0167235495 0.0303754266 0.0034129693 0.0071672355 0.0098976109 0.0040955631 \n          NY           OH           OK           OR           PA           RI \n0.0208191126 0.0300341297 0.0249146758 0.0112627986 0.0221843003 0.0017064846 \n          SC           SD           TN           TX           UT           VA \n0.0156996587 0.0177474403 0.0310580205 0.0754266212 0.0085324232 0.0313993174 \n          VT           WA           WI           WV           WY \n0.0047781570 0.0133105802 0.0235494881 0.0180887372 0.0075085324 \n\n\nCode\n#2nd dataset birds\n\ndim(birds) \n\n\n[1] 30977    14\n\n\nCode\ncolnames(birds)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nCode\nstr(birds)\n\n\nspec_tbl_df [30,977 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Domain Code     : chr [1:30977] \"QA\" \"QA\" \"QA\" \"QA\" ...\n $ Domain          : chr [1:30977] \"Live Animals\" \"Live Animals\" \"Live Animals\" \"Live Animals\" ...\n $ Area Code       : num [1:30977] 2 2 2 2 2 2 2 2 2 2 ...\n $ Area            : chr [1:30977] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ Element Code    : num [1:30977] 5112 5112 5112 5112 5112 ...\n $ Element         : chr [1:30977] \"Stocks\" \"Stocks\" \"Stocks\" \"Stocks\" ...\n $ Item Code       : num [1:30977] 1057 1057 1057 1057 1057 ...\n $ Item            : chr [1:30977] \"Chickens\" \"Chickens\" \"Chickens\" \"Chickens\" ...\n $ Year Code       : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Year            : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Unit            : chr [1:30977] \"1000 Head\" \"1000 Head\" \"1000 Head\" \"1000 Head\" ...\n $ Value           : num [1:30977] 4700 4900 5000 5300 5500 5800 6600 6290 6300 6000 ...\n $ Flag            : chr [1:30977] \"F\" \"F\" \"F\" \"F\" ...\n $ Flag Description: chr [1:30977] \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Domain Code` = col_character(),\n  ..   Domain = col_character(),\n  ..   `Area Code` = col_double(),\n  ..   Area = col_character(),\n  ..   `Element Code` = col_double(),\n  ..   Element = col_character(),\n  ..   `Item Code` = col_double(),\n  ..   Item = col_character(),\n  ..   `Year Code` = col_double(),\n  ..   Year = col_double(),\n  ..   Unit = col_character(),\n  ..   Value = col_double(),\n  ..   Flag = col_character(),\n  ..   `Flag Description` = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nCode\nbirds <- transform(birds,value1 = as.numeric(Value))\nsum(is.na(birds))\n\n\n[1] 12845\n\n\nCode\nbirds<-na.omit(birds)\ndim(birds)\n\n\n[1] 19168    15\n\n\nCode\narea<-select(birds,Area)\ntable(area)\n\n\nArea\n                                         Afghanistan \n                                                  31 \n                                              Africa \n                                                 290 \n                                             Albania \n                                                  62 \n                                             Algeria \n                                                 208 \n                                      American Samoa \n                                                  53 \n                                            Americas \n                                                 232 \n                                              Angola \n                                                  49 \n                                 Antigua and Barbuda \n                                                  53 \n                                           Argentina \n                                                 132 \n                                             Armenia \n                                                  39 \n                                                Asia \n                                                 290 \n                                           Australia \n                                                  47 \n                           Australia and New Zealand \n                                                 232 \n                                             Austria \n                                                  63 \n                                          Azerbaijan \n                                                  46 \n                                             Bahamas \n                                                  57 \n                                             Bahrain \n                                                  48 \n                                          Bangladesh \n                                                  46 \n                                            Barbados \n                                                  79 \n                                             Belarus \n                                                  68 \n                                             Belgium \n                                                  36 \n                                              Belize \n                                                 151 \n                                               Benin \n                                                  24 \n                                             Bermuda \n                                                  44 \n                                              Bhutan \n                                                  29 \n                    Bolivia (Plurinational State of) \n                                                 125 \n                              Bosnia and Herzegovina \n                                                  80 \n                                            Botswana \n                                                  29 \n                                              Brazil \n                                                  94 \n                                   Brunei Darussalam \n                                                  35 \n                                            Bulgaria \n                                                  59 \n                                        Burkina Faso \n                                                  40 \n                                             Burundi \n                                                  45 \n                                          Cabo Verde \n                                                  38 \n                                            Cambodia \n                                                  65 \n                                            Cameroon \n                                                  31 \n                                              Canada \n                                                 146 \n                                           Caribbean \n                                                 232 \n                                      Cayman Islands \n                                                  31 \n                            Central African Republic \n                                                  69 \n                                     Central America \n                                                 174 \n                                        Central Asia \n                                                 108 \n                                                Chad \n                                                  58 \n                                               Chile \n                                                  53 \n                                China, Hong Kong SAR \n                                                 124 \n                                    China, Macao SAR \n                                                  58 \n                                     China, mainland \n                                                 168 \n                                            Colombia \n                                                  45 \n                                             Comoros \n                                                  58 \n                                               Congo \n                                                  41 \n                                        Cook Islands \n                                                  55 \n                                          Costa Rica \n                                                  43 \n                                       Côte d'Ivoire \n                                                  28 \n                                             Croatia \n                                                  27 \n                                                Cuba \n                                                   3 \n                                              Cyprus \n                                                 196 \n                                             Czechia \n                                                   1 \n                                      Czechoslovakia \n                                                   1 \n               Democratic People's Republic of Korea \n                                                  60 \n                    Democratic Republic of the Congo \n                                                  17 \n                                             Denmark \n                                                   2 \n                                            Dominica \n                                                  58 \n                                  Dominican Republic \n                                                  41 \n                                      Eastern Africa \n                                                 232 \n                                        Eastern Asia \n                                                 290 \n                                      Eastern Europe \n                                                 232 \n                                             Ecuador \n                                                 147 \n                                               Egypt \n                                                  92 \n                                         El Salvador \n                                                  31 \n                                   Equatorial Guinea \n                                                 112 \n                                             Eritrea \n                                                  23 \n                                             Estonia \n                                                  90 \n                                            Eswatini \n                                                  22 \n                                            Ethiopia \n                                                   8 \n                                        Ethiopia PDR \n                                                  16 \n                                              Europe \n                                                 290 \n                         Falkland Islands (Malvinas) \n                                                  25 \n                                                Fiji \n                                                 171 \n                                             Finland \n                                                  16 \n                                              France \n                                                  74 \n                                       French Guyana \n                                                  80 \n                                    French Polynesia \n                                                 115 \n                                               Gabon \n                                                  49 \n                                              Gambia \n                                                  30 \n                                             Georgia \n                                                  43 \n                                             Germany \n                                                  40 \n                                               Ghana \n                                                  17 \n                                              Greece \n                                                  25 \n                                             Grenada \n                                                  51 \n                                          Guadeloupe \n                                                 140 \n                                                Guam \n                                                  48 \n                                           Guatemala \n                                                  29 \n                                              Guinea \n                                                  26 \n                                       Guinea-Bissau \n                                                  44 \n                                              Guyana \n                                                  48 \n                                               Haiti \n                                                 218 \n                                            Honduras \n                                                  33 \n                                               India \n                                                  79 \n                                           Indonesia \n                                                  13 \n                          Iran (Islamic Republic of) \n                                                 214 \n                                                Iraq \n                                                  48 \n                                             Ireland \n                                                 108 \n                                              Israel \n                                                  94 \n                                               Italy \n                                                 103 \n                                             Jamaica \n                                                  56 \n                                               Japan \n                                                  60 \n                                              Jordan \n                                                 137 \n                                          Kazakhstan \n                                                  41 \n                                               Kenya \n                                                  22 \n                                            Kiribati \n                                                  56 \n                                              Kuwait \n                                                  13 \n                                          Kyrgyzstan \n                                                  51 \n                    Lao People's Democratic Republic \n                                                 117 \n                                              Latvia \n                                                  13 \n                                             Lebanon \n                                                  46 \n                                             Lesotho \n                                                  39 \n                                             Liberia \n                                                 116 \n                                               Libya \n                                                  37 \n                                           Lithuania \n                                                  24 \n                                          Madagascar \n                                                 218 \n                                              Malawi \n                                                  50 \n                                            Malaysia \n                                                  50 \n                                                Mali \n                                                  30 \n                                               Malta \n                                                  76 \n                                          Martinique \n                                                  65 \n                                          Mauritania \n                                                  53 \n                                           Mauritius \n                                                 193 \n                                           Melanesia \n                                                 174 \n                                              Mexico \n                                                  68 \n                                          Micronesia \n                                                 111 \n                    Micronesia (Federated States of) \n                                                  48 \n                                       Middle Africa \n                                                 174 \n                                            Mongolia \n                                                   2 \n                                          Montenegro \n                                                   3 \n                                          Montserrat \n                                                  53 \n                                             Morocco \n                                                  70 \n                                          Mozambique \n                                                  98 \n                                             Myanmar \n                                                 112 \n                                             Namibia \n                                                  70 \n                                               Nauru \n                                                  58 \n                                               Nepal \n                                                  48 \n                                         Netherlands \n                                                  10 \n                       Netherlands Antilles (former) \n                                                  58 \n                                       New Caledonia \n                                                  48 \n                                         New Zealand \n                                                 161 \n                                           Nicaragua \n                                                  45 \n                                               Niger \n                                                  17 \n                                             Nigeria \n                                                  13 \n                                                Niue \n                                                  47 \n                                     North Macedonia \n                                                   2 \n                                     Northern Africa \n                                                 290 \n                                    Northern America \n                                                 232 \n                                     Northern Europe \n                                                 232 \n                                              Norway \n                                                  17 \n                                             Oceania \n                                                 232 \n                                                Oman \n                                                  52 \n                     Pacific Islands Trust Territory \n                                                  28 \n                                            Pakistan \n                                                  94 \n                                           Palestine \n                                                  19 \n                                              Panama \n                                                 106 \n                                    Papua New Guinea \n                                                 140 \n                                            Paraguay \n                                                  87 \n                                         Philippines \n                                                 108 \n                                           Polynesia \n                                                 116 \n                                            Portugal \n                                                  92 \n                                         Puerto Rico \n                                                  15 \n                                               Qatar \n                                                  32 \n                                   Republic of Korea \n                                                  12 \n                                 Republic of Moldova \n                                                  52 \n                                             Réunion \n                                                 107 \n                                             Romania \n                                                 156 \n                                  Russian Federation \n                                                  45 \n                                              Rwanda \n                                                  73 \n        Saint Helena, Ascension and Tristan da Cunha \n                                                  30 \n                               Saint Kitts and Nevis \n                                                  55 \n                                         Saint Lucia \n                                                  40 \n                           Saint Pierre and Miquelon \n                                                  41 \n                    Saint Vincent and the Grenadines \n                                                  44 \n                                               Samoa \n                                                  50 \n                               Sao Tome and Principe \n                                                 168 \n                                        Saudi Arabia \n                                                  88 \n                                             Senegal \n                                                   5 \n                               Serbia and Montenegro \n                                                  53 \n                                          Seychelles \n                                                 100 \n                                        Sierra Leone \n                                                  78 \n                                           Singapore \n                                                  73 \n                                            Slovakia \n                                                   2 \n                                            Slovenia \n                                                  36 \n                                     Solomon Islands \n                                                  57 \n                                             Somalia \n                                                  58 \n                                        South Africa \n                                                 193 \n                                       South America \n                                                 232 \n                                         South Sudan \n                                                   7 \n                                  South-eastern Asia \n                                                 290 \n                                     Southern Africa \n                                                 261 \n                                       Southern Asia \n                                                 232 \n                                     Southern Europe \n                                                 290 \n                                               Spain \n                                                  91 \n                                           Sri Lanka \n                                                  10 \n                                      Sudan (former) \n                                                  30 \n                                            Suriname \n                                                  47 \n                                              Sweden \n                                                  49 \n                                         Switzerland \n                                                  20 \n                                Syrian Arab Republic \n                                                  32 \n                                            Thailand \n                                                  17 \n                                         Timor-Leste \n                                                  46 \n                                                Togo \n                                                  11 \n                                             Tokelau \n                                                  56 \n                                               Tonga \n                                                  32 \n                                 Trinidad and Tobago \n                                                  47 \n                                             Tunisia \n                                                  50 \n                                              Turkey \n                                                  56 \n                                        Turkmenistan \n                                                  49 \n                                              Tuvalu \n                                                  45 \n                                              Uganda \n                                                  24 \n                                United Arab Emirates \n                                                  49 \nUnited Kingdom of Great Britain and Northern Ireland \n                                                  28 \n                         United Republic of Tanzania \n                                                  97 \n                            United States of America \n                                                 116 \n                        United States Virgin Islands \n                                                  41 \n                                             Uruguay \n                                                 211 \n                                                USSR \n                                                  12 \n                                          Uzbekistan \n                                                  54 \n                                             Vanuatu \n                                                  51 \n                  Venezuela (Bolivarian Republic of) \n                                                  29 \n                                            Viet Nam \n                                                  79 \n                           Wallis and Futuna Islands \n                                                  54 \n                                      Western Africa \n                                                 116 \n                                        Western Asia \n                                                 290 \n                                      Western Europe \n                                                 290 \n                                               World \n                                                 290 \n                                               Yemen \n                                                  52 \n                                        Yugoslav SFR \n                                                   4 \n                                              Zambia \n                                                  49 \n                                            Zimbabwe \n                                                 158 \n\n\nCode\nitem<-select(birds,Item)\ntable(item)\n\n\nItem\n              Chickens                  Ducks Geese and guinea fowls \n                  7698                   4357                   2599 \n  Pigeons, other birds                Turkeys \n                   903                   3611 \n\n\nCode\ndomain<-select(birds,Domain)\ntable(domain)\n\n\nDomain\nLive Animals \n       19168 \n\n\nCode\nplot(value1~Year,birds)\n\n\n\n\n\nCode\nbirds_USA<-birds%>% filter(`Area`=='United States of America')\nplot(value1~Year,birds_USA)"
  },
  {
    "objectID": "posts/Yakub_HW2_.html",
    "href": "posts/Yakub_HW2_.html",
    "title": "HW 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\nLoaded the Fed Funds Dataset\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/Yakub_HW2_.html#describe-the-data",
    "href": "posts/Yakub_HW2_.html#describe-the-data",
    "title": "HW 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nDoing a Head of the Dataset to get a view of what the Data Looks like.\n\n\nCode\nhead(fedfunds)\n\n\n# A tibble: 6 × 10\n   Year Month   Day Federal Fu…¹ Feder…² Feder…³ Effec…⁴ Real …⁵ Unemp…⁶ Infla…⁷\n  <dbl> <dbl> <dbl>        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1  1954     7     1           NA      NA      NA    0.8      4.6     5.8      NA\n2  1954     8     1           NA      NA      NA    1.22    NA       6        NA\n3  1954     9     1           NA      NA      NA    1.06    NA       6.1      NA\n4  1954    10     1           NA      NA      NA    0.85     8       5.7      NA\n5  1954    11     1           NA      NA      NA    0.83    NA       5.3      NA\n6  1954    12     1           NA      NA      NA    1.28    NA       5        NA\n# … with abbreviated variable names ¹​`Federal Funds Target Rate`,\n#   ²​`Federal Funds Upper Target`, ³​`Federal Funds Lower Target`,\n#   ⁴​`Effective Federal Funds Rate`, ⁵​`Real GDP (Percent Change)`,\n#   ⁶​`Unemployment Rate`, ⁷​`Inflation Rate`\n\n\nGetting the column names\n\n\nCode\ncolnames(fedfunds)\n\n\n [1] \"Year\"                         \"Month\"                       \n [3] \"Day\"                          \"Federal Funds Target Rate\"   \n [5] \"Federal Funds Upper Target\"   \"Federal Funds Lower Target\"  \n [7] \"Effective Federal Funds Rate\" \"Real GDP (Percent Change)\"   \n [9] \"Unemployment Rate\"            \"Inflation Rate\"              \n\n\nRenaming columns as I had difficulty invoking the column name in code.\n\n\nCode\nnames(fedfunds)[names(fedfunds) == \"Real GDP (Percent Change)\"] <- \"Real GDP\"\ncolnames(fedfunds)\n\n\n [1] \"Year\"                         \"Month\"                       \n [3] \"Day\"                          \"Federal Funds Target Rate\"   \n [5] \"Federal Funds Upper Target\"   \"Federal Funds Lower Target\"  \n [7] \"Effective Federal Funds Rate\" \"Real GDP\"                    \n [9] \"Unemployment Rate\"            \"Inflation Rate\"              \n\n\nSummary of the Datset and found a lot of NAs to clean up.\n\n\nCode\nsummary(fedfunds)\n\n\n      Year          Month             Day         Federal Funds Target Rate\n Min.   :1954   Min.   : 1.000   Min.   : 1.000   Min.   : 1.000           \n 1st Qu.:1973   1st Qu.: 4.000   1st Qu.: 1.000   1st Qu.: 3.750           \n Median :1988   Median : 7.000   Median : 1.000   Median : 5.500           \n Mean   :1987   Mean   : 6.598   Mean   : 3.598   Mean   : 5.658           \n 3rd Qu.:2001   3rd Qu.:10.000   3rd Qu.: 1.000   3rd Qu.: 7.750           \n Max.   :2017   Max.   :12.000   Max.   :31.000   Max.   :11.500           \n                                                  NA's   :442              \n Federal Funds Upper Target Federal Funds Lower Target\n Min.   :0.2500             Min.   :0.0000            \n 1st Qu.:0.2500             1st Qu.:0.0000            \n Median :0.2500             Median :0.0000            \n Mean   :0.3083             Mean   :0.0583            \n 3rd Qu.:0.2500             3rd Qu.:0.0000            \n Max.   :1.0000             Max.   :0.7500            \n NA's   :801                NA's   :801               \n Effective Federal Funds Rate    Real GDP       Unemployment Rate\n Min.   : 0.070               Min.   :-10.000   Min.   : 3.400   \n 1st Qu.: 2.428               1st Qu.:  1.400   1st Qu.: 4.900   \n Median : 4.700               Median :  3.100   Median : 5.700   \n Mean   : 4.911               Mean   :  3.138   Mean   : 5.979   \n 3rd Qu.: 6.580               3rd Qu.:  4.875   3rd Qu.: 7.000   \n Max.   :19.100               Max.   : 16.500   Max.   :10.800   \n NA's   :152                  NA's   :654       NA's   :152      \n Inflation Rate  \n Min.   : 0.600  \n 1st Qu.: 2.000  \n Median : 2.800  \n Mean   : 3.733  \n 3rd Qu.: 4.700  \n Max.   :13.600  \n NA's   :194     \n\n\nDropping all NAs\n\n\nCode\nyear_metrics <- fedfunds %>%\n  drop_na(Year,`Real GDP`,`Inflation Rate`,`Unemployment Rate`,`Effective Federal Funds Rate`)\nyear_metrics\n\n\n# A tibble: 236 × 10\n    Year Month   Day Federal F…¹ Feder…² Feder…³ Effec…⁴ Real …⁵ Unemp…⁶ Infla…⁷\n   <dbl> <dbl> <dbl>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  1958     1     1          NA      NA      NA    2.72   -10       5.8     3.2\n 2  1958     4     1          NA      NA      NA    1.26     2.6     7.4     2.4\n 3  1958     7     1          NA      NA      NA    0.68     9.6     7.5     2.4\n 4  1958    10     1          NA      NA      NA    1.8      9.7     6.7     1.7\n 5  1959     1     1          NA      NA      NA    2.48     7.7     6       1.7\n 6  1959     4     1          NA      NA      NA    2.96    10.1     5.2     1.7\n 7  1959     7     1          NA      NA      NA    3.47    -0.8     5.1     2  \n 8  1959    10     1          NA      NA      NA    3.98     1.6     5.7     2.7\n 9  1960     1     1          NA      NA      NA    3.99     9.2     5.2     2  \n10  1960     4     1          NA      NA      NA    3.92    -1.5     5.2     2  \n# … with 226 more rows, and abbreviated variable names\n#   ¹​`Federal Funds Target Rate`, ²​`Federal Funds Upper Target`,\n#   ³​`Federal Funds Lower Target`, ⁴​`Effective Federal Funds Rate`,\n#   ⁵​`Real GDP`, ⁶​`Unemployment Rate`, ⁷​`Inflation Rate`\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\nsummary(year_metrics)\n\n\n      Year          Month            Day    Federal Funds Target Rate\n Min.   :1958   Min.   : 1.00   Min.   :1   Min.   : 1.000           \n 1st Qu.:1972   1st Qu.: 3.25   1st Qu.:1   1st Qu.: 3.750           \n Median :1987   Median : 5.50   Median :1   Median : 5.250           \n Mean   :1987   Mean   : 5.50   Mean   :1   Mean   : 5.407           \n 3rd Qu.:2002   3rd Qu.: 7.75   3rd Qu.:1   3rd Qu.: 7.000           \n Max.   :2016   Max.   :10.00   Max.   :1   Max.   :11.000           \n                                            NA's   :131              \n Federal Funds Upper Target Federal Funds Lower Target\n Min.   :0.2500             Min.   :0.00000           \n 1st Qu.:0.2500             1st Qu.:0.00000           \n Median :0.2500             Median :0.00000           \n Mean   :0.2812             Mean   :0.03125           \n 3rd Qu.:0.2500             3rd Qu.:0.00000           \n Max.   :0.5000             Max.   :0.25000           \n NA's   :204                NA's   :204               \n Effective Federal Funds Rate    Real GDP       Unemployment Rate\n Min.   : 0.070               Min.   :-10.000   Min.   : 3.400   \n 1st Qu.: 2.655               1st Qu.:  1.400   1st Qu.: 5.000   \n Median : 4.845               Median :  3.100   Median : 5.700   \n Mean   : 5.084               Mean   :  3.116   Mean   : 6.074   \n 3rd Qu.: 6.875               3rd Qu.:  4.800   3rd Qu.: 7.100   \n Max.   :19.080               Max.   : 16.500   Max.   :10.400   \n                                                                 \n Inflation Rate  \n Min.   : 0.600  \n 1st Qu.: 2.000  \n Median : 2.800  \n Mean   : 3.740  \n 3rd Qu.: 4.725  \n Max.   :13.000  \n                 \n\n\nI tried to use a line graph but it looks messy.\n\n\nCode\nlibrary(ggplot2)\nggplot(data=year_metrics, aes(x=`Year`, y=`Real GDP`))+\n  geom_line()\n\n\n\n\n\nThis Bar Graph came out much cleaner looking.\n\n\nCode\nperctgdp<-ggplot(year_metrics, aes(x =`Year`, y =`Real GDP`,fill=`Real GDP`)) + geom_bar(stat = \"identity\")\nperctgdp\n\n\n\n\n\nI added limits and breaks to see the most recent year 2017.\n\n\nCode\nperctgdp<-perctgdp+scale_x_continuous(limits=c(1954,2017),breaks=seq(1954,2017,5))\n\n\nRelabeled the Y axis to make to clear this not that actual GDP rate but the Percentage change of the GDP\n\n\nCode\nprint(perctgdp + labs(\n  title = \"GDP Percentage Change by Year\",\n  y = \"GDP Percentage Change\", x = \"Year\"\n))\n\n\n\n\n\n\n\nCode\nggplot(year_metrics) +\n  aes(x = `Unemployment Rate`, y = `Inflation Rate`) +\n  geom_point(colour = \"#0c4c8a\") +\n  theme_minimal()\n\n\n\n\n\nI did a Correlation test of Unemployment rate and the Inflation rate. It got only a moderate Correlation.\n\n\nCode\ncor(year_metrics$`Unemployment Rate`,year_metrics$`Inflation Rate`)\n\n\n[1] 0.2095474\n\n\nTrying to Graph both the Unemployment Rate and Inflation Rate by Year\n\n\nCode\ndf <- year_metrics %>%\n  select(Year, `Unemployment Rate`, `Inflation Rate`) %>%\n  gather(key = \"variable\", value = \"value\", -Year)\n\n\nGraph that shows inflation rate vs Unemployment Rate by Year\nAs shown here, there can be periods of High Inflation and low unemployment which is why there isn’t a strong correlation.\n\n\nCode\nggplot(df, aes(x =`Year`, y = value)) + \n  geom_line(aes(color = variable, linetype = variable)) + \n  scale_color_manual(values = c(\"darkred\", \"steelblue\"))+labs(\n  title = \"Inflation and UnEmployment Rates by Year\",\n  y = \"Variable Rates\", x = \"Year\")"
  },
  {
    "objectID": "posts/challenge5_Mekhala Kumar.html",
    "href": "posts/challenge5_Mekhala Kumar.html",
    "title": "Challenge 5",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(readr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_Mekhala Kumar.html#public-school-characteristics",
    "href": "posts/challenge5_Mekhala Kumar.html#public-school-characteristics",
    "title": "Challenge 5",
    "section": "Public School Characteristics",
    "text": "Public School Characteristics\n\nPublicSchoolChar <- read_csv(\"_data/Public_School_Characteristics_2017-18.csv\")\ndim(PublicSchoolChar)\n\n[1] 100729     79\n\nprint(summarytools::dfSummary(PublicSchoolChar,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nPublicSchoolChar\nDimensions: 100729 x 79\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      X\n[numeric]\n      Mean (sd) : -92.9 (16.9)min ≤ med ≤ max:-176.6 ≤ -89.3 ≤ 144.9IQR (CV) : 20.2 (-0.2)\n      97136 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Y\n[numeric]\n      Mean (sd) : 37.8 (5.8)min ≤ med ≤ max:-14.3 ≤ 38.8 ≤ 71.3IQR (CV) : 7.7 (0.2)\n      97136 distinct values\n      \n      0\n(0.0%)\n    \n    \n      OBJECTID\n[numeric]\n      Mean (sd) : 50365 (29078.1)min ≤ med ≤ max:1 ≤ 50365 ≤ 100729IQR (CV) : 50364 (0.6)\n      100729 distinct values\n      \n      0\n(0.0%)\n    \n    \n      NCESSCH\n[character]\n      1. 0100005008702. 0100005008713. 0100005008794. 0100005008895. 0100005016166. 0100005021507. 0100006001938. 0100006008729. 01000060087610. 010000600877[ 100719 others ]\n      1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)100719(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      NMCNTY\n[character]\n      1. Los Angeles County2. Cook County3. Maricopa County4. Harris County5. Orange County6. Jefferson County7. Montgomery County8. Washington County9. Wayne County10. Dallas County[ 1949 others ]\n      2264(2.2%)1388(1.4%)1256(1.2%)1142(1.1%)1074(1.1%)980(1.0%)888(0.9%)848(0.8%)817(0.8%)814(0.8%)89258(88.6%)\n      \n      0\n(0.0%)\n    \n    \n      SURVYEAR\n[character]\n      1. 2017-2018\n      100729(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      STABR\n[character]\n      1. CA2. TX3. NY4. FL5. IL6. MI7. OH8. PA9. NC10. NJ[ 46 others ]\n      10323(10.2%)9320(9.3%)4808(4.8%)4375(4.3%)4245(4.2%)3734(3.7%)3610(3.6%)2990(3.0%)2691(2.7%)2595(2.6%)52038(51.7%)\n      \n      0\n(0.0%)\n    \n    \n      LEAID\n[character]\n      1. 72000302. 06227103. 17099304. 12003905. 32000606. 12001807. 12008708. 15000309. 482364010. 1201500[ 17451 others ]\n      1121(1.1%)1009(1.0%)655(0.7%)537(0.5%)381(0.4%)336(0.3%)320(0.3%)294(0.3%)284(0.3%)268(0.3%)95524(94.8%)\n      \n      0\n(0.0%)\n    \n    \n      ST_LEAID\n[character]\n      1. PR-012. CA-19647333. IL-15-016-2990-254. FL-135. NV-026. FL-067. FL-298. HI-0019. TX-10191210. FL-50[ 17451 others ]\n      1121(1.1%)1009(1.0%)655(0.7%)537(0.5%)381(0.4%)336(0.3%)320(0.3%)294(0.3%)284(0.3%)268(0.3%)95524(94.8%)\n      \n      0\n(0.0%)\n    \n    \n      LEA_NAME\n[character]\n      1. PUERTO RICO DEPARTMENT OF2. Los Angeles Unified3. City of Chicago SD 2994. DADE5. CLARK COUNTY SCHOOL DISTR6. BROWARD7. HILLSBOROUGH8. Hawaii Department of Educ9. HOUSTON ISD10. PALM BEACH[ 17147 others ]\n      1121(1.1%)1009(1.0%)655(0.7%)537(0.5%)381(0.4%)336(0.3%)320(0.3%)294(0.3%)284(0.3%)268(0.3%)95524(94.8%)\n      \n      0\n(0.0%)\n    \n    \n      SCH_NAME\n[character]\n      1. Lincoln Elementary School2. Lincoln Elementary3. Jefferson Elementary4. Washington Elementary5. Washington Elementary Sch6. Central Elementary School7. Jefferson Elementary Scho8. Lincoln Elem School9. Central High School10. Roosevelt Elementary[ 88366 others ]\n      64(0.1%)61(0.1%)53(0.1%)49(0.0%)46(0.0%)42(0.0%)33(0.0%)33(0.0%)32(0.0%)32(0.0%)100284(99.6%)\n      \n      0\n(0.0%)\n    \n    \n      LSTREET1\n[character]\n      1. 6420 E. Broadway Blvd. Su2. Box DOE3. 2405 FAIRVIEW SCHOOL RD4. 1820 XENIUM LN N5. Main St6. 335 ALTERNATIVE LN7. 2101 N TWYMAN RD8. 720 9TH AVE9. 50 Moreland Rd.10. 951 W Snowflake Blvd[ 92384 others ]\n      33(0.0%)28(0.0%)22(0.0%)19(0.0%)13(0.0%)12(0.0%)11(0.0%)11(0.0%)10(0.0%)10(0.0%)100560(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      LSTREET2\n[character]\n      1. Suite B2. Ste. 1003. P.O. Box 14974. Suite A5. Suite 2006. Building B7. Ste. 1028. Ste. A9. Suite 110. SUITE 111 HART[ 482 others ]\n      8(1.4%)7(1.2%)6(1.0%)6(1.0%)5(0.8%)4(0.7%)4(0.7%)4(0.7%)4(0.7%)4(0.7%)540(91.2%)\n      \n      100137\n(99.4%)\n    \n    \n      LSTREET3\n[logical]\n      All NA's\n      \n      \n      100729\n(100.0%)\n    \n    \n      LCITY\n[character]\n      1. HOUSTON2. Chicago3. Los Angeles4. BROOKLYN5. SAN ANTONIO6. Phoenix7. BRONX8. DALLAS9. NEW YORK10. Tucson[ 14624 others ]\n      783(0.8%)664(0.7%)577(0.6%)569(0.6%)520(0.5%)446(0.4%)441(0.4%)378(0.4%)359(0.4%)330(0.3%)95662(95.0%)\n      \n      0\n(0.0%)\n    \n    \n      LSTATE\n[character]\n      1. CA2. TX3. NY4. FL5. IL6. MI7. OH8. PA9. NC10. NJ[ 45 others ]\n      10325(10.3%)9320(9.3%)4808(4.8%)4377(4.3%)4245(4.2%)3736(3.7%)3610(3.6%)2990(3.0%)2693(2.7%)2595(2.6%)52030(51.7%)\n      \n      0\n(0.0%)\n    \n    \n      LZIP\n[character]\n      1. 857102. 104563. 853644. 785215. 785726. 785777. 007318. 104579. 7853910. 60623[ 22526 others ]\n      53(0.1%)45(0.0%)44(0.0%)43(0.0%)42(0.0%)41(0.0%)39(0.0%)37(0.0%)37(0.0%)36(0.0%)100312(99.6%)\n      \n      0\n(0.0%)\n    \n    \n      LZIP4\n[character]\n      1. 88882. 11993. 12994. 98015. 20996. 13997. 16998. 15999. 149910. 1899[ 8615 others ]\n      899(1.5%)113(0.2%)111(0.2%)106(0.2%)104(0.2%)101(0.2%)100(0.2%)99(0.2%)94(0.2%)89(0.2%)57411(96.9%)\n      \n      41502\n(41.2%)\n    \n    \n      PHONE\n[character]\n      1. (505)880-37442. (520)225-60603. (505)721-10514. (480)461-40005. (972)316-36636. (505)527-58007. (520)745-45888. (480)497-33009. (623)445-500010. (480)484-6100[ 91818 others ]\n      141(0.1%)63(0.1%)36(0.0%)35(0.0%)34(0.0%)33(0.0%)33(0.0%)29(0.0%)28(0.0%)27(0.0%)100270(99.5%)\n      \n      0\n(0.0%)\n    \n    \n      GSLO\n[character]\n      1. PK2. KG3. 094. 065. 076. 057. 038. 049. M10. 01[ 8 others ]\n      31179(31.0%)23839(23.7%)16627(16.5%)12912(12.8%)5441(5.4%)2578(2.6%)1581(1.6%)1165(1.2%)1113(1.1%)964(1.0%)3330(3.3%)\n      \n      0\n(0.0%)\n    \n    \n      GSHI\n[character]\n      1. 052. 123. 084. 065. 046. 027. 038. PK9. M10. N[ 9 others ]\n      28039(27.8%)26443(26.3%)21860(21.7%)10873(10.8%)3938(3.9%)1591(1.6%)1446(1.4%)1430(1.4%)1113(1.1%)796(0.8%)3200(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      VIRTUAL\n[character]\n      1. A virtual school2. Missing3. Not a virtual school4. Not Applicable\n      656(0.7%)183(0.2%)99049(98.3%)841(0.8%)\n      \n      0\n(0.0%)\n    \n    \n      TOTFRL\n[numeric]\n      Mean (sd) : 249.4 (275.2)min ≤ med ≤ max:-9 ≤ 178 ≤ 9626IQR (CV) : 297 (1.1)\n      1906 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FRELCH\n[numeric]\n      Mean (sd) : 221.6 (253.9)min ≤ med ≤ max:-9 ≤ 149 ≤ 7581IQR (CV) : 272 (1.1)\n      1765 distinct values\n      \n      0\n(0.0%)\n    \n    \n      REDLCH\n[numeric]\n      Mean (sd) : 26 (36.9)min ≤ med ≤ max:-9 ≤ 16 ≤ 2045IQR (CV) : 37 (1.4)\n      399 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PK\n[numeric]\n      Mean (sd) : 34.8 (53.5)min ≤ med ≤ max:0 ≤ 22 ≤ 1912IQR (CV) : 43 (1.5)\n      468 distinct values\n      \n      64621\n(64.2%)\n    \n    \n      KG\n[numeric]\n      Mean (sd) : 65 (46.9)min ≤ med ≤ max:0 ≤ 62 ≤ 948IQR (CV) : 57 (0.7)\n      393 distinct values\n      \n      43684\n(43.4%)\n    \n    \n      G01\n[numeric]\n      Mean (sd) : 64.4 (44.8)min ≤ med ≤ max:0 ≤ 62 ≤ 1408IQR (CV) : 56 (0.7)\n      353 distinct values\n      \n      43333\n(43.0%)\n    \n    \n      G02\n[numeric]\n      Mean (sd) : 64.6 (44.4)min ≤ med ≤ max:0 ≤ 63 ≤ 688IQR (CV) : 56 (0.7)\n      345 distinct values\n      \n      43268\n(43.0%)\n    \n    \n      G03\n[numeric]\n      Mean (sd) : 66.4 (46.3)min ≤ med ≤ max:0 ≤ 64 ≤ 783IQR (CV) : 59 (0.7)\n      358 distinct values\n      \n      43253\n(42.9%)\n    \n    \n      G04\n[numeric]\n      Mean (sd) : 67.9 (48.7)min ≤ med ≤ max:0 ≤ 65 ≤ 877IQR (CV) : 61 (0.7)\n      382 distinct values\n      \n      43470\n(43.2%)\n    \n    \n      G05\n[numeric]\n      Mean (sd) : 69.7 (56.7)min ≤ med ≤ max:0 ≤ 64 ≤ 985IQR (CV) : 65 (0.8)\n      494 distinct values\n      \n      44673\n(44.3%)\n    \n    \n      G06\n[numeric]\n      Mean (sd) : 91.5 (108.4)min ≤ med ≤ max:0 ≤ 56 ≤ 1155IQR (CV) : 111 (1.2)\n      641 distinct values\n      \n      58585\n(58.2%)\n    \n    \n      G07\n[numeric]\n      Mean (sd) : 102.7 (126.2)min ≤ med ≤ max:0 ≤ 52 ≤ 1439IQR (CV) : 153 (1.2)\n      687 distinct values\n      \n      63682\n(63.2%)\n    \n    \n      G08\n[numeric]\n      Mean (sd) : 101.9 (127.1)min ≤ med ≤ max:0 ≤ 50 ≤ 1608IQR (CV) : 152 (1.2)\n      700 distinct values\n      \n      63449\n(63.0%)\n    \n    \n      G09\n[numeric]\n      Mean (sd) : 124.7 (185.8)min ≤ med ≤ max:0 ≤ 40 ≤ 2799IQR (CV) : 166 (1.5)\n      987 distinct values\n      \n      68499\n(68.0%)\n    \n    \n      G10\n[numeric]\n      Mean (sd) : 120.4 (178.1)min ≤ med ≤ max:0 ≤ 39 ≤ 1837IQR (CV) : 157 (1.5)\n      945 distinct values\n      \n      68706\n(68.2%)\n    \n    \n      G11\n[numeric]\n      Mean (sd) : 115.4 (170.1)min ≤ med ≤ max:0 ≤ 40 ≤ 1719IQR (CV) : 149 (1.5)\n      914 distinct values\n      \n      68720\n(68.2%)\n    \n    \n      G12\n[numeric]\n      Mean (sd) : 114.1 (165.5)min ≤ med ≤ max:0 ≤ 43 ≤ 2580IQR (CV) : 150 (1.5)\n      891 distinct values\n      \n      68814\n(68.3%)\n    \n    \n      G13\n[logical]\n      1. FALSE2. TRUE\n      36(97.3%)1(2.7%)\n      \n      100692\n(100.0%)\n    \n    \n      TOTAL\n[numeric]\n      Mean (sd) : 515.7 (450.2)min ≤ med ≤ max:0 ≤ 434 ≤ 14286IQR (CV) : 408 (0.9)\n      2945 distinct values\n      \n      2229\n(2.2%)\n    \n    \n      MEMBER\n[numeric]\n      Mean (sd) : 515.6 (449.9)min ≤ med ≤ max:0 ≤ 434 ≤ 14286IQR (CV) : 408 (0.9)\n      2944 distinct values\n      \n      2229\n(2.2%)\n    \n    \n      AM\n[numeric]\n      Mean (sd) : 6.7 (30.3)min ≤ med ≤ max:0 ≤ 1 ≤ 1395IQR (CV) : 4 (4.5)\n      424 distinct values\n      \n      20609\n(20.5%)\n    \n    \n      HI\n[numeric]\n      Mean (sd) : 142.5 (240.6)min ≤ med ≤ max:0 ≤ 49 ≤ 4677IQR (CV) : 160 (1.7)\n      1745 distinct values\n      \n      3852\n(3.8%)\n    \n    \n      BL\n[numeric]\n      Mean (sd) : 83 (151.4)min ≤ med ≤ max:0 ≤ 19 ≤ 5088IQR (CV) : 90 (1.8)\n      1166 distinct values\n      \n      8325\n(8.3%)\n    \n    \n      WH\n[numeric]\n      Mean (sd) : 247.9 (275.1)min ≤ med ≤ max:0 ≤ 182 ≤ 8146IQR (CV) : 312 (1.1)\n      1839 distinct values\n      \n      3993\n(4.0%)\n    \n    \n      HP\n[numeric]\n      Mean (sd) : 3.1 (24.7)min ≤ med ≤ max:0 ≤ 0 ≤ 1394IQR (CV) : 2 (8)\n      305 distinct values\n      \n      30008\n(29.8%)\n    \n    \n      TR\n[numeric]\n      Mean (sd) : 20.7 (27.3)min ≤ med ≤ max:0 ≤ 12 ≤ 1228IQR (CV) : 24 (1.3)\n      307 distinct values\n      \n      7137\n(7.1%)\n    \n    \n      FTE\n[numeric]\n      Mean (sd) : 32.6 (25.6)min ≤ med ≤ max:0 ≤ 27.6 ≤ 1419IQR (CV) : 24 (0.8)\n      10066 distinct values\n      \n      5233\n(5.2%)\n    \n    \n      LATCOD\n[numeric]\n      Mean (sd) : 37.8 (5.8)min ≤ med ≤ max:-14.3 ≤ 38.8 ≤ 71.3IQR (CV) : 7.7 (0.2)\n      96746 distinct values\n      \n      0\n(0.0%)\n    \n    \n      LONCOD\n[numeric]\n      Mean (sd) : -92.9 (16.9)min ≤ med ≤ max:-176.6 ≤ -89.3 ≤ 144.9IQR (CV) : 20.2 (-0.2)\n      96911 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ULOCALE\n[character]\n      1. 21-Suburb: Large2. 11-City: Large3. 41-Rural: Fringe4. 42-Rural: Distant5. 13-City: Small6. 43-Rural: Remote7. 32-Town: Distant8. 12-City: Mid-size9. 33-Town: Remote10. 22-Suburb: Mid-size[ 2 others ]\n      26772(26.6%)14851(14.7%)11179(11.1%)10279(10.2%)6635(6.6%)6412(6.4%)6266(6.2%)5876(5.8%)4138(4.1%)3305(3.3%)5016(5.0%)\n      \n      0\n(0.0%)\n    \n    \n      STUTERATIO\n[numeric]\n      Mean (sd) : 16.9 (85.7)min ≤ med ≤ max:0 ≤ 15.3 ≤ 22350IQR (CV) : 5.3 (5.1)\n      3854 distinct values\n      \n      6835\n(6.8%)\n    \n    \n      STITLEI\n[character]\n      1. Missing2. No3. Not Applicable4. Yes\n      864(0.9%)14596(14.5%)29199(29.0%)56070(55.7%)\n      \n      0\n(0.0%)\n    \n    \n      AMALM\n[numeric]\n      Mean (sd) : 3.7 (16.1)min ≤ med ≤ max:0 ≤ 1 ≤ 743IQR (CV) : 2 (4.4)\n      268 distinct values\n      \n      26365\n(26.2%)\n    \n    \n      AMALF\n[numeric]\n      Mean (sd) : 3.6 (15.5)min ≤ med ≤ max:0 ≤ 1 ≤ 652IQR (CV) : 2 (4.4)\n      263 distinct values\n      \n      26708\n(26.5%)\n    \n    \n      ASALM\n[numeric]\n      Mean (sd) : 15.9 (45.2)min ≤ med ≤ max:0 ≤ 3 ≤ 1997IQR (CV) : 11 (2.8)\n      522 distinct values\n      \n      16162\n(16.0%)\n    \n    \n      ASALF\n[numeric]\n      Mean (sd) : 15.1 (42.5)min ≤ med ≤ max:0 ≤ 3 ≤ 1532IQR (CV) : 11 (2.8)\n      495 distinct values\n      \n      16080\n(16.0%)\n    \n    \n      HIALM\n[numeric]\n      Mean (sd) : 73.7 (123.5)min ≤ med ≤ max:0 ≤ 25 ≤ 2292IQR (CV) : 83 (1.7)\n      1073 distinct values\n      \n      4774\n(4.7%)\n    \n    \n      HIALF\n[numeric]\n      Mean (sd) : 70.5 (118.7)min ≤ med ≤ max:0 ≤ 24 ≤ 2461IQR (CV) : 79 (1.7)\n      1047 distinct values\n      \n      5121\n(5.1%)\n    \n    \n      BLALM\n[numeric]\n      Mean (sd) : 43.5 (77.3)min ≤ med ≤ max:0 ≤ 11 ≤ 2473IQR (CV) : 48 (1.8)\n      687 distinct values\n      \n      10801\n(10.7%)\n    \n    \n      BLALF\n[numeric]\n      Mean (sd) : 42.1 (76.8)min ≤ med ≤ max:0 ≤ 10 ≤ 2615IQR (CV) : 46 (1.8)\n      693 distinct values\n      \n      11485\n(11.4%)\n    \n    \n      WHALM\n[numeric]\n      Mean (sd) : 128.6 (140.5)min ≤ med ≤ max:0 ≤ 95 ≤ 3854IQR (CV) : 160 (1.1)\n      1046 distinct values\n      \n      4502\n(4.5%)\n    \n    \n      WHALF\n[numeric]\n      Mean (sd) : 120.8 (135.6)min ≤ med ≤ max:0 ≤ 88 ≤ 4292IQR (CV) : 152 (1.1)\n      1030 distinct values\n      \n      4682\n(4.6%)\n    \n    \n      HPALM\n[numeric]\n      Mean (sd) : 1.7 (13.4)min ≤ med ≤ max:0 ≤ 0 ≤ 751IQR (CV) : 1 (7.9)\n      210 distinct values\n      \n      34182\n(33.9%)\n    \n    \n      HPALF\n[numeric]\n      Mean (sd) : 1.6 (12.2)min ≤ med ≤ max:0 ≤ 0 ≤ 643IQR (CV) : 1 (7.7)\n      212 distinct values\n      \n      34563\n(34.3%)\n    \n    \n      TRALM\n[numeric]\n      Mean (sd) : 10.8 (13.9)min ≤ med ≤ max:0 ≤ 6 ≤ 512IQR (CV) : 13 (1.3)\n      174 distinct values\n      \n      9200\n(9.1%)\n    \n    \n      TRALF\n[numeric]\n      Mean (sd) : 10.5 (14)min ≤ med ≤ max:0 ≤ 6 ≤ 716IQR (CV) : 12 (1.3)\n      183 distinct values\n      \n      9477\n(9.4%)\n    \n    \n      TOTMENROL\n[numeric]\n      Mean (sd) : 264.9 (229)min ≤ med ≤ max:0 ≤ 224 ≤ 6890IQR (CV) : 210 (0.9)\n      1691 distinct values\n      \n      2296\n(2.3%)\n    \n    \n      TOTFENROL\n[numeric]\n      Mean (sd) : 251.1 (222.8)min ≤ med ≤ max:0 ≤ 211 ≤ 7396IQR (CV) : 200 (0.9)\n      1646 distinct values\n      \n      2362\n(2.3%)\n    \n    \n      STATUS\n[numeric]\n      Mean (sd) : 1.1 (0.6)min ≤ med ≤ max:1 ≤ 1 ≤ 8IQR (CV) : 0 (0.5)\n      1:98557(97.8%)3:1103(1.1%)4:77(0.1%)5:110(0.1%)6:500(0.5%)7:341(0.3%)8:41(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      UG\n[numeric]\n      Mean (sd) : 11.2 (33.6)min ≤ med ≤ max:0 ≤ 2 ≤ 1017IQR (CV) : 10 (3)\n      217 distinct values\n      \n      88689\n(88.0%)\n    \n    \n      AE\n[logical]\n      1. FALSE2. TRUE\n      60(93.8%)4(6.2%)\n      \n      100665\n(99.9%)\n    \n    \n      SCHOOL_TYPE_TEXT\n[character]\n      1. Alternative/other school2. Regular school3. Special education school4. Vocational school\n      5531(5.5%)91737(91.1%)1948(1.9%)1513(1.5%)\n      \n      0\n(0.0%)\n    \n    \n      SY_STATUS_TEXT\n[character]\n      1. Currently operational2. New school3. School has changed agency4. School has reopened5. School temporarily closed6. School to be operational 7. School was operational bu\n      98557(97.8%)1103(1.1%)110(0.1%)41(0.0%)500(0.5%)341(0.3%)77(0.1%)\n      \n      0\n(0.0%)\n    \n    \n      SCHOOL_LEVEL\n[character]\n      1. Adult Education2. Elementary3. High4. Middle5. Not Applicable6. Not Reported7. Other8. Prekindergarten9. Secondary10. Ungraded\n      28(0.0%)53287(52.9%)22977(22.8%)16506(16.4%)796(0.8%)1113(1.1%)3824(3.8%)1430(1.4%)602(0.6%)166(0.2%)\n      \n      0\n(0.0%)\n    \n    \n      AS\n[numeric]\n      Mean (sd) : 29.8 (85.8)min ≤ med ≤ max:0 ≤ 5 ≤ 3529IQR (CV) : 21 (2.9)\n      850 distinct values\n      \n      12717\n(12.6%)\n    \n    \n      CHARTER_TEXT\n[character]\n      1. No2. Not Applicable3. Yes\n      87007(86.4%)6387(6.3%)7335(7.3%)\n      \n      0\n(0.0%)\n    \n    \n      MAGNET_TEXT\n[character]\n      1. Missing2. No3. Not Applicable4. Yes\n      6256(6.2%)77531(77.0%)13520(13.4%)3422(3.4%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23"
  },
  {
    "objectID": "posts/challenge5_Mekhala Kumar.html#briefly-describe-the-data-1",
    "href": "posts/challenge5_Mekhala Kumar.html#briefly-describe-the-data-1",
    "title": "Challenge 5",
    "section": "Briefly describe the data",
    "text": "Briefly describe the data\nThe dataset contains details about the public school education. It has 100729 observations and 79 variables. The data contains many variables that could be renamed for the sake of understanding easily. Some of the variables need to be turned into factors. In this challenge, I will be focusing on the variables of state and school level so I will be performing the changes to only these two variables.I also made a smaller dataframe with two states in order to compare observations between the two states.\nOne issue faced while changing the categories in the School Education Level variable was that there were secondary, middle and high school mentioned. I assumed that secondary should include both middle and high school, but the number of observations for middle and high school do not add up to the observations present for secondary school. Hence, I have kept all three in the dataset,"
  },
  {
    "objectID": "posts/challenge5_Mekhala Kumar.html#tidy-data-as-needed",
    "href": "posts/challenge5_Mekhala Kumar.html#tidy-data-as-needed",
    "title": "Challenge 5",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\n\nPublicSchoolChar<-PublicSchoolChar%>%\n  rename( State= STABR )\nPublicSchoolChar<-PublicSchoolChar%>%select(State,SCHOOL_LEVEL,everything())\nlevel <- unique(PublicSchoolChar$SCHOOL_LEVEL)\nlevel\n\n [1] \"Elementary\"      \"High\"            \"Other\"           \"Not Reported\"   \n [5] \"Middle\"          \"Secondary\"       \"Prekindergarten\" \"Not Applicable\" \n [9] \"Ungraded\"        \"Adult Education\"\n\nPublicSchoolChar<-PublicSchoolChar%>%\n  mutate(Levels = factor(SCHOOL_LEVEL, \n                       labels=level[c(4,8,9,7,1,6,5,2,10,3)]))%>%\n  select(-SCHOOL_LEVEL)\nrm(level)\n\ntable(PublicSchoolChar$Levels)\n\n\n   Not Reported  Not Applicable        Ungraded Prekindergarten      Elementary \n             28           53287           22977           16506             796 \n      Secondary          Middle            High Adult Education           Other \n           1113            3824            1430             602             166 \n\nState2<-PublicSchoolChar%>%filter(State == \"MA\"|State==\"NJ\")\nState2"
  },
  {
    "objectID": "posts/challenge5_Mekhala Kumar.html#univariate-visualisations-1",
    "href": "posts/challenge5_Mekhala Kumar.html#univariate-visualisations-1",
    "title": "Challenge 5",
    "section": "Univariate Visualisations",
    "text": "Univariate Visualisations\nHere the number of observations in each state can be seen. The distribution of the different school levels is also visible. However, it can be seen that the majority of the observations are not applicable, so essentially they are missing.\n\nggplot(PublicSchoolChar, aes(State)) + geom_bar()\n\n\n\nggplot(PublicSchoolChar, aes(Levels)) + geom_bar()"
  },
  {
    "objectID": "posts/challenge5_Mekhala Kumar.html#bivariate-visualisations-doubt",
    "href": "posts/challenge5_Mekhala Kumar.html#bivariate-visualisations-doubt",
    "title": "Challenge 5",
    "section": "Bivariate Visualisations (Doubt)",
    "text": "Bivariate Visualisations (Doubt)\nIn order to make a bivariate visualisation, a continuous variable is also required. However, I am unable to understand what data is represented from the column names in the dataset and hence was unable to complete this step.\n:::"
  },
  {
    "objectID": "posts/challenge8_instructions.html",
    "href": "posts/challenge8_instructions.html",
    "title": "Challenge 8 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge8_instructions.html#challenge-overview",
    "href": "posts/challenge8_instructions.html#challenge-overview",
    "title": "Challenge 8 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in multiple data sets, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\njoin two or more data sets and analyze some aspect of the joined data\n\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge8_instructions.html#read-in-data",
    "href": "posts/challenge8_instructions.html#read-in-data",
    "title": "Challenge 8 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nfaostat ⭐⭐\nrailroads ⭐⭐⭐\nfed_rate ⭐⭐⭐\ndebt ⭐⭐⭐\nus_hh ⭐⭐⭐⭐\nsnl ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge8_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge8_instructions.html#tidy-data-as-needed",
    "title": "Challenge 8 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge8_instructions.html#join-data",
    "href": "posts/challenge8_instructions.html#join-data",
    "title": "Challenge 8 Instructions",
    "section": "Join Data",
    "text": "Join Data\nBe sure to include a sanity check, and double-check that case count is correct!"
  },
  {
    "objectID": "posts/challenge4_LindsayJones.html",
    "href": "posts/challenge4_LindsayJones.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_LindsayJones.html#read-in-data",
    "href": "posts/challenge4_LindsayJones.html#read-in-data",
    "title": "Challenge 4",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\npoultry_tidy <- read_csv(\"_data/poultry_tidy.csv\",\n                         show_col_types = FALSE)\n\n\n\nBriefly describe the data\nData set shows the price in dollars for 5 different cuts of poultry during each month from 2004 to 2013."
  },
  {
    "objectID": "posts/challenge4_LindsayJones.html#tidy-data-as-needed",
    "href": "posts/challenge4_LindsayJones.html#tidy-data-as-needed",
    "title": "Challenge 4",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nThe data is NOT tidy. I will use pivot_wider to correct this.\n\n\nCode\npoultry_tidy_wider <- pivot_wider(poultry_tidy, \n                                  names_from = Product, \n                                  values_from = Price_Dollar)\nprint(poultry_tidy_wider)\n\n\n# A tibble: 120 × 7\n    Year Month     Whole `B/S Breast` `Bone-in Breast` `Whole Legs` Thighs\n   <dbl> <chr>     <dbl>        <dbl>            <dbl>        <dbl>  <dbl>\n 1  2013 January    2.38         7.04             3.90         2.04   2.16\n 2  2013 February   2.38         7.04             3.90         2.04   2.16\n 3  2013 March      2.38         7.04             3.90         2.04   2.16\n 4  2013 April      2.38         7.04             3.90         2.04   2.16\n 5  2013 May        2.38         7.04             3.90         2.04   2.16\n 6  2013 June       2.38         7.04             3.90         2.04   2.16\n 7  2013 July       2.38         7.04             3.90         2.04   2.16\n 8  2013 August     2.38         7.04             3.90         2.04   2.16\n 9  2013 September  2.38         7.04             3.90         2.04   2.16\n10  2013 October    2.38         7.04             3.90         2.04   2.16\n# … with 110 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge4_LindsayJones.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_LindsayJones.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nI wasn’t able to identify any variables that need to be mutated. Factors also didn’t seem necessary given the type of data we’re working with.\nI would like to rename the “Whole Legs” column to something not containing the word “whole” to avoid confusion with the other variable, but my attempts to rename the column didn’t work."
  },
  {
    "objectID": "posts/challenge2_jerinjacob.html",
    "href": "posts/challenge2_jerinjacob.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\nThe railroad data contain 2931 county-level aggregated counts of the number of railroad employees in 2012. Counties are embedded within States, and all 50 states plus Canada, overseas addresses in Asia and Europe, and Washington, DC are represented."
  },
  {
    "objectID": "posts/challenge2_jerinjacob.html#reading-railroad-dataset",
    "href": "posts/challenge2_jerinjacob.html#reading-railroad-dataset",
    "title": "Challenge 2",
    "section": "Reading Railroad dataset",
    "text": "Reading Railroad dataset\nTo include the data of Canada too in the analysis, we have mutated the county column and renamed the county name to CANADA.\n\n\nCode\nlibrary(readxl)\nrailroad <- read_excel(\"_data/StateCounty2012.xls\", skip = 4, col_names = c(\"State\", \"Delete\", \"County\", \"Delete\", \"Employees\")) %>%\n  select(!contains(\"Delete\")) %>%\n  filter(!str_detect(State, \"Total\"))\n\nrailroad<-head(railroad, -2)%>%\n  mutate(County = ifelse(State==\"CANADA\", \"CANADA\", County))\n\nrailroad"
  },
  {
    "objectID": "posts/challenge2_jerinjacob.html#describe-the-data",
    "href": "posts/challenge2_jerinjacob.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\nrailroad %>%\n  summarise(across(c(State, County), n_distinct))\n\n\n\n\n  \n\n\n\nThere are 2931 state-county cases but only 1710 distinct county names. This means that there are many county names that repeats in different states.\n\n\nCode\nrailroad %>%\n  summarise(total_employees = sum(Employees))\n\n\n\n\n  \n\n\n\nThere are 256094 railroad employees in 2012 dataset across the counties"
  },
  {
    "objectID": "posts/challenge2_jerinjacob.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_jerinjacob.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\n\n\nCode\nrailroad %>%\n  filter(Employees >= 1000) %>%\n  arrange(desc(Employees))\n\n\n\n\n  \n\n\n\nCook, IL has the most number of employees with a head count of 8207. 27 counties have more than 1000 employees\n\n\nCode\nrailroad %>%\n  summarise(min(Employees))\n\n\n\n\n  \n\n\n\n\n\nCode\nrailroad %>%\n  filter(Employees == 1)\n\n\n\n\n  \n\n\n\nThere are 145 counties where there is only 1 employee working in the rail road department."
  },
  {
    "objectID": "posts/challenge2_jerinjacob.html#explain-and-interpret",
    "href": "posts/challenge2_jerinjacob.html#explain-and-interpret",
    "title": "Challenge 2",
    "section": "Explain and Interpret",
    "text": "Explain and Interpret\n\n\nCode\nrailroad %>%\n  group_by(State) %>%\n  summarise(Total_Employees = sum(Employees), num_counties = n()) %>%\n  arrange(desc(Total_Employees))\n\n\n\n\n  \n\n\n\nTexas has the most number of employees working in rail road followed by IL, NY NE and CA in the consecutive top 5 positions. When we closely look on the data, the top states are populous and geographically large. But there are exceptions and are not directly propotionate."
  },
  {
    "objectID": "posts/challenge1_AnanyaPujary.html",
    "href": "posts/challenge1_AnanyaPujary.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_AnanyaPujary.html#read-in-the-data",
    "href": "posts/challenge1_AnanyaPujary.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")"
  },
  {
    "objectID": "posts/challenge1_AnanyaPujary.html#describe-the-data",
    "href": "posts/challenge1_AnanyaPujary.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nI’ll be working with the ‘railroad_2012_clean_county.csv’ dataset.\n\n\nCode\ndim(railroad) #describing the 'railroad' dataset's dimensions\n\n\n[1] 2930    3\n\n\nFrom this command, we learn that the ‘railroad_2012_clean_county.csv’ dataset has 3 columns and 2930 rows.\n\n\nCode\ncolnames(railroad)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nCode\nhead(railroad)\n\n\n# A tibble: 6 × 3\n  state county               total_employees\n  <chr> <chr>                          <dbl>\n1 AE    APO                                2\n2 AK    ANCHORAGE                          7\n3 AK    FAIRBANKS NORTH STAR               2\n4 AK    JUNEAU                             3\n5 AK    MATANUSKA-SUSITNA                  2\n6 AK    SITKA                              1\n\n\nThe columns in ‘railroad’ are: ‘state’ (datatype: character), ‘county’(datatype: character), and ‘total_employees’(datatype: double class - numeric values with decimal points). These data were probably collected as part of a large-scale survey of the number of railroad employees by county and state in the United States.\n\n\nCode\nrailroad_arranged <- railroad %>%\n  arrange(desc(total_employees)) # arranging data to find the county with the most number of employees\nhead(railroad_arranged)\n\n\n# A tibble: 6 × 3\n  state county           total_employees\n  <chr> <chr>                      <dbl>\n1 IL    COOK                        8207\n2 TX    TARRANT                     4235\n3 NE    DOUGLAS                     3797\n4 NY    SUFFOLK                     3685\n5 VA    INDEPENDENT CITY            3249\n6 FL    DUVAL                       3073\n\n\nCook county in Illinois has the highest number of railroad employees (8207).\n\n\nCode\nrailroads<- railroad %>%\n  group_by(state) %>%  # grouping the data by state\n  select(total_employees) %>% # looking only at the 'total_employees' column\n  summarize_all(sum, na.rm=TRUE)%>% # adding the number of employees in the counties state-wise\n  arrange(desc(total_employees)) # arranging the states from highest to lowest number of employees\n\nhead(railroads)\n\n\n# A tibble: 6 × 2\n  state total_employees\n  <chr>           <dbl>\n1 TX              19839\n2 IL              19131\n3 NY              17050\n4 NE              13176\n5 CA              13137\n6 PA              12769\n\n\nTexas has the most railroad employees (19839) and the Armed Forces Pacific has the least (1)."
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html#challenge-overview",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html#challenge-overview",
    "title": "Challenge 3",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html#read-in-data",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\nRead the data regarding eggs\n\n\nCode\neggs <- read_csv(\"_data/eggs_tidy.csv\")\neggs\n\n\n# A tibble: 120 × 6\n   month      year large_half_dozen large_dozen extra_large_half_dozen extra_l…¹\n   <chr>     <dbl>            <dbl>       <dbl>                  <dbl>     <dbl>\n 1 January    2004             126         230                    132       230 \n 2 February   2004             128.        226.                   134.      230 \n 3 March      2004             131         225                    137       230 \n 4 April      2004             131         225                    137       234.\n 5 May        2004             131         225                    137       236 \n 6 June       2004             134.        231.                   137       241 \n 7 July       2004             134.        234.                   137       241 \n 8 August     2004             134.        234.                   137       241 \n 9 September  2004             130.        234.                   136.      241 \n10 October    2004             128.        234.                   136.      241 \n# … with 110 more rows, and abbreviated variable name ¹​extra_large_dozen\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nBriefly describe the data\nThis dataset shows that the price of eggs categorizied by their size from 2004 I think. Because this data has 6 columns, it cannot be easily recognized at once. So I will pivot it. The 4 columns about the types will be pivoted to “types” column."
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html#anticipate-the-end-result",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html#anticipate-the-end-result",
    "title": "Challenge 3",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nIn this ‘eggs’ dataset 2 of the variables are used to identify a case. So expected rows are 480 and columns are 4.\n\n\nCode\n#existing rows/cases\nnrow(eggs)\n\n\n[1] 120\n\n\nCode\n#existing columns/cases\nncol(eggs)\n\n\n[1] 6\n\n\nCode\n#expected rows/cases\nnrow(eggs) * (ncol(eggs)-2)\n\n\n[1] 480\n\n\nCode\n# expected columns \n2 + 2\n\n\n[1] 4"
  },
  {
    "objectID": "posts/challenge3_instructions-Youngsoo Choi.html#pivot-the-data",
    "href": "posts/challenge3_instructions-Youngsoo Choi.html#pivot-the-data",
    "title": "Challenge 3",
    "section": "Pivot the Data",
    "text": "Pivot the Data\n\n\nCode\n#pivot data\n\npivot_eggs<-pivot_longer(eggs,col=c(large_half_dozen, large_dozen, extra_large_half_dozen, extra_large_dozen), names_to=\"types\", values_to=\"price\")\npivot_eggs\n\n\n# A tibble: 480 × 4\n   month     year types                  price\n   <chr>    <dbl> <chr>                  <dbl>\n 1 January   2004 large_half_dozen        126 \n 2 January   2004 large_dozen             230 \n 3 January   2004 extra_large_half_dozen  132 \n 4 January   2004 extra_large_dozen       230 \n 5 February  2004 large_half_dozen        128.\n 6 February  2004 large_dozen             226.\n 7 February  2004 extra_large_half_dozen  134.\n 8 February  2004 extra_large_dozen       230 \n 9 March     2004 large_half_dozen        131 \n10 March     2004 large_dozen             225 \n# … with 470 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n#change the order of columns\npivot_eggs<-pivot_eggs[c(2,1,3,4)]\npivot_eggs\n\n\n# A tibble: 480 × 4\n    year month    types                  price\n   <dbl> <chr>    <chr>                  <dbl>\n 1  2004 January  large_half_dozen        126 \n 2  2004 January  large_dozen             230 \n 3  2004 January  extra_large_half_dozen  132 \n 4  2004 January  extra_large_dozen       230 \n 5  2004 February large_half_dozen        128.\n 6  2004 February large_dozen             226.\n 7  2004 February extra_large_half_dozen  134.\n 8  2004 February extra_large_dozen       230 \n 9  2004 March    large_half_dozen        131 \n10  2004 March    large_dozen             225 \n# … with 470 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAny additional comments?\nIt has changed to 480 rows and 4 columns dataset."
  },
  {
    "objectID": "posts/challenge1_stevenoneill.html",
    "href": "posts/challenge1_stevenoneill.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_stevenoneill.html#read-in-the-data",
    "href": "posts/challenge1_stevenoneill.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\n\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nI chose the State & County railroad employment dataset.\n\n\nCode\nlibrary(readxl)\n\n\nFirst I imported the dataset, removing the first two rows of unhelpful data:\n\n\nCode\ndf1 <- read_xls(\"_data/StateCounty2012.xls\", skip = 2)\n\n\nNext, I removed un-used columns:\n\n\nCode\ndf2 <- df1 %>% select(STATE,COUNTY,TOTAL)\n\n\nAfter, I removed unhelpful pre-calculated totals:\n\n\nCode\ndf3=df2[grepl(\"^[a-zA-Z][a-zA-Z]$\",df2$STATE),]"
  },
  {
    "objectID": "posts/challenge1_stevenoneill.html#describe-the-data",
    "href": "posts/challenge1_stevenoneill.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nThis data describes railroad employment in U.S. states and territories. In this instance, the original ‘cases’ are the counties and the original ‘variables’ are their parent states and the total number of persons employed in the railroad industry in those counties.\nYou may notice the data contains the uncommon state codes “AE” and “AP”, as well as the recurring “APO” county name. These represent military addresses:\n\n\n\nState Code\nLocation\n\n\n\n\nAE\nEurope, Middle East, Africa, Canada\n\n\nAP\nAsia Pacific\n\n\nAA\nAmericas (excluding Canada)\n\n\n\n‘APO’ refers to “Army Post Office”.\nI have calculated some basic statistics:\n\n\nCode\n#Group by state, but first, add the largest county to the dataframe\nby_state <- df3 %>% group_by(STATE) %>% \n    mutate(largest.county.name = COUNTY[which.max(TOTAL)]) %>% \n      mutate(smallest.county.name = COUNTY[which.min(TOTAL)])\n\n#Group by state, then summarize the total of all county employees, per-state:\nby_state <- by_state %>% summarise(\n  total.state.employees = sum(TOTAL),\n  median.county.employees = median(TOTAL),\n  smallest.county = min(TOTAL),\n  smallest.county.name = first(smallest.county.name),\n  largest.county = max(TOTAL),\n  largest.county.name = first(largest.county.name),\n  standard.dev = sd(TOTAL)\n)\n\nby_state\n\n\n\n\n  \n\n\n\nA few things stand out:\n\nTexas has the largest amount of railroad employees combined, at 19,839.\nHowever, Illinois has the largest single county of railroad employees in Cook County, at 8207. That’s almost double the next-largest in Tarrant, TX.\nIllinois also possesses one of the smallest counties by the same metric - Hardin County, with only one employee. It has the highest standard deviation among in-state counties.\n\nLooking forward to the next steps in analysis."
  },
  {
    "objectID": "posts/challenge1_solutions.html",
    "href": "posts/challenge1_solutions.html",
    "title": "Challenge 1 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/challenge1_solutions.html#working-with-tabular-data",
    "href": "posts/challenge1_solutions.html#working-with-tabular-data",
    "title": "Challenge 1 Solution",
    "section": "Working with Tabular Data",
    "text": "Working with Tabular Data\nOur advanced datasets ( ⭐⭐⭐ and higher) are tabular data (i.e., tables) that are often published based on government sources or by other organizations. Tabular data is often made available in Excel format (.xls or .xlsx) and is formatted for ease of reading - but this can make it tricky to read into R and reshape into a usable dataset.\nReading in tabular data will follow the same general work flow or work process regardless of formatting differences. We will work through the steps in detail this week (and in future weeks as new datasets are introduced), but this is an outline of the basic process. Note that not every step is needed for every file.\n\nIdentify grouping variables and values to extract from the table\nIdentify formatting issues that need to be addressed or eliminated\nIdentify column issues to be addressed during data read-in\nChoose column names to allow pivoting or future analysis\nAddress issues in rows using filter (and stringr package)\nCreate or mutate new variables as required, using separate, pivot_longer, etc\n\n\nRailroad ⭐FAOSTAT ⭐⭐Wild Birds ⭐⭐⭐Railroad (xls) ⭐⭐⭐⭐\n\n\nIt is hard to get much information about the data source or contents from a .csv file - as compared to the formatted .xlsx version of the same data described below.\n\nRead the Data\n\n\nCode\nrailroad<-read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nRows: 2930 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, county\ndbl (1): total_employees\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\nFrom inspection, we can that the three variables are named state, county, and total employees. Combined with the name of the fail, this appears to be the aggregated data on the number of employees working for the railroad in each county 2012. We assume that the 2930 cases - which are counties embedded within states1 - consist only of counties where there are railroad employees?\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nWith a few simple commands, we can confirm that there are 53 “states” represented in the data. To identify the additional non-state areas (probably District of Columbia, plus some combination of Puerto Rico and/or overseas addresses), we can print out a list of unique state names.\n\n1: We can identify case variables because both are character variables, which in tidy lingo are grouping variables not values.\n\n\n\nOnce again, a .csv file lacks any of the additional information that might be present in a published Excel table. So, we know the data are likely to be about birds, but will we be looking at individual pet birds, prices of bird breeds sold in stores, the average flock size of wild birds - who knows!\nThe FAOSTAT*.csv files have some additional information - the FAO - which a Google search reveals to be the Food and Agriculture Association of the United Nations publishes country-level data regularly in a database called FAOSTAT. So my best guess at this point is that we are going to be looking at country-level estaimtes of the number of birds that are raised for eggs and poultry, but we will see if this is right by inspecting the data.\n\nRead the Data\n\n\nCode\nbirds<-read_csv(\"_data/birds.csv\")\n\n\nRows: 30977 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nbirds\n\n\n\n\n  \n\n\n\nCode\nchickens<-read_csv(\"_data/FAOSTAT_egg_chicken.csv\")\n\n\nRows: 38170 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nchickens\n\n\n\n\n  \n\n\n\nIt is pretty difficult to get a handle on what data are being captured by any of the FAOSTAT* (including the birds.csv) data sets simply from a quick scan of the tibble after read in. It was easy with the railroad data, but now we are going to have to work harder to describe exactly what comprises a case in these data and what values are present for each case. We can see that there are 30,970 rows in the birds data (and 38,170 rows in the chickens) - but this might not mean that there are 30,970 (or 38,170) cases because we aren’t sure what constitutes a case at this point.\n\n\nWhat is a case?\nOne approach to figuring out what constitutes a case is to identify the value variables and assume that what is leftover are the grouping variables. Unfortunately, there are six double variables (from the column descriptions that are automatically returned), and it appears that most of them are not grouping variables. For example, the variable “Area Code” is a double - but doesn’t appear to be a value that varies across rows. Thus, it is a grouping variable rather than a true value in tidy nomenclature. Similar issues can be found with Year and “Item Code” - both appear to be grouping variables. Ironically, it is the variable called Value which appears to the sole value in the data set - but what is it the value of?\nAnother approach to identifying a case is to look for variation (or lack of variation) in just the first few cases of the tibble. (Think of this as the basis for a minimal reproducible example.) In the first few cases, the variables of the first 10 cases appear to be identical until we get to Year and Year Code (which appear to be identical to each other.) So it appears that Value is varying by country-year - but perhaps also by information in one of the other variables. It also appears that many of the doubles are just numeric codes, so lets drop those variables to simplify (I’m going to drop down to just showing the birds data for now.)\n\n\nCode\nbirds.sm<-birds%>%\n  select(-contains(\"Code\"))\nbirds.sm\n\n\n\n\n  \n\n\n\nCode\nchickens.sm<-chickens%>%\n  select(-contains(\"Code\"))\n\n\n\n\nVisual Summary of Data Set\nBefore we go doing detailed cross-tabs to figure out where there is variation, lets do a high level summary of the dataset to see if - for example - there are multiple values in the Element variable - or if we only have a dataset with records containing estimates of Chicken Stocks (from Element + Item.)\nTo get a better grasp of the data, lets do a quick skim or summary of the dataset and see if we can find out more about our data at a glance. I am using the dfSummary function from the summarytools package -one of the more attractive ways to quickly summarise a dataset. I am using a few options to allow it to render directly to html.\n\n\nCode\nprint(summarytools::dfSummary(birds.sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nbirds.sm\nDimensions: 30977 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Live Animals\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Africa2. Asia3. Eastern Asia4. Egypt5. Europe6. France7. Greece8. Myanmar9. Northern Africa10. South-eastern Asia[ 238 others ]\n      290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)28077(90.6%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Stocks\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Chickens2. Ducks3. Geese and guinea fowls4. Pigeons, other birds5. Turkeys\n      13074(42.2%)6909(22.3%)4136(13.4%)1165(3.8%)5693(18.4%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.6 (16.7)min ≤ med ≤ max:1961 ≤ 1992 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 99410.6 (720611.4)min ≤ med ≤ max:0 ≤ 1800 ≤ 23707134IQR (CV) : 15233 (7.2)\n      11495 distinct values\n      \n      1036\n(3.3%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Im5. M\n      1494(7.4%)6488(32.1%)10007(49.5%)1213(6.0%)1002(5.0%)\n      \n      10773\n(34.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Data not available3. FAO data based on imputat4. FAO estimate5. Official data6. Unofficial figure\n      6488(20.9%)1002(3.2%)1213(3.9%)10007(32.3%)10773(34.8%)1494(4.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-22\n\n\n\nFinally - we have a much better grasp on what is going on. First, we know that all records in this data set are of the number of Live Animal Stocks (Domain + Element), with the value expressed as 1000 heads (Unit). These three variables are grouping variables but DO NOT vary in this particular data extract - but are probably used to create data extracts from the larger FAOSTAT database.. To see if we are correct, we will have to checkout the same fields in the chickens data below.\nSecond, we can now guess that a case consists of a country-year-animal record - as captured in the variables Area, Year and Item, respectively - estimate of the number of live animals (Value.) ALso, as a side note, it appears that the estimated number of animals may have a long right-hand tail - just looking at the mini-histogram. So we can now say that we have estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) in 248 areas (countries??) for 58 years between 1961-2018.\nThe only minor concern is that we are still not entirely sure what information is being captured in the Flag (and matching Flag Description) variable. It appears unlikely that there is more than one estimate per country-year-animal case (see the summary of Area where all countries have 290 observations.) An assumption of one type of estimate (the content of Flag Description) per year is also consistent with the histogram of Year, which is pretty consistent although more countries were clearly added later in the series and data collection is not complete for the most recent time period.\nWe can dig a bit more, and find the description of the Flag field on the FAOSTAT website.. Sure enough, this confirms that the flags correspond to what type of estimate is being used (e.g., official data vs an estimate by FAOSTAT or imputed data.)\nWe can also confirm that NOT all cases are countries, as there is a Flag value, A, described as aggregated data. A quick inspection of the areas using this flag confirm that all of the “countries” are actually regional aggregations, and should be filtered out of the dataset as they are not the same “type” of case as a country-level case. To fix these data into true tidy format, we would need to filter out the aggregates, then merge on the country group definitions from FAOSTAT to create new country-group or regional variables that could be used to recreate aggregated estimates with dplyr.\n\n\nCode\nbirds.sm%>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n\n\n  \n\n\n\n\n\nFAOstat*.csv\nLets take a quick look at our chickens data to see if it follows the same basic pattern as the birds data. Sure enough, it looks like we have a different domain (livestock products) but that the cases remain similar country-year-product, with three slightly different estimates related to egg-laying (instead of the five types of poultry.)\n\n\nCode\nprint(summarytools::dfSummary(chickens.sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nchickens.sm\nDimensions: 38170 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Livestock Primary\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Afghanistan2. Africa3. Albania4. Algeria5. American Samoa6. Americas7. Angola8. Antigua and Barbuda9. Argentina10. Asia[ 235 others ]\n      174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)36430(95.4%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Laying2. Production3. Yield\n      12679(33.2%)12840(33.6%)12651(33.1%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Eggs, hen, in shell\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.5 (16.7)min ≤ med ≤ max:1961 ≤ 1991 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head2. 100mg/An3. tonnes\n      12679(33.2%)12651(33.1%)12840(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 291341.2 (2232761)min ≤ med ≤ max:1 ≤ 31996 ≤ 76769955IQR (CV) : 91235.8 (7.7)\n      21325 distinct values\n      \n      40\n(0.1%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Fc5. Im6. M\n      1435(4.7%)3186(10.4%)10538(34.4%)13344(43.6%)2079(6.8%)40(0.1%)\n      \n      7548\n(19.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Calculated data3. Data not available4. FAO data based on imputat5. FAO estimate6. Official data7. Unofficial figure\n      3186(8.3%)13344(35.0%)40(0.1%)2079(5.4%)10538(27.6%)7548(19.8%)1435(3.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-22\n\n\n\n\n\n\nThe “wild_bird_data” sheet is in Excel format (.xlsx) instead of the .csv format of the earlier data sets. In theory, it should be no harder to read in an Excel worksheet (or even workbook) as compared to a .csv file - there is a package called read_xl that is part of the tidyverse that easily reads in excel files.\nHowever, in practice, most people use Excel sheets as a publication format - not a way to store data, so there is almost always a ton of “junk” in the file that is NOT part of the data table that we want to read in. Sometimes the additional “junk” is incredibly useful - it might include table notes or information about data sources. However, we still need a systematic way to identify this junk and get rid of it during the data reading step.\nFor example, lets see what happens here if we just read in the wild bird data straight from excel.\n\n\nCode\nwildbirds<-read_excel(\"_data/wild_bird_data.xlsx\")\nwildbirds\n\n\n\n\n  \n\n\n\nHm, this doesn’t seem quite right. It is clear that the first “case” has information in it that looks more like variable labels. Lets take a quick look at the raw data.\n\n\n\nWild Bird Excel File\n\n\nSure enough the Excel file first row does contain additional information, a pointer to the article that this data was drawn from, and a quick Google reveals the article is [Nee, S., Read, A., Greenwood, J. et al. The relationship between abundance and body size in British birds. Nature 351, 312–313 (1991)] (https://www.nature.com/articles/351312a0)\n\nSkipping a row\nWe could try to manually adjust things - remove the first row, change the column names, and then change the column types. But this is both a lot of work, and not really a best practice for data management. Lets instead re-read the data in with the skip option from read_excel, and see if it fixes all of our problems!\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 1)\nwildbirds\n\n\n\n\n  \n\n\n\nThis now looks great! Both variables are numeric, and now they correctly show up as double or (). The variable names might be a bit tough to work with, though, so it can be easier to assign new column names on the read in - and then manually adjust axis labels, etc once you are working on your publication-quality graphs.\nNote that I skip two rows this time, and apply my own column names.\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 2, \n                        col_names = c(\"weight\", \"pop_size\"))\nwildbirds\n\n\n\n\n  \n\n\n\n\n\n\nThe railroad data set is our most challenging data to read in this week, but is (by comparison) a fairly straightforward formatted table published by the Railroad Retirement Board. The value variable is a count of the number of employees in each county and state combination. \nLooking at the excel file, we can see that there are only a few issues: 1. There are three rows at the top of the sheet that are not needed 2. There are blank columns that are not needed. 3. There are Total rows for each state that are not needed\n\nSkipping title rows\nFor the first issue, we use the “skip” option on read_excel from the readxl package to skip the rows at the top.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 3)\n\n\nNew names:\n• `` -> `...2`\n• `` -> `...4`\n\n\n\n\n  \n\n\n\n\n\nRemoving empty columns\nFor the second issue, I name the blank columns “delete” to make is easy to remove the unwanted columns. I then use select (with the ! sign to designate the complement or NOT) to select columns we wish to keep in the dataset - the rest are removed. Note that I skip 4 rows this time as I do not need the original header row.\nThere are other approaches you could use for this task (e.g., remove all columns that have no valid volues), but hard coding of variable names and types during data read in is not considered a violation of best practices and - if used strategically - can often make later data cleaning much easier.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...4`\n\n\n\n\n  \n\n\n\n\n\nFiltering “total” rows\nFor the third issue, we are going to use filter to identify (and drop the rows that have the word “Total” in the State column). str_detect can be used to find specific rows within a column that have the designated “pattern”, while the “!” designates the complement of the selected rows (i.e., those without the “pattern” we are searching for.)\nThe str_detect command is from the stringr package, and is a powerful and easy to use implementation of grep and regex in the tidyverse - the base R functions (grep, gsub, etc) are classic but far more difficult to use, particularly for those not in practice. Be sure to explore the stringr package on your own.\n\n\nCode\nrailroad<-read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...4`\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\n\n\nRemove any table notes\nTables often have notes in the last few table rows. You can check table limits and use this information during data read-in to not read the notes by setting the n-max option at the total number of rows to read, or less commonly, the range option to specify the spreadsheet range in standard excel naming (e.g., “B4:R142”). If you didn’t handle this on read in, you can use the tail command to check for notes and either tail or head to keep only the rows that you need.\n\n\nCode\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\nCode\n#remove the last two observations\nrailroad <-head(railroad, -2)\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\n\n\nConfirm cases\nAnd that is all it takes! The data are now ready for analysis. Lets see if we get the same number of unique states that were in the cleaned data in exercise 1.\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  n_distinct(.)\n\n\n[1] 54\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nOh my goodness! It seems that we have an additional “State” - it looks like Canada is in the full excel data and not the tidy data. This is one example of why it is good practice to always work from the original data source!"
  },
  {
    "objectID": "posts/challenge4_AnimeshSengupta.html",
    "href": "posts/challenge4_AnimeshSengupta.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(skimr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_AnimeshSengupta.html#challenge-overview",
    "href": "posts/challenge4_AnimeshSengupta.html#challenge-overview",
    "title": "Challenge 4",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_AnimeshSengupta.html#read-in-data",
    "href": "posts/challenge4_AnimeshSengupta.html#read-in-data",
    "title": "Challenge 4",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ⭐\npoultry_tidy.csv⭐⭐\nFedFundsRate.csv⭐⭐⭐\nhotel_bookings.csv⭐⭐⭐⭐\ndebt_in_trillions ⭐⭐⭐⭐⭐\n\n\n\nCode\nlibrary(readxl)\ndebt_data <- read_excel(\"../posts/_data/debt_in_trillions.xlsx\", .name_repair = \"universal\")\nhead(debt_data,10)\n\n\n# A tibble: 10 × 8\n   Year.and.Quarter Mortgage HE.Revolving Auto.Loan Credit…¹ Stude…² Other Total\n   <chr>               <dbl>        <dbl>     <dbl>    <dbl>   <dbl> <dbl> <dbl>\n 1 03:Q1                4.94        0.242     0.641    0.688   0.241 0.478  7.23\n 2 03:Q2                5.08        0.26      0.622    0.693   0.243 0.486  7.38\n 3 03:Q3                5.18        0.269     0.684    0.693   0.249 0.477  7.56\n 4 03:Q4                5.66        0.302     0.704    0.698   0.253 0.449  8.07\n 5 04:Q1                5.84        0.328     0.72     0.695   0.260 0.446  8.29\n 6 04:Q2                5.97        0.367     0.743    0.697   0.263 0.423  8.46\n 7 04:Q3                6.21        0.426     0.751    0.706   0.33  0.41   8.83\n 8 04:Q4                6.36        0.468     0.728    0.717   0.346 0.423  9.04\n 9 05:Q1                6.51        0.502     0.725    0.71    0.364 0.394  9.21\n10 05:Q2                6.70        0.528     0.774    0.717   0.374 0.402  9.49\n# … with abbreviated variable names ¹​Credit.Card, ²​Student.Loan\n\n\n\nBriefly describe the data\nThe data represents the debt statistics quarterly across different asset class."
  },
  {
    "objectID": "posts/challenge4_AnimeshSengupta.html#tidy-data-as-needed",
    "href": "posts/challenge4_AnimeshSengupta.html#tidy-data-as-needed",
    "title": "Challenge 4",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nThe data is nearly tidy, we just need to mutate the Year and quarter column and make sure all the numerical values are uniform across the dataset.\n\n\nCode\ncolnames(debt_data)\n\n\n[1] \"Year.and.Quarter\" \"Mortgage\"         \"HE.Revolving\"     \"Auto.Loan\"       \n[5] \"Credit.Card\"      \"Student.Loan\"     \"Other\"            \"Total\"           \n\n\nCode\ndebt_data1 <- debt_data%>%\n  mutate(\n    Date= parse_date_time(Year.and.Quarter,\"yq\"),\n    across(where(is.numeric), round, 4)\n  )\nhead(debt_data1,20)\n\n\n# A tibble: 20 × 9\n   Year.and.Quarter Mortgage HE.Revolving Auto.Loan Credit…¹ Stude…² Other Total\n   <chr>               <dbl>        <dbl>     <dbl>    <dbl>   <dbl> <dbl> <dbl>\n 1 03:Q1                4.94        0.242     0.641    0.688   0.241 0.478  7.23\n 2 03:Q2                5.08        0.26      0.622    0.693   0.243 0.486  7.38\n 3 03:Q3                5.18        0.269     0.684    0.693   0.249 0.477  7.56\n 4 03:Q4                5.66        0.302     0.704    0.698   0.253 0.449  8.07\n 5 04:Q1                5.84        0.328     0.72     0.695   0.260 0.446  8.29\n 6 04:Q2                5.97        0.367     0.743    0.697   0.263 0.423  8.46\n 7 04:Q3                6.21        0.426     0.751    0.706   0.33  0.41   8.83\n 8 04:Q4                6.36        0.468     0.728    0.717   0.346 0.423  9.04\n 9 05:Q1                6.51        0.502     0.725    0.71    0.364 0.394  9.21\n10 05:Q2                6.70        0.528     0.774    0.717   0.374 0.402  9.49\n11 05:Q3                6.91        0.541     0.83     0.732   0.378 0.405  9.79\n12 05:Q4                7.10        0.565     0.792    0.736   0.392 0.416 10.0 \n13 06:Q1                7.44        0.582     0.788    0.723   0.434 0.418 10.4 \n14 06:Q2                7.76        0.59      0.796    0.739   0.439 0.423 10.7 \n15 06:Q3                8.04        0.603     0.821    0.754   0.447 0.442 11.1 \n16 06:Q4                8.23        0.604     0.821    0.767   0.482 0.406 11.3 \n17 07:Q1                8.42        0.605     0.794    0.764   0.506 0.404 11.5 \n18 07:Q2                8.71        0.619     0.807    0.796   0.514 0.408 11.8 \n19 07:Q3                8.93        0.631     0.818    0.817   0.528 0.413 12.1 \n20 07:Q4                9.10        0.647     0.815    0.839   0.548 0.422 12.4 \n# … with 1 more variable: Date <dttm>, and abbreviated variable names\n#   ¹​Credit.Card, ²​Student.Loan\n# ℹ Use `colnames()` to see all variable names\n\n\nAny additional comments? So here we converted the 03:Q1 format of date to human readable date using the parse_date_time. Also , we rounded of all the numeric data to 4 decimal places for uniformity."
  },
  {
    "objectID": "posts/challenge4_AnimeshSengupta.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_AnimeshSengupta.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\n\n\nCode\ndebt_checks<-debt_data1%>%rowwise()%>%\n  mutate(Expected_sum=sum(across(.cols=c(Mortgage,HE.Revolving,Auto.Loan,Credit.Card,Student.Loan,Other))),\n  Difference=abs(Total-Expected_sum))\n\nhead(debt_checks,10)\n\n\n# A tibble: 10 × 11\n# Rowwise: \n   Year.and.Quarter Mortgage HE.Revolving Auto.Loan Credit…¹ Stude…² Other Total\n   <chr>               <dbl>        <dbl>     <dbl>    <dbl>   <dbl> <dbl> <dbl>\n 1 03:Q1                4.94        0.242     0.641    0.688   0.241 0.478  7.23\n 2 03:Q2                5.08        0.26      0.622    0.693   0.243 0.486  7.38\n 3 03:Q3                5.18        0.269     0.684    0.693   0.249 0.477  7.56\n 4 03:Q4                5.66        0.302     0.704    0.698   0.253 0.449  8.07\n 5 04:Q1                5.84        0.328     0.72     0.695   0.260 0.446  8.29\n 6 04:Q2                5.97        0.367     0.743    0.697   0.263 0.423  8.46\n 7 04:Q3                6.21        0.426     0.751    0.706   0.33  0.41   8.83\n 8 04:Q4                6.36        0.468     0.728    0.717   0.346 0.423  9.04\n 9 05:Q1                6.51        0.502     0.725    0.71    0.364 0.394  9.21\n10 05:Q2                6.70        0.528     0.774    0.717   0.374 0.402  9.49\n# … with 3 more variables: Date <dttm>, Expected_sum <dbl>, Difference <dbl>,\n#   and abbreviated variable names ¹​Credit.Card, ²​Student.Loan\n# ℹ Use `colnames()` to see all variable names\n\n\nAny additional comments? As part of Sanity checks, calculation of total debt across column needs to be verified. Across() function was used rowwise to calculate the total debt and compared the absolute value between each other. As per the data, the difference is near to zero hence the total computations are veritable."
  },
  {
    "objectID": "posts/challenge2_WillMunson.html",
    "href": "posts/challenge2_WillMunson.html",
    "title": "Challenge 2 Will Munson",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_WillMunson.html#challenge-overview",
    "href": "posts/challenge2_WillMunson.html#challenge-overview",
    "title": "Challenge 2 Will Munson",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_WillMunson.html#read-in-the-data",
    "href": "posts/challenge2_WillMunson.html#read-in-the-data",
    "title": "Challenge 2 Will Munson",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\nCode\nFAOstat <- read_csv(\"_data/FAOSTAT_livestock.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_WillMunson.html#describe-the-data",
    "href": "posts/challenge2_WillMunson.html#describe-the-data",
    "title": "Challenge 2 Will Munson",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nSo, essentially, this data is interpreting the value of livestock around the world. The values are either official or unofficial data, or FAO estimates. \n\nThe data has a total of over 82k slots. Value is the only quantitative variable. Each set of data was recorded between the years of 1961 and 2018. \n\n\nCode\nsummary(FAOstat)\n\n\n Domain Code           Domain            Area Code          Area          \n Length:82116       Length:82116       Min.   :   1.0   Length:82116      \n Class :character   Class :character   1st Qu.:  73.0   Class :character  \n Mode  :character   Mode  :character   Median : 146.0   Mode  :character  \n                                       Mean   : 912.7                     \n                                       3rd Qu.: 221.0                     \n                                       Max.   :5504.0                     \n                                                                          \n  Element Code    Element            Item Code        Item          \n Min.   :5111   Length:82116       Min.   : 866   Length:82116      \n 1st Qu.:5111   Class :character   1st Qu.: 976   Class :character  \n Median :5111   Mode  :character   Median :1034   Mode  :character  \n Mean   :5111                      Mean   :1018                     \n 3rd Qu.:5111                      3rd Qu.:1096                     \n Max.   :5111                      Max.   :1126                     \n                                                                    \n   Year Code         Year          Unit               Value          \n Min.   :1961   Min.   :1961   Length:82116       Min.   :0.000e+00  \n 1st Qu.:1976   1st Qu.:1976   Class :character   1st Qu.:1.250e+04  \n Median :1991   Median :1991   Mode  :character   Median :2.247e+05  \n Mean   :1990   Mean   :1990                      Mean   :1.163e+07  \n 3rd Qu.:2005   3rd Qu.:2005                      3rd Qu.:2.377e+06  \n Max.   :2018   Max.   :2018                      Max.   :1.490e+09  \n                                                  NA's   :1301       \n     Flag           Flag Description  \n Length:82116       Length:82116      \n Class :character   Class :character  \n Mode  :character   Mode  :character"
  },
  {
    "objectID": "posts/challenge2_WillMunson.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_WillMunson.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Will Munson",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nFAOstat %>%\n  group_by(Year) %>%\n  filter(Item == 'Sheep' & `Flag Description` == 'Official data') %>%\n  summarize(mean = mean(Value, na.rm = TRUE), sd = sd(Value, na.rm = TRUE))\n\n\n# A tibble: 58 × 3\n    Year     mean        sd\n   <dbl>    <dbl>     <dbl>\n 1  1961 9172935. 22937872.\n 2  1962 8667943. 23653127.\n 3  1963 8787122. 24057267.\n 4  1964 8612111. 23824573.\n 5  1965 8492538. 23745915.\n 6  1966 9094943. 23326897.\n 7  1967 7984239. 22894441.\n 8  1968 8688126. 24101623.\n 9  1969 8518124. 24300499.\n10  1970 7955016. 24019630.\n# … with 48 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nI added a chart here to get a better understanding of how the mean official data has changed overtime. Just having the numbers here isn't as helpful as having a chart.\n\n\nCode\nFAO <- FAOstat %>%\n  group_by(Year) %>%\n  filter(Item == 'Sheep' & `Flag Description` == 'Official data') %>%\n  summarize(mean = mean(Value, na.rm = TRUE), sd = sd(Value, na.rm = TRUE))\n\nplot(x = FAO$Year, y = FAO$mean)\n\n\n\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included.\nThis specific group I chose involved sheep and official data. Since the value changes every year, I chose to take the average value and standard deviation of sheep for each year. What appears to be happening here is the mean is the average value of sheep appears to be relatively stagnant between the years 2000 and 2010. After 2010, the official data shows a major surplus in the value of sheep."
  },
  {
    "objectID": "posts/challenge2_EmmaRasmussen.html",
    "href": "posts/challenge2_EmmaRasmussen.html",
    "title": "Challenge 2 Attempt",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n##Reading in the Data/Tidying\nReading the data set into R and skipping the first 3 rows so the header includes state, county, and total\nSelecting for columns that contain variables:\nRemoving state employee total rows:\nRenaming the TOTAL column to more accurate describe the variable:\nThe code below returns the highest and lowest number of counties (with railroad employees) by state."
  },
  {
    "objectID": "posts/challenge2_EmmaRasmussen.html#describe-the-data",
    "href": "posts/challenge2_EmmaRasmussen.html#describe-the-data",
    "title": "Challenge 2 Attempt",
    "section": "Describe the data",
    "text": "Describe the data\nThis data was likely gathered from government employment records or employment records directly from the railroads or Department of Transportation. Cases- counties within states, variables- states, and the number of employees at railroads within each county.\nDimensions of the “tidied” dataset:\n\n\nCode\ndim(StateCounty2012)\n\n\n[1] 2930    3\n\n\nThe data set has 2930 rows (counties with railroads) and 3 columns (State, County, and number of employees).\nThe code below creates a table with a count of counties by state. For example, Florida (FL) has 67 counties with railroads.\n\n\nCode\ntable(StateCounty2012$STATE)\n\n\n\n AE  AK  AL  AP  AR  AZ  CA  CO  CT  DC  DE  FL  GA  HI  IA  ID  IL  IN  KS  KY \n  1   6  67   1  72  15  55  57   8   1   3  67 152   3  99  36 103  92  95 119 \n LA  MA  MD  ME  MI  MN  MO  MS  MT  NC  ND  NE  NH  NJ  NM  NV  NY  OH  OK  OR \n 63  12  24  16  78  86 115  78  53  94  49  89  10  21  29  12  61  88  73  33 \n PA  RI  SC  SD  TN  TX  UT  VA  VT  WA  WI  WV  WY \n 65   5  46  52  91 221  25  92  14  39  69  53  22 \n\n\nI was shocked to see that Texas has 221 counties with people who work for railroads, compared to MA with 12 counties. A quick Google Search says Texas actually has 254 counties. Since Texas has the greatest number of counties with railroad employees, I will look closer at the distribution of employees across Texas counties."
  },
  {
    "objectID": "posts/challenge2_EmmaRasmussen.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_EmmaRasmussen.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Attempt",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nFiltering the subgroup of New Hampshire counties: (I used to live on the NH border)\n\n\nCode\nfilter(StateCounty2012, STATE == \"NH\")\n\n\n# A tibble: 10 × 3\n   STATE COUNTY       Total_employees\n   <chr> <chr>                  <dbl>\n 1 NH    BELKNAP                    2\n 2 NH    CARROLL                   12\n 3 NH    CHESHIRE                  28\n 4 NH    COOS                      19\n 5 NH    GRAFTON                    7\n 6 NH    HILLSBOROUGH             136\n 7 NH    MERRIMACK                  9\n 8 NH    ROCKINGHAM               146\n 9 NH    STRAFFORD                 27\n10 NH    SULLIVAN                   7\n\n\nCalculating the Mode:\n\n\nCode\nNHTable<-filter(StateCounty2012, STATE == \"NH\")%>%\n  arrange(Total_employees)%>%\n  count(Total_employees)%>%\n  arrange(desc(n))\nNHTable$Total_employees[1]\n\n\n[1] 7\n\n\nWhile not super helpful, the most common number of employees at NH railroads by county is 7.\nOther summary statistics for the subgroup of NH counties:\n\n\nCode\nStateCounty2012\n\n\n# A tibble: 2,930 × 3\n   STATE COUNTY               Total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nStateCounty2012%>%\n  filter(STATE == \"NH\") %>%\n  summarize(\"NHmin\"= min(Total_employees), \"NHmax\"= max(Total_employees), \"NHmean\"= mean(Total_employees), \"NHmedian\"= median(Total_employees), \"NHmode\"= mode(Total_employees), \"NHsd\" = sd(Total_employees), \"NHIQR\"= IQR(Total_employees))\n\n\n# A tibble: 1 × 7\n  NHmin NHmax NHmean NHmedian NHmode   NHsd NHIQR\n  <dbl> <dbl>  <dbl>    <dbl> <chr>   <dbl> <dbl>\n1     2   146   39.3     15.5 numeric  54.3  20.2\n\n\nFinding the county in each state with the largest number of employees:\n\n\nCode\nStateCountyLarge<-group_by(StateCounty2012, STATE)%>%\n  arrange(StateCounty2012, (\"STATE\"), desc(\"Total_employees\"))%>%\n  slice(1)\nStateCountyLarge\n\n\n# A tibble: 53 × 3\n# Groups:   STATE [53]\n   STATE COUNTY        Total_employees\n   <chr> <chr>                   <dbl>\n 1 AE    APO                         2\n 2 AK    ANCHORAGE                   7\n 3 AL    AUTAUGA                   102\n 4 AP    APO                         1\n 5 AR    ARKANSAS                   11\n 6 AZ    APACHE                    270\n 7 CA    ALAMEDA                   346\n 8 CO    ADAMS                     553\n 9 CT    FAIRFIELD                 486\n10 DC    WASHINGTON DC             279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nFinding the mean of the largest total employee numbers in each state (this is a pretty useless stat but I was just trying to find some summary statistics from a grouped/sliced data set). I don’t love this code, there are too many variables but it’s not working when I edit it any more to try to calculate the median etc.\n\n\nCode\nStateMax<-summarize(StateCountyLarge, mt= mean(Total_employees))\nTotalStateMean<-summarize(StateMax, mt=mean(mt))\nprint(TotalStateMean)\n\n\n# A tibble: 1 × 1\n     mt\n  <dbl>\n1  93.0\n\n\nThe mean of the largest total employee numbers across the 53 included states/territories is 93.03774 employees.\n\nExplain and Interpret\nI chose NH because I was curious about the distribution of railroad employees by county in that area (I grew up on the NH border). There is lots of variability in NH railroad employment. Rockingham county has 146 employees, while Belknap county has only 2 railroad employees. I then tried to slice out the counties with the most employees by state to see how many employees there are on average at the “biggest” railroad counties in each state. I was surprised comparing the NH max to the mean of the largest railroads by county in each state(the second data analyzed that was sliced). I was surprise given the mean number of employees at the “largest” railroad/county in each state was only 93, but the max for NH is 146. I don’t think of NH has a big state or having a lot of railroads. One thing that might expplain this is that states like Texas (over 100) have more counties than NH (10) skewing the mean (93.0377). In comparison, NH would have larger employment numbers by county given it only has 10 counties."
  },
  {
    "objectID": "posts/challenge3_instructions.html",
    "href": "posts/challenge3_instructions.html",
    "title": "Challenge 3 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions.html#challenge-overview",
    "href": "posts/challenge3_instructions.html#challenge-overview",
    "title": "Challenge 3 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions.html#read-in-data",
    "href": "posts/challenge3_instructions.html#read-in-data",
    "title": "Challenge 3 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\n\n\nBriefly describe the data\nDescribe the data, and be sure to comment on why you are planning to pivot it to make it “tidy”"
  },
  {
    "objectID": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "href": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "title": "Challenge 3 Instructions",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA    1394.    1265.\n2 USA      1990 NAFTA    1114.    1114.\n3 France   1980 EU        497.   -1433.\n4 Mexico   1990 NAFTA     409.     469.\n5 USA      1980 NAFTA    1155.    1570.\n6 France   1990 EU        652.     345.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge3_instructions.html#pivot-the-data",
    "href": "posts/challenge3_instructions.html#pivot-the-data",
    "title": "Challenge 3 Instructions",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1394.\n 2 Mexico   1980 NAFTA incoming              1265.\n 3 USA      1990 NAFTA outgoing              1114.\n 4 USA      1990 NAFTA incoming              1114.\n 5 France   1980 EU    outgoing               497.\n 6 France   1980 EU    incoming             -1433.\n 7 Mexico   1990 NAFTA outgoing               409.\n 8 Mexico   1990 NAFTA incoming               469.\n 9 USA      1980 NAFTA outgoing              1155.\n10 USA      1980 NAFTA incoming              1570.\n11 France   1990 EU    outgoing               652.\n12 France   1990 EU    incoming               345.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge2_ManiShankerKamarapu.html",
    "href": "posts/challenge2_ManiShankerKamarapu.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_ManiShankerKamarapu.html#read-in-the-data",
    "href": "posts/challenge2_ManiShankerKamarapu.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\nFAOSTAT_cattle_dairy <- read_csv(\"_data/FAOSTAT_cattle_dairy.csv\")\nView(FAOSTAT_cattle_dairy)\n\n\nAs we read the data, there are many interesting variables and a lot of observations based on yearly analysis of the various cattle products."
  },
  {
    "objectID": "posts/challenge2_ManiShankerKamarapu.html#describe-the-data",
    "href": "posts/challenge2_ManiShankerKamarapu.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\ndim(FAOSTAT_cattle_dairy)\n\n\n[1] 36449    14\n\n\nWe can see that there are 36449 rows and 14 columns in the data. We can see from observations from different variables that “Value” variable is pretty much the only real valuable data we are getting from this data set and remaining variables are either unchanged(Domain and Item) or grouping variables(Area, Element, Year and Flag) which can be used to summarize data and find the categories we are interested in and also can be used to find specific information we need and let’s try to get to some more useful info using some other functions.\n\n\nCode\ncolnames(FAOSTAT_cattle_dairy)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nThese are the variables available in the data set.\n\n\nCode\nsummary(filter(select(FAOSTAT_cattle_dairy, Value)))\n\n\n     Value          \n Min.   :        7  \n 1st Qu.:     7849  \n Median :    43266  \n Mean   :  4410235  \n 3rd Qu.:   700000  \n Max.   :683217055  \n NA's   :74         \n\n\nIt is the summary of the Value variable of the data set which is the only real data variable in data set.\n\n\nCode\nArea_num <- distinct(FAOSTAT_cattle_dairy, Area)\nElement_num <- distinct(FAOSTAT_cattle_dairy, Element)\nYear_num <- distinct(FAOSTAT_cattle_dairy, Year)\nFlag_num <- distinct(FAOSTAT_cattle_dairy, Flag)\nDistinct_count <- c(nrow(Area_num), nrow(Element_num), nrow(Year_num), nrow(Flag_num))\nDistinct_count\n\n\n[1] 232   3  58   7\n\n\nWe can observe that data set has the data from 1961-2018 on 3 elements at 232 different areas.\nI am interested in finding in more information about Flag and how it corresponds to the description they have given and can we group the values by using it.\n\n\nCode\nFLag_info <- FAOSTAT_cattle_dairy %>%\n  select(Flag,`Flag Description`)\nunique(FLag_info)\n\n\n# A tibble: 7 × 2\n  Flag  `Flag Description`                                                      \n  <chr> <chr>                                                                   \n1 F     FAO estimate                                                            \n2 Fc    Calculated data                                                         \n3 <NA>  Official data                                                           \n4 *     Unofficial figure                                                       \n5 Im    FAO data based on imputation methodology                                \n6 M     Data not available                                                      \n7 A     Aggregate, may include official, semi-official, estimated or calculated…\n\n\nFrom the above observation we got to know there are 7 types of flags and it is based on data provided and type of data and values they are and how is has been taken."
  },
  {
    "objectID": "posts/challenge2_ManiShankerKamarapu.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_ManiShankerKamarapu.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\n\nArea and Element wise analysis\n\n\nCode\nAE_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Area, Element) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE))\nAE_analysis\n\n\n# A tibble: 695 × 8\n# Groups:   Area [232]\n   Area        Element      mean_value median_…¹ sd_va…² min_v…³ max_v…⁴ IQR_v…⁵\n   <chr>       <chr>             <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Afghanistan Milk Animals   1790863.  1150000   1.08e6  7   e5  4.05e6  1.57e6\n 2 Afghanistan Production      914876.   600000   5.01e5  3   e5  1.87e6  8.86e5\n 3 Afghanistan Yield             5246.     5128   9.06e2  3.59e3  7.33e3  6.35e2\n 4 Africa      Milk Animals  37629052. 33173522.  1.75e7  1.70e7  7.13e7  3.00e7\n 5 Africa      Production    18949137. 15058869   1.02e7  7.65e6  3.68e7  1.88e7\n 6 Africa      Yield             4869.     4600   4.39e2  4.31e3  5.84e3  6.93e2\n 7 Albania     Milk Animals    288774.   284650   1.13e5  1.17e5  4.88e5  1.92e5\n 8 Albania     Production      521367.   392500   3.37e5  7.83e4  9.83e5  6.78e5\n 9 Albania     Yield            16197     13857   6.61e3  6.35e3  2.84e4  8.11e3\n10 Algeria     Milk Animals    659719.   640000   2.50e5  2.63e5  1.11e6  4.14e5\n# … with 685 more rows, and abbreviated variable names ¹​median_value,\n#   ²​sd_value, ³​min_value, ⁴​max_value, ⁵​IQR_value\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs per the analysis, in each area the we have three kinds of elements and there are a lot of variations and mean value is not dependent to element itself in any area but it differs in each area and we can get a deeper understanding when we analyse the area and element separately.\n\n\nYear and Element wise analysis\n\n\nCode\nYE_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Year, Element) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE))\nYE_analysis\n\n\n# A tibble: 174 × 8\n# Groups:   Year [58]\n    Year Element      mean_value median_value  sd_value min_va…¹ max_v…² IQR_v…³\n   <dbl> <chr>             <dbl>        <dbl>     <dbl>    <dbl>   <dbl>   <dbl>\n 1  1961 Milk Animals   3582516.       200550 15152245.        8  1.77e8  1.23e6\n 2  1961 Production     6335891.       100368 28456161.        7  3.14e8  1.74e6\n 3  1961 Yield            11949.         8812     9905.     1200  4.29e4  1.41e4\n 4  1962 Milk Animals   3567116.       200850 15117111.       17  1.77e8  1.21e6\n 5  1962 Production     6384721.       100000 28667301.       23  3.16e8  1.76e6\n 6  1962 Yield            12105.         8899    10173.     1200  4.43e4  1.41e4\n 7  1963 Milk Animals   3578137.       219550 15189434.       20  1.77e8  1.21e6\n 8  1963 Production     6319684.       102500 28305726.       20  3.13e8  1.78e6\n 9  1963 Yield            12147.         9051    10241.     1191  4.59e4  1.44e4\n10  1964 Milk Animals   3554094.       215650 15090377.       25  1.76e8  1.25e6\n# … with 164 more rows, and abbreviated variable names ¹​min_value, ²​max_value,\n#   ³​IQR_value\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs per the data, we can say that the elements has been increased by passing year, there are some fluctuations in between but there is an overall increase by the year.\n\n\nElement-wise analysis\n\n\nCode\nE_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Element) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE)) %>%\n  arrange(desc(mean_value))\nE_analysis\n\n\n# A tibble: 3 × 7\n  Element      mean_value median_value  sd_value min_value max_value IQR_value\n  <chr>             <dbl>        <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n1 Production     9001419.       295500 40268994.         7 683217055   2736860\n2 Milk Animals   4205410.       295000 18041595.         8 276573845   1525233\n3 Yield            19329.        13218    19361.       923    134121     21367\n\n\nAs per the above observation, the mean value of the production is maximum and yield is the minimum. It shows that the production value is more with the less yield and it leads to an overall profit.\n\n\nArea-wise analysis\n\n\nCode\nA_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Area) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE)) %>%\n  arrange(desc(mean_value))\nA_analysis\n\n\n# A tibble: 232 × 7\n   Area                     mean_value median_…¹ sd_va…² min_v…³ max_v…⁴ IQR_v…⁵\n   <chr>                         <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 World                    230162236.    2.20e8  2.02e8   17662  6.83e8  3.87e8\n 2 Europe                    99143192.    8.26e7  9.82e7   21977  2.80e8  2.09e8\n 3 Americas                  56060267.    4.51e7  5.50e7   22342  1.84e8  9.12e7\n 4 Asia                      48224349.    3.71e7  5.08e7    6034  2.13e8  7.49e7\n 5 Eastern Europe            45406646.    4.82e7  4.33e7   16644  1.40e8  7.46e7\n 6 USSR                      42870195.    4.18e7  3.68e7   15900  1.08e8  8.09e7\n 7 Western Europe            30380343.    1.67e7  3.27e7   30476  8.74e7  7.21e7\n 8 Northern America          29827044.    1.14e7  3.49e7   32302  1.06e8  6.43e7\n 9 Southern Asia             28165903.    2.50e7  2.75e7    3824  1.17e8  4.64e7\n10 United States of America  26633354.    1.00e7  3.15e7   33068  9.87e7  5.61e7\n# … with 222 more rows, and abbreviated variable names ¹​median_value,\n#   ²​sd_value, ³​min_value, ⁴​max_value, ⁵​IQR_value\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs per the analysis, Europe has the highest cattle dairy and British Virgin Islands has the minimum, we can say none.\n\n\nYear-wise analysis\n\n\nCode\nY_analysis <- FAOSTAT_cattle_dairy %>%\n  group_by(Year) %>%\n  summarise(mean_value=mean(Value, na.rm = TRUE), median_value=median(Value, na.rm = TRUE), sd_value=sd(Value, na.rm = TRUE), min_value=min(Value, na.rm = TRUE), max_value=max(Value, na.rm = TRUE), IQR_value=IQR(Value, na.rm = TRUE)) %>%\n  arrange(desc(mean_value))\nY_analysis\n\n\n# A tibble: 58 × 7\n    Year mean_value median_value  sd_value min_value max_value IQR_value\n   <dbl>      <dbl>        <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1  2017   5682309.       80522. 33349614.        20 677670685   958680 \n 2  2018   5654115.       79029  33488177.        21 683217055   938124 \n 3  2016   5617407.       77378. 32809823.        20 665596536   981852.\n 4  2015   5586308.       78062. 32577146.        20 661430554   964494 \n 5  2014   5534018.       77490  32246947.        20 655245580   947544.\n 6  2013   5411995.       75958. 31386003.        20 635379383   964509 \n 7  2012   5372784.       74057  31140061.        20 630244839   946680 \n 8  2011   5287165.       71750  30561395.        20 616177381   953420.\n 9  2010   5185483.       70586  29911569.        20 601868328   928111 \n10  2009   5088955.       70375  29357924.         9 590471016   895700 \n# … with 48 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs per the data, the cattle dairy production has been increased by the passing year, there are some fluctuations in middle but the overall it is increasing, we can see from the data that the increase is nearly 85 percentage from 1961 to 2018.\n\n\nExplain and Interpret\nI chose Element, Area and Year subgroups from data set. The reason for choosing them is there importance in analysis and also tried to analysis using two variables to know the dependency of values on different variables. So I have done Area and Element wise analysis, Year and Element wise analysis, Element-wise analysis, Area-wise analysis and Year-wise analysis. Conclusion to my analysis would be Europe has the highest of the cattle dairy and the production of the dairy is increasing gradually as by the year."
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html",
    "href": "posts/challenge1_TylerTewksbury.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html#challenge-overview",
    "href": "posts/challenge1_TylerTewksbury.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html#read-in-the-data",
    "href": "posts/challenge1_TylerTewksbury.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nrailroad_data <- read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_TylerTewksbury.html#describe-the-data",
    "href": "posts/challenge1_TylerTewksbury.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nrailroad_data %>%\nsummary()\n\n\n    state              county          total_employees  \n Length:2930        Length:2930        Min.   :   1.00  \n Class :character   Class :character   1st Qu.:   7.00  \n Mode  :character   Mode  :character   Median :  21.00  \n                                       Mean   :  87.18  \n                                       3rd Qu.:  65.00  \n                                       Max.   :8207.00  \n\n\nCode\nlength(unique(railroad_data$state))\n\n\n[1] 53\n\n\nThe dataset is a simple, pre cleaned spreadsheet consisting of three columns: State, County, and Total_Employees. These three columns are self explanatory, labeling the state, county, and the amount of employees in said railroad system. Using the summary function,we can see that there are 2930 reported counties within 53 states (including DC, Armed Forces Europe, Armed Forces Pacific). This data could be useful to determine the size of specific railroad systems based on employment. The dataset could be enhanced by adding in overall population data so one can answer questions such as the percent of people in a county who work on railroads."
  },
  {
    "objectID": "posts/challenge4_Akhilesh.html",
    "href": "posts/challenge4_Akhilesh.html",
    "title": "Challenge 4 Akhilesh",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_Akhilesh.html#challenge-overview",
    "href": "posts/challenge4_Akhilesh.html#challenge-overview",
    "title": "Challenge 4 Akhilesh",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_Akhilesh.html#read-in-data",
    "href": "posts/challenge4_Akhilesh.html#read-in-data",
    "title": "Challenge 4 Akhilesh",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data\n\n\nCode\nprint(summarytools::dfSummary(animal_weight,\n                              varnumbers = FALSE,\n                              plain.ascii  = FALSE,\n                              style        = \"grid\",\n                              graph.magnif = 0.50,\n                              valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nanimal_weight\nDimensions: 9 x 17\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      IPCC Area\n[character]\n      1. Africa2. Asia3. Eastern Europe4. Indian Subcontinent5. Latin America6. Middle east7. Northern America8. Oceania9. Western Europe\n      1(11.1%)1(11.1%)1(11.1%)1(11.1%)1(11.1%)1(11.1%)1(11.1%)1(11.1%)1(11.1%)\n      \n      0\n(0.0%)\n    \n    \n      Cattle - dairy\n[numeric]\n      Mean (sd) : 425.4 (140.4)min ≤ med ≤ max:275 ≤ 400 ≤ 604IQR (CV) : 275 (0.3)\n      275:3(33.3%)350:1(11.1%)400:1(11.1%)500:1(11.1%)550:1(11.1%)600:1(11.1%)604:1(11.1%)\n      \n      0\n(0.0%)\n    \n    \n      Cattle - non-dairy\n[numeric]\n      Mean (sd) : 298 (116.3)min ≤ med ≤ max:110 ≤ 330 ≤ 420IQR (CV) : 218 (0.4)\n      110:1(11.1%)173:2(22.2%)305:1(11.1%)330:1(11.1%)389:1(11.1%)391:2(22.2%)420:1(11.1%)\n      \n      0\n(0.0%)\n    \n    \n      Buffaloes\n[numeric]\n      Min  : 295Mean : 370.6Max  : 380\n      295:1(11.1%)380:8(88.9%)\n      \n      0\n(0.0%)\n    \n    \n      Swine - market\n[numeric]\n      Mean (sd) : 39.2 (10.8)min ≤ med ≤ max:28 ≤ 45 ≤ 50IQR (CV) : 22 (0.3)\n      28:4(44.4%)45:1(11.1%)46:1(11.1%)50:3(33.3%)\n      \n      0\n(0.0%)\n    \n    \n      Swine - breeding\n[numeric]\n      Mean (sd) : 116.4 (84.2)min ≤ med ≤ max:28 ≤ 180 ≤ 198IQR (CV) : 152 (0.7)\n      28:4(44.4%)180:3(33.3%)198:2(22.2%)\n      \n      0\n(0.0%)\n    \n    \n      Chicken - Broilers\n[numeric]\n      1 distinct value\n      0.90:9(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Chicken - Layers\n[numeric]\n      1 distinct value\n      1.80:9(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Ducks\n[numeric]\n      1 distinct value\n      2.70:9(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Turkeys\n[numeric]\n      1 distinct value\n      6.80:9(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Sheep\n[numeric]\n      Min  : 28Mean : 39.4Max  : 48.5\n      28.00:4(44.4%)48.50:5(55.6%)\n      \n      0\n(0.0%)\n    \n    \n      Goats\n[numeric]\n      Min  : 30Mean : 34.7Max  : 38.5\n      30.00:4(44.4%)38.50:5(55.6%)\n      \n      0\n(0.0%)\n    \n    \n      Horses\n[numeric]\n      Min  : 238Mean : 315.2Max  : 377\n      238:4(44.4%)377:5(55.6%)\n      \n      0\n(0.0%)\n    \n    \n      Asses\n[numeric]\n      1 distinct value\n      130:9(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Mules\n[numeric]\n      1 distinct value\n      130:9(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Camels\n[numeric]\n      1 distinct value\n      217:9(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Llamas\n[numeric]\n      1 distinct value\n      217:9(100.0%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\n\nColumn names of the dataframe\n\n\nCode\ncolnames(animal_weight)\n\n\n [1] \"IPCC Area\"          \"Cattle - dairy\"     \"Cattle - non-dairy\"\n [4] \"Buffaloes\"          \"Swine - market\"     \"Swine - breeding\"  \n [7] \"Chicken - Broilers\" \"Chicken - Layers\"   \"Ducks\"             \n[10] \"Turkeys\"            \"Sheep\"              \"Goats\"             \n[13] \"Horses\"             \"Asses\"              \"Mules\"             \n[16] \"Camels\"             \"Llamas\"            \n\n\n\n\nColumn classes of the dataframe\n\n\nCode\ncol_classes = data.frame(t(data.frame(lapply(animal_weight,class))))\ncol_classes\n\n\n                   t.data.frame.lapply.animal_weight..class...\nIPCC.Area                                            character\nCattle...dairy                                         numeric\nCattle...non.dairy                                     numeric\nBuffaloes                                              numeric\nSwine...market                                         numeric\nSwine...breeding                                       numeric\nChicken...Broilers                                     numeric\nChicken...Layers                                       numeric\nDucks                                                  numeric\nTurkeys                                                numeric\nSheep                                                  numeric\nGoats                                                  numeric\nHorses                                                 numeric\nAsses                                                  numeric\nMules                                                  numeric\nCamels                                                 numeric\nLlamas                                                 numeric\n\n\n\n\nSummary, Dataframe\n\n\nCode\nsummary(animal_weight)\n\n\n  IPCC Area         Cattle - dairy  Cattle - non-dairy   Buffaloes    \n Length:9           Min.   :275.0   Min.   :110        Min.   :295.0  \n Class :character   1st Qu.:275.0   1st Qu.:173        1st Qu.:380.0  \n Mode  :character   Median :400.0   Median :330        Median :380.0  \n                    Mean   :425.4   Mean   :298        Mean   :370.6  \n                    3rd Qu.:550.0   3rd Qu.:391        3rd Qu.:380.0  \n                    Max.   :604.0   Max.   :420        Max.   :380.0  \n Swine - market  Swine - breeding Chicken - Broilers Chicken - Layers\n Min.   :28.00   Min.   : 28.0    Min.   :0.9        Min.   :1.8     \n 1st Qu.:28.00   1st Qu.: 28.0    1st Qu.:0.9        1st Qu.:1.8     \n Median :45.00   Median :180.0    Median :0.9        Median :1.8     \n Mean   :39.22   Mean   :116.4    Mean   :0.9        Mean   :1.8     \n 3rd Qu.:50.00   3rd Qu.:180.0    3rd Qu.:0.9        3rd Qu.:1.8     \n Max.   :50.00   Max.   :198.0    Max.   :0.9        Max.   :1.8     \n     Ducks        Turkeys        Sheep           Goats           Horses     \n Min.   :2.7   Min.   :6.8   Min.   :28.00   Min.   :30.00   Min.   :238.0  \n 1st Qu.:2.7   1st Qu.:6.8   1st Qu.:28.00   1st Qu.:30.00   1st Qu.:238.0  \n Median :2.7   Median :6.8   Median :48.50   Median :38.50   Median :377.0  \n Mean   :2.7   Mean   :6.8   Mean   :39.39   Mean   :34.72   Mean   :315.2  \n 3rd Qu.:2.7   3rd Qu.:6.8   3rd Qu.:48.50   3rd Qu.:38.50   3rd Qu.:377.0  \n Max.   :2.7   Max.   :6.8   Max.   :48.50   Max.   :38.50   Max.   :377.0  \n     Asses         Mules         Camels        Llamas   \n Min.   :130   Min.   :130   Min.   :217   Min.   :217  \n 1st Qu.:130   1st Qu.:130   1st Qu.:217   1st Qu.:217  \n Median :130   Median :130   Median :217   Median :217  \n Mean   :130   Mean   :130   Mean   :217   Mean   :217  \n 3rd Qu.:130   3rd Qu.:130   3rd Qu.:217   3rd Qu.:217  \n Max.   :130   Max.   :130   Max.   :217   Max.   :217"
  },
  {
    "objectID": "posts/challenge4_Akhilesh.html#tidy-data-as-needed",
    "href": "posts/challenge4_Akhilesh.html#tidy-data-as-needed",
    "title": "Challenge 4 Akhilesh",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\n\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\ntidy data using pivot_longer, so that obervations represent individual observation and columns represent individual variable\n\n\nCode\nanimal_weight_pivot <- pivot_longer(animal_weight, col = names(animal_weight)[2:17], names_to = 'animal_name', values_to = 'weight')\nanimal_weight_pivot\n\n\n# A tibble: 144 × 3\n   `IPCC Area`         animal_name        weight\n   <chr>               <chr>               <dbl>\n 1 Indian Subcontinent Cattle - dairy      275  \n 2 Indian Subcontinent Cattle - non-dairy  110  \n 3 Indian Subcontinent Buffaloes           295  \n 4 Indian Subcontinent Swine - market       28  \n 5 Indian Subcontinent Swine - breeding     28  \n 6 Indian Subcontinent Chicken - Broilers    0.9\n 7 Indian Subcontinent Chicken - Layers      1.8\n 8 Indian Subcontinent Ducks                 2.7\n 9 Indian Subcontinent Turkeys               6.8\n10 Indian Subcontinent Sheep                28  \n# … with 134 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge4_Akhilesh.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_Akhilesh.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Akhilesh",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\n\ncol_classes below provide column wise class of dataframe animal_weight_pivot\n\n\n‘IPCC Area’ and ‘animal_name’ are character class, and converted to factor class using mutate_at\n\n\nCode\ncol_classes = data.frame(t(data.frame(lapply(animal_weight_pivot,class))))\n\n\nanimal_weight_pivot %>% \n  mutate_at(c('IPCC Area', 'animal_name'), factor)\n\n\n# A tibble: 144 × 3\n   `IPCC Area`         animal_name        weight\n   <fct>               <fct>               <dbl>\n 1 Indian Subcontinent Cattle - dairy      275  \n 2 Indian Subcontinent Cattle - non-dairy  110  \n 3 Indian Subcontinent Buffaloes           295  \n 4 Indian Subcontinent Swine - market       28  \n 5 Indian Subcontinent Swine - breeding     28  \n 6 Indian Subcontinent Chicken - Broilers    0.9\n 7 Indian Subcontinent Chicken - Layers      1.8\n 8 Indian Subcontinent Ducks                 2.7\n 9 Indian Subcontinent Turkeys               6.8\n10 Indian Subcontinent Sheep                28  \n# … with 134 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\ncol_classes for sanity check\n\n\nCode\ncol_classes = data.frame(t(data.frame(lapply(animal_weight_pivot,class))))\ncol_classes\n\n\n            t.data.frame.lapply.animal_weight_pivot..class...\nIPCC.Area                                           character\nanimal_name                                         character\nweight                                                numeric"
  },
  {
    "objectID": "posts/challenge3_Akhilesh.html",
    "href": "posts/challenge3_Akhilesh.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_Akhilesh.html#challenge-overview",
    "href": "posts/challenge3_Akhilesh.html#challenge-overview",
    "title": "Challenge 3",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_Akhilesh.html#read-in-data",
    "href": "posts/challenge3_Akhilesh.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data\nprint(summarytools::dfSummary(hotel_bookings, varnumbers = FALSE, plain.ascii = FALSE, style = “grid”, graph.magnif = 0.70, valid.col = FALSE), method = ‘render’, table.classes = ‘table-condensed’)\n\nThe data is has 9 rows and 17 columns\nThe dataset contains, weight data of 16 different animal categories across 9 different Intergovernmental Panel on Climate Change (IPCC) Participants.\nIndividual rows don’t represent individual observations\nAnd Also all columns contain same weight variable\nThis a good example to apply pivot_longer(), as animal-type column along with animal_weight will be created through pivot_longer, where individual row will represent individual observations for a set of IPCC & animal type"
  },
  {
    "objectID": "posts/challenge3_Akhilesh.html#anticipate-the-end-result",
    "href": "posts/challenge3_Akhilesh.html#anticipate-the-end-result",
    "title": "Challenge 3",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\n\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\n\n\nCode\n# The pivoted output will be of 12 * 5 dimension, i.e 12 rows and 5 columns and also cases = 1\n\n# number of rows of the pivoted dataset\n\nn_row_pivot = nrow(df) * (ncol(df)-3)\n\n#number of columns of the pivoted dataset\n\nn_col_pivot = (ncol(df)-2)+2\n\n12*5 # 12 rows and 5 columns\n\n\n[1] 60"
  },
  {
    "objectID": "posts/challenge3_Akhilesh.html#pivot-the-data",
    "href": "posts/challenge3_Akhilesh.html#pivot-the-data",
    "title": "Challenge 3",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\ndf_pivot<-pivot_longer(df, col = c(‘outgoing’, ‘incoming’), names_to=“trade_direction”, values_to = “trade_value”) df_pivot\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1323.\n 2 Mexico   1980 NAFTA incoming               517.\n 3 USA      1990 NAFTA outgoing              1784.\n 4 USA      1990 NAFTA incoming              1033.\n 5 France   1980 EU    outgoing              1463.\n 6 France   1980 EU    incoming              1086.\n 7 Mexico   1990 NAFTA outgoing              1106.\n 8 Mexico   1990 NAFTA incoming               723.\n 9 USA      1980 NAFTA outgoing               905.\n10 USA      1980 NAFTA incoming              3686.\n11 France   1990 EU    outgoing               942.\n12 France   1990 EU    incoming              1825.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\nCode\ncase =1 \n\nanimal_weight_pivot <- pivot_longer(animal_weight, col = names(animal_weight)[2:17], names_to = 'animal_name', values_to = 'weight')\n\n\"Now each column is an individual variable and each row is a individual observation, so it can be called a tidy data\"\n\n\n[1] \"Now each column is an individual variable and each row is a individual observation, so it can be called a tidy data\"\n\n\nCode\n#number of rows in animal_weight dataset = 9\n#number of columns in animal_weight dataset = 17\n#case = 1\n#$n = 9, $k-1 = 16\n\n# pivot_longer output dataset\n  #number_of_rows = 9 * 16 = 144\n  #number_of_columns = 1 +2 = 3\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html",
    "href": "posts/challenge2_TylerTewksbury.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#challenge-overview",
    "href": "posts/challenge2_TylerTewksbury.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#read-in-the-data",
    "href": "posts/challenge2_TylerTewksbury.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\nCode\ndf <- read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#describe-the-data",
    "href": "posts/challenge2_TylerTewksbury.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nsummary(df)\n\n\n    state              county          total_employees  \n Length:2930        Length:2930        Min.   :   1.00  \n Class :character   Class :character   1st Qu.:   7.00  \n Mode  :character   Mode  :character   Median :  21.00  \n                                       Mean   :  87.18  \n                                       3rd Qu.:  65.00  \n                                       Max.   :8207.00  \n\n\nThis summary shows the amount of values in the three columns, as well as a high level overview of said values. State and county are both characters, which likely was taken from an existing survey data spreadsheet. Summary also gives us a brief insight into the total_employees, showing the max, min, median, etc. Just with this summary, questions can be asked about the data. How many counties correlate to each state? Do states with more counties have a lower average of employees? Many questions can be asked, but this correlation between county count and employee count average will be looked into."
  },
  {
    "objectID": "posts/challenge2_TylerTewksbury.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_TylerTewksbury.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nstates_mean <- df %>%\n  group_by(state) %>%\n  summarise(mean_employee = mean(total_employees)) %>%\n  arrange(desc(mean_employee), .by_group = TRUE)\n\nstate_county_count <- df %>%\n  count(state) %>%\n  arrange(n)\n\nstates_mean <- df %>%\n  group_by(state) %>%\n  summarise(mean_employee = mean(total_employees)) %>%\n  arrange(desc(mean_employee), .by_group = TRUE)\n\nstate_county_count <- df %>%\n  count(state) %>%\n  arrange(n)\n\nstate_info <- inner_join(state_county_count, states_mean)\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included.\nI performed three different analyses to get a deeper understanding of the data. First, I grouped by state and calculated the mean of total employees per state, and sorted by descending. This allowed me to see the states with, what can be presumed to be, the highest concentration of employees. Next, to get a better understanding at the state level, I calculated the amount of counties in each state, sorting by ascending. This would allow me to confirm if a state with a high mean would have few counties as well. The theory seemed to be true, as there were a few I recognized by manually looking. However, I could confirm this using R. To do this, I simply repeated the first two analayses, this time creating new data frames out of them. I then ran an inner join to join via state, creating a new table with all the data I calculated. Using this table, visualizations can be made to test/prove the hypothesis of low county count = higher number of employees in their counties."
  },
  {
    "objectID": "posts/challenge4_jerinjacob.html",
    "href": "posts/challenge4_jerinjacob.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_jerinjacob.html#read-in-data",
    "href": "posts/challenge4_jerinjacob.html#read-in-data",
    "title": "Challenge 4",
    "section": "Read in data",
    "text": "Read in data\nThe debt dataset is the category wise debt data in trillions from the first quarter of the year 2003 to the second quarter of 2021.\n\n\nCode\ndebt <-read_excel(\"_data/debt_in_trillions.xlsx\", skip= 1, col_names = c(\"Year_Quarter\", \"Mortgage\", \"HE_Revolving\", \"Auto_Loan\", \"Credit_Card\", \"Student_Loan\", \"Other\", \"Total\")) \ndebt\n\n\n# A tibble: 74 × 8\n   Year_Quarter Mortgage HE_Revolving Auto_Loan Credit_Card Studen…¹ Other Total\n   <chr>           <dbl>        <dbl>     <dbl>       <dbl>    <dbl> <dbl> <dbl>\n 1 03:Q1            4.94        0.242     0.641       0.688    0.241 0.478  7.23\n 2 03:Q2            5.08        0.26      0.622       0.693    0.243 0.486  7.38\n 3 03:Q3            5.18        0.269     0.684       0.693    0.249 0.477  7.56\n 4 03:Q4            5.66        0.302     0.704       0.698    0.253 0.449  8.07\n 5 04:Q1            5.84        0.328     0.72        0.695    0.260 0.446  8.29\n 6 04:Q2            5.97        0.367     0.743       0.697    0.263 0.423  8.46\n 7 04:Q3            6.21        0.426     0.751       0.706    0.33  0.41   8.83\n 8 04:Q4            6.36        0.468     0.728       0.717    0.346 0.423  9.04\n 9 05:Q1            6.51        0.502     0.725       0.71     0.364 0.394  9.21\n10 05:Q2            6.70        0.528     0.774       0.717    0.374 0.402  9.49\n# … with 64 more rows, and abbreviated variable name ¹​Student_Loan\n# ℹ Use `print(n = ...)` to see more rows\n\n\nIt has 6 types of debt variables namely Mortgage, Home Equity revolving debt, Auto Loan, Credit Card, Student Loan and a column for all other debts. The total debt got almost doubled over the period from 2003 to 2021."
  },
  {
    "objectID": "posts/challenge4_jerinjacob.html#tidy-data-as-needed",
    "href": "posts/challenge4_jerinjacob.html#tidy-data-as-needed",
    "title": "Challenge 4",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\n\n\nCode\ndebt<-debt%>%\n  separate(\"Year_Quarter\", into=c(\"Year\", \"Quarter\"), sep=\":\")%>%\n  fill(Quarter)\n#debt\n\n\n\n\nCode\ndebt <- debt %>%\n  filter(Quarter == \"Q4\") %>%\n  select(!contains(\"Quarter\"))\n#debt\n\n\nThe data has been filtered so that we can take the debt details at the year-ending quarter.\n\n\nCode\nattach(debt)\npar(mfrow=c(3,2))\nplot(Year, Mortgage, main=\"Scatterplot of Year vs. Mortgage\")\nplot(Year, HE_Revolving, main=\"Scatterplot of Year vs HE_Revolving\")\nplot(Year, Auto_Loan, main=\"Scatterplot of Year vs Auto_Loan\")\nplot(Year, Credit_Card, main=\"Scatterplot of Year vs Credit_Card\")\nplot(Year, Student_Loan, main=\"Scatterplot of Year vs Student_Loan\")\nplot(Year, Other, main=\"Scatterplot of Year vs Other\")\n\n\n\n\n\nWhen the data is being visualized, we get a more clear idea about the trend for each type of debts over the years. Student loan showed a steady increase in the whole period. Mortgage, Auto Loan and Credit card showed an up trend from the year 2003 to 2008 and then had a decline for a couple of years and again started to regain the uptrend. HE Revolving, eventhough doubled in the period till 2008, continuously declined after that."
  },
  {
    "objectID": "posts/challenge4_jerinjacob.html#contribution-to-the-total-debt",
    "href": "posts/challenge4_jerinjacob.html#contribution-to-the-total-debt",
    "title": "Challenge 4",
    "section": "Contribution to the total debt",
    "text": "Contribution to the total debt\nEach variables’ contribution to the total debt is taken as percentage values.\n\n\nCode\ndebt_in_percentage <- debt %>%\n  mutate(Mortgage = (Mortgage/Total)*100, \n         HE_Rev = (HE_Revolving/Total)*100, \n         AutoLoan = (Auto_Loan/Total)*100, \n         CreditCard = (Credit_Card/Total)*100,\n        StudentLoan = (Student_Loan/Total)*100,\n        Others = (Other/Total)*100)\ndebt_in_percentage <- debt_in_percentage %>%\n    select(Year, Mortgage, HE_Rev, AutoLoan, CreditCard, StudentLoan, Others)\ndebt_in_percentage\n\n\n# A tibble: 18 × 7\n   Year  Mortgage HE_Rev AutoLoan CreditCard StudentLoan Others\n   <chr>    <dbl>  <dbl>    <dbl>      <dbl>       <dbl>  <dbl>\n 1 03        70.2   3.74     8.73       8.65        3.14   5.56\n 2 04        70.3   5.18     8.05       7.93        3.82   4.68\n 3 05        71.0   5.65     7.92       7.36        3.92   4.15\n 4 06        72.8   5.34     7.26       6.78        4.26   3.59\n 5 07        73.6   5.23     6.59       6.78        4.43   3.41\n 6 08        73.1   5.56     6.24       6.84        5.05   3.25\n 7 09        72.7   5.81     5.93       6.53        5.93   3.11\n 8 10        72.2   5.70     6.07       6.23        6.93   2.91\n 9 11        71.7   5.44     6.36       6.10        7.57   2.86\n10 12        70.8   4.96     6.90       5.99        8.52   2.80\n11 13        69.9   4.59     7.49       5.93        9.37   2.75\n12 14        69.1   4.31     8.07       5.92        9.78   2.83\n13 15        68.1   4.02     8.78       6.05       10.2    2.90\n14 16        67.4   3.76     9.20       6.19       10.4    3.00\n15 17        67.6   3.38     9.29       6.34       10.5    2.96\n16 18        67.4   3.04     9.41       6.42       10.8    3.01\n17 19        67.6   2.76     9.41       6.55       10.7    3.05\n18 20        69.0   2.40     9.44       5.63       10.7    2.88\n\n\nIn any given year, Mortgage contributes the highest to the total debt. The contribution of Student Loan is increasing over the years and in 2020 it has reached above 10 % of the total debt."
  },
  {
    "objectID": "posts/challenge3_AnimeshSengupta.html",
    "href": "posts/challenge3_AnimeshSengupta.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_AnimeshSengupta.html#challenge-overview",
    "href": "posts/challenge3_AnimeshSengupta.html#challenge-overview",
    "title": "Challenge 3",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_AnimeshSengupta.html#read-in-data",
    "href": "posts/challenge3_AnimeshSengupta.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟"
  },
  {
    "objectID": "posts/challenge3_AnimeshSengupta.html#challenge",
    "href": "posts/challenge3_AnimeshSengupta.html#challenge",
    "title": "Challenge 3",
    "section": "Challenge",
    "text": "Challenge\nFor this challenge we choose the USA Households Data. The following section shows how the data is loaded, processed and pivoted to make it a tidier data.\n##Processing\n\n\nCode\n#! label: Data loading\nUS_household_data <- read_excel(\"../posts/_data/USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\",skip = 5, n_max = 353, col_names = c(\"Year\",\"Number\",\"Total\",\"pd_<15000\",\"pd_15000-24999\",\"pd_25000-34999\",\"pd_35000-49999\",\"pd_50000-74999\",\"pd_75000-99999\",\"pd_100000-149999\",\"pd_150000-199999\",\"pd_>200000\",\"median_income_estimate\",\"median_income_moe\",\"mean_income_estimate\",\"mean_income_moe\"))\n\n\n###Data Preprocessing\n\n\nCode\n#! label: Data processing\nUS_processed_data <- US_household_data%>%\n  rowwise()%>% #to ensure the following operation runs row wise\n  mutate(Race=case_when(\n    is.na(Number) ~ Year\n  ))%>%\n  ungroup()%>% # to stop rowwise operation\n  fill(Race,.direction = \"down\")%>%\n  subset(!is.na(Number))%>%\n  rowwise()%>%\n  mutate(\n    Year=strsplit(Year,' ')[[1]][1],\n    Race=ifelse(grepl(\"[0-9]\", Race ,perl=TRUE)[1],strsplit(Race,\" \\\\s*(?=[^ ]+$)\",perl=TRUE)[[1]][1],Race)\n  )\n\n#head(US_processed_data,10)\n\n\n\nBriefly describe the data\nThe US household data gives an insight of the income statistics of a generic household in USA. The feature set of the data is the following Year, Number, Total, pd_<15000, pd_15000-24999, pd_25000-34999, pd_35000-49999, pd_50000-74999, pd_75000-99999, pd_100000-149999, pd_150000-199999, pd_>200000, median_income_estimate, median_income_moe, mean_income_estimate, mean_income_moe, Race. Out of the following columns , the percent distribution of income classes can be pivoted longer. This transformation can be done using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_AnimeshSengupta.html#anticipate-the-end-result",
    "href": "posts/challenge3_AnimeshSengupta.html#anticipate-the-end-result",
    "title": "Challenge 3",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA     623.     713.\n2 USA      1990 NAFTA    2363.     800.\n3 France   1980 EU        439.    2396.\n4 Mexico   1990 NAFTA    1181.    1353.\n5 USA      1980 NAFTA     605.     877.\n6 France   1990 EU        927.     875.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\nCode\n#! label: Data Dimensions\nnrow(US_processed_data)\n\n\n[1] 340\n\n\nCode\nncol(US_processed_data)\n\n\n[1] 17\n\n\nCode\nexpected_columns <- ncol(US_processed_data)-9+2\nexpected_rows <- nrow(US_processed_data) * (9)\nexpected_columns\n\n\n[1] 10\n\n\nCode\nexpected_rows\n\n\n[1] 3060\n\n\nAny additional comments?\nOur dataset had 17 columns initially and after pivoting it will come down to 10 features."
  },
  {
    "objectID": "posts/challenge3_AnimeshSengupta.html#pivot-the-data",
    "href": "posts/challenge3_AnimeshSengupta.html#pivot-the-data",
    "title": "Challenge 3",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing               623.\n 2 Mexico   1980 NAFTA incoming               713.\n 3 USA      1990 NAFTA outgoing              2363.\n 4 USA      1990 NAFTA incoming               800.\n 5 France   1980 EU    outgoing               439.\n 6 France   1980 EU    incoming              2396.\n 7 Mexico   1990 NAFTA outgoing              1181.\n 8 Mexico   1990 NAFTA incoming              1353.\n 9 USA      1980 NAFTA outgoing               605.\n10 USA      1980 NAFTA incoming               877.\n11 France   1990 EU    outgoing               927.\n12 France   1990 EU    incoming               875.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\nCode\n#! label: pivoting\nUS_pivot_data<-US_processed_data%>%\n  pivot_longer(\n    cols = starts_with(\"pd\"),\n    names_to = \"income_range\",\n    values_to = \"percent distribution\",\n    names_prefix=\"pd_\"\n  )\nhead(US_pivot_data,10)\n\n\n# A tibble: 10 × 10\n   Year  Number Total median_inc…¹ media…² mean_…³ mean_…⁴ Race  incom…⁵ perce…⁶\n   <chr> <chr>  <dbl>        <dbl>   <dbl> <chr>   <chr>   <chr> <chr>     <dbl>\n 1 2019  128451   100        68703     904 98088   1042    ALL … <15000      9.1\n 2 2019  128451   100        68703     904 98088   1042    ALL … 15000-…     8  \n 3 2019  128451   100        68703     904 98088   1042    ALL … 25000-…     8.3\n 4 2019  128451   100        68703     904 98088   1042    ALL … 35000-…    11.7\n 5 2019  128451   100        68703     904 98088   1042    ALL … 50000-…    16.5\n 6 2019  128451   100        68703     904 98088   1042    ALL … 75000-…    12.3\n 7 2019  128451   100        68703     904 98088   1042    ALL … 100000…    15.5\n 8 2019  128451   100        68703     904 98088   1042    ALL … 150000…     8.3\n 9 2019  128451   100        68703     904 98088   1042    ALL … >200000    10.3\n10 2018  128579   100        64324     704 91652   914     ALL … <15000     10.1\n# … with abbreviated variable names ¹​median_income_estimate,\n#   ²​median_income_moe, ³​mean_income_estimate, ⁴​mean_income_moe, ⁵​income_range,\n#   ⁶​`percent distribution`\n\n\nCode\ndim(US_pivot_data)\n\n\n[1] 3060   10\n\n\nAny additional comments?\nOur pivoted data has been properly processed and the dimensions also matches the expected row and column measures. The challenge is successfully completed"
  },
  {
    "objectID": "posts/challenge3_MirandaManka.html",
    "href": "posts/challenge3_MirandaManka.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_MirandaManka.html#challenge-overview",
    "href": "posts/challenge3_MirandaManka.html#challenge-overview",
    "title": "Challenge 3",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_MirandaManka.html#read-in-data",
    "href": "posts/challenge3_MirandaManka.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\norganic_eggs = read_excel(\"_data/organiceggpoultry.xls\",\n  sheet = \"Data\", \n  range = \"B6:F125\", \n  col_names = c(\"date\", \"extralarge_dozen\", \"extralarge_halfdozen\", \n     \"large_dozen\", \"large_halfdozen\"))\n\norganic_eggs = organic_eggs %>% \n  mutate(date = str_remove(date, \" /1\"))\n\norganic_eggs = organic_eggs %>% \n  separate(date, into = c(\"month\", \"year\"), sep = \" \") %>% \n  fill(year)\n\ndim(organic_eggs)\n\n\n[1] 120   6\n\n\nCode\nhead(organic_eggs)\n\n\n# A tibble: 6 × 6\n  month    year  extralarge_dozen extralarge_halfdozen large_dozen large_halfd…¹\n  <chr>    <chr>            <dbl>                <dbl>       <dbl>         <dbl>\n1 Jan      2004              230                  132         230           126 \n2 February 2004              230                  134.        226.          128.\n3 March    2004              230                  137         225           131 \n4 April    2004              234.                 137         225           131 \n5 May      2004              236                  137         225           131 \n6 June     2004              241                  137         231.          134.\n# … with abbreviated variable name ¹​large_halfdozen\n\n\n\nBriefly describe the data\nAfter reading in the data and doing some cleaning, the dataset is ready to move on to the next step. This is a dataset about organic egg costs over time for different sizes and quantities of eggs (both large and extra large, dozen and half dozen eggs, for each month from 2004 to 2013), with 120 rows and 6 columns. The 6 columns are month, year, extralarge_dozen, extralarge_halfdozen, large_dozen, and large_halfdozen. As explained in class, these names make it easier to pivot data in the future (think about what the final pivoted data should look like and name accordingly). This data needs to be pivoted because right now each row does not represent a unique observation (for example Jan 2004 has prices for 4 different types of eggs)."
  },
  {
    "objectID": "posts/challenge3_MirandaManka.html#find-current-data-dimensions-anticipate-the-end-result",
    "href": "posts/challenge3_MirandaManka.html#find-current-data-dimensions-anticipate-the-end-result",
    "title": "Challenge 3",
    "section": "Find current data dimensions & Anticipate the end result",
    "text": "Find current data dimensions & Anticipate the end result\nThis dataset has n = 120 rows and k = 6 variables (the dimensions were found in the code above). Since 2 of the variables are used to identify a case (month and year), k - 2 = 6 - 2 = 4 variables will be pivoted into a longer format. Therefore, there should be n * (k - 2) = 120 * (6 - 2) = 120 * 4 = 480 rows in the pivoted dataframe.\n\nPivot the data & Describe the final dimensions\n\n\nCode\norganic_eggs = organic_eggs %>% \n  pivot_longer(cols = contains(\"dozen\"), names_to = c(\"size\", \"quantity\"), \n     names_sep=\"_\", values_to = \"price\")\n\ndim(organic_eggs)\n\n\n[1] 480   5\n\n\nAs expected, the final dataset has 480 rows. Each row (case) is a unique observation that contains the month, year, size, quantity, and price. The data has been successfully pivoted so that further analysis can take place. The data is now tidy because each row now contains a single observation/case (month and year information, ttype and quantity, and price) instead of what the dataset started at (date and 4 different columns for the different size and quantities). The naming conventions and thinking about what the final dataset will be, helped to pivot smoothly to better rows and columns."
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html",
    "href": "posts/challenge2_AnimeshSengupta.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#challenge-overview",
    "href": "posts/challenge2_AnimeshSengupta.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#read-in-the-data",
    "href": "posts/challenge2_AnimeshSengupta.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nFAOstat*.csv ⭐⭐⭐\n\n\n\nCode\n#! label: Read Data\nFAOstat_livestock <- read_csv(\"../posts/_data/FAOSTAT_livestock.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#describe-the-data",
    "href": "posts/challenge2_AnimeshSengupta.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\n#Data Dimensions\ndim(FAOstat_livestock)\n\n\n[1] 82116    14\n\n\nCode\n#Data Columns\ncolnames(FAOstat_livestock)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nCode\n#Data Preview\nhead(FAOstat_livestock,3)\n\n\n# A tibble: 3 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1961  1961 Head \n2 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1962  1962 Head \n3 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1963  1963 Head \n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names"
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#processed-data-for-analysis-and-some-information-about-it",
    "href": "posts/challenge2_AnimeshSengupta.html#processed-data-for-analysis-and-some-information-about-it",
    "title": "Challenge 2",
    "section": "Processed Data for analysis and some information about it",
    "text": "Processed Data for analysis and some information about it\n\n\nCode\n#! label: Isolate data for analysis\nFAOstat_livestock_main <- select(FAOstat_livestock,\"Area\",\"Item\",\"Year\",\"Unit\",\"Value\")\nhead(FAOstat_livestock_main,5)\n\n\n# A tibble: 5 × 5\n  Area        Item   Year Unit    Value\n  <chr>       <chr> <dbl> <chr>   <dbl>\n1 Afghanistan Asses  1961 Head  1300000\n2 Afghanistan Asses  1962 Head   851850\n3 Afghanistan Asses  1963 Head  1001112\n4 Afghanistan Asses  1964 Head  1150000\n5 Afghanistan Asses  1965 Head  1300000\n\n\nCode\ncountries_num<-n_distinct(FAOstat_livestock_main$Area)\nyear_vector<-unique(FAOstat_livestock_main$Year)\nunique(FAOstat_livestock_main$Item)\n\n\n[1] \"Asses\"     \"Camels\"    \"Cattle\"    \"Goats\"     \"Horses\"    \"Mules\"    \n[7] \"Sheep\"     \"Buffaloes\" \"Pigs\"     \n\n\nThe data collected for FAO was between 1961 to 2018 for Areas in total.\nI am interested in finding the expenditure statistics on livestock for these countries. Lets find out below."
  },
  {
    "objectID": "posts/challenge2_AnimeshSengupta.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_AnimeshSengupta.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nct_analysis <- FAOstat_livestock_main %>%\n                group_by(Area,Item)%>%\n                summarise(mean_val=mean(Value,na.rm = TRUE),\n                          median_val=median(Value,na.rm = TRUE),\n                          .groups = 'drop')\ndim(ct_analysis)\n\n\n[1] 1566    4\n\n\nCode\ncolnames(ct_analysis)\n\n\n[1] \"Area\"       \"Item\"       \"mean_val\"   \"median_val\"\n\n\n##Item wise by analysis\n\n\nCode\nct_item_group <- ct_analysis %>%\n  group_by(Item)%>%\n  summarise(mean_val=mean(mean_val,na.rm = TRUE))%>%\n  arrange(desc(mean_val))\n\nhead(ct_item_group,5)\n\n\n# A tibble: 5 × 2\n  Item       mean_val\n  <chr>         <dbl>\n1 Cattle    21058163.\n2 Sheep     20137879.\n3 Pigs      14207291.\n4 Goats     10801180.\n5 Buffaloes  8583673.\n\n\nAs per the data , Cattle generates the most value for any country , with the mean value being 2.1058163^{7}. We can speculatively attribute this high value to cattle due to its important role in society. For example, a cattle generates value not only be meat consumption but also dairy production. On the other hand, the Mule with lowest mean value of 4.4022093^{5}, has lesser societal value.\n##Area wise analysis\n\n\nCode\nct_area_group <- ct_analysis %>%\n  group_by(Area)%>%\n  summarise(mean_val=mean(mean_val,na.rm = TRUE))%>%\n  arrange(desc(mean_val))\n\nhead(ct_area_group,15)\n\n\n# A tibble: 15 × 2\n   Area                        mean_val\n   <chr>                          <dbl>\n 1 World                     449961866.\n 2 Asia                      190587493.\n 3 Americas                   95795716.\n 4 Eastern Asia               80092992.\n 5 Southern Asia              78704652.\n 6 Africa                     78159910.\n 7 China, mainland            73083831.\n 8 Europe                     71745016.\n 9 South America              56288859.\n10 India                      48618161.\n11 USSR                       36287113.\n12 Australia and New Zealand  36010992.\n13 Eastern Europe             34383679.\n14 Oceania                    31265613.\n15 Northern America           29793900.\n\n\nAmong the countries, Mainland China and India are the biggest mean producer of livestock value.\n##Time wise analysis of largest producer for most valuable livestock\n\n\nCode\nct_time_series <- FAOstat_livestock_main %>%\n  filter(Item == 'Cattle' & Area == 'China, mainland')%>%\n  group_by(Year)%>%\n  summarise(mean_val=mean(Value,na.rm = TRUE))%>%\n  arrange(Year)\n\nggplot(data = ct_time_series, aes(x = Year, y = mean_val)) +\n  geom_point()\n\n\n\n\n\nAs per the time chart plot, the cattle production started to increase substantially during the late 1900s and saw a peak during early 2000. Since then cattle production has a seen a downfall, probably attributing to advent of technology\n\nExplain and Interpret\nFor my analysis I chose the subgroup of Area, Item, Time and Value from the FAO Livestock dataset. The reason for choosing such a group of features was because of its high meaningfulness. This would also allow me to conduct Area wise, Item wise and time series Analysis. Conclusion to my analysis would be that Cattle is the MVP while China is the largest producer of livestock."
  },
  {
    "objectID": "posts/Challenge1_KimDarkenwald.html",
    "href": "posts/Challenge1_KimDarkenwald.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Challenge1_KimDarkenwald.html#describe-the-data",
    "href": "posts/Challenge1_KimDarkenwald.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nFound within this dataset are three columns: “state,” “county,” and “total_employees”. There are 2390 rows containing information for these three columns.\n\n\n\nData was collected in the year 2012 determining the amount of railroad employees across the country. According to the data, there are 2930 counties within the country that contain railroad employees. Some states have multiple counties with railroad employees."
  },
  {
    "objectID": "posts/challenge7_instructions.html",
    "href": "posts/challenge7_instructions.html",
    "title": "Challenge 7 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge7_instructions.html#challenge-overview",
    "href": "posts/challenge7_instructions.html#challenge-overview",
    "title": "Challenge 7 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\nRecreate at least two graphs from previous exercises, but introduce at least one additional dimension that you omitted before using ggplot functionality (color, shape, line, facet, etc) The goal is not to create unneeded chart ink (Tufte), but to concisely capture variation in additional dimensions that were collapsed in your earlier 2 or 3 dimensional graphs.\n\n\nExplain why you choose the specific graph type\n\n\nIf you haven’t tried in previous weeks, work this week to make your graphs “publication” ready with titles, captions, and pretty axis labels and other viewer-friendly features\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code. And anyone not familiar with Edward Tufte should check out his fantastic books and courses on data visualizaton.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge7_instructions.html#read-in-data",
    "href": "posts/challenge7_instructions.html#read-in-data",
    "title": "Challenge 7 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\neggs ⭐\nabc_poll ⭐⭐\naustralian_marriage ⭐⭐\nhotel_bookings ⭐⭐⭐\nair_bnb ⭐⭐⭐\nus_hh ⭐⭐⭐⭐\nfaostat ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge7_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge7_instructions.html#tidy-data-as-needed",
    "title": "Challenge 7 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge7_instructions.html#visualization-with-multiple-dimensions",
    "href": "posts/challenge7_instructions.html#visualization-with-multiple-dimensions",
    "title": "Challenge 7 Instructions",
    "section": "Visualization with Multiple Dimensions",
    "text": "Visualization with Multiple Dimensions"
  },
  {
    "objectID": "posts/challenge3_WillMunson.html",
    "href": "posts/challenge3_WillMunson.html",
    "title": "Challenge 3 Will Munson",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_WillMunson.html#challenge-overview",
    "href": "posts/challenge3_WillMunson.html#challenge-overview",
    "title": "Challenge 3 Will Munson",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_WillMunson.html#read-in-data",
    "href": "posts/challenge3_WillMunson.html#read-in-data",
    "title": "Challenge 3 Will Munson",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data\nDescribe the data, and be sure to comment on why you are planning to pivot it to make it “tidy”\nOkay, so this data is basically explained by both the row AND the column. The observed variables (Item and Weight) are not listed, and instead, we see the items listed as variables for each column, and each weight represents these variables. What we need to do is reorganize this dataset so that there are columns that represent each variable. There should be three columns instead of 17."
  },
  {
    "objectID": "posts/challenge3_WillMunson.html#anticipate-the-end-result",
    "href": "posts/challenge3_WillMunson.html#anticipate-the-end-result",
    "title": "Challenge 3 Will Munson",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA    1036.    1121.\n2 USA      1990 NAFTA    1829.     611.\n3 France   1980 EU        963.    -334.\n4 Mexico   1990 NAFTA     195.     986.\n5 USA      1980 NAFTA    1663.    1141.\n6 France   1990 EU        785.    2205.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\nCode\nnrow(animal_weight)\n\n\n[1] 9\n\n\nCode\nncol(animal_weight)\n\n\n[1] 17\n\n\nCode\n#Number of rows\nnrow(animal_weight)*(ncol(animal_weight)-1)\n\n\n[1] 144\n\n\nAny additional comments? There are way too many columns in the original dataset. Let’s change this so we only get three of them. ## Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1036.\n 2 Mexico   1980 NAFTA incoming              1121.\n 3 USA      1990 NAFTA outgoing              1829.\n 4 USA      1990 NAFTA incoming               611.\n 5 France   1980 EU    outgoing               963.\n 6 France   1980 EU    incoming              -334.\n 7 Mexico   1990 NAFTA outgoing               195.\n 8 Mexico   1990 NAFTA incoming               986.\n 9 USA      1980 NAFTA outgoing              1663.\n10 USA      1980 NAFTA incoming              1141.\n11 France   1990 EU    outgoing               785.\n12 France   1990 EU    incoming              2205.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\nCode\nanimal_weight <- pivot_longer(animal_weight, col = c(`Cattle - dairy`, `Cattle - non-dairy`, Buffaloes, `Swine - market`, `Swine - breeding`, `Chicken - Broilers`, `Chicken - Layers`, Ducks, Turkeys, Sheep, Goats, Horses, Asses, Mules, Camels, Llamas),\n                              names_to = \"Animal Type\",\n                              values_to = \"Weight in lb\")\n\nanimal_weight\n\n\n# A tibble: 144 × 3\n   `IPCC Area`         `Animal Type`      `Weight in lb`\n   <chr>               <chr>                       <dbl>\n 1 Indian Subcontinent Cattle - dairy              275  \n 2 Indian Subcontinent Cattle - non-dairy          110  \n 3 Indian Subcontinent Buffaloes                   295  \n 4 Indian Subcontinent Swine - market               28  \n 5 Indian Subcontinent Swine - breeding             28  \n 6 Indian Subcontinent Chicken - Broilers            0.9\n 7 Indian Subcontinent Chicken - Layers              1.8\n 8 Indian Subcontinent Ducks                         2.7\n 9 Indian Subcontinent Turkeys                       6.8\n10 Indian Subcontinent Sheep                        28  \n# … with 134 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAny additional comments?\nThis was a very valuable lesson to learn when it comes to working with data in R. While it may seem more aesthetically pleasing to look at a dataset where you have variables in both the first row and the first column, it's not the most efficient way to analyze the data."
  },
  {
    "objectID": "posts/challenge5_stevenoneill.html",
    "href": "posts/challenge5_stevenoneill.html",
    "title": "Challenge 5",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_stevenoneill.html#usa-households",
    "href": "posts/challenge5_stevenoneill.html#usa-households",
    "title": "Challenge 5",
    "section": "USA Households",
    "text": "USA Households\n\nUSA Households ⭐⭐⭐⭐⭐\n\nI’m choosing the USA Households by Total Money Income, Race, and Hispanic Origin of Householder dataset.\nRight away I am changing the row names manually. It doesn’t take long and wouldn’t make sense to automate. I assume these variables do not change year-to-year.\n\nhouseholds <- read_xlsx(\"_data/USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\",\n                        range = cell_rows(6:357),\n                        col_names = c(\"year_or_race\",\n                                      \"number\",\n                                      \"pct_total\",\n                                      \"pct_under_15k\",\n                                      \"pct_15k_to_24999\",\n                                      \"pct_25k_to_34999\",\n                                      \"pct_35k_to_49999\",\n                                      \"pct_50k_to_74999\",\n                                      \"pct_75k_to_99999\",\n                                      \"pct_100k_to_149999\",\n                                      \"pct_150k_to_199999\",\n                                      \"pct_over_200k\",\n                                      \"median_income_estimate\",\n                                      \"median_income_moe\",\n                                      \"mean_income_estimate\",\n                                      \"mean_income_moe\" ))\nhouseholds\n\n\n\n  \n\n\n\nWoah, tons of extra numbers are appended to the years. And… several years are repeated twice.\n\nhouseholds %>% distinct(year_or_race)\n\n\n\n  \n\n\n\nOn the original spreadsheet they appear to lead to footnotes with extra information. For example:\n3 The 2014 CPS ASEC included redesigned questions for income and health insurance coverage. All of the approximately 98,000 addresses were eligible to receive the redesigned set of health insurance coverage questions. The redesigned income questions were implemented to a subsample of these 98,000 addresses using a probability split panel design. Approximately 68,000 addresses were eligible to receive a set of income questions similar to those used in the 2013 CPS ASEC and the remaining 30,000 addresses were eligible to receive the redesigned income questions. The source of these 2013 estimates is the portion of the CPS ASEC sample which received the redesigned income questions, approximately 30,000 addresses.\n4 The source of these 2013 estimates is the portion of the CPS ASEC sample which received the income questions consistent with the 2013 CPS ASEC, approximately 68,000 addresses.\n… and so on.\nI will deal with the duplicate years after I remove the footnotes.\nIn the code below, the regex \"\\\\d{4}\" looks for years and extracts them into the column calc_year.\nAfter, it seeks all values (.*) between alphabetic characters, creates a race column, and fills lower cells until the race variable is overwritten.\n\nhouseholds <- households %>% mutate(year = str_extract(year_or_race, \"\\\\d{4}\"),\n                      race = str_extract(year_or_race, \"[:alpha:](.*)[:alpha:].\")) %>% fill(race)\nhouseholds\n\n\n\n  \n\n\n\nNext, I can remove the rows which only told me what race we were looking at. Goodbye, rows with no number. Also, begone, year_or_race and pct_total column.\n\nhouseholds <- households %>% drop_na(number) %>% select(-year_or_race, -pct_total) %>% relocate(year, race)\nhouseholds\n\n\n\n  \n\n\n\n\nSanity checks\nThere are still some “race” data that have multiple years, like 2017. Mostly, according to the document’s footnote, the studies that occur a second time in a year have some marginal improvement in the way they are conducted. So I’ll just keep the more “recent” study and discard the other one for now.\nBy combing year and race, I can remove duplicates. I’ll use separate to put things back as they were.\n\nhouseholds <- households %>% unite(\"race_year\", year:race) %>% filter(!duplicated(race_year)) %>% separate(race_year, into=c(\"race\", \"year\"), sep=\"_\")\nhouseholds\n\n\n\n  \n\n\n\nAdditionally, as in challenge 3, we see that survey data was not collected with uniform “race” data during certain years. 2001 to 2002 had the biggest change\nFor example, “White” was collected until 2002, when it was disambiguated into WHITE ALONE, or WHITE ALONE, NOT HISPANIC.\n\nhouseholds %>% filter(year %in% c(1967, 1970, 1972, 1980, 1985, 1995, 2001, 2002, 2008, 2010, 2017)) %>% select(race, number, year) %>% pivot_wider(values_from=\"number\", names_from = \"year\")\n\n\n\n  \n\n\n\nAlso, ASIAN AND PACIFIC ISLANDER were not reliably collected."
  },
  {
    "objectID": "posts/challenge5_stevenoneill.html#univariate-visualizations",
    "href": "posts/challenge5_stevenoneill.html#univariate-visualizations",
    "title": "Challenge 5",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations\nHere is median income for the “Black alone or in Combination” subset, from 2002 to 2019 when data was available:\n\nblack_alone <- households %>% filter(year %in% 2002:2019 ) %>% filter(race == \"BLACK ALONE OR IN COMBINATION\")\n\nblack_alone %>% ggplot(aes(x=year, y=median_income_estimate)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\nCompared to “Asian Alone” demographics in the same time period:\n\nasian_alone <- households %>% filter(year %in% 2002:2019 ) %>% filter(race == \"ASIAN ALONE OR IN COMBINATION\")\n\nasian_alone %>% ggplot(aes(x=year, y=median_income_estimate)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\nAlthough incomes are at different levels, we can see the effect of the financial recession (2008-2011) in both groups."
  },
  {
    "objectID": "posts/challenge5_stevenoneill.html#bivariate-visualization",
    "href": "posts/challenge5_stevenoneill.html#bivariate-visualization",
    "title": "Challenge 5",
    "section": "Bivariate Visualization",
    "text": "Bivariate Visualization\nThis plot visualizes the difference between the three groups better:\n\nalone_demographics <- households %>% filter(race %in% c(\"ASIAN ALONE \",\"WHITE ALONE \", \"BLACK ALONE \"))\n\nalone_demographics %>% ggplot(aes(x=year, y=median_income_estimate, fill = race)) + \n  geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "posts/challenge3_jerinjacob.html",
    "href": "posts/challenge3_jerinjacob.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_jerinjacob.html#read-in-data",
    "href": "posts/challenge3_jerinjacob.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\nanimal_weight\n\n\n# A tibble: 9 × 17\n  IPCC A…¹ Cattl…² Cattl…³ Buffa…⁴ Swine…⁵ Swine…⁶ Chick…⁷ Chick…⁸ Ducks Turkeys\n  <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl>\n1 Indian …     275     110     295      28      28     0.9     1.8   2.7     6.8\n2 Eastern…     550     391     380      50     180     0.9     1.8   2.7     6.8\n3 Africa       275     173     380      28      28     0.9     1.8   2.7     6.8\n4 Oceania      500     330     380      45     180     0.9     1.8   2.7     6.8\n5 Western…     600     420     380      50     198     0.9     1.8   2.7     6.8\n6 Latin A…     400     305     380      28      28     0.9     1.8   2.7     6.8\n7 Asia         350     391     380      50     180     0.9     1.8   2.7     6.8\n8 Middle …     275     173     380      28      28     0.9     1.8   2.7     6.8\n9 Norther…     604     389     380      46     198     0.9     1.8   2.7     6.8\n# … with 7 more variables: Sheep <dbl>, Goats <dbl>, Horses <dbl>, Asses <dbl>,\n#   Mules <dbl>, Camels <dbl>, Llamas <dbl>, and abbreviated variable names\n#   ¹​`IPCC Area`, ²​`Cattle - dairy`, ³​`Cattle - non-dairy`, ⁴​Buffaloes,\n#   ⁵​`Swine - market`, ⁶​`Swine - breeding`, ⁷​`Chicken - Broilers`,\n#   ⁸​`Chicken - Layers`\n# ℹ Use `colnames()` to see all variable names"
  },
  {
    "objectID": "posts/challenge3_jerinjacob.html#describing-the-data",
    "href": "posts/challenge3_jerinjacob.html#describing-the-data",
    "title": "Challenge 3",
    "section": "Describing the data",
    "text": "Describing the data\nThe animal weight data has the weight of 16 types of livestocks collected from 9 geographical areas.\nThe pivoted dataset will have 144 rows/ cases and 3 columns of region, animal type and weight.\n\n\nCode\nanimal_weight_longer<-pivot_longer(animal_weight, \n                                    col=-`IPCC Area`,\n                                    names_to = \"Livestock\",\n                                    values_to = \"Weight\")\nanimal_weight_longer\n\n\n# A tibble: 144 × 3\n   `IPCC Area`         Livestock          Weight\n   <chr>               <chr>               <dbl>\n 1 Indian Subcontinent Cattle - dairy      275  \n 2 Indian Subcontinent Cattle - non-dairy  110  \n 3 Indian Subcontinent Buffaloes           295  \n 4 Indian Subcontinent Swine - market       28  \n 5 Indian Subcontinent Swine - breeding     28  \n 6 Indian Subcontinent Chicken - Broilers    0.9\n 7 Indian Subcontinent Chicken - Layers      1.8\n 8 Indian Subcontinent Ducks                 2.7\n 9 Indian Subcontinent Turkeys               6.8\n10 Indian Subcontinent Sheep                28  \n# … with 134 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge1_AnimeshSengupta.html",
    "href": "posts/challenge1_AnimeshSengupta.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_AnimeshSengupta.html#challenge-overview",
    "href": "posts/challenge1_AnimeshSengupta.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_AnimeshSengupta.html#read-in-the-data",
    "href": "posts/challenge1_AnimeshSengupta.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\ndebt_data <- read_excel(\"../posts/_data/debt_in_trillions.xlsx\")\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_AnimeshSengupta.html#describe-the-data",
    "href": "posts/challenge1_AnimeshSengupta.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\ndim(debt_data)\n\n\n[1] 74  8\n\n\nCode\ncolnames(debt_data)\n\n\n[1] \"Year and Quarter\" \"Mortgage\"         \"HE Revolving\"     \"Auto Loan\"       \n[5] \"Credit Card\"      \"Student Loan\"     \"Other\"            \"Total\"           \n\n\nCode\nhead(debt_data,5)\n\n\n# A tibble: 5 × 8\n  `Year and Quarter` Mortgage `HE Revolving` Auto …¹ Credi…² Stude…³ Other Total\n  <chr>                 <dbl>          <dbl>   <dbl>   <dbl>   <dbl> <dbl> <dbl>\n1 03:Q1                  4.94          0.242   0.641   0.688   0.241 0.478  7.23\n2 03:Q2                  5.08          0.26    0.622   0.693   0.243 0.486  7.38\n3 03:Q3                  5.18          0.269   0.684   0.693   0.249 0.477  7.56\n4 03:Q4                  5.66          0.302   0.704   0.698   0.253 0.449  8.07\n5 04:Q1                  5.84          0.328   0.72    0.695   0.260 0.446  8.29\n# … with abbreviated variable names ¹​`Auto Loan`, ²​`Credit Card`,\n#   ³​`Student Loan`"
  },
  {
    "objectID": "posts/challenge2_Akhilesh.html",
    "href": "posts/challenge2_Akhilesh.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(Hmisc)\nlibrary(psych)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_Akhilesh.html#challenge-overview",
    "href": "posts/challenge2_Akhilesh.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_Akhilesh.html#read-in-the-data",
    "href": "posts/challenge2_Akhilesh.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nhotel_bookings ⭐⭐⭐\nFAOstat*.csv ⭐⭐⭐⭐⭐ (join FAOSTAT_country_groups)\n\n\n\nCode\n#### Read dataset 'hotel_bookings.csv', available in the `posts/_data` folder, using the read_.csv command.\n\nhotel_bookings <- read.csv('_data/hotel_bookings.csv', stringsAsFactors = TRUE, header = TRUE)\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_Akhilesh.html#describe-the-data",
    "href": "posts/challenge2_Akhilesh.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\nCheck first 6 rows in the dataset, to get primary understanding of the dataframe structure\n\n\nCode\nhead(hotel_bookings)\n\n\n         hotel is_canceled lead_time arrival_date_year arrival_date_month\n1 Resort Hotel           0       342              2015               July\n2 Resort Hotel           0       737              2015               July\n3 Resort Hotel           0         7              2015               July\n4 Resort Hotel           0        13              2015               July\n5 Resort Hotel           0        14              2015               July\n6 Resort Hotel           0        14              2015               July\n  arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights\n1                       27                         1                       0\n2                       27                         1                       0\n3                       27                         1                       0\n4                       27                         1                       0\n5                       27                         1                       0\n6                       27                         1                       0\n  stays_in_week_nights adults children babies meal country market_segment\n1                    0      2        0      0   BB     PRT         Direct\n2                    0      2        0      0   BB     PRT         Direct\n3                    1      1        0      0   BB     GBR         Direct\n4                    1      1        0      0   BB     GBR      Corporate\n5                    2      2        0      0   BB     GBR      Online TA\n6                    2      2        0      0   BB     GBR      Online TA\n  distribution_channel is_repeated_guest previous_cancellations\n1               Direct                 0                      0\n2               Direct                 0                      0\n3               Direct                 0                      0\n4            Corporate                 0                      0\n5                TA/TO                 0                      0\n6                TA/TO                 0                      0\n  previous_bookings_not_canceled reserved_room_type assigned_room_type\n1                              0                  C                  C\n2                              0                  C                  C\n3                              0                  A                  C\n4                              0                  A                  A\n5                              0                  A                  A\n6                              0                  A                  A\n  booking_changes deposit_type agent company days_in_waiting_list customer_type\n1               3   No Deposit  NULL    NULL                    0     Transient\n2               4   No Deposit  NULL    NULL                    0     Transient\n3               0   No Deposit  NULL    NULL                    0     Transient\n4               0   No Deposit   304    NULL                    0     Transient\n5               0   No Deposit   240    NULL                    0     Transient\n6               0   No Deposit   240    NULL                    0     Transient\n  adr required_car_parking_spaces total_of_special_requests reservation_status\n1   0                           0                         0          Check-Out\n2   0                           0                         0          Check-Out\n3  75                           0                         0          Check-Out\n4  75                           0                         0          Check-Out\n5  98                           0                         1          Check-Out\n6  98                           0                         1          Check-Out\n  reservation_status_date\n1              2015-07-01\n2              2015-07-01\n3              2015-07-02\n4              2015-07-02\n5              2015-07-03\n6              2015-07-03\n\n\n\nView Dataframe in table view\n\n\nCode\nView(hotel_bookings)\n\n\n\n\n\nRow and column dimentions of dataframe\n\n\nCode\ndim(hotel_bookings)\n\n\n[1] 119390     32\n\n\n\n\nColumn names of the dataframe\n\n\nCode\ncolnames(hotel_bookings)\n\n\n [1] \"hotel\"                          \"is_canceled\"                   \n [3] \"lead_time\"                      \"arrival_date_year\"             \n [5] \"arrival_date_month\"             \"arrival_date_week_number\"      \n [7] \"arrival_date_day_of_month\"      \"stays_in_weekend_nights\"       \n [9] \"stays_in_week_nights\"           \"adults\"                        \n[11] \"children\"                       \"babies\"                        \n[13] \"meal\"                           \"country\"                       \n[15] \"market_segment\"                 \"distribution_channel\"          \n[17] \"is_repeated_guest\"              \"previous_cancellations\"        \n[19] \"previous_bookings_not_canceled\" \"reserved_room_type\"            \n[21] \"assigned_room_type\"             \"booking_changes\"               \n[23] \"deposit_type\"                   \"agent\"                         \n[25] \"company\"                        \"days_in_waiting_list\"          \n[27] \"customer_type\"                  \"adr\"                           \n[29] \"required_car_parking_spaces\"    \"total_of_special_requests\"     \n[31] \"reservation_status\"             \"reservation_status_date\"       \n\n\n\n\nColumn classes of the dataframe\n\n\nCode\ncol_classes = data.frame(t(data.frame(lapply(hotel_bookings,class))))\n\n\n\n\nDescribe, Dataframe\n\n\nCode\ndescribe(hotel_bookings)\n\n\n                               vars      n    mean     sd  median trimmed\nhotel*                            1 119390    1.34   0.47    1.00    1.29\nis_canceled                       2 119390    0.37   0.48    0.00    0.34\nlead_time                         3 119390  104.01 106.86   69.00   87.24\narrival_date_year                 4 119390 2016.16   0.71 2016.00 2016.20\narrival_date_month*               5 119390    6.49   3.54    7.00    6.49\narrival_date_week_number          6 119390   27.17  13.61   28.00   27.16\narrival_date_day_of_month         7 119390   15.80   8.78   16.00   15.80\nstays_in_weekend_nights           8 119390    0.93   1.00    1.00    0.84\nstays_in_week_nights              9 119390    2.50   1.91    2.00    2.29\nadults                           10 119390    1.86   0.58    2.00    1.88\nchildren                         11 119386    0.10   0.40    0.00    0.00\nbabies                           12 119390    0.01   0.10    0.00    0.00\nmeal*                            13 119390    1.56   1.07    1.00    1.31\ncountry*                         14 119390   94.59  45.09   82.00   96.85\nmarket_segment*                  15 119390    5.93   1.27    6.00    6.11\ndistribution_channel*            16 119390    3.59   0.91    4.00    3.80\nis_repeated_guest                17 119390    0.03   0.18    0.00    0.00\nprevious_cancellations           18 119390    0.09   0.84    0.00    0.00\nprevious_bookings_not_canceled   19 119390    0.14   1.50    0.00    0.00\nreserved_room_type*              20 119390    1.99   1.70    1.00    1.64\nassigned_room_type*              21 119390    2.33   1.88    1.00    2.02\nbooking_changes                  22 119390    0.22   0.65    0.00    0.06\ndeposit_type*                    23 119390    1.12   0.33    1.00    1.03\nagent*                           24 119390  211.82 122.75  296.00  221.86\ncompany*                         25 119390  342.22  50.06  353.00  353.00\ndays_in_waiting_list             26 119390    2.32  17.59    0.00    0.00\ncustomer_type*                   27 119390    3.14   0.58    3.00    3.14\nadr                              28 119390  101.83  50.54   94.58   97.71\nrequired_car_parking_spaces      29 119390    0.06   0.25    0.00    0.00\ntotal_of_special_requests        30 119390    0.57   0.79    0.00    0.43\nreservation_status*              31 119390    1.65   0.50    2.00    1.67\nreservation_status_date*         32 119390  517.75 227.48  525.00  521.18\n                                  mad     min  max   range  skew kurtosis   se\nhotel*                           0.00    1.00    2    1.00  0.70    -1.51 0.00\nis_canceled                      0.00    0.00    1    1.00  0.54    -1.71 0.00\nlead_time                       88.96    0.00  737  737.00  1.35     1.70 0.31\narrival_date_year                1.48 2015.00 2017    2.00 -0.23    -0.99 0.00\narrival_date_month*              4.45    1.00   12   11.00 -0.04    -1.23 0.01\narrival_date_week_number        16.31    1.00   53   52.00 -0.01    -0.99 0.04\narrival_date_day_of_month       11.86    1.00   31   30.00  0.00    -1.19 0.03\nstays_in_weekend_nights          1.48    0.00   19   19.00  1.38     7.17 0.00\nstays_in_week_nights             1.48    0.00   50   50.00  2.86    24.28 0.01\nadults                           0.00    0.00   55   55.00 18.32  1352.04 0.00\nchildren                         0.00    0.00   10   10.00  4.11    18.67 0.00\nbabies                           0.00    0.00   10   10.00 24.65  1633.85 0.00\nmeal*                            0.00    1.00    5    4.00  1.59     0.96 0.00\ncountry*                        81.54    1.00  178  177.00 -0.16    -1.53 0.13\nmarket_segment*                  1.48    1.00    8    7.00 -1.00     0.15 0.00\ndistribution_channel*            0.00    1.00    5    4.00 -1.87     1.86 0.00\nis_repeated_guest                0.00    0.00    1    1.00  5.33    26.37 0.00\nprevious_cancellations           0.00    0.00   26   26.00 24.46   674.03 0.00\nprevious_bookings_not_canceled   0.00    0.00   72   72.00 23.54   767.20 0.00\nreserved_room_type*              0.00    1.00   10    9.00  1.44     0.89 0.00\nassigned_room_type*              0.00    1.00   12   11.00  1.15     0.53 0.01\nbooking_changes                  0.00    0.00   21   21.00  6.00    79.39 0.00\ndeposit_type*                    0.00    1.00    3    2.00  2.38     4.07 0.00\nagent*                          56.34    1.00  334  333.00 -0.37    -1.53 0.36\ncompany*                         0.00    1.00  353  352.00 -4.93    23.77 0.14\ndays_in_waiting_list             0.00    0.00  391  391.00 11.94   186.78 0.05\ncustomer_type*                   0.00    1.00    4    3.00 -1.08     4.55 0.00\nadr                             41.25   -6.38 5400 5406.38 10.53  1013.13 0.15\nrequired_car_parking_spaces      0.00    0.00    8    8.00  4.16    30.00 0.00\ntotal_of_special_requests        0.00    0.00    5    5.00  1.35     1.49 0.00\nreservation_status*              0.00    1.00    3    2.00 -0.38    -1.25 0.00\nreservation_status_date*       275.76    1.00  926  925.00 -0.12    -1.02 0.66\n\n\nCode\nsummary(hotel_bookings)\n\n\n          hotel        is_canceled       lead_time   arrival_date_year\n City Hotel  :79330   Min.   :0.0000   Min.   :  0   Min.   :2015     \n Resort Hotel:40060   1st Qu.:0.0000   1st Qu.: 18   1st Qu.:2016     \n                      Median :0.0000   Median : 69   Median :2016     \n                      Mean   :0.3704   Mean   :104   Mean   :2016     \n                      3rd Qu.:1.0000   3rd Qu.:160   3rd Qu.:2017     \n                      Max.   :1.0000   Max.   :737   Max.   :2017     \n                                                                      \n arrival_date_month arrival_date_week_number arrival_date_day_of_month\n August :13877      Min.   : 1.00            Min.   : 1.0             \n July   :12661      1st Qu.:16.00            1st Qu.: 8.0             \n May    :11791      Median :28.00            Median :16.0             \n October:11160      Mean   :27.17            Mean   :15.8             \n April  :11089      3rd Qu.:38.00            3rd Qu.:23.0             \n June   :10939      Max.   :53.00            Max.   :31.0             \n (Other):47873                                                        \n stays_in_weekend_nights stays_in_week_nights     adults      \n Min.   : 0.0000         Min.   : 0.0         Min.   : 0.000  \n 1st Qu.: 0.0000         1st Qu.: 1.0         1st Qu.: 2.000  \n Median : 1.0000         Median : 2.0         Median : 2.000  \n Mean   : 0.9276         Mean   : 2.5         Mean   : 1.856  \n 3rd Qu.: 2.0000         3rd Qu.: 3.0         3rd Qu.: 2.000  \n Max.   :19.0000         Max.   :50.0         Max.   :55.000  \n                                                              \n    children           babies                 meal          country     \n Min.   : 0.0000   Min.   : 0.000000   BB       :92310   PRT    :48590  \n 1st Qu.: 0.0000   1st Qu.: 0.000000   FB       :  798   GBR    :12129  \n Median : 0.0000   Median : 0.000000   HB       :14463   FRA    :10415  \n Mean   : 0.1039   Mean   : 0.007949   SC       :10650   ESP    : 8568  \n 3rd Qu.: 0.0000   3rd Qu.: 0.000000   Undefined: 1169   DEU    : 7287  \n Max.   :10.0000   Max.   :10.000000                     ITA    : 3766  \n NA's   :4                                               (Other):28635  \n       market_segment  distribution_channel is_repeated_guest\n Online TA    :56477   Corporate: 6677      Min.   :0.00000  \n Offline TA/TO:24219   Direct   :14645      1st Qu.:0.00000  \n Groups       :19811   GDS      :  193      Median :0.00000  \n Direct       :12606   TA/TO    :97870      Mean   :0.03191  \n Corporate    : 5295   Undefined:    5      3rd Qu.:0.00000  \n Complementary:  743                        Max.   :1.00000  \n (Other)      :  239                                         \n previous_cancellations previous_bookings_not_canceled reserved_room_type\n Min.   : 0.00000       Min.   : 0.0000                A      :85994     \n 1st Qu.: 0.00000       1st Qu.: 0.0000                D      :19201     \n Median : 0.00000       Median : 0.0000                E      : 6535     \n Mean   : 0.08712       Mean   : 0.1371                F      : 2897     \n 3rd Qu.: 0.00000       3rd Qu.: 0.0000                G      : 2094     \n Max.   :26.00000       Max.   :72.0000                B      : 1118     \n                                                       (Other): 1551     \n assigned_room_type booking_changes       deposit_type        agent      \n A      :74053      Min.   : 0.0000   No Deposit:104641   9      :31961  \n D      :25322      1st Qu.: 0.0000   Non Refund: 14587   NULL   :16340  \n E      : 7806      Median : 0.0000   Refundable:   162   240    :13922  \n F      : 3751      Mean   : 0.2211                       1      : 7191  \n G      : 2553      3rd Qu.: 0.0000                       14     : 3640  \n C      : 2375      Max.   :21.0000                       7      : 3539  \n (Other): 3530                                            (Other):42797  \n    company       days_in_waiting_list         customer_type  \n NULL   :112593   Min.   :  0.000      Contract       : 4076  \n 40     :   927   1st Qu.:  0.000      Group          :  577  \n 223    :   784   Median :  0.000      Transient      :89613  \n 67     :   267   Mean   :  2.321      Transient-Party:25124  \n 45     :   250   3rd Qu.:  0.000                             \n 153    :   215   Max.   :391.000                             \n (Other):  4354                                               \n      adr          required_car_parking_spaces total_of_special_requests\n Min.   :  -6.38   Min.   :0.00000             Min.   :0.0000           \n 1st Qu.:  69.29   1st Qu.:0.00000             1st Qu.:0.0000           \n Median :  94.58   Median :0.00000             Median :0.0000           \n Mean   : 101.83   Mean   :0.06252             Mean   :0.5714           \n 3rd Qu.: 126.00   3rd Qu.:0.00000             3rd Qu.:1.0000           \n Max.   :5400.00   Max.   :8.00000             Max.   :5.0000           \n                                                                        \n reservation_status reservation_status_date\n Canceled :43017    2015-10-21:  1461      \n Check-Out:75166    2015-07-06:   805      \n No-Show  : 1207    2016-11-25:   790      \n                    2015-01-01:   763      \n                    2016-01-18:   625      \n                    2015-07-02:   469      \n                    (Other)   :114477      \n\n\n\n\nSummary of dataframe\n\n\nCode\nsummary(hotel_bookings)\n\n\n          hotel        is_canceled       lead_time   arrival_date_year\n City Hotel  :79330   Min.   :0.0000   Min.   :  0   Min.   :2015     \n Resort Hotel:40060   1st Qu.:0.0000   1st Qu.: 18   1st Qu.:2016     \n                      Median :0.0000   Median : 69   Median :2016     \n                      Mean   :0.3704   Mean   :104   Mean   :2016     \n                      3rd Qu.:1.0000   3rd Qu.:160   3rd Qu.:2017     \n                      Max.   :1.0000   Max.   :737   Max.   :2017     \n                                                                      \n arrival_date_month arrival_date_week_number arrival_date_day_of_month\n August :13877      Min.   : 1.00            Min.   : 1.0             \n July   :12661      1st Qu.:16.00            1st Qu.: 8.0             \n May    :11791      Median :28.00            Median :16.0             \n October:11160      Mean   :27.17            Mean   :15.8             \n April  :11089      3rd Qu.:38.00            3rd Qu.:23.0             \n June   :10939      Max.   :53.00            Max.   :31.0             \n (Other):47873                                                        \n stays_in_weekend_nights stays_in_week_nights     adults      \n Min.   : 0.0000         Min.   : 0.0         Min.   : 0.000  \n 1st Qu.: 0.0000         1st Qu.: 1.0         1st Qu.: 2.000  \n Median : 1.0000         Median : 2.0         Median : 2.000  \n Mean   : 0.9276         Mean   : 2.5         Mean   : 1.856  \n 3rd Qu.: 2.0000         3rd Qu.: 3.0         3rd Qu.: 2.000  \n Max.   :19.0000         Max.   :50.0         Max.   :55.000  \n                                                              \n    children           babies                 meal          country     \n Min.   : 0.0000   Min.   : 0.000000   BB       :92310   PRT    :48590  \n 1st Qu.: 0.0000   1st Qu.: 0.000000   FB       :  798   GBR    :12129  \n Median : 0.0000   Median : 0.000000   HB       :14463   FRA    :10415  \n Mean   : 0.1039   Mean   : 0.007949   SC       :10650   ESP    : 8568  \n 3rd Qu.: 0.0000   3rd Qu.: 0.000000   Undefined: 1169   DEU    : 7287  \n Max.   :10.0000   Max.   :10.000000                     ITA    : 3766  \n NA's   :4                                               (Other):28635  \n       market_segment  distribution_channel is_repeated_guest\n Online TA    :56477   Corporate: 6677      Min.   :0.00000  \n Offline TA/TO:24219   Direct   :14645      1st Qu.:0.00000  \n Groups       :19811   GDS      :  193      Median :0.00000  \n Direct       :12606   TA/TO    :97870      Mean   :0.03191  \n Corporate    : 5295   Undefined:    5      3rd Qu.:0.00000  \n Complementary:  743                        Max.   :1.00000  \n (Other)      :  239                                         \n previous_cancellations previous_bookings_not_canceled reserved_room_type\n Min.   : 0.00000       Min.   : 0.0000                A      :85994     \n 1st Qu.: 0.00000       1st Qu.: 0.0000                D      :19201     \n Median : 0.00000       Median : 0.0000                E      : 6535     \n Mean   : 0.08712       Mean   : 0.1371                F      : 2897     \n 3rd Qu.: 0.00000       3rd Qu.: 0.0000                G      : 2094     \n Max.   :26.00000       Max.   :72.0000                B      : 1118     \n                                                       (Other): 1551     \n assigned_room_type booking_changes       deposit_type        agent      \n A      :74053      Min.   : 0.0000   No Deposit:104641   9      :31961  \n D      :25322      1st Qu.: 0.0000   Non Refund: 14587   NULL   :16340  \n E      : 7806      Median : 0.0000   Refundable:   162   240    :13922  \n F      : 3751      Mean   : 0.2211                       1      : 7191  \n G      : 2553      3rd Qu.: 0.0000                       14     : 3640  \n C      : 2375      Max.   :21.0000                       7      : 3539  \n (Other): 3530                                            (Other):42797  \n    company       days_in_waiting_list         customer_type  \n NULL   :112593   Min.   :  0.000      Contract       : 4076  \n 40     :   927   1st Qu.:  0.000      Group          :  577  \n 223    :   784   Median :  0.000      Transient      :89613  \n 67     :   267   Mean   :  2.321      Transient-Party:25124  \n 45     :   250   3rd Qu.:  0.000                             \n 153    :   215   Max.   :391.000                             \n (Other):  4354                                               \n      adr          required_car_parking_spaces total_of_special_requests\n Min.   :  -6.38   Min.   :0.00000             Min.   :0.0000           \n 1st Qu.:  69.29   1st Qu.:0.00000             1st Qu.:0.0000           \n Median :  94.58   Median :0.00000             Median :0.0000           \n Mean   : 101.83   Mean   :0.06252             Mean   :0.5714           \n 3rd Qu.: 126.00   3rd Qu.:0.00000             3rd Qu.:1.0000           \n Max.   :5400.00   Max.   :8.00000             Max.   :5.0000           \n                                                                        \n reservation_status reservation_status_date\n Canceled :43017    2015-10-21:  1461      \n Check-Out:75166    2015-07-06:   805      \n No-Show  : 1207    2016-11-25:   790      \n                    2015-01-01:   763      \n                    2016-01-18:   625      \n                    2015-07-02:   469      \n                    (Other)   :114477      \n\n\n\n\ngroup_by summary hotel wise\n\n\nCode\n    a <- hotel_bookings%>%\n      group_by('hotel')\n    \ndescribe(a)\n\n\n                               vars      n    mean     sd  median trimmed\nhotel*                            1 119390    1.34   0.47    1.00    1.29\nis_canceled                       2 119390    0.37   0.48    0.00    0.34\nlead_time                         3 119390  104.01 106.86   69.00   87.24\narrival_date_year                 4 119390 2016.16   0.71 2016.00 2016.20\narrival_date_month*               5 119390    6.49   3.54    7.00    6.49\narrival_date_week_number          6 119390   27.17  13.61   28.00   27.16\narrival_date_day_of_month         7 119390   15.80   8.78   16.00   15.80\nstays_in_weekend_nights           8 119390    0.93   1.00    1.00    0.84\nstays_in_week_nights              9 119390    2.50   1.91    2.00    2.29\nadults                           10 119390    1.86   0.58    2.00    1.88\nchildren                         11 119386    0.10   0.40    0.00    0.00\nbabies                           12 119390    0.01   0.10    0.00    0.00\nmeal*                            13 119390    1.56   1.07    1.00    1.31\ncountry*                         14 119390   94.59  45.09   82.00   96.85\nmarket_segment*                  15 119390    5.93   1.27    6.00    6.11\ndistribution_channel*            16 119390    3.59   0.91    4.00    3.80\nis_repeated_guest                17 119390    0.03   0.18    0.00    0.00\nprevious_cancellations           18 119390    0.09   0.84    0.00    0.00\nprevious_bookings_not_canceled   19 119390    0.14   1.50    0.00    0.00\nreserved_room_type*              20 119390    1.99   1.70    1.00    1.64\nassigned_room_type*              21 119390    2.33   1.88    1.00    2.02\nbooking_changes                  22 119390    0.22   0.65    0.00    0.06\ndeposit_type*                    23 119390    1.12   0.33    1.00    1.03\nagent*                           24 119390  211.82 122.75  296.00  221.86\ncompany*                         25 119390  342.22  50.06  353.00  353.00\ndays_in_waiting_list             26 119390    2.32  17.59    0.00    0.00\ncustomer_type*                   27 119390    3.14   0.58    3.00    3.14\nadr                              28 119390  101.83  50.54   94.58   97.71\nrequired_car_parking_spaces      29 119390    0.06   0.25    0.00    0.00\ntotal_of_special_requests        30 119390    0.57   0.79    0.00    0.43\nreservation_status*              31 119390    1.65   0.50    2.00    1.67\nreservation_status_date*         32 119390  517.75 227.48  525.00  521.18\n\"hotel\"*                         33 119390    1.00   0.00    1.00    1.00\n                                  mad     min  max   range  skew kurtosis   se\nhotel*                           0.00    1.00    2    1.00  0.70    -1.51 0.00\nis_canceled                      0.00    0.00    1    1.00  0.54    -1.71 0.00\nlead_time                       88.96    0.00  737  737.00  1.35     1.70 0.31\narrival_date_year                1.48 2015.00 2017    2.00 -0.23    -0.99 0.00\narrival_date_month*              4.45    1.00   12   11.00 -0.04    -1.23 0.01\narrival_date_week_number        16.31    1.00   53   52.00 -0.01    -0.99 0.04\narrival_date_day_of_month       11.86    1.00   31   30.00  0.00    -1.19 0.03\nstays_in_weekend_nights          1.48    0.00   19   19.00  1.38     7.17 0.00\nstays_in_week_nights             1.48    0.00   50   50.00  2.86    24.28 0.01\nadults                           0.00    0.00   55   55.00 18.32  1352.04 0.00\nchildren                         0.00    0.00   10   10.00  4.11    18.67 0.00\nbabies                           0.00    0.00   10   10.00 24.65  1633.85 0.00\nmeal*                            0.00    1.00    5    4.00  1.59     0.96 0.00\ncountry*                        81.54    1.00  178  177.00 -0.16    -1.53 0.13\nmarket_segment*                  1.48    1.00    8    7.00 -1.00     0.15 0.00\ndistribution_channel*            0.00    1.00    5    4.00 -1.87     1.86 0.00\nis_repeated_guest                0.00    0.00    1    1.00  5.33    26.37 0.00\nprevious_cancellations           0.00    0.00   26   26.00 24.46   674.03 0.00\nprevious_bookings_not_canceled   0.00    0.00   72   72.00 23.54   767.20 0.00\nreserved_room_type*              0.00    1.00   10    9.00  1.44     0.89 0.00\nassigned_room_type*              0.00    1.00   12   11.00  1.15     0.53 0.01\nbooking_changes                  0.00    0.00   21   21.00  6.00    79.39 0.00\ndeposit_type*                    0.00    1.00    3    2.00  2.38     4.07 0.00\nagent*                          56.34    1.00  334  333.00 -0.37    -1.53 0.36\ncompany*                         0.00    1.00  353  352.00 -4.93    23.77 0.14\ndays_in_waiting_list             0.00    0.00  391  391.00 11.94   186.78 0.05\ncustomer_type*                   0.00    1.00    4    3.00 -1.08     4.55 0.00\nadr                             41.25   -6.38 5400 5406.38 10.53  1013.13 0.15\nrequired_car_parking_spaces      0.00    0.00    8    8.00  4.16    30.00 0.00\ntotal_of_special_requests        0.00    0.00    5    5.00  1.35     1.49 0.00\nreservation_status*              0.00    1.00    3    2.00 -0.38    -1.25 0.00\nreservation_status_date*       275.76    1.00  926  925.00 -0.12    -1.02 0.66\n\"hotel\"*                         0.00    1.00    1    0.00   NaN      NaN 0.00\n\n\nCode\nsummary(a)\n\n\n          hotel        is_canceled       lead_time   arrival_date_year\n City Hotel  :79330   Min.   :0.0000   Min.   :  0   Min.   :2015     \n Resort Hotel:40060   1st Qu.:0.0000   1st Qu.: 18   1st Qu.:2016     \n                      Median :0.0000   Median : 69   Median :2016     \n                      Mean   :0.3704   Mean   :104   Mean   :2016     \n                      3rd Qu.:1.0000   3rd Qu.:160   3rd Qu.:2017     \n                      Max.   :1.0000   Max.   :737   Max.   :2017     \n                                                                      \n arrival_date_month arrival_date_week_number arrival_date_day_of_month\n August :13877      Min.   : 1.00            Min.   : 1.0             \n July   :12661      1st Qu.:16.00            1st Qu.: 8.0             \n May    :11791      Median :28.00            Median :16.0             \n October:11160      Mean   :27.17            Mean   :15.8             \n April  :11089      3rd Qu.:38.00            3rd Qu.:23.0             \n June   :10939      Max.   :53.00            Max.   :31.0             \n (Other):47873                                                        \n stays_in_weekend_nights stays_in_week_nights     adults      \n Min.   : 0.0000         Min.   : 0.0         Min.   : 0.000  \n 1st Qu.: 0.0000         1st Qu.: 1.0         1st Qu.: 2.000  \n Median : 1.0000         Median : 2.0         Median : 2.000  \n Mean   : 0.9276         Mean   : 2.5         Mean   : 1.856  \n 3rd Qu.: 2.0000         3rd Qu.: 3.0         3rd Qu.: 2.000  \n Max.   :19.0000         Max.   :50.0         Max.   :55.000  \n                                                              \n    children           babies                 meal          country     \n Min.   : 0.0000   Min.   : 0.000000   BB       :92310   PRT    :48590  \n 1st Qu.: 0.0000   1st Qu.: 0.000000   FB       :  798   GBR    :12129  \n Median : 0.0000   Median : 0.000000   HB       :14463   FRA    :10415  \n Mean   : 0.1039   Mean   : 0.007949   SC       :10650   ESP    : 8568  \n 3rd Qu.: 0.0000   3rd Qu.: 0.000000   Undefined: 1169   DEU    : 7287  \n Max.   :10.0000   Max.   :10.000000                     ITA    : 3766  \n NA's   :4                                               (Other):28635  \n       market_segment  distribution_channel is_repeated_guest\n Online TA    :56477   Corporate: 6677      Min.   :0.00000  \n Offline TA/TO:24219   Direct   :14645      1st Qu.:0.00000  \n Groups       :19811   GDS      :  193      Median :0.00000  \n Direct       :12606   TA/TO    :97870      Mean   :0.03191  \n Corporate    : 5295   Undefined:    5      3rd Qu.:0.00000  \n Complementary:  743                        Max.   :1.00000  \n (Other)      :  239                                         \n previous_cancellations previous_bookings_not_canceled reserved_room_type\n Min.   : 0.00000       Min.   : 0.0000                A      :85994     \n 1st Qu.: 0.00000       1st Qu.: 0.0000                D      :19201     \n Median : 0.00000       Median : 0.0000                E      : 6535     \n Mean   : 0.08712       Mean   : 0.1371                F      : 2897     \n 3rd Qu.: 0.00000       3rd Qu.: 0.0000                G      : 2094     \n Max.   :26.00000       Max.   :72.0000                B      : 1118     \n                                                       (Other): 1551     \n assigned_room_type booking_changes       deposit_type        agent      \n A      :74053      Min.   : 0.0000   No Deposit:104641   9      :31961  \n D      :25322      1st Qu.: 0.0000   Non Refund: 14587   NULL   :16340  \n E      : 7806      Median : 0.0000   Refundable:   162   240    :13922  \n F      : 3751      Mean   : 0.2211                       1      : 7191  \n G      : 2553      3rd Qu.: 0.0000                       14     : 3640  \n C      : 2375      Max.   :21.0000                       7      : 3539  \n (Other): 3530                                            (Other):42797  \n    company       days_in_waiting_list         customer_type  \n NULL   :112593   Min.   :  0.000      Contract       : 4076  \n 40     :   927   1st Qu.:  0.000      Group          :  577  \n 223    :   784   Median :  0.000      Transient      :89613  \n 67     :   267   Mean   :  2.321      Transient-Party:25124  \n 45     :   250   3rd Qu.:  0.000                             \n 153    :   215   Max.   :391.000                             \n (Other):  4354                                               \n      adr          required_car_parking_spaces total_of_special_requests\n Min.   :  -6.38   Min.   :0.00000             Min.   :0.0000           \n 1st Qu.:  69.29   1st Qu.:0.00000             1st Qu.:0.0000           \n Median :  94.58   Median :0.00000             Median :0.0000           \n Mean   : 101.83   Mean   :0.06252             Mean   :0.5714           \n 3rd Qu.: 126.00   3rd Qu.:0.00000             3rd Qu.:1.0000           \n Max.   :5400.00   Max.   :8.00000             Max.   :5.0000           \n                                                                        \n reservation_status reservation_status_date   \"hotel\"         \n Canceled :43017    2015-10-21:  1461       Length:119390     \n Check-Out:75166    2015-07-06:   805       Class :character  \n No-Show  : 1207    2016-11-25:   790       Mode  :character  \n                    2015-01-01:   763                         \n                    2016-01-18:   625                         \n                    2015-07-02:   469                         \n                    (Other)   :114477                         \n\n\n\n\nDataframe summary description using summarytools::dfSummary\n\n\nCode\nprint(summarytools::dfSummary(hotel_bookings,\n                              varnumbers = FALSE,\n                              plain.ascii  = FALSE,\n                              style        = \"grid\",\n                              graph.magnif = 0.70,\n                              valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nhotel_bookings\nDimensions: 119390 x 32\n  Duplicates: 31994\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      hotel\n[factor]\n      1. City Hotel2. Resort Hotel\n      79330(66.4%)40060(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      is_canceled\n[integer]\n      Min  : 0Mean : 0.4Max  : 1\n      0:75166(63.0%)1:44224(37.0%)\n      \n      0\n(0.0%)\n    \n    \n      lead_time\n[integer]\n      Mean (sd) : 104 (106.9)min ≤ med ≤ max:0 ≤ 69 ≤ 737IQR (CV) : 142 (1)\n      479 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_year\n[integer]\n      Mean (sd) : 2016.2 (0.7)min ≤ med ≤ max:2015 ≤ 2016 ≤ 2017IQR (CV) : 1 (0)\n      2015:21996(18.4%)2016:56707(47.5%)2017:40687(34.1%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_month\n[factor]\n      1. April2. August3. December4. February5. January6. July7. June8. March9. May10. November[ 2 others ]\n      11089(9.3%)13877(11.6%)6780(5.7%)8068(6.8%)5929(5.0%)12661(10.6%)10939(9.2%)9794(8.2%)11791(9.9%)6794(5.7%)21668(18.1%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_week_number\n[integer]\n      Mean (sd) : 27.2 (13.6)min ≤ med ≤ max:1 ≤ 28 ≤ 53IQR (CV) : 22 (0.5)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_day_of_month\n[integer]\n      Mean (sd) : 15.8 (8.8)min ≤ med ≤ max:1 ≤ 16 ≤ 31IQR (CV) : 15 (0.6)\n      31 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_weekend_nights\n[integer]\n      Mean (sd) : 0.9 (1)min ≤ med ≤ max:0 ≤ 1 ≤ 19IQR (CV) : 2 (1.1)\n      17 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_week_nights\n[integer]\n      Mean (sd) : 2.5 (1.9)min ≤ med ≤ max:0 ≤ 2 ≤ 50IQR (CV) : 2 (0.8)\n      35 distinct values\n      \n      0\n(0.0%)\n    \n    \n      adults\n[integer]\n      Mean (sd) : 1.9 (0.6)min ≤ med ≤ max:0 ≤ 2 ≤ 55IQR (CV) : 0 (0.3)\n      14 distinct values\n      \n      0\n(0.0%)\n    \n    \n      children\n[integer]\n      Mean (sd) : 0.1 (0.4)min ≤ med ≤ max:0 ≤ 0 ≤ 10IQR (CV) : 0 (3.8)\n      0:110796(92.8%)1:4861(4.1%)2:3652(3.1%)3:76(0.1%)10:1(0.0%)\n      \n      4\n(0.0%)\n    \n    \n      babies\n[integer]\n      Mean (sd) : 0 (0.1)min ≤ med ≤ max:0 ≤ 0 ≤ 10IQR (CV) : 0 (12.3)\n      0:118473(99.2%)1:900(0.8%)2:15(0.0%)9:1(0.0%)10:1(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      meal\n[factor]\n      1. BB2. FB3. HB4. SC5. Undefined\n      92310(77.3%)798(0.7%)14463(12.1%)10650(8.9%)1169(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      country\n[factor]\n      1. ABW2. AGO3. AIA4. ALB5. AND6. ARE7. ARG8. ARM9. ASM10. ATA[ 168 others ]\n      2(0.0%)362(0.3%)1(0.0%)12(0.0%)7(0.0%)51(0.0%)214(0.2%)8(0.0%)1(0.0%)2(0.0%)118730(99.4%)\n      \n      0\n(0.0%)\n    \n    \n      market_segment\n[factor]\n      1. Aviation2. Complementary3. Corporate4. Direct5. Groups6. Offline TA/TO7. Online TA8. Undefined\n      237(0.2%)743(0.6%)5295(4.4%)12606(10.6%)19811(16.6%)24219(20.3%)56477(47.3%)2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      distribution_channel\n[factor]\n      1. Corporate2. Direct3. GDS4. TA/TO5. Undefined\n      6677(5.6%)14645(12.3%)193(0.2%)97870(82.0%)5(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      is_repeated_guest\n[integer]\n      Min  : 0Mean : 0Max  : 1\n      0:115580(96.8%)1:3810(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      previous_cancellations\n[integer]\n      Mean (sd) : 0.1 (0.8)min ≤ med ≤ max:0 ≤ 0 ≤ 26IQR (CV) : 0 (9.7)\n      15 distinct values\n      \n      0\n(0.0%)\n    \n    \n      previous_bookings_not_canceled\n[integer]\n      Mean (sd) : 0.1 (1.5)min ≤ med ≤ max:0 ≤ 0 ≤ 72IQR (CV) : 0 (10.9)\n      73 distinct values\n      \n      0\n(0.0%)\n    \n    \n      reserved_room_type\n[factor]\n      1. A2. B3. C4. D5. E6. F7. G8. H9. L10. P\n      85994(72.0%)1118(0.9%)932(0.8%)19201(16.1%)6535(5.5%)2897(2.4%)2094(1.8%)601(0.5%)6(0.0%)12(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      assigned_room_type\n[factor]\n      1. A2. B3. C4. D5. E6. F7. G8. H9. I10. K[ 2 others ]\n      74053(62.0%)2163(1.8%)2375(2.0%)25322(21.2%)7806(6.5%)3751(3.1%)2553(2.1%)712(0.6%)363(0.3%)279(0.2%)13(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      booking_changes\n[integer]\n      Mean (sd) : 0.2 (0.7)min ≤ med ≤ max:0 ≤ 0 ≤ 21IQR (CV) : 0 (2.9)\n      21 distinct values\n      \n      0\n(0.0%)\n    \n    \n      deposit_type\n[factor]\n      1. No Deposit2. Non Refund3. Refundable\n      104641(87.6%)14587(12.2%)162(0.1%)\n      \n      0\n(0.0%)\n    \n    \n      agent\n[factor]\n      1. 12. 103. 1034. 1045. 1056. 1067. 1078. 119. 11010. 111[ 324 others ]\n      7191(6.0%)260(0.2%)21(0.0%)53(0.0%)14(0.0%)2(0.0%)2(0.0%)395(0.3%)12(0.0%)16(0.0%)111424(93.3%)\n      \n      0\n(0.0%)\n    \n    \n      company\n[factor]\n      1. 102. 1003. 1014. 1025. 1036. 1047. 1058. 1069. 10710. 108[ 343 others ]\n      1(0.0%)1(0.0%)1(0.0%)1(0.0%)16(0.0%)1(0.0%)8(0.0%)2(0.0%)9(0.0%)11(0.0%)119339(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      days_in_waiting_list\n[integer]\n      Mean (sd) : 2.3 (17.6)min ≤ med ≤ max:0 ≤ 0 ≤ 391IQR (CV) : 0 (7.6)\n      128 distinct values\n      \n      0\n(0.0%)\n    \n    \n      customer_type\n[factor]\n      1. Contract2. Group3. Transient4. Transient-Party\n      4076(3.4%)577(0.5%)89613(75.1%)25124(21.0%)\n      \n      0\n(0.0%)\n    \n    \n      adr\n[numeric]\n      Mean (sd) : 101.8 (50.5)min ≤ med ≤ max:-6.4 ≤ 94.6 ≤ 5400IQR (CV) : 56.7 (0.5)\n      8879 distinct values\n      \n      0\n(0.0%)\n    \n    \n      required_car_parking_spaces\n[integer]\n      Mean (sd) : 0.1 (0.2)min ≤ med ≤ max:0 ≤ 0 ≤ 8IQR (CV) : 0 (3.9)\n      0:111974(93.8%)1:7383(6.2%)2:28(0.0%)3:3(0.0%)8:2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      total_of_special_requests\n[integer]\n      Mean (sd) : 0.6 (0.8)min ≤ med ≤ max:0 ≤ 0 ≤ 5IQR (CV) : 1 (1.4)\n      0:70318(58.9%)1:33226(27.8%)2:12969(10.9%)3:2497(2.1%)4:340(0.3%)5:40(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status\n[factor]\n      1. Canceled2. Check-Out3. No-Show\n      43017(36.0%)75166(63.0%)1207(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status_date\n[factor]\n      1. 2014-10-172. 2014-11-183. 2015-01-014. 2015-01-025. 2015-01-186. 2015-01-207. 2015-01-218. 2015-01-229. 2015-01-2810. 2015-01-29[ 916 others ]\n      180(0.2%)1(0.0%)763(0.6%)16(0.0%)1(0.0%)2(0.0%)91(0.1%)6(0.0%)1(0.0%)1(0.0%)118328(99.1%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\n\n\nBooking details for two hotels, namely “City Hotel &”Resort Hotel” is given in the dataset hotel_bookings\n\n\nHotel Bookings can be classified on following columns:\n\n\nCode\ntable(hotel_bookings$arrival_date_month)\n\n\n\n    April    August  December  February   January      July      June     March \n    11089     13877      6780      8068      5929     12661     10939      9794 \n      May  November   October September \n    11791      6794     11160     10508 \n\n\n\n\nHotel_bookings year wise\n\n\nCode\ntable(hotel_bookings$arrival_date_year)\n\n\n\n 2015  2016  2017 \n21996 56707 40687"
  },
  {
    "objectID": "posts/challenge2_Akhilesh.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_Akhilesh.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\nChange class of certain columns to factors to represent categorical data\n\n\nCode\n#select columns for factorization\ncols_factor <- c(\"is_canceled\", 'arrival_date_year', 'arrival_date_month', 'arrival_date_week_number', 'arrival_date_day_of_month', 'is_repeated_guest' ) \n\n# converting class of columns: \"is_canceled\", 'arrival_date_year', 'arrival_date_month', 'arrival_date_week_number', 'arrival_date_day_of_month' to factor 'is_repeated_guest'\n\nhotel_bookings <-hotel_bookings%>%\n  mutate_at(vars(is_canceled, arrival_date_year, arrival_date_month, arrival_date_week_number, arrival_date_day_of_month, is_repeated_guest), funs(factor))\n\n# verify class of hotel_bookings: \n\nsapply(hotel_bookings, class)  \n\n\n                         hotel                    is_canceled \n                      \"factor\"                       \"factor\" \n                     lead_time              arrival_date_year \n                     \"integer\"                       \"factor\" \n            arrival_date_month       arrival_date_week_number \n                      \"factor\"                       \"factor\" \n     arrival_date_day_of_month        stays_in_weekend_nights \n                      \"factor\"                      \"integer\" \n          stays_in_week_nights                         adults \n                     \"integer\"                      \"integer\" \n                      children                         babies \n                     \"integer\"                      \"integer\" \n                          meal                        country \n                      \"factor\"                       \"factor\" \n                market_segment           distribution_channel \n                      \"factor\"                       \"factor\" \n             is_repeated_guest         previous_cancellations \n                      \"factor\"                      \"integer\" \nprevious_bookings_not_canceled             reserved_room_type \n                     \"integer\"                       \"factor\" \n            assigned_room_type                booking_changes \n                      \"factor\"                      \"integer\" \n                  deposit_type                          agent \n                      \"factor\"                       \"factor\" \n                       company           days_in_waiting_list \n                      \"factor\"                      \"integer\" \n                 customer_type                            adr \n                      \"factor\"                      \"numeric\" \n   required_car_parking_spaces      total_of_special_requests \n                     \"integer\"                      \"integer\" \n            reservation_status        reservation_status_date \n                      \"factor\"                       \"factor\" \n\n\n\n\nHotel wise, year wise, month wise, reservation status wise number of bookings\n\n\nCode\nbooking_hymrs<-hotel_bookings%>%\n  group_by(hotel, arrival_date_year, arrival_date_month, reservation_status)%>%\n  summarise(no_of_bookings=n(),.groups = 'keep') %>% \n  pivot_wider(names_from = c(hotel, reservation_status), values_from = no_of_bookings)\n  \nbooking_hymrs\n\n\n# A tibble: 26 × 8\n# Groups:   arrival_date_year, arrival_date_month [26]\n   arrival_date_year arrival_d…¹ City …² City …³ City …⁴ Resor…⁵ Resor…⁶ Resor…⁷\n   <fct>             <fct>         <int>   <int>   <int>   <int>   <int>   <int>\n 1 2015              August         1217    1248      15     361    1043       5\n 2 2015              December        646     986      22     278     961      27\n 3 2015              July            918     459      21     314    1058       6\n 4 2015              November        277     934      24     170     920      15\n 5 2015              October        1311    2065      10     403    1160       8\n 6 2015              September      1513    1986      30     543    1034       8\n 7 2016              April          1492    2022      47     515    1345       7\n 8 2016              August         1214    2131      33     572    1107       6\n 9 2016              December       1049    1406      23     316    1056      10\n10 2016              February        796    1441     134     396    1113      11\n# … with 16 more rows, and abbreviated variable names ¹​arrival_date_month,\n#   ²​`City Hotel_Canceled`, ³​`City Hotel_Check-Out`, ⁴​`City Hotel_No-Show`,\n#   ⁵​`Resort Hotel_Canceled`, ⁶​`Resort Hotel_Check-Out`,\n#   ⁷​`Resort Hotel_No-Show`\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nHotel year wise, month wise, reservation status wise number of cancelled/normal bookings\n\n\nis_cancelled: 1 = cancelled, 0= normal booking\n\n\nCode\nbooking_hymc<-hotel_bookings%>%\n  group_by(hotel, arrival_date_year, arrival_date_month)%>%\n  summarise(no_of_bookings=sum(!as.numeric(is_canceled)),no_of_cancelled=sum(as.numeric(is_canceled)), .groups = 'keep') %>% \n  pivot_wider(names_from = hotel, values_from = c(no_of_bookings, no_of_cancelled))\n\nbooking_hymc\n\n\n# A tibble: 26 × 6\n# Groups:   arrival_date_year, arrival_date_month [26]\n   arrival_date_year arrival_date_month no_of_bookings…¹ no_of…² no_of…³ no_of…⁴\n   <fct>             <fct>                         <int>   <int>   <dbl>   <dbl>\n 1 2015              August                            0       0    3712    1775\n 2 2015              December                          0       0    2322    1571\n 3 2015              July                              0       0    2337    1698\n 4 2015              November                          0       0    1536    1290\n 5 2015              October                           0       0    4707    1982\n 6 2015              September                         0       0    5072    2136\n 7 2016              April                             0       0    5100    2389\n 8 2016              August                            0       0    4625    2263\n 9 2016              December                          0       0    3550    1708\n10 2016              February                          0       0    3301    1927\n# … with 16 more rows, and abbreviated variable names\n#   ¹​`no_of_bookings_City Hotel`, ²​`no_of_bookings_Resort Hotel`,\n#   ³​`no_of_cancelled_City Hotel`, ⁴​`no_of_cancelled_Resort Hotel`\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n\nHotel wise: Number of cancelled ticket: Canceled and No-Show\n\n\nCode\nbooking_can <-hotel_bookings%>%\n  filter(is_canceled==1, reservation_status %in% c('No-Show', 'Canceled'))%>%\n  group_by(hotel,reservation_status)%>%\n  summarise(cancelled_ticket_status=n(), .groups = 'keep') %>% \n  pivot_wider(names_from = hotel, values_from = c(cancelled_ticket_status))\n\nbooking_can\n\n\n# A tibble: 2 × 3\n# Groups:   reservation_status [2]\n  reservation_status `City Hotel` `Resort Hotel`\n  <fct>                     <int>          <int>\n1 Canceled                  32186          10831\n2 No-Show                     916            291\n\n\n\n\nHotel year wise, month wise, reservation status wise number of cancelled/normal bookings\n\n\nCode\nbooking_cym <-hotel_bookings%>%\n  filter(is_canceled==1, reservation_status %in% c('No-Show', 'Canceled'))%>%\n  group_by(hotel, arrival_date_year, arrival_date_month, reservation_status)%>%\n  summarise(cancelled_ticket_status=n(), .groups = 'keep') %>% \n  pivot_wider(names_from = hotel, values_from = c(cancelled_ticket_status))\n\nbooking_cym\n\n\n# A tibble: 52 × 5\n# Groups:   arrival_date_year, arrival_date_month, reservation_status [52]\n   arrival_date_year arrival_date_month reservation_status `City Hotel` Resort…¹\n   <fct>             <fct>              <fct>                     <int>    <int>\n 1 2015              August             Canceled                   1217      361\n 2 2015              August             No-Show                      15        5\n 3 2015              December           Canceled                    646      278\n 4 2015              December           No-Show                      22       27\n 5 2015              July               Canceled                    918      314\n 6 2015              July               No-Show                      21        6\n 7 2015              November           Canceled                    277      170\n 8 2015              November           No-Show                      24       15\n 9 2015              October            Canceled                   1311      403\n10 2015              October            No-Show                      10        8\n# … with 42 more rows, and abbreviated variable name ¹​`Resort Hotel`\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nHotel year wise, month wise, country wise reservation status wise number of normal bookings/not-cancelled\n\n\nTo identify and undertand the hotel wise, year wise, month wise, country wise Average Daily Rate for the hotel for the bookings undertaken\n\n\nMean, median, standard deviation, minimum, maximum, quantile of adr for hotel > arrival_date_year\n\n\nTo identify the hotel wise, year wise, distribution of Average Daily Rate for the hotel for the bookings undertaken\n\n\nCode\nbooking_adr <-hotel_bookings%>%\n  filter(is_canceled==0, reservation_status %in% c('No-Show', 'Check-Out'))%>%\n  group_by(hotel, arrival_date_year)%>%\n  summarise(median_adr=median(adr),avg_adr=mean(adr),sd_adr=sd(adr), min_adr = min(adr),max_adr = max(adr), quantile(adr, c(0.25, 0.5, 0.75)), .groups = 'keep')\n\nbooking_adr\n\n\n# A tibble: 18 × 8\n# Groups:   hotel, arrival_date_year [6]\n   hotel        arrival_date_year media…¹ avg_adr sd_adr min_adr max_adr quant…²\n   <fct>        <fct>               <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n 1 City Hotel   2015                 85.5    87.9   36.1    0       268.    65  \n 2 City Hotel   2015                 85.5    87.9   36.1    0       268.    85.5\n 3 City Hotel   2015                 85.5    87.9   36.1    0       268.   105. \n 4 City Hotel   2016                 99     104.    37.7    0       452.    80.8\n 5 City Hotel   2016                 99     104.    37.7    0       452.    99  \n 6 City Hotel   2016                 99     104.    37.7    0       452.   123. \n 7 City Hotel   2017                112     117.    43.1    0       510     89.1\n 8 City Hotel   2017                112     117.    43.1    0       510    112  \n 9 City Hotel   2017                112     117.    43.1    0       510    140. \n10 Resort Hotel 2015                 75.5    89.8   53.7    0       508     48  \n11 Resort Hotel 2015                 75.5    89.8   53.7    0       508     75.5\n12 Resort Hotel 2015                 75.5    89.8   53.7    0       508    124. \n13 Resort Hotel 2016                 66.3    83.9   55.9    0       367     48  \n14 Resort Hotel 2016                 66.3    83.9   55.9    0       367     66.3\n15 Resort Hotel 2016                 66.3    83.9   55.9    0       367    100  \n16 Resort Hotel 2017                 80.9   102.    66.0   -6.38    426.    53.0\n17 Resort Hotel 2017                 80.9   102.    66.0   -6.38    426.    80.9\n18 Resort Hotel 2017                 80.9   102.    66.0   -6.38    426.   135  \n# … with abbreviated variable names ¹​median_adr,\n#   ²​`quantile(adr, c(0.25, 0.5, 0.75))`\n\n\n\n\nLeadtime for cancelled booking\n\n\nTo understand if higher lead time is causing cancellation of booking\n\n\nCode\nbooking_canc_lt<- hotel_bookings%>%\n  select(hotel, is_canceled, lead_time)%>%\n  filter(is_canceled==1)%>%\n  group_by(hotel)%>%\n  summarise(max_leadtime_cancelled = max(lead_time), min_leadtime_cancelled= min(lead_time), sd_leadtime_cancelled = sd(lead_time), mean_lead_time_cancelled = mean(lead_time), .groups = 'keep')\n\nbooking_canc_lt\n\n\n# A tibble: 2 × 5\n# Groups:   hotel [2]\n  hotel        max_leadtime_cancelled min_leadtime_cancelled sd_leadti…¹ mean_…²\n  <fct>                         <int>                  <int>       <dbl>   <dbl>\n1 City Hotel                      629                      0       124.     150.\n2 Resort Hotel                    471                      0        98.8    129.\n# … with abbreviated variable names ¹​sd_leadtime_cancelled,\n#   ²​mean_lead_time_cancelled\n\n\n\n\nLeadtime for booking undertaken\n\n\nTo understand if the lead time for bookings undertaken and compare the same with cancelled booking as calculated above\n\n\nCode\nbooking_ut_lt<- hotel_bookings%>%\n  select(hotel, is_canceled, lead_time)%>%\n  filter(is_canceled==0)%>%\n  group_by(hotel)%>%\n  summarise(max_leadtime = max(lead_time), min_leadtime= min(lead_time), sd_leadtime = sd(lead_time), mean_lead_time = mean(lead_time), .groups = 'keep')\n\nbooking_ut_lt\n\n\n# A tibble: 2 × 5\n# Groups:   hotel [2]\n  hotel        max_leadtime min_leadtime sd_leadtime mean_lead_time\n  <fct>               <int>        <int>       <dbl>          <dbl>\n1 City Hotel            518            0        89.9           80.7\n2 Resort Hotel          737            0        93.1           78.8\n\n\n\n\nHotel wise, market segment wise repeated guest bookings undertaken\n\n\nTo identify any specific market segment is generating more number of bookings for the repeated guests.\n\n\nCode\nbooking_rg<- hotel_bookings%>%\n  select(hotel, market_segment, is_repeated_guest, is_canceled)%>%\n  filter(is_repeated_guest==1, is_canceled==0)%>%\n  group_by(hotel,market_segment)%>%\n  summarise(repeat_guest=n(),.groups = 'keep')\n\nbooking_rg\n\n\n# A tibble: 13 × 3\n# Groups:   hotel, market_segment [13]\n   hotel        market_segment repeat_guest\n   <fct>        <fct>                 <int>\n 1 City Hotel   Aviation                 53\n 2 City Hotel   Complementary           193\n 3 City Hotel   Corporate               842\n 4 City Hotel   Direct                  158\n 5 City Hotel   Groups                    7\n 6 City Hotel   Offline TA/TO           142\n 7 City Hotel   Online TA               196\n 8 Resort Hotel Complementary            18\n 9 Resort Hotel Corporate               537\n10 Resort Hotel Direct                  581\n11 Resort Hotel Groups                   39\n12 Resort Hotel Offline TA/TO           142\n13 Resort Hotel Online TA               350\n\n\n\n\nHotel wise, market segment wise repeated guest bookings cancelled\n\n\nTo identify any specific market segment is generating more number of cancelled bookings for the repeated guests.\n\n\nCode\nbooking_rg_can<- hotel_bookings%>%\n  select(hotel, market_segment, is_repeated_guest, is_canceled)%>%\n  filter(is_repeated_guest==1, is_canceled==1)%>%\n  group_by(hotel,market_segment)%>%\n  summarise(repeat_guest_cancel=n(),.groups = 'keep') \n\nbooking_rg_can\n\n\n# A tibble: 12 × 3\n# Groups:   hotel, market_segment [12]\n   hotel        market_segment repeat_guest_cancel\n   <fct>        <fct>                        <int>\n 1 City Hotel   Aviation                        11\n 2 City Hotel   Complementary                   19\n 3 City Hotel   Corporate                       71\n 4 City Hotel   Direct                          33\n 5 City Hotel   Groups                         194\n 6 City Hotel   Offline TA/TO                   77\n 7 City Hotel   Online TA                       36\n 8 Resort Hotel Corporate                       24\n 9 Resort Hotel Direct                          24\n10 Resort Hotel Groups                          30\n11 Resort Hotel Offline TA/TO                   18\n12 Resort Hotel Online TA                       15\n\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included."
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html",
    "href": "posts/challenge3_EmmaRasmussen.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html#challenge-overview",
    "href": "posts/challenge3_EmmaRasmussen.html#challenge-overview",
    "title": "Challenge 3",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html#read-in-data",
    "href": "posts/challenge3_EmmaRasmussen.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\nanimal_weightOG<-animal_weight#saving a copy of the original data set\nanimal_weight\n\n\n# A tibble: 9 × 17\n  IPCC A…¹ Cattl…² Cattl…³ Buffa…⁴ Swine…⁵ Swine…⁶ Chick…⁷ Chick…⁸ Ducks Turkeys\n  <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl>\n1 Indian …     275     110     295      28      28     0.9     1.8   2.7     6.8\n2 Eastern…     550     391     380      50     180     0.9     1.8   2.7     6.8\n3 Africa       275     173     380      28      28     0.9     1.8   2.7     6.8\n4 Oceania      500     330     380      45     180     0.9     1.8   2.7     6.8\n5 Western…     600     420     380      50     198     0.9     1.8   2.7     6.8\n6 Latin A…     400     305     380      28      28     0.9     1.8   2.7     6.8\n7 Asia         350     391     380      50     180     0.9     1.8   2.7     6.8\n8 Middle …     275     173     380      28      28     0.9     1.8   2.7     6.8\n9 Norther…     604     389     380      46     198     0.9     1.8   2.7     6.8\n# … with 7 more variables: Sheep <dbl>, Goats <dbl>, Horses <dbl>, Asses <dbl>,\n#   Mules <dbl>, Camels <dbl>, Llamas <dbl>, and abbreviated variable names\n#   ¹​`IPCC Area`, ²​`Cattle - dairy`, ³​`Cattle - non-dairy`, ⁴​Buffaloes,\n#   ⁵​`Swine - market`, ⁶​`Swine - breeding`, ⁷​`Chicken - Broilers`,\n#   ⁸​`Chicken - Layers`\n# ℹ Use `colnames()` to see all variable names\n\n\n\nBriefly describe the data\nDescribe the data, and be sure to comment on why you are planning to pivot it to make it “tidy”\nThe data appears to illustrate (average?) animal weights by region. To tidy the data we will pivot the columns with animal names into a single column. Each “case” is an animal type within a region, and the values/dependent variable is the weight."
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html#anticipate-the-end-result",
    "href": "posts/challenge3_EmmaRasmussen.html#anticipate-the-end-result",
    "title": "Challenge 3",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA     673.     766.\n2 USA      1990 NAFTA     455.    1477.\n3 France   1980 EU       1383.     314.\n4 Mexico   1990 NAFTA     594.     711.\n5 USA      1980 NAFTA     974.    1255.\n6 France   1990 EU        881.    1590.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\nOG Dataset has k=17 columns and n=9 rows. 17-1(1 existing variable to describe each case (country), the other 16 columns need to be pivoted) We wll now have three columns, one region, one animalm(new col) (together the IV), one weight(the DV) (new col) 9*16 rows expected in data frame= 144 3col byt 144 rows rows expected\n\n\nCode\nnrow(animal_weightOG)\n\n\n[1] 9\n\n\nCode\nncol(animal_weightOG)\n\n\n[1] 17\n\n\nCode\n#expected rows/cases\nnrow(animal_weightOG)*(ncol(animal_weightOG)-1)\n\n\n[1] 144\n\n\nCode\n#expected columns\n1+2\n\n\n[1] 3\n\n\nAny additional comments? See comment at bottom"
  },
  {
    "objectID": "posts/challenge3_EmmaRasmussen.html#pivot-the-data",
    "href": "posts/challenge3_EmmaRasmussen.html#pivot-the-data",
    "title": "Challenge 3",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing               673.\n 2 Mexico   1980 NAFTA incoming               766.\n 3 USA      1990 NAFTA outgoing               455.\n 4 USA      1990 NAFTA incoming              1477.\n 5 France   1980 EU    outgoing              1383.\n 6 France   1980 EU    incoming               314.\n 7 Mexico   1990 NAFTA outgoing               594.\n 8 Mexico   1990 NAFTA incoming               711.\n 9 USA      1980 NAFTA outgoing               974.\n10 USA      1980 NAFTA incoming              1255.\n11 France   1990 EU    outgoing               881.\n12 France   1990 EU    incoming              1590.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nA case will be an animal from a particular region. It meets the requirements for tidy data because each case has its own row, and each variable has its own column.\n\n\nCode\npivot_longer(animal_weight, col = c(2:17),\n                 names_to=\"animal_type\",\n                 values_to = \"weight\")\n\n\n# A tibble: 144 × 3\n   `IPCC Area`         animal_type        weight\n   <chr>               <chr>               <dbl>\n 1 Indian Subcontinent Cattle - dairy      275  \n 2 Indian Subcontinent Cattle - non-dairy  110  \n 3 Indian Subcontinent Buffaloes           295  \n 4 Indian Subcontinent Swine - market       28  \n 5 Indian Subcontinent Swine - breeding     28  \n 6 Indian Subcontinent Chicken - Broilers    0.9\n 7 Indian Subcontinent Chicken - Layers      1.8\n 8 Indian Subcontinent Ducks                 2.7\n 9 Indian Subcontinent Turkeys               6.8\n10 Indian Subcontinent Sheep                28  \n# … with 134 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAny additional comments?\nI am a little confused by the calculation, I tried to work out everything above in a way that made sense. But we essentially start with: - how many columns will remain unpivoted (variables that start in the correct place/column) -how many columns are being pivoted (the rest or starting number of col minus number above) -The number of columns being pivoted*number of rows = new number of rows -the new number of columns is the unchanged columns plus 1 for the variables contained in the pivot plus one for the values?? So unchanged col+2"
  },
  {
    "objectID": "posts/Challenge4_KimDarkenwald.html",
    "href": "posts/Challenge4_KimDarkenwald.html",
    "title": "Challenge 4 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Challenge4_KimDarkenwald.html#challenge-overview",
    "href": "posts/Challenge4_KimDarkenwald.html#challenge-overview",
    "title": "Challenge 4 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/Challenge4_KimDarkenwald.html#read-in-data",
    "href": "posts/Challenge4_KimDarkenwald.html#read-in-data",
    "title": "Challenge 4 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ⭐\npoultry_tidy.csv⭐⭐\nFedFundsRate.csv⭐⭐⭐\nhotel_bookings.csv⭐⭐⭐⭐\ndebt_in_trillions ⭐⭐⭐⭐⭐\n\n\n\nCode\nABC <- read.csv(\"_data/abc_poll_2021.csv\")\nview(ABC)\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/Challenge4_KimDarkenwald.html#tidy-data-as-needed",
    "href": "posts/Challenge4_KimDarkenwald.html#tidy-data-as-needed",
    "title": "Challenge 4 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/Challenge4_KimDarkenwald.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/Challenge4_KimDarkenwald.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Instructions",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge1_nickboonstra.html",
    "href": "posts/challenge1_nickboonstra.html",
    "title": "Nick Boonstra Challenge 1 Resubmit",
    "section": "",
    "text": "This challenge involves reading in and cleaning up data from the “birds” data set.\n\n\n\n\nCode\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_nickboonstra.html#reading-in-and-cleaning-up",
    "href": "posts/challenge1_nickboonstra.html#reading-in-and-cleaning-up",
    "title": "Nick Boonstra Challenge 1 Resubmit",
    "section": "Reading In and Cleaning Up",
    "text": "Reading In and Cleaning Up\n\n\nCode\nbirds<-read_csv(\"_data/birds.csv\")\nbirds\n\n\n# A tibble: 30,977 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1961  1961\n 2 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1962  1962\n 3 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1963  1963\n 4 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1964  1964\n 5 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1965  1965\n 6 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1966  1966\n 7 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1967  1967\n 8 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1968  1968\n 9 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1969  1969\n10 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1970  1970\n# … with 30,967 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nFortunately, all of the data came in tidy, meaning:\n\nEach column measured observations of only one variable;\nEach row provided values for only one observation; and,\nEach cell contained only one value.\n\n\nData Cleanup\nHowever, while every column was nominally tidy-compliant, a number of them were extraneous, in that they provided redundant or useless data. These columns, specifically, were “Domain,” “Domain Code,” “Element,” “Element Code,” “Unit,” and “Year Code.”\n“Year Code” was redundant, in that its values were equal to “Year” for every observation. Of course, attempting to verify this by hand would be borderline impossible, but luckily this was easily testable by some quick code:\n\n\nCode\nbirds_test <- read_csv(\"_data/birds.csv\")\nbirds_test <- birds_test %>%\n  mutate(year_test = case_when(\n    Year == `Year Code` ~ 1,\n    TRUE ~ 0\n  ))\ncount(birds_test,year_test)\n\n\n# A tibble: 1 × 2\n  year_test     n\n      <dbl> <int>\n1         1 30977\n\n\nCode\nrm(birds_test)\n\n\nBecause the value of value of the dummy “year_test” variable is equal to 1 for all observations, we can know that “Year Code” was equal to “Year” for all observations, and thus eliminate “Year Code” without losing any information.\nIn the case of the other five columns named above, all observations contained the same value, making the columns practically useless. Once again, this assertion was easily testable:\n\n\nCode\ncount(birds,Domain)\n\n\n# A tibble: 1 × 2\n  Domain           n\n  <chr>        <int>\n1 Live Animals 30977\n\n\nCode\ncount(birds,`Domain Code`)\n\n\n# A tibble: 1 × 2\n  `Domain Code`     n\n  <chr>         <int>\n1 QA            30977\n\n\nCode\ncount(birds,Element)\n\n\n# A tibble: 1 × 2\n  Element     n\n  <chr>   <int>\n1 Stocks  30977\n\n\nCode\ncount(birds,`Element Code`)\n\n\n# A tibble: 1 × 2\n  `Element Code`     n\n           <dbl> <int>\n1           5112 30977\n\n\nCode\ncount(birds,Unit)\n\n\n# A tibble: 1 × 2\n  Unit          n\n  <chr>     <int>\n1 1000 Head 30977\n\n\nThus, with each column only carrying one value throughout the data set, these columns could be deleted without pratically losing information. In this case, however, the values being removed from the dataframe are not already being kept somewhere else in the dataframe, as opposed to the duplication found in the Year/Year Code case. Thus, recording these values in a separate location may be desirable, depending upon the exact nature of the dataset and the desired analysis.\nBeyond removing these extraneous columns, the only other adjustments I found necessary were to rename the remaining columns to abide by “snake_case.” This was done for practicality (some of the column names had spaces), consistency, and personal preference.\nAll of these changes, then, are seen here:\n\n\nCode\nbirds<-birds %>%\n  select(-starts_with(\"Domain\")) %>% ## All values identical for all obs\n  rename(area_code = `Area Code`) %>%\n  rename(area = Area) %>%\n  select(-starts_with(\"Element\")) %>% ## All values identical for all obs\n  rename(item_code = `Item Code`) %>%\n  rename(item = Item) %>%\n  select(-`Year Code`) %>% ## Values identical to Year for all obs\n  rename(year = Year) %>%\n  select(-Unit) %>% ## All values identical for all obs\n  rename(value = Value) %>%\n  rename(flag = Flag) %>%\n  rename(flag_desc = `Flag Description`)\n\nbirds\n\n\n# A tibble: 30,977 × 8\n   area_code area        item_code item      year value flag  flag_desc    \n       <dbl> <chr>           <dbl> <chr>    <dbl> <dbl> <chr> <chr>        \n 1         2 Afghanistan      1057 Chickens  1961  4700 F     FAO estimate \n 2         2 Afghanistan      1057 Chickens  1962  4900 F     FAO estimate \n 3         2 Afghanistan      1057 Chickens  1963  5000 F     FAO estimate \n 4         2 Afghanistan      1057 Chickens  1964  5300 F     FAO estimate \n 5         2 Afghanistan      1057 Chickens  1965  5500 F     FAO estimate \n 6         2 Afghanistan      1057 Chickens  1966  5800 F     FAO estimate \n 7         2 Afghanistan      1057 Chickens  1967  6600 F     FAO estimate \n 8         2 Afghanistan      1057 Chickens  1968  6290 <NA>  Official data\n 9         2 Afghanistan      1057 Chickens  1969  6300 F     FAO estimate \n10         2 Afghanistan      1057 Chickens  1970  6000 F     FAO estimate \n# … with 30,967 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge1_nickboonstra.html#describing-the-data",
    "href": "posts/challenge1_nickboonstra.html#describing-the-data",
    "title": "Nick Boonstra Challenge 1 Resubmit",
    "section": "Describing the data",
    "text": "Describing the data\nThese data appear to be recording the populations of various types of birds across a number of countries and years.\n\n\nCode\nnames(birds)\n\n\n[1] \"area_code\" \"area\"      \"item_code\" \"item\"      \"year\"      \"value\"    \n[7] \"flag\"      \"flag_desc\"\n\n\nCode\nbirds %>%\n  group_by(item) %>%\n  summarise(\"Median Values by Type\" = median(value,na.rm=T))\n\n\n# A tibble: 5 × 2\n  item                   `Median Values by Type`\n  <chr>                                    <dbl>\n1 Chickens                                10784.\n2 Ducks                                     510 \n3 Geese and guinea fowls                    258 \n4 Pigeons, other birds                     2800 \n5 Turkeys                                   528 \n\n\nCode\nbirds %>%\n  group_by(year) %>%\n  summarise(\"Median Values by Year\" = median(value,na.rm=T))\n\n\n# A tibble: 58 × 2\n    year `Median Values by Year`\n   <dbl>                   <dbl>\n 1  1961                   1033 \n 2  1962                   1014 \n 3  1963                   1106 \n 4  1964                   1103 \n 5  1965                   1104 \n 6  1966                   1088.\n 7  1967                   1193 \n 8  1968                   1252.\n 9  1969                   1267 \n10  1970                   1259 \n# … with 48 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nbirds %>%\n  group_by(area) %>%\n  summarise(\"Median Values by Area\" = median(value,na.rm=T))\n\n\n# A tibble: 248 × 2\n   area                `Median Values by Area`\n   <chr>                                 <dbl>\n 1 Afghanistan                          6700  \n 2 Africa                              12910. \n 3 Albania                              1300  \n 4 Algeria                                42.5\n 5 American Samoa                         38  \n 6 Americas                            66924. \n 7 Angola                               6075  \n 8 Antigua and Barbuda                    85  \n 9 Argentina                            2355  \n10 Armenia                              1528. \n# … with 238 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html",
    "href": "posts/challenge1_QuinnHe.html",
    "title": "Challenge 1 Quinn He",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html#challenge-overview",
    "href": "posts/challenge1_QuinnHe.html#challenge-overview",
    "title": "Challenge 1 Quinn He",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html#read-in-the-data",
    "href": "posts/challenge1_QuinnHe.html#read-in-the-data",
    "title": "Challenge 1 Quinn He",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nbirds <- read_csv(\"_data/birds.csv\")\n\nview(birds)\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_QuinnHe.html#describe-the-data",
    "href": "posts/challenge1_QuinnHe.html#describe-the-data",
    "title": "Challenge 1 Quinn He",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nThe birds data set contains a wide range of range of entries. With the function below we can see all the column names listed. A few are hard to figure out what exactly the represent and just how important they are.\n\n\nCode\ncolnames(birds)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nIt appears the data set was taken from a farm organization. The data is definitely a little messy, but makes sense on the data entry side. Each country has descending rows of chickens, ducks, and fowls from 1961 to 2018. This is mostly a bit redundant. This whole data set keeps track of the value of these three types of birds in a 60 year window. There is also a possibility this data set came from a larger set with other types of animals because the “Domain” column lists ‘Livestock’ throughout the entire data set.\n\n\nCode\nsummary(birds)\n\n\n Domain Code           Domain            Area Code        Area          \n Length:30977       Length:30977       Min.   :   1   Length:30977      \n Class :character   Class :character   1st Qu.:  79   Class :character  \n Mode  :character   Mode  :character   Median : 156   Mode  :character  \n                                       Mean   :1202                     \n                                       3rd Qu.: 231                     \n                                       Max.   :5504                     \n                                                                        \n  Element Code    Element            Item Code        Item          \n Min.   :5112   Length:30977       Min.   :1057   Length:30977      \n 1st Qu.:5112   Class :character   1st Qu.:1057   Class :character  \n Median :5112   Mode  :character   Median :1068   Mode  :character  \n Mean   :5112                      Mean   :1066                     \n 3rd Qu.:5112                      3rd Qu.:1072                     \n Max.   :5112                      Max.   :1083                     \n                                                                    \n   Year Code         Year          Unit               Value         \n Min.   :1961   Min.   :1961   Length:30977       Min.   :       0  \n 1st Qu.:1976   1st Qu.:1976   Class :character   1st Qu.:     171  \n Median :1992   Median :1992   Mode  :character   Median :    1800  \n Mean   :1991   Mean   :1991                      Mean   :   99411  \n 3rd Qu.:2005   3rd Qu.:2005                      3rd Qu.:   15404  \n Max.   :2018   Max.   :2018                      Max.   :23707134  \n                                                  NA's   :1036      \n     Flag           Flag Description  \n Length:30977       Length:30977      \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n\nCode\ndim(birds)\n\n\n[1] 30977    14\n\n\nCode\nhead(birds)\n\n\n# A tibble: 6 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1961  1961 1000…\n2 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1962  1962 1000…\n3 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1963  1963 1000…\n4 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1964  1964 1000…\n5 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1965  1965 1000…\n6 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1966  1966 1000…\n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names\n\n\nCode\ntail(birds)\n\n\n# A tibble: 6 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2013  2013 1000…\n2 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2014  2014 1000…\n3 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2015  2015 1000…\n4 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2016  2016 1000…\n5 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2017  2017 1000…\n6 QA      Live …    5504 Poly…    5112 Stocks     1068 Ducks    2018  2018 1000…\n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names\n\n\nCode\n#table(birds)\n\nggplot(birds, mapping = aes(x = 'Year', y = 'Value'))\n\n\n\n\n\nI’m wondering if I should use the %>% function here. I’m also having an issue with my functions because they don’t run correctly. Is it from my the “delimiter” error above? It may also be an issue with my working directory.\nI commented out ‘table(birds)’ because it was giving me an error when I rendered it the function.\n\n\nCode\nbirds %>%\n  select(Item)\n\n\n# A tibble: 30,977 × 1\n   Item    \n   <chr>   \n 1 Chickens\n 2 Chickens\n 3 Chickens\n 4 Chickens\n 5 Chickens\n 6 Chickens\n 7 Chickens\n 8 Chickens\n 9 Chickens\n10 Chickens\n# … with 30,967 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge2_solutions.html",
    "href": "posts/challenge2_solutions.html",
    "title": "Challenge 2 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_solutions.html#challenge-overview",
    "href": "posts/challenge2_solutions.html#challenge-overview",
    "title": "Challenge 2 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics\n\n\nRailroad ⭐FAOstat* ⭐⭐⭐Hotel Bookings ⭐⭐⭐⭐\n\n\nThe railroad data contain 2931 county-level aggregated counts of the number of railroad employees in 2012. Counties are embedded within States, and all 50 states plus Canada, overseas addresses in Asia and Europe, and Washington, DC are represented.\n\n\n\n\n\n\nData Summaries\n\n\n\nThis is a concise summary of more extensive work we did in Challenge 1, and is an example of how you should describe data in public-facing work. A “public-facing” version of your work contains all critical details needed to replicate your work, but doesn’t contain a point-by-point rundown of the mental process you went through to get to that point. (Your internal analysis file should probably walk through that mental process, however!)\n\n\n\nRead the data\nHere, we are just reusing the code from Challenge 1. We are using the excel version, to ensure that we get Canada, and are renaming the missing data in county for Canada so that we don’t accidently filter that observation out.\n\nrailroad<-read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"state\", \"delete\",  \"county\",\n                                  \"delete\", \"employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(state, \"Total\"))\n\nrailroad<-head(railroad, -2)%>%\n  mutate(county = ifelse(state==\"CANADA\", \"CANADA\", county))\n\nrailroad\n\n\n\n  \n\n\n\n\n\nHow many values does X take on?\nNow, lets practice grouping our data and using other dplyr commands that make data wrangling super easy. First, lets take a closer look at how we counted the number of unique states last week. First, we selected the state column. Then we used the n_distinct command - which replicates the base R commands length(unique(var)).\n\n\n\n\n\n\nacross()\n\n\n\nInstead of counting the number of distinct values one at a time, I am doing an operation on two columns at the same time using across.\n\n\n\nrailroad%>%\n  summarise(across(c(state,county), n_distinct))\n\n\n\n  \n\n\n\nCheck this out - many counties have the same name! There are 2931 state-county cases, but only 1710 distinct county names. This is one reason it is so critical to understand “what is a case” when you are working with your data - otherwise you might accidently collapse or group information that isn’t intenced to be grouped.\n\n\nHow many total X are in group Y?\nSuppose we want to know the total number of railroad employees was in 2012, what is the best way to sum up all of the values in the data? The summarize function is useful for doing calculcations across some or all of a data set.\n\nrailroad%>%\n  summarise(total_employees = sum(employees))\n\n\n\n  \n\n\n\nAround a quarter of a million people were employed in the railroad industry in 2012. While this may seem like a lot, it was a significant decrease in employment from a few decades earlier, according to official Bureau of Labor Statistics (BLS) estimates.\nYou may notice that the BLS estimates are significantly lower than the ones we are using, provided by the Railroad Retirement Board. Given that the Railroad Retirement Board has “gold-standard” data on railroad employees, this discrepancy suggests that many people who work in the railroad industry are being classified in a different way by BLS statistics.\n\n\nWhich X have the most Y?\nSuppose we are interested in which county names are duplicated most often, or in which states have the most railroad employees. We can use the same basic approch to answer both “Which X have the most Y?” style questions\n\n\n\n\n\n\ndf-print: paged (YAML)\n\n\n\nWhen you are using df-print: paged in your yaml header, or are using tibbles, there is no need to rely on the head(data) commmand to limit your results to the top 10 of a list.\n\n\n\nrailroad%>%\n  group_by(state)%>%\n  summarise(total_employees = sum(employees),\n            num_counties = n())%>%\n  arrange(desc(total_employees))\n\n\n\n  \n\n\n\nLooking at the top 10 states in terms of total railroad employment, a few trends emerge. Several of the top 10 states with geographical activity are highly populous and geographically large. California, Texas, New York, Pennsylvania, Ohio, Illinois, and Georgia are all amonst the top-10 largest states - so it would make sense if there are more railroad employees in large states.\nBut railroads are spread out along geography, and thus we might also expect square mileage within a state to be related to state railroad employment - not just state population. For example, Texas is around 65% larger (in area) than California, and has around 50% more railroad employees.\nThere appear to be multiple exceptions to both rules, however. If geography plus population were the primary factors explaining railroad employment, then California would be ranked higher than New York and Illinois, and New York would likely rank higher than ILlinois. However, Illinois - Chicago in particular - is a hub of railroad activity, and thus Illinois’ higher ranking is likely reflecting hub activity and employment. New York is a hub for the East Coast in particular. While California may have hubs of train activity in Los Angeles or San Francisco, the Northeast has a higher density of train stations and almost certainly generates more passenger and freight miles than the larger and more populous California.\nThis final factor - the existence of heavily used train routes probably explains the high railroad employment in states like Nebraska, Indiana and Missouri - all of which lay along a major railway route between New York and Chicago, and then out to California. Anyway who has played Ticket to Ride probably recognizes many of these routes!\n\n\n\n\n\n\nGo further\n\n\n\nA fun exercise once you are comfortable with joins and map-based visualizatinos would be to join the railroad employment data to information about state population and geographic area, and also mapping information about railway routes, to get more insight into the factors driving railroad employment.\n\n\n\n\n\nThe FAOSTAT sheets are excerpts of the FAOSTAT database provided by the Food and Agriculture Association, an agency of the United Nations. We are using the file birds.csv that includes estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) for 248 areas for 58 years between 1961-2018. Estimated stocks are given in 1000 head.\nBecause we know (from challenge 1) that several of those areas include aggregated data (e.g., ) we are going to remove the aggregations, remove the unnecessary variables, and only work with the grouping variables available in the data. In a future challenge, we will join back on more data from the FAO to recreate regional groupings.\n\nbirds<-read_csv(\"_data/birds.csv\")%>%\n  select(-c(contains(\"Code\"), Element, Domain, Unit))%>%\n  filter(Flag!=\"A\")\nbirds\n\n\n\n  \n\n\n\n\nWhat is the average of Y for X groups?\nLets suppose we are starting off and know nothing about poultry stocks around the world, where could we start? Perhaps we could try to get a sense of the relative sizes of stocks of each of the five types of poultry, identified in the variable Item. Additionally, because some of the values may be missing, lets find out how many of the estimates are missing.\n\nbirds%>%\n  group_by(Item)%>%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE),\n            n_missing = sum(is.na(Value)))\n\n\n\n  \n\n\n\nOn average, we can see that countries have far more chickens as livestock (\\(\\bar{x}\\)=58.4million head) than other livestock birds (average stocks range between 2 and 10 million head). However, the information from the median stock counts suggest that there is significant variation across countries along with a strong right hand skew with regards to chicken stocks. The median number of chickens in a country is 3.8 million head - significantly less than the mean of almost 60 million. Overall, missing data doesn’t seem to be a huge issue, so we will just use na.rm=TRUE and not worry too much about the missingness for now.\nIt could be that stock head counts have changed over time, so lets try selecting two points in time and seeing whether or not average livestock counts are changing.\n\n\n\n\n\n\npivot-wider\n\n\n\nIt can be difficult to visually report data in tidy format. For example, it is tough to compare two values when they are on different rows. In this example, I use pivot-wider to swap a tidy grouping variable into multiple columns to be more “table-like.” I then do some manual formatting to make it easy to compare the grouped estimates.\n\n\n\nt1<-birds%>%\n  filter(Year %in% c(1966, 2016))%>%\n  group_by(Year, Item)%>%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE))%>%\n  pivot_wider(names_from = Year, values_from = c(avg_stocks, med_stocks))\n\nknitr::kable(t1,\n             digits=0,format.args = list(big.mark = \",\"),\n             col.names = c(\"Type\", \"1966\", \"2016\",\n                           \"1996\", \"2016\"))%>%\n  kableExtra::kable_styling(htmltable_class = \"lightable-minimal\")%>%\n  kableExtra::add_header_above(c(\" \" = 1, \"Mean Stock (1000)\" = 2,\n                                 \"Median Stock (1000)\" = 2))\n\n\n\n \n\n\nMean Stock (1000)\nMedian Stock (1000)\n\n  \n    Type \n    1966 \n    2016 \n    1996 \n    2016 \n  \n \n\n  \n    Chickens \n    25,264 \n    105,437 \n    2,315 \n    7,854 \n  \n  \n    Ducks \n    5,053 \n    14,842 \n    110 \n    236 \n  \n  \n    Geese and guinea fowls \n    2,468 \n    11,750 \n    97 \n    73 \n  \n  \n    Pigeons, other birds \n    3,314 \n    2,874 \n    3,000 \n    1,194 \n  \n  \n    Turkeys \n    843 \n    2,858 \n    74 \n    194 \n  \n\n\n\n\n\n\n\n\n\n\n\nkable and kableExtra\n\n\n\nI manually adjust table formatting (column names, setting significant digits, adding a comma) using kable and add a header row using kableExtra. Because df-print=paged option is set to make it easier to scroll through longer data frames, I need to directly specify that I want to produce an htmltable - not a default kable/rmarkdown table - when I use kable and kableExtra formatting directly.\n\n\nSure enough, it does look like stocks have changed significantly over time. The expansion of country-level chicken stocks over five decades between 1966 and 2016 are most noteworthy, with both average and median stock count going up by a factor of 4. Pigeons have never been very popular, and average stocks have actually decreased over the same time period while the other less popular bird - turkeys - saw significant incrases in stock count. Some countries increased specialization in goose and/or guinea fowl production, as the average stock count went up but the median went down over the same period.\n\n\n\n\n\n\nGo further\n\n\n\nIt could be really interesting to graph the rise and fall of poultry stocks overtime with these data, and match these changes to changes in population size and country GDP. Another option would be to match the countries back to regional groupings available from the UN FAO, a future “data join” challenge.\n\n\n\n\n\nThis data set contains 119,390 hotel bookings from two hotels (“City Hotel” and “Resort Hotel”) with an arrival date between July 2015 and August 2017 (more detail needed), including bookings that were later cancelled. Each row contains extensive information about a single booking:\n\nthe booking process (e.g., lead time, booking agent, deposit, changes made)\nbooking details (e.g., scheduled arrival date, length of stay)\nguest requests (e.g., type of room, meal(s) included, car parking)\nbooking channel (e.g., distribution, market segment, corporate affiliation for )\nguest information (e.g., child/adult, passport country)\nguest prior bookings (e.g., repeat customer, prior cancellations)\n\nThe data are a de-identified extract of real hotel demand data, made available by the authors.\n\nRead and make sense of the data\nThe hotel bookings data set is new to challenge 2, so we need to go through the same process we did during challenge 1 to find out more about the data. Lets read in the data and use the summmaryTools package to get an overview of the data set.\n\nbookings<-read_csv(\"_data/hotel_bookings.csv\")\n\nprint(summarytools::dfSummary(bookings,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE,\n                        style        = \"grid\",\n                        graph.magnif = 0.70,\n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nbookings\nDimensions: 119390 x 32\n  Duplicates: 31994\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      hotel\n[character]\n      1. City Hotel2. Resort Hotel\n      79330(66.4%)40060(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      is_canceled\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:75166(63.0%)1:44224(37.0%)\n      \n      0\n(0.0%)\n    \n    \n      lead_time\n[numeric]\n      Mean (sd) : 104 (106.9)min ≤ med ≤ max:0 ≤ 69 ≤ 737IQR (CV) : 142 (1)\n      479 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_year\n[numeric]\n      Mean (sd) : 2016.2 (0.7)min ≤ med ≤ max:2015 ≤ 2016 ≤ 2017IQR (CV) : 1 (0)\n      2015:21996(18.4%)2016:56707(47.5%)2017:40687(34.1%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_month\n[character]\n      1. August2. July3. May4. October5. April6. June7. September8. March9. February10. November[ 2 others ]\n      13877(11.6%)12661(10.6%)11791(9.9%)11160(9.3%)11089(9.3%)10939(9.2%)10508(8.8%)9794(8.2%)8068(6.8%)6794(5.7%)12709(10.6%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_week_number\n[numeric]\n      Mean (sd) : 27.2 (13.6)min ≤ med ≤ max:1 ≤ 28 ≤ 53IQR (CV) : 22 (0.5)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_day_of_month\n[numeric]\n      Mean (sd) : 15.8 (8.8)min ≤ med ≤ max:1 ≤ 16 ≤ 31IQR (CV) : 15 (0.6)\n      31 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_weekend_nights\n[numeric]\n      Mean (sd) : 0.9 (1)min ≤ med ≤ max:0 ≤ 1 ≤ 19IQR (CV) : 2 (1.1)\n      17 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_week_nights\n[numeric]\n      Mean (sd) : 2.5 (1.9)min ≤ med ≤ max:0 ≤ 2 ≤ 50IQR (CV) : 2 (0.8)\n      35 distinct values\n      \n      0\n(0.0%)\n    \n    \n      adults\n[numeric]\n      Mean (sd) : 1.9 (0.6)min ≤ med ≤ max:0 ≤ 2 ≤ 55IQR (CV) : 0 (0.3)\n      14 distinct values\n      \n      0\n(0.0%)\n    \n    \n      children\n[numeric]\n      Mean (sd) : 0.1 (0.4)min ≤ med ≤ max:0 ≤ 0 ≤ 10IQR (CV) : 0 (3.8)\n      0:110796(92.8%)1:4861(4.1%)2:3652(3.1%)3:76(0.1%)10:1(0.0%)\n      \n      4\n(0.0%)\n    \n    \n      babies\n[numeric]\n      Mean (sd) : 0 (0.1)min ≤ med ≤ max:0 ≤ 0 ≤ 10IQR (CV) : 0 (12.3)\n      0:118473(99.2%)1:900(0.8%)2:15(0.0%)9:1(0.0%)10:1(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      meal\n[character]\n      1. BB2. FB3. HB4. SC5. Undefined\n      92310(77.3%)798(0.7%)14463(12.1%)10650(8.9%)1169(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      country\n[character]\n      1. PRT2. GBR3. FRA4. ESP5. DEU6. ITA7. IRL8. BEL9. BRA10. NLD[ 168 others ]\n      48590(40.7%)12129(10.2%)10415(8.7%)8568(7.2%)7287(6.1%)3766(3.2%)3375(2.8%)2342(2.0%)2224(1.9%)2104(1.8%)18590(15.6%)\n      \n      0\n(0.0%)\n    \n    \n      market_segment\n[character]\n      1. Aviation2. Complementary3. Corporate4. Direct5. Groups6. Offline TA/TO7. Online TA8. Undefined\n      237(0.2%)743(0.6%)5295(4.4%)12606(10.6%)19811(16.6%)24219(20.3%)56477(47.3%)2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      distribution_channel\n[character]\n      1. Corporate2. Direct3. GDS4. TA/TO5. Undefined\n      6677(5.6%)14645(12.3%)193(0.2%)97870(82.0%)5(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      is_repeated_guest\n[numeric]\n      Min  : 0Mean : 0Max  : 1\n      0:115580(96.8%)1:3810(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      previous_cancellations\n[numeric]\n      Mean (sd) : 0.1 (0.8)min ≤ med ≤ max:0 ≤ 0 ≤ 26IQR (CV) : 0 (9.7)\n      15 distinct values\n      \n      0\n(0.0%)\n    \n    \n      previous_bookings_not_canceled\n[numeric]\n      Mean (sd) : 0.1 (1.5)min ≤ med ≤ max:0 ≤ 0 ≤ 72IQR (CV) : 0 (10.9)\n      73 distinct values\n      \n      0\n(0.0%)\n    \n    \n      reserved_room_type\n[character]\n      1. A2. B3. C4. D5. E6. F7. G8. H9. L10. P\n      85994(72.0%)1118(0.9%)932(0.8%)19201(16.1%)6535(5.5%)2897(2.4%)2094(1.8%)601(0.5%)6(0.0%)12(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      assigned_room_type\n[character]\n      1. A2. D3. E4. F5. G6. C7. B8. H9. I10. K[ 2 others ]\n      74053(62.0%)25322(21.2%)7806(6.5%)3751(3.1%)2553(2.1%)2375(2.0%)2163(1.8%)712(0.6%)363(0.3%)279(0.2%)13(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      booking_changes\n[numeric]\n      Mean (sd) : 0.2 (0.7)min ≤ med ≤ max:0 ≤ 0 ≤ 21IQR (CV) : 0 (2.9)\n      21 distinct values\n      \n      0\n(0.0%)\n    \n    \n      deposit_type\n[character]\n      1. No Deposit2. Non Refund3. Refundable\n      104641(87.6%)14587(12.2%)162(0.1%)\n      \n      0\n(0.0%)\n    \n    \n      agent\n[character]\n      1. 92. NULL3. 2404. 15. 146. 77. 68. 2509. 24110. 28[ 324 others ]\n      31961(26.8%)16340(13.7%)13922(11.7%)7191(6.0%)3640(3.0%)3539(3.0%)3290(2.8%)2870(2.4%)1721(1.4%)1666(1.4%)33250(27.8%)\n      \n      0\n(0.0%)\n    \n    \n      company\n[character]\n      1. NULL2. 403. 2234. 675. 456. 1537. 1748. 2199. 28110. 154[ 343 others ]\n      112593(94.3%)927(0.8%)784(0.7%)267(0.2%)250(0.2%)215(0.2%)149(0.1%)141(0.1%)138(0.1%)133(0.1%)3793(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      days_in_waiting_list\n[numeric]\n      Mean (sd) : 2.3 (17.6)min ≤ med ≤ max:0 ≤ 0 ≤ 391IQR (CV) : 0 (7.6)\n      128 distinct values\n      \n      0\n(0.0%)\n    \n    \n      customer_type\n[character]\n      1. Contract2. Group3. Transient4. Transient-Party\n      4076(3.4%)577(0.5%)89613(75.1%)25124(21.0%)\n      \n      0\n(0.0%)\n    \n    \n      adr\n[numeric]\n      Mean (sd) : 101.8 (50.5)min ≤ med ≤ max:-6.4 ≤ 94.6 ≤ 5400IQR (CV) : 56.7 (0.5)\n      8879 distinct values\n      \n      0\n(0.0%)\n    \n    \n      required_car_parking_spaces\n[numeric]\n      Mean (sd) : 0.1 (0.2)min ≤ med ≤ max:0 ≤ 0 ≤ 8IQR (CV) : 0 (3.9)\n      0:111974(93.8%)1:7383(6.2%)2:28(0.0%)3:3(0.0%)8:2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      total_of_special_requests\n[numeric]\n      Mean (sd) : 0.6 (0.8)min ≤ med ≤ max:0 ≤ 0 ≤ 5IQR (CV) : 1 (1.4)\n      0:70318(58.9%)1:33226(27.8%)2:12969(10.9%)3:2497(2.1%)4:340(0.3%)5:40(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status\n[character]\n      1. Canceled2. Check-Out3. No-Show\n      43017(36.0%)75166(63.0%)1207(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status_date\n[Date]\n      min : 2014-10-17med : 2016-08-07max : 2017-09-14range : 2y 10m 28d\n      926 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-22\n\n\n\nWow - there is a lot of information available here. Lets scan it and see what jumps out. First we can see that the summary function claims that there are almost 32,000 duplicates in the data. However, this is likely an artifact of the way that the bookings have been de-identified, and may reflect bookings with identical details but different individuals who made the bookings.\nWe can see that we are provided with limited information about the hotel. Hotels are identified only as “City” Hotel” or a “Resort Hotel”. Maybe we have bookings from only two hotels? Lets tentatively add that to our data description.\nThere is a flag for whether a booking is cancelled. This means that our universe of cases includes bookings where the guests showed up, as well as bookings that were later cancelled - we can add that to our data description.\nThere are multiple fields with the arrival date - year, month, etc. For now, we can tell that the arrival date of the bookings ranges between 2015 and 2017. More precise identification of the date range could be more easily done next challenge when we can recode the arrival date information using lubridate.But maybe it is possible to find out which values of month co-occur with specific years?\n\n\nWhich values of Y are nested within X?\nTo approach this question, we can narrow the dataset down to just the two variables of interest, and then use the distinct command.\n\nbookings%>%\n  select(arrival_date_year, arrival_date_month)%>%\n  distinct()\n\n\n\n  \n\n\n\nGreat - now we now that all bookings have arrival dates between June 2015 and August 2017, and can add that to the data description. Just for fun, lets see if we can confirm that the dates are the same for both hotels.\n\n\n\n\n\n\nslice()\n\n\n\nThis would be easier to investigate with proper date variables, but I am using slice to find the first and last row for each hotel, by position. This avoids printing out a long data list we have to scroll through, but would fail if the hotels had different sets of arrival month-year pairs.\n\n\n\nd<-bookings%>%\n  select(arrival_date_year, arrival_date_month)%>%\n  n_distinct\n\nbookings%>%\n  select(hotel, arrival_date_year, arrival_date_month)%>%\n  distinct()%>%\n  slice(c(1, d, d+1, d*2))\n\n\n\n  \n\n\n\nLets suppose we want to know whether or not the two hotels offer the same types of rooms? This is another query of the sort Which values of X are nested in y?\n\nbookings%>%\n  group_by(hotel)%>%\n  count(reserved_room_type)\n\n\n\n  \n\n\n\nIn this case, however, it is tough to directly compare - it appears that the hotel-roomtype pairs are not as consistent as the year-month pairs for the same hotels. A quick pivot-wider makes this comparison a little easier to visualize. Here we can see that the Resort Hotel has two additional room types: “H” and “L”.\n\nbookings%>%\n  group_by(hotel)%>%\n  count(reserved_room_type)%>%\n  pivot_wider(names_from= hotel, values_from = n)\n\n\n\n  \n\n\n\n\n\nWhat is the average of Y for group X?\nThe breakdown of rooms by hotel doesn’t shed much light on the room codes and what they might mean. Lets see if we can find average number of occupants and average price for each room type, and see if we can learn more about our data.\n\n\n\n\n\n\nmean(., na.rm=TRUE)\n\n\n\nI am using the mean function with the option na.rm=TRUE to deal with the four NA values in the children field, identified in the summary table above.\n\n\n\nt1<-bookings%>%\n  group_by(hotel, reserved_room_type)%>%\n  summarise(price = mean(adr),\n            adults = mean(adults),\n            children = mean(children+babies, na.rm=TRUE)\n            )%>%\n  pivot_wider(names_from= hotel,\n              values_from = c(price, adults, children))\n\nknitr::kable(t1,\n             digits=1,\n             col.names = c(\"Type\", \"City\", \"Resort\",\n                           \"City\", \"Resort\", \"City\", \"Resort\"))%>%\n  kableExtra::kable_styling(htmltable_class = \"lightable-minimal\")%>%\n  kableExtra::add_header_above(c(\"Room\" = 1, \"Price\" = 2,\n                                 \"Adults\" = 2, \"Children & Babies\" = 2))\n\n\nAverage Price and Occupancy, by hotel and room type\n \n\nRoom\nPrice\nAdults\nChildren & Babies\n\n  \n    Type \n    City \n    Resort \n    City \n    Resort \n    City \n    Resort \n  \n \n\n  \n    A \n    96.2 \n    76.2 \n    1.8 \n    1.8 \n    0.0 \n    0.0 \n  \n  \n    B \n    90.3 \n    104.7 \n    1.6 \n    2.0 \n    0.6 \n    0.0 \n  \n  \n    C \n    85.5 \n    161.4 \n    1.5 \n    2.0 \n    0.1 \n    1.4 \n  \n  \n    D \n    131.5 \n    103.6 \n    2.2 \n    2.0 \n    0.0 \n    0.1 \n  \n  \n    E \n    156.8 \n    114.5 \n    2.1 \n    2.0 \n    0.3 \n    0.0 \n  \n  \n    F \n    189.3 \n    132.8 \n    2.0 \n    2.0 \n    1.6 \n    0.1 \n  \n  \n    G \n    201.8 \n    168.2 \n    2.3 \n    2.0 \n    1.1 \n    1.4 \n  \n  \n    P \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    H \n    NA \n    188.2 \n    NA \n    2.7 \n    NA \n    1.0 \n  \n  \n    L \n    NA \n    124.7 \n    NA \n    2.2 \n    NA \n    0.0 \n  \n\n\n\n\n\n\n\n\n\n\n\nkable & kableExtra\n\n\n\nI manually adjust table formatting (column names, plus adding a header row) using kable and kableExtra package, respectively. Also, because df-print=paged is the option set in the YAML header, I need to directly specify that I want to produce an htmltable - not a default kable/rmarkdown table.\n\n\nBased on these descriptives broken down by hotel and room type, we can speculate that the “H” and “L” room types at the resort are likely some sort of multi-bedroom suite (because the average number of adults is over 2.) Similarly, we can speculate that the difference between ABC and DEF may be something related to room size or quality (e.g., number and size of beds) and/or related to meals included with the rooms - but this would require further investigation to pin down!\n\n\n\n\n\n\nGo further\n\n\n\nThere is lots more to explore in the hotel bookings dataset, but it will be a lot easier once we recode the date fields using lubridate."
  },
  {
    "objectID": "posts/challenge4_WillMunson.html",
    "href": "posts/challenge4_WillMunson.html",
    "title": "Challenge 4 Will Munson",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_WillMunson.html#challenge-overview",
    "href": "posts/challenge4_WillMunson.html#challenge-overview",
    "title": "Challenge 4 Will Munson",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_WillMunson.html#read-in-data",
    "href": "posts/challenge4_WillMunson.html#read-in-data",
    "title": "Challenge 4 Will Munson",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ⭐\npoultry_tidy.csv⭐⭐\nFedFundsRate.csv⭐⭐⭐\nhotel_bookings.csv⭐⭐⭐⭐\ndebt_in_trillions ⭐⭐⭐⭐⭐\n\n\n\nCode\npoultry_tidy<-read_csv(\"_data/poultry_tidy.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge4_WillMunson.html#tidy-data-as-needed",
    "href": "posts/challenge4_WillMunson.html#tidy-data-as-needed",
    "title": "Challenge 4 Will Munson",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAny additional comments?\nI believe the data is already tidy. Each value has its own column, and there's not much to do. However, I noticed that the year column seems to be backwards, and the price values don't round up to two digits. I'm not sure if that's a problem."
  },
  {
    "objectID": "posts/challenge4_WillMunson.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_WillMunson.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Will Munson",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\n\n\nCode\npoultry_tidy %>%\n  arrange(Product, Year) %>%\n  mutate_at(vars(Price_Dollar), funs(round(., digit = 2)))\n\n\n# A tibble: 600 × 4\n   Product     Year Month     Price_Dollar\n   <chr>      <dbl> <chr>            <dbl>\n 1 B/S Breast  2004 January           6.46\n 2 B/S Breast  2004 February          6.42\n 3 B/S Breast  2004 March             6.42\n 4 B/S Breast  2004 April             6.42\n 5 B/S Breast  2004 May               6.42\n 6 B/S Breast  2004 June              6.41\n 7 B/S Breast  2004 July              6.42\n 8 B/S Breast  2004 August            6.42\n 9 B/S Breast  2004 September         6.42\n10 B/S Breast  2004 October           6.42\n# … with 590 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAny additional comments?\nWhile the data might be tidy, you should also check and make sure the values are entered in a way that makes sense. Otherwise, there's"
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html",
    "href": "posts/challenge2_instructions-youngsoo choi.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html#challenge-overview",
    "href": "posts/challenge2_instructions-youngsoo choi.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html#read-in-the-data",
    "href": "posts/challenge2_instructions-youngsoo choi.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nI choosed ’StateCounty2012.xls’file and read it. But it has first 3 rows about title and any others. So I removed the first 3 rows using skip code.\n\n\nCode\nlibrary(readxl)\nstate2012 <- read_xls(\"_data/StateCounty2012.xls\", skip=3)\nstate2012\n\n\n# A tibble: 2,990 × 5\n   STATE     ...2  COUNTY               ...4  TOTAL\n   <chr>     <lgl> <chr>                <lgl> <dbl>\n 1 AE        NA    APO                  NA        2\n 2 AE Total1 NA    <NA>                 NA        2\n 3 AK        NA    ANCHORAGE            NA        7\n 4 AK        NA    FAIRBANKS NORTH STAR NA        2\n 5 AK        NA    JUNEAU               NA        3\n 6 AK        NA    MATANUSKA-SUSITNA    NA        2\n 7 AK        NA    SITKA                NA        1\n 8 AK        NA    SKAGWAY MUNICIPALITY NA       88\n 9 AK Total  NA    <NA>                 NA      103\n10 AL        NA    AUTAUGA              NA      102\n# … with 2,980 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\ncolnames(state2012)\n\n\n[1] \"STATE\"  \"...2\"   \"COUNTY\" \"...4\"   \"TOTAL\" \n\n\nThis dataset has 2990 rows and 5 columns. And each column name is “STATE”, “…2”, “COUNTY”, “…4”, “TOTAL”."
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html#describe-the-data",
    "href": "posts/challenge2_instructions-youngsoo choi.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nThis dataset is about the number of workers related to railroad jobs in 2012 I think. And this dataset contains data of 2990 county.\n\n\nCode\nsummary(state2012)\n\n\n    STATE             ...2            COUNTY            ...4        \n Length:2990        Mode:logical   Length:2990        Mode:logical  \n Class :character   NA's:2990      Class :character   NA's:2990     \n Mode  :character                  Mode  :character                 \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n     TOTAL         \n Min.   :     1.0  \n 1st Qu.:     7.0  \n Median :    22.0  \n Mean   :   256.9  \n 3rd Qu.:    71.0  \n Max.   :255432.0  \n NA's   :5"
  },
  {
    "objectID": "posts/challenge2_instructions-youngsoo choi.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions-youngsoo choi.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nI choosed MA and CA because MA is the state where I live in and CA is much bigger state I think. I used filter code to figure out MA’s and CA’s central tendency numbers.\n\n\nCode\nMA<-filter(state2012, STATE==\"MA\")\nsummary(MA)\n\n\n    STATE             ...2            COUNTY            ...4        \n Length:12          Mode:logical   Length:12          Mode:logical  \n Class :character   NA's:12        Class :character   NA's:12       \n Mode  :character                  Mode  :character                 \n                                                                    \n                                                                    \n                                                                    \n     TOTAL      \n Min.   : 44.0  \n 1st Qu.:101.8  \n Median :271.0  \n Mean   :281.6  \n 3rd Qu.:396.8  \n Max.   :673.0  \n\n\nCode\nCA<-filter(state2012, STATE==\"CA\")\nsummary(CA)\n\n\n    STATE             ...2            COUNTY            ...4        \n Length:55          Mode:logical   Length:55          Mode:logical  \n Class :character   NA's:55        Class :character   NA's:55       \n Mode  :character                  Mode  :character                 \n                                                                    \n                                                                    \n                                                                    \n     TOTAL       \n Min.   :   1.0  \n 1st Qu.:  12.5  \n Median :  61.0  \n Mean   : 238.9  \n 3rd Qu.: 200.5  \n Max.   :2888.0  \n\n\n\nExplain and Interpret\nMA has 12 county and average total is 286.1 and CA has 55 county and average total is 238.9. So I figure CA has more county than MA but MA’s average total is larger than CA’s."
  },
  {
    "objectID": "posts/challenge2_QuinnHe.html",
    "href": "posts/challenge2_QuinnHe.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_QuinnHe.html#describe-the-data",
    "href": "posts/challenge2_QuinnHe.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nThe data includes information on two separate hotels “Resort Hotel” and “City Hotel”. Just from summarizing it we can see many variables with tons of information. From this I can see the data set spans from 2015 to 2017. I’m sure there would be another easier way to visualize this, but for now I’m only looking through the “summary” function.\nThis just lets me glance at some interesting numbers without getting too in depth. For example, someone waited 391 days on the wait list for one of the hotels which is bizarre. You can tell there was a guest that had previously cancelled rooms 26 times! Now what the data doe\n\n\nCode\nsummary(h_bookings)\n\n\n    hotel            is_canceled       lead_time   arrival_date_year\n Length:119390      Min.   :0.0000   Min.   :  0   Min.   :2015     \n Class :character   1st Qu.:0.0000   1st Qu.: 18   1st Qu.:2016     \n Mode  :character   Median :0.0000   Median : 69   Median :2016     \n                    Mean   :0.3704   Mean   :104   Mean   :2016     \n                    3rd Qu.:1.0000   3rd Qu.:160   3rd Qu.:2017     \n                    Max.   :1.0000   Max.   :737   Max.   :2017     \n                                                                    \n arrival_date_month arrival_date_week_number arrival_date_day_of_month\n Length:119390      Min.   : 1.00            Min.   : 1.0             \n Class :character   1st Qu.:16.00            1st Qu.: 8.0             \n Mode  :character   Median :28.00            Median :16.0             \n                    Mean   :27.17            Mean   :15.8             \n                    3rd Qu.:38.00            3rd Qu.:23.0             \n                    Max.   :53.00            Max.   :31.0             \n                                                                      \n stays_in_weekend_nights stays_in_week_nights     adults      \n Min.   : 0.0000         Min.   : 0.0         Min.   : 0.000  \n 1st Qu.: 0.0000         1st Qu.: 1.0         1st Qu.: 2.000  \n Median : 1.0000         Median : 2.0         Median : 2.000  \n Mean   : 0.9276         Mean   : 2.5         Mean   : 1.856  \n 3rd Qu.: 2.0000         3rd Qu.: 3.0         3rd Qu.: 2.000  \n Max.   :19.0000         Max.   :50.0         Max.   :55.000  \n                                                              \n    children           babies              meal             country         \n Min.   : 0.0000   Min.   : 0.000000   Length:119390      Length:119390     \n 1st Qu.: 0.0000   1st Qu.: 0.000000   Class :character   Class :character  \n Median : 0.0000   Median : 0.000000   Mode  :character   Mode  :character  \n Mean   : 0.1039   Mean   : 0.007949                                        \n 3rd Qu.: 0.0000   3rd Qu.: 0.000000                                        \n Max.   :10.0000   Max.   :10.000000                                        \n NA's   :4                                                                  \n market_segment     distribution_channel is_repeated_guest\n Length:119390      Length:119390        Min.   :0.00000  \n Class :character   Class :character     1st Qu.:0.00000  \n Mode  :character   Mode  :character     Median :0.00000  \n                                         Mean   :0.03191  \n                                         3rd Qu.:0.00000  \n                                         Max.   :1.00000  \n                                                          \n previous_cancellations previous_bookings_not_canceled reserved_room_type\n Min.   : 0.00000       Min.   : 0.0000                Length:119390     \n 1st Qu.: 0.00000       1st Qu.: 0.0000                Class :character  \n Median : 0.00000       Median : 0.0000                Mode  :character  \n Mean   : 0.08712       Mean   : 0.1371                                  \n 3rd Qu.: 0.00000       3rd Qu.: 0.0000                                  \n Max.   :26.00000       Max.   :72.0000                                  \n                                                                         \n assigned_room_type booking_changes   deposit_type          agent          \n Length:119390      Min.   : 0.0000   Length:119390      Length:119390     \n Class :character   1st Qu.: 0.0000   Class :character   Class :character  \n Mode  :character   Median : 0.0000   Mode  :character   Mode  :character  \n                    Mean   : 0.2211                                        \n                    3rd Qu.: 0.0000                                        \n                    Max.   :21.0000                                        \n                                                                           \n   company          days_in_waiting_list customer_type           adr         \n Length:119390      Min.   :  0.000      Length:119390      Min.   :  -6.38  \n Class :character   1st Qu.:  0.000      Class :character   1st Qu.:  69.29  \n Mode  :character   Median :  0.000      Mode  :character   Median :  94.58  \n                    Mean   :  2.321                         Mean   : 101.83  \n                    3rd Qu.:  0.000                         3rd Qu.: 126.00  \n                    Max.   :391.000                         Max.   :5400.00  \n                                                                             \n required_car_parking_spaces total_of_special_requests reservation_status\n Min.   :0.00000             Min.   :0.0000            Length:119390     \n 1st Qu.:0.00000             1st Qu.:0.0000            Class :character  \n Median :0.00000             Median :0.0000            Mode  :character  \n Mean   :0.06252             Mean   :0.5714                              \n 3rd Qu.:0.00000             3rd Qu.:1.0000                              \n Max.   :8.00000             Max.   :5.0000                              \n                                                                         \n reservation_status_date\n Length:119390          \n Class :character       \n Mode  :character       \n                        \n                        \n                        \n                        \n\n\nWith booking data, I think it is important to really know what types of customers are booking rooms because some of the data is hard to understand if you don’t know who’s booking it. For example, if I see a booking with 50+ adults, it’s nice to know what kind of customer is do that. This function lets me see all the types of customers there are within the data set.\nIf I do it with the “agent” column I’ll be able to see how many agents are working for a booking agency or the hotel.\n\n\nCode\nh_bookings %>%\n  select(\"customer_type\") %>%\n  distinct()\n\n\n    customer_type\n1       Transient\n2        Contract\n3 Transient-Party\n4           Group"
  },
  {
    "objectID": "posts/challenge2_QuinnHe.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_QuinnHe.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\nHere I just want to look at the amount of adults in a booking because then I see some bookings have multiple children or even babies in a single booking, but no adults. This could be because the adults are in another room, think a family booking two rooms, but aside from that I’m unsure if this is common or not in the hotel industry.\n\n\nCode\nno_adults <- filter(h_bookings, adults < 1)\n\nalot_adults <- filter(h_bookings, adults > 20)\n\nno_adults\n\n\n           hotel is_canceled lead_time arrival_date_year arrival_date_month\n1   Resort Hotel           0         1              2015            October\n2   Resort Hotel           0         0              2015            October\n3   Resort Hotel           0        36              2015           November\n4   Resort Hotel           0       165              2015           December\n5   Resort Hotel           0       165              2015           December\n6   Resort Hotel           1         0              2016           February\n7   Resort Hotel           1         0              2016           November\n8   Resort Hotel           0        31              2016           December\n9   Resort Hotel           0         4              2017            January\n10  Resort Hotel           0        46              2017            January\n11  Resort Hotel           0        15              2017              March\n12  Resort Hotel           0        15              2017              March\n13  Resort Hotel           0         1              2017               June\n14    City Hotel           0       132              2015               July\n15    City Hotel           0         0              2015             August\n16    City Hotel           0         1              2015             August\n17    City Hotel           0         0              2015             August\n18    City Hotel           0       104              2015             August\n19    City Hotel           0         0              2015             August\n20    City Hotel           0         3              2015             August\n21    City Hotel           0        15              2015             August\n22    City Hotel           1         1              2015          September\n23    City Hotel           0         0              2015          September\n24    City Hotel           0         0              2015          September\n25    City Hotel           0         4              2015          September\n26    City Hotel           1        48              2015            October\n27    City Hotel           1         6              2015           December\n28    City Hotel           0         6              2015           December\n29    City Hotel           0         1              2015           December\n30    City Hotel           1        12              2015           December\n31    City Hotel           0         7              2015           December\n32    City Hotel           0        30              2016            January\n33    City Hotel           0        33              2016            January\n34    City Hotel           0         0              2016            January\n35    City Hotel           0        40              2016            January\n36    City Hotel           0        40              2016            January\n37    City Hotel           0        40              2016            January\n38    City Hotel           1        11              2016            January\n39    City Hotel           1        16              2016            January\n40    City Hotel           1        43              2016           February\n41    City Hotel           0        35              2016           February\n42    City Hotel           0        46              2016           February\n43    City Hotel           1        44              2016           February\n44    City Hotel           0        64              2016           February\n45    City Hotel           1         1              2016           February\n46    City Hotel           0        40              2016           February\n47    City Hotel           0         0              2016           February\n48    City Hotel           0        37              2016           February\n49    City Hotel           0        73              2016           February\n50    City Hotel           0        48              2016           February\n51    City Hotel           1        65              2016           February\n52    City Hotel           1         7              2016              March\n53    City Hotel           1        72              2016              March\n54    City Hotel           1        75              2016              March\n55    City Hotel           0       107              2016              March\n56    City Hotel           0       108              2016              March\n57    City Hotel           0        34              2016              March\n58    City Hotel           1       123              2016              March\n59    City Hotel           0       113              2016              March\n60    City Hotel           0       109              2016              April\n61    City Hotel           1       147              2016              April\n62    City Hotel           0        91              2016              April\n63    City Hotel           1       173              2016              April\n64    City Hotel           0       123              2016              April\n65    City Hotel           1        68              2016              April\n66    City Hotel           0       122              2016                May\n67    City Hotel           0       122              2016                May\n68    City Hotel           1         9              2016                May\n69    City Hotel           1        23              2016                May\n70    City Hotel           0       150              2016                May\n71    City Hotel           1        45              2016               June\n72    City Hotel           1       115              2016               June\n73    City Hotel           0       193              2016               July\n74    City Hotel           0       198              2016               July\n75    City Hotel           1       244              2016               July\n76    City Hotel           1       244              2016               July\n77    City Hotel           1       244              2016               July\n78    City Hotel           0       187              2016               July\n79    City Hotel           1       187              2016               July\n80    City Hotel           0       187              2016               July\n81    City Hotel           1       194              2016               July\n82    City Hotel           0       193              2016               July\n83    City Hotel           0       197              2016               July\n84    City Hotel           0       201              2016               July\n85    City Hotel           1        59              2016               July\n86    City Hotel           1       230              2016               July\n87    City Hotel           0       236              2016               July\n88    City Hotel           1       225              2016               July\n89    City Hotel           1        36              2016             August\n90    City Hotel           1       237              2016             August\n91    City Hotel           1       219              2016             August\n92    City Hotel           1        14              2016             August\n93    City Hotel           1       251              2016             August\n94    City Hotel           1       151              2016             August\n95    City Hotel           0       256              2016             August\n96    City Hotel           1       178              2016             August\n97    City Hotel           1       247              2016             August\n98    City Hotel           1       156              2016             August\n99    City Hotel           1       179              2016          September\n100   City Hotel           1        62              2016          September\n101   City Hotel           1        40              2016          September\n102   City Hotel           1        17              2016          September\n103   City Hotel           1        32              2016          September\n104   City Hotel           1        31              2016          September\n105   City Hotel           1        76              2016          September\n106   City Hotel           1       158              2016            October\n107   City Hotel           1       255              2016           November\n108   City Hotel           1         0              2016           November\n109   City Hotel           1        44              2016           November\n110   City Hotel           1        30              2016           November\n111   City Hotel           1       136              2016           November\n112   City Hotel           1       215              2016           November\n113   City Hotel           1       195              2016           December\n114   City Hotel           1         0              2016           December\n115   City Hotel           1         0              2016           December\n116   City Hotel           1         0              2016           December\n117   City Hotel           1         9              2016           December\n118   City Hotel           1       381              2016           December\n119   City Hotel           1       102              2016           December\n120   City Hotel           1       251              2016           December\n121   City Hotel           1       111              2017            January\n122   City Hotel           1       268              2017            January\n123   City Hotel           1         1              2017            January\n124   City Hotel           1       148              2017           February\n125   City Hotel           1         4              2017           February\n126   City Hotel           1       134              2017           February\n127   City Hotel           1       148              2017           February\n128   City Hotel           1       144              2017              March\n129   City Hotel           1         0              2017              March\n130   City Hotel           1       160              2017              March\n131   City Hotel           1       200              2017              March\n132   City Hotel           1       200              2017              March\n133   City Hotel           1       172              2017              March\n134   City Hotel           1       160              2017              March\n135   City Hotel           1       230              2017              April\n136   City Hotel           1       179              2017              April\n137   City Hotel           1       212              2017              April\n138   City Hotel           1         0              2017              April\n139   City Hotel           1         0              2017              April\n140   City Hotel           1         0              2017              April\n141   City Hotel           1         0              2017              April\n142   City Hotel           1       219              2017              April\n143   City Hotel           1       204              2017              April\n144   City Hotel           1       232              2017              April\n145   City Hotel           1       266              2017                May\n146   City Hotel           1       209              2017                May\n147   City Hotel           1       273              2017                May\n148   City Hotel           1       310              2017                May\n149   City Hotel           1       311              2017                May\n150   City Hotel           1       310              2017                May\n151   City Hotel           1       434              2017               June\n152   City Hotel           1       238              2017               June\n153   City Hotel           1       257              2017               July\n154   City Hotel           1       315              2017               July\n155   City Hotel           1       315              2017               July\n156   City Hotel           1       315              2017               July\n157   City Hotel           1       340              2017               July\n158   City Hotel           1         0              2017             August\n159   City Hotel           1         0              2017             August\n160   City Hotel           1       308              2017             August\n161   City Hotel           1       316              2017             August\n162   City Hotel           0       110              2015               July\n163   City Hotel           0         0              2015             August\n164   City Hotel           0         0              2015             August\n165   City Hotel           0         0              2015             August\n166   City Hotel           0         0              2015            October\n167   City Hotel           0         3              2015          September\n168   City Hotel           0         1              2015            October\n169   City Hotel           0         0              2015           November\n170   City Hotel           0         0              2015            October\n171   City Hotel           0        64              2015            October\n172   City Hotel           0       296              2015            October\n173   City Hotel           1       244              2016               July\n174   City Hotel           1       244              2016               July\n175   City Hotel           0         0              2015           November\n176   City Hotel           0        13              2015           December\n177   City Hotel           1        22              2015           December\n178   City Hotel           0         4              2015           December\n179   City Hotel           1        13              2015           December\n180   City Hotel           0         0              2015           December\n181   City Hotel           0         0              2015           December\n182   City Hotel           0         0              2015           December\n183   City Hotel           0         0              2015           December\n184   City Hotel           0         0              2017            January\n185   City Hotel           0         5              2015           December\n186   City Hotel           1       150              2016              March\n187   City Hotel           0         0              2016            January\n188   City Hotel           0        13              2016            January\n189   City Hotel           0         0              2016           February\n190   City Hotel           0        28              2016           February\n191   City Hotel           0         0              2016           February\n192   City Hotel           0       121              2016           February\n193   City Hotel           0        19              2016           February\n194   City Hotel           0        39              2016           February\n195   City Hotel           0         1              2016           February\n196   City Hotel           0         0              2016              March\n197   City Hotel           0         0              2016              March\n198   City Hotel           0         0              2016              March\n199   City Hotel           0         9              2016              March\n200   City Hotel           0       130              2016              March\n201   City Hotel           0         0              2016              April\n202   City Hotel           0        70              2016              April\n203   City Hotel           0        16              2016              March\n204   City Hotel           0         5              2016              April\n205   City Hotel           0        30              2016                May\n206   City Hotel           0        40              2016              April\n207   City Hotel           0        52              2016              April\n208   City Hotel           0        38              2016                May\n209   City Hotel           0        53              2016                May\n210   City Hotel           0         0              2016           December\n211   City Hotel           0         0              2016           December\n212   City Hotel           0        84              2016                May\n213   City Hotel           0        87              2016                May\n214   City Hotel           0        87              2016                May\n215   City Hotel           0        14              2016               June\n216   City Hotel           0        14              2016               June\n217   City Hotel           0       107              2016               June\n218   City Hotel           0        34              2016               June\n219   City Hotel           0         3              2016               June\n220   City Hotel           0         0              2016               June\n221   City Hotel           0       131              2016               June\n222   City Hotel           0       120              2016               June\n223   City Hotel           0       108              2016               July\n224   City Hotel           0        55              2017              March\n225   City Hotel           0       159              2016               July\n226   City Hotel           0         0              2016               July\n227   City Hotel           0         1              2016               July\n228   City Hotel           0       237              2016               July\n229   City Hotel           0        82              2016               July\n230   City Hotel           0         0              2016               July\n231   City Hotel           0         0              2016               July\n232   City Hotel           0        72              2016               July\n233   City Hotel           0        85              2016               July\n234   City Hotel           0         1              2016               July\n235   City Hotel           0       242              2016               July\n236   City Hotel           0        65              2016             August\n237   City Hotel           0        94              2016             August\n238   City Hotel           0         4              2016             August\n239   City Hotel           0         1              2016             August\n240   City Hotel           0        93              2016             August\n241   City Hotel           0        14              2016             August\n242   City Hotel           0       136              2016             August\n243   City Hotel           0        14              2016             August\n244   City Hotel           0       145              2016             August\n245   City Hotel           0       143              2016             August\n246   City Hotel           0        11              2016             August\n247   City Hotel           0        88              2016             August\n248   City Hotel           0       180              2016             August\n249   City Hotel           0       120              2016             August\n250   City Hotel           0        78              2016             August\n251   City Hotel           0        21              2016             August\n252   City Hotel           0       142              2016             August\n253   City Hotel           0       178              2016             August\n254   City Hotel           0       185              2016             August\n255   City Hotel           0        65              2016             August\n256   City Hotel           0        36              2016             August\n257   City Hotel           0       186              2016          September\n258   City Hotel           0       125              2016             August\n259   City Hotel           0        34              2016          September\n260   City Hotel           0         3              2016          September\n261   City Hotel           0        33              2016          September\n262   City Hotel           0        42              2016             August\n263   City Hotel           0       107              2016          September\n264   City Hotel           0         1              2016          September\n265   City Hotel           0        71              2016            October\n266   City Hotel           0        15              2016            October\n267   City Hotel           0        88              2016            October\n268   City Hotel           0       338              2016            October\n269   City Hotel           0       125              2016            October\n270   City Hotel           0       114              2016            October\n271   City Hotel           0       165              2016            October\n272   City Hotel           0        57              2016            October\n273   City Hotel           0        66              2016            October\n274   City Hotel           0       130              2016            October\n275   City Hotel           0        10              2016            October\n276   City Hotel           0       208              2016            October\n277   City Hotel           0       216              2016            October\n278   City Hotel           0         0              2016            October\n279   City Hotel           0        88              2016            October\n280   City Hotel           0         0              2016            October\n281   City Hotel           0        92              2016            October\n282   City Hotel           0         0              2016           November\n283   City Hotel           0       241              2016            October\n284   City Hotel           0       200              2016            October\n285   City Hotel           0       182              2016            October\n286   City Hotel           0         0              2016           November\n287   City Hotel           0        57              2016           November\n288   City Hotel           0        74              2016           November\n289   City Hotel           0       140              2016          September\n290   City Hotel           0         0              2016           November\n291   City Hotel           0         0              2016           November\n292   City Hotel           0        37              2016           November\n293   City Hotel           0         1              2016           November\n294   City Hotel           0         0              2016           November\n295   City Hotel           0         0              2016           November\n296   City Hotel           0       216              2016           November\n297   City Hotel           0        14              2016           November\n298   City Hotel           0         0              2016           November\n299   City Hotel           0         0              2016           November\n300   City Hotel           0        19              2016           November\n301   City Hotel           0         0              2016           November\n302   City Hotel           0         0              2016           November\n303   City Hotel           0         2              2016           November\n304   City Hotel           0         0              2016           December\n305   City Hotel           0        53              2016           November\n306   City Hotel           0         0              2016           December\n307   City Hotel           0        38              2016           December\n308   City Hotel           0         0              2016           December\n309   City Hotel           0         0              2016           December\n310   City Hotel           0        56              2016           December\n311   City Hotel           0        61              2016           December\n312   City Hotel           0        56              2016           December\n313   City Hotel           0         0              2016           December\n314   City Hotel           0         0              2016           December\n315   City Hotel           0       109              2016           December\n316   City Hotel           0       197              2016           December\n317   City Hotel           0       104              2016           December\n318   City Hotel           0       104              2016           December\n319   City Hotel           0       247              2016           December\n320   City Hotel           0       255              2016           December\n321   City Hotel           0         0              2017            January\n322   City Hotel           0       110              2017            January\n323   City Hotel           0       177              2016           December\n324   City Hotel           0         0              2017            January\n325   City Hotel           0       128              2017            January\n326   City Hotel           0       125              2017            January\n327   City Hotel           0       137              2017            January\n328   City Hotel           0        76              2017            January\n329   City Hotel           0        16              2017            January\n330   City Hotel           0         0              2017            January\n331   City Hotel           0         5              2017           February\n332   City Hotel           0         0              2017           February\n333   City Hotel           0         7              2017           February\n334   City Hotel           0         0              2017           February\n335   City Hotel           0         0              2017           February\n336   City Hotel           0         0              2017           February\n337   City Hotel           0        11              2017            January\n338   City Hotel           0         1              2017           February\n339   City Hotel           0         0              2017           February\n340   City Hotel           0         0              2017              March\n341   City Hotel           0         0              2017              March\n342   City Hotel           0       201              2017           February\n343   City Hotel           0        50              2017              March\n344   City Hotel           0        37              2017           February\n345   City Hotel           0         2              2017              March\n346   City Hotel           0         0              2017              March\n347   City Hotel           0        51              2017              March\n348   City Hotel           0         0              2017              March\n349   City Hotel           0        17              2017              March\n350   City Hotel           0         0              2017              March\n351   City Hotel           0         0              2017              March\n352   City Hotel           0         0              2017              March\n353   City Hotel           0         0              2017              March\n354   City Hotel           0         0              2017              March\n355   City Hotel           0         0              2017              March\n356   City Hotel           0         0              2017              March\n357   City Hotel           0         0              2017              March\n358   City Hotel           0        34              2017              March\n359   City Hotel           0        82              2017              April\n360   City Hotel           0         2              2017              March\n361   City Hotel           0       177              2017              April\n362   City Hotel           0       202              2017              April\n363   City Hotel           0       182              2017              April\n364   City Hotel           0       181              2017              April\n365   City Hotel           0        10              2017              April\n366   City Hotel           0       255              2017              April\n367   City Hotel           1       132              2017              April\n368   City Hotel           0         7              2017              April\n369   City Hotel           0       202              2017              April\n370   City Hotel           0         0              2017                May\n371   City Hotel           0        17              2017              April\n372   City Hotel           0        33              2017                May\n373   City Hotel           0        35              2017                May\n374   City Hotel           0        31              2017                May\n375   City Hotel           0         6              2017                May\n376   City Hotel           0         0              2017                May\n377   City Hotel           0        75              2017                May\n378   City Hotel           0         1              2017                May\n379   City Hotel           0       248              2017                May\n380   City Hotel           0       159              2017                May\n381   City Hotel           0         0              2017               June\n382   City Hotel           0       239              2017               June\n383   City Hotel           0        28              2017               June\n384   City Hotel           0        28              2017               June\n385   City Hotel           0        26              2017               June\n386   City Hotel           0         1              2017               June\n387   City Hotel           0         1              2017               June\n388   City Hotel           0       107              2017               June\n389   City Hotel           0         1              2017               June\n390   City Hotel           0       250              2017               July\n391   City Hotel           0       315              2017               July\n392   City Hotel           0        44              2017               July\n393   City Hotel           0         2              2017               July\n394   City Hotel           0       295              2017               July\n395   City Hotel           0        14              2017               July\n396   City Hotel           0       170              2017               July\n397   City Hotel           0       351              2017               July\n398   City Hotel           0       329              2017               July\n399   City Hotel           0       296              2017               July\n400   City Hotel           0       276              2017               July\n401   City Hotel           0       291              2017               July\n402   City Hotel           0       159              2017               July\n403   City Hotel           0        10              2017             August\n    arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights\n1                         41                         6                       0\n2                         42                        12                       0\n3                         47                        20                       1\n4                         53                        30                       1\n5                         53                        30                       2\n6                          8                        15                       0\n7                         48                        21                       0\n8                         53                        27                       2\n9                          2                        14                       0\n10                         4                        25                       3\n11                        13                        30                       2\n12                        13                        30                       2\n13                        25                        18                       2\n14                        30                        23                       2\n15                        33                         9                       2\n16                        33                        10                       1\n17                        33                        10                       9\n18                        33                        11                       0\n19                        33                        11                       0\n20                        34                        16                       2\n21                        35                        28                       0\n22                        37                         6                       2\n23                        37                         7                       1\n24                        37                        11                       0\n25                        37                        11                       0\n26                        43                        19                       1\n27                        51                        13                       1\n28                        51                        18                       0\n29                        52                        23                       0\n30                        52                        24                       0\n31                        52                        26                       2\n32                         2                         3                       1\n33                         2                         6                       0\n34                         2                         7                       0\n35                         3                        14                       0\n36                         3                        14                       0\n37                         3                        14                       0\n38                         3                        15                       2\n39                         4                        18                       1\n40                         6                         5                       1\n41                         6                         6                       1\n42                         7                         7                       2\n43                         7                         8                       1\n44                         7                         9                       0\n45                         7                        11                       0\n46                         7                        11                       1\n47                         7                        12                       0\n48                         7                        12                       1\n49                         9                        21                       2\n50                         9                        21                       2\n51                         9                        27                       2\n52                        10                         4                       2\n53                        12                        19                       0\n54                        12                        19                       2\n55                        13                        23                       2\n56                        13                        24                       2\n57                        13                        25                       2\n58                        14                        30                       0\n59                        14                        31                       0\n60                        14                         1                       2\n61                        15                         4                       1\n62                        15                         4                       1\n63                        15                         8                       2\n64                        16                        12                       0\n65                        18                        24                       2\n66                        19                         5                       2\n67                        19                         5                       2\n68                        21                        18                       0\n69                        21                        21                       2\n70                        23                        31                       0\n71                        25                        17                       2\n72                        27                        26                       2\n73                        28                         4                       3\n74                        28                         6                       0\n75                        28                         6                       2\n76                        28                         6                       2\n77                        28                         6                       2\n78                        28                         7                       0\n79                        28                         7                       0\n80                        28                         7                       0\n81                        28                         9                       2\n82                        29                        15                       0\n83                        30                        17                       1\n84                        30                        21                       0\n85                        31                        24                       2\n86                        31                        26                       0\n87                        31                        28                       2\n88                        31                        28                       2\n89                        32                         2                       0\n90                        32                         5                       0\n91                        33                        10                       0\n92                        33                        13                       1\n93                        34                        14                       2\n94                        34                        19                       2\n95                        35                        21                       3\n96                        35                        23                       0\n97                        35                        27                       1\n98                        36                        31                       0\n99                        36                         2                       2\n100                       37                         4                       2\n101                       37                         4                       2\n102                       37                         9                       1\n103                       38                        12                       1\n104                       38                        16                       2\n105                       39                        21                       0\n106                       45                        30                       2\n107                       46                        12                       2\n108                       47                        17                       4\n109                       47                        19                       0\n110                       48                        23                       0\n111                       49                        28                       1\n112                       49                        30                       0\n113                       49                         2                       0\n114                       50                         6                       0\n115                       50                         6                       0\n116                       50                         6                       0\n117                       50                        10                       0\n118                       52                        23                       1\n119                       53                        26                       1\n120                       53                        28                       2\n121                        1                         2                       1\n122                        1                         2                       1\n123                        4                        25                       0\n124                        6                         5                       2\n125                        8                        21                       0\n126                        8                        22                       2\n127                        8                        24                       2\n128                        9                         4                       2\n129                       11                        16                       0\n130                       11                        17                       2\n131                       12                        24                       2\n132                       12                        24                       2\n133                       12                        24                       2\n134                       13                        30                       1\n135                       14                         6                       0\n136                       14                         7                       1\n137                       14                         7                       1\n138                       15                        10                       0\n139                       15                        10                       0\n140                       15                        10                       0\n141                       15                        10                       0\n142                       15                        12                       0\n143                       16                        21                       0\n144                       17                        27                       2\n145                       20                        17                       0\n146                       21                        21                       2\n147                       21                        24                       0\n148                       21                        24                       0\n149                       21                        24                       0\n150                       21                        24                       0\n151                       22                         3                       2\n152                       23                         5                       2\n153                       27                         4                       0\n154                       27                         7                       0\n155                       27                         7                       0\n156                       27                         7                       0\n157                       28                         9                       2\n158                       32                        11                       0\n159                       32                        11                       0\n160                       33                        15                       0\n161                       33                        16                       0\n162                       30                        25                       2\n163                       33                        11                       2\n164                       35                        26                       0\n165                       35                        27                       0\n166                       42                        15                       0\n167                       39                        24                       2\n168                       40                         3                       2\n169                       49                        30                       1\n170                       42                        14                       0\n171                       42                        16                       2\n172                       43                        24                       2\n173                       28                         6                       2\n174                       28                         6                       2\n175                       46                        11                       0\n176                       49                         4                       1\n177                       50                         9                       1\n178                       50                        11                       0\n179                       53                        29                       2\n180                       52                        24                       0\n181                       52                        24                       0\n182                       52                        24                       0\n183                       52                        24                       0\n184                        3                        21                       0\n185                       52                        22                       0\n186                       13                        24                       1\n187                        2                         8                       0\n188                        4                        21                       0\n189                        6                         4                       0\n190                        6                         5                       2\n191                        7                        11                       0\n192                        8                        15                       1\n193                        8                        14                       3\n194                        9                        22                       1\n195                        9                        25                       0\n196                       12                        16                       0\n197                       12                        16                       0\n198                       11                        12                       2\n199                       12                        16                       2\n200                       13                        25                       0\n201                       15                         8                       0\n202                       16                        12                       0\n203                       11                        11                      14\n204                       18                        27                       2\n205                       19                         1                       2\n206                       18                        30                       2\n207                       18                        30                       2\n208                       20                        13                       1\n209                       21                        16                       1\n210                       52                        21                       0\n211                       52                        21                       0\n212                       22                        26                       0\n213                       23                        29                       2\n214                       23                        29                       2\n215                       24                         9                       0\n216                       24                         9                       0\n217                       23                         3                       3\n218                       25                        12                       2\n219                       25                        16                       0\n220                       26                        21                       0\n221                       27                        26                       2\n222                       27                        30                       0\n223                       28                         5                       2\n224                       10                         5                       2\n225                       28                         3                       5\n226                       30                        19                       0\n227                       30                        20                       0\n228                       31                        24                       1\n229                       30                        23                       2\n230                       31                        26                       0\n231                       31                        26                       0\n232                       31                        27                       0\n233                       31                        26                       0\n234                       29                        16                       4\n235                       31                        29                       0\n236                       32                         4                       0\n237                       32                         4                       0\n238                       32                         5                       2\n239                       32                         3                       2\n240                       33                        13                       0\n241                       33                        13                       1\n242                       34                        15                       1\n243                       34                        17                       0\n244                       34                        15                       1\n245                       34                        18                       0\n246                       34                        19                       0\n247                       34                        16                       0\n248                       34                        20                       0\n249                       34                        16                       1\n250                       34                        19                       1\n251                       34                        20                       2\n252                       35                        21                       2\n253                       35                        27                       1\n254                       35                        25                       2\n255                       36                        31                       1\n256                       35                        27                       3\n257                       37                         4                       2\n258                       35                        25                       4\n259                       39                        18                       2\n260                       39                        20                       0\n261                       39                        20                       0\n262                       35                        27                       8\n263                       39                        24                       2\n264                       38                        17                       4\n265                       41                         7                       0\n266                       41                         6                       2\n267                       41                         8                       2\n268                       41                         7                       2\n269                       42                         9                       2\n270                       42                        12                       1\n271                       44                        28                       0\n272                       42                        15                       2\n273                       43                        20                       0\n274                       43                        21                       0\n275                       42                        14                       4\n276                       44                        23                       2\n277                       44                        23                       2\n278                       44                        28                       0\n279                       41                         7                       6\n280                       44                        27                       0\n281                       44                        29                       1\n282                       45                         1                       0\n283                       44                        29                       2\n284                       45                        31                       1\n285                       45                        31                       1\n286                       46                        11                       0\n287                       46                         9                       0\n288                       46                         7                       1\n289                       39                        22                      16\n290                       47                        19                       0\n291                       47                        19                       0\n292                       47                        17                       1\n293                       47                        18                       1\n294                       48                        23                       0\n295                       48                        23                       0\n296                       48                        22                       0\n297                       47                        18                       2\n298                       48                        24                       0\n299                       48                        24                       0\n300                       48                        26                       4\n301                       48                        26                       0\n302                       48                        26                       0\n303                       47                        16                       2\n304                       49                         3                       0\n305                       49                        29                       0\n306                       50                         5                       0\n307                       49                         2                       2\n308                       50                         6                       0\n309                       50                         7                       0\n310                       50                         7                       0\n311                       50                         8                       0\n312                       50                         8                       0\n313                       52                        18                       0\n314                       52                        20                       0\n315                       51                        16                       2\n316                       52                        21                       1\n317                       53                        25                       2\n318                       53                        25                       2\n319                       52                        24                       2\n320                       53                        27                       0\n321                        1                         3                       0\n322                        1                         3                       0\n323                       53                        30                       2\n324                        2                         9                       0\n325                        1                         6                       1\n326                        1                         1                       4\n327                        2                        13                       2\n328                        3                        15                       2\n329                        3                        20                       2\n330                        5                        30                       0\n331                        5                         2                       0\n332                        5                         4                       0\n333                        7                        13                       0\n334                        7                        15                       0\n335                        7                        15                       0\n336                        9                        27                       0\n337                        3                        15                      13\n338                        7                        12                       5\n339                        9                        27                       0\n340                        9                         1                       0\n341                        9                         1                       0\n342                        9                        27                       1\n343                        9                         1                       0\n344                        8                        22                       3\n345                       10                         6                       0\n346                       10                         7                       0\n347                       10                         8                       0\n348                       10                         9                       0\n349                        9                         3                       3\n350                       11                        15                       0\n351                        9                         4                       4\n352                       11                        17                       0\n353                       12                        21                       0\n354                       10                         7                       4\n355                       12                        24                       0\n356                       12                        24                       0\n357                       12                        24                       0\n358                       12                        21                       2\n359                       13                         1                       2\n360                       10                         9                       8\n361                       14                         7                       2\n362                       15                         9                       2\n363                       15                        11                       0\n364                       15                        10                       1\n365                       15                        14                       2\n366                       15                        13                       2\n367                       17                        25                       0\n368                       16                        20                       2\n369                       17                        23                       2\n370                       18                         5                       0\n371                       17                        27                       2\n372                       19                         9                       0\n373                       19                        10                       0\n374                       20                        14                       0\n375                       20                        14                       0\n376                       20                        17                       0\n377                       20                        20                       2\n378                       21                        23                       0\n379                       21                        25                       0\n380                       21                        24                       0\n381                       23                         4                       0\n382                       23                         4                       2\n383                       24                        13                       0\n384                       24                        13                       0\n385                       25                        19                       1\n386                       26                        28                       0\n387                       26                        28                       0\n388                       26                        27                       0\n389                       26                        30                       0\n390                       27                         8                       2\n391                       28                        11                       0\n392                       28                        15                       1\n393                       28                        15                       2\n394                       29                        20                       0\n395                       30                        24                       1\n396                       30                        27                       0\n397                       30                        27                       0\n398                       30                        24                       1\n399                       30                        27                       1\n400                       31                        30                       2\n401                       30                        29                       2\n402                       31                        31                       1\n403                       32                        12                       2\n    stays_in_week_nights adults children babies meal country market_segment\n1                      3      0        0      0   SC     PRT      Corporate\n2                      0      0        0      0   SC     PRT      Corporate\n3                      2      0        0      0   SC     ESP         Groups\n4                      4      0        0      0   SC     PRT         Groups\n5                      4      0        0      0   SC     PRT         Groups\n6                      0      0        0      0   SC    NULL  Offline TA/TO\n7                      0      0        0      0   SC    NULL  Offline TA/TO\n8                      8      0        0      0   BB     PRT         Direct\n9                      1      0        0      0   SC     PRT         Groups\n10                     9      0        0      0   SC     FRA         Direct\n11                     6      0        0      0   SC     PRT      Corporate\n12                     6      0        0      0   SC     PRT      Corporate\n13                     5      0        0      0   BB     GBR         Direct\n14                     5      0        0      0   HB     PRT         Groups\n15                     0      0        0      0   BB     FRA      Online TA\n16                     1      0        3      0   BB     PRT         Direct\n17                    20      0        0      0   SC     PRT      Online TA\n18                     3      0        2      0   BB     FRA      Online TA\n19                     4      0        0      0   BB     PRT      Online TA\n20                     0      0        2      0   BB     PRT         Direct\n21                     1      0        2      0   BB     PRT  Complementary\n22                     0      0        0      0   SC     PRT         Groups\n23                     4      0        0      0   BB     SWE      Online TA\n24                     2      0        0      0   BB     FRA         Groups\n25                     1      0        0      0   BB     PRT         Direct\n26                     3      0        2      0   BB     PRT  Offline TA/TO\n27                     0      0        3      0   BB     PRT      Online TA\n28                     1      0        2      1   BB     PRT      Online TA\n29                     3      0        2      0   BB     DEU      Online TA\n30                     2      0        2      0   BB     BEL      Online TA\n31                     1      0        2      0   BB     ESP      Online TA\n32                     0      0        2      0   BB     PRT      Online TA\n33                     2      0        2      0   BB     BRA      Online TA\n34                     1      0        2      0   BB     FRA      Online TA\n35                     3      0        2      0   BB     ESP      Online TA\n36                     3      0        2      0   BB     ESP      Online TA\n37                     3      0        2      0   BB     ESP      Online TA\n38                     2      0        2      0   BB     OMN      Online TA\n39                     3      0        2      0   BB     BRA      Online TA\n40                     2      0        2      0   BB     BEL      Online TA\n41                     1      0        2      0   BB     PRT      Online TA\n42                     3      0        1      0   BB     PRT         Direct\n43                     3      0        2      0   BB     ITA      Online TA\n44                     3      0        2      0   BB     POL      Online TA\n45                     3      0        0      0   BB     GBR      Online TA\n46                     3      0        2      0   BB     KOR         Direct\n47                     1      0        0      0   SC     PRT  Complementary\n48                     2      0        2      0   BB     USA      Online TA\n49                     1      0        2      0   BB     PRT      Online TA\n50                     4      0        0      0   SC     CHE         Direct\n51                     2      0        2      0   BB     ITA      Online TA\n52                     2      0        2      0   BB     CHN      Online TA\n53                     1      0        2      0   BB     PRT      Online TA\n54                     1      0        2      0   BB     MEX      Online TA\n55                     5      0        2      0   BB     CHE         Direct\n56                     3      0        2      0   BB     PRT      Online TA\n57                     2      0        2      0   BB     AUT      Online TA\n58                     3      0        2      0   BB     PRT      Online TA\n59                     3      0        2      0   BB     BEL      Online TA\n60                     4      0        1      0   BB     BEL      Online TA\n61                     3      0        2      0   BB     PRT      Online TA\n62                     3      0        2      0   BB     BEL      Online TA\n63                     3      0        0      0   BB     PRT  Offline TA/TO\n64                     5      0        2      0   BB     FRA      Online TA\n65                     2      0        2      0   BB     NLD      Online TA\n66                     4      0        0      0   SC     PRT      Online TA\n67                     4      0        0      0   SC     CHE      Online TA\n68                     2      0        2      0   BB     GBR      Online TA\n69                     4      0        2      0   BB     IND      Online TA\n70                     3      0        2      0   BB     GBR      Online TA\n71                     2      0        2      0   BB     PRT         Direct\n72                     2      0        2      0   BB     SWE      Online TA\n73                    10      0        2      0   BB     DEU      Online TA\n74                     4      0        0      0   BB     GBR      Online TA\n75                     4      0        0      0   SC     PRT  Offline TA/TO\n76                     4      0        0      0   SC     PRT  Offline TA/TO\n77                     4      0        0      0   SC     PRT  Offline TA/TO\n78                     2      0        2      0   BB     PRT      Online TA\n79                     2      0        2      0   BB     GBR      Online TA\n80                     2      0        2      0   BB     PRT      Online TA\n81                     1      0        2      0   BB     NLD      Online TA\n82                     1      0        2      0   BB     CHE      Online TA\n83                     0      0        2      0   BB     GBR      Online TA\n84                     1      0        2      0   BB     PRT      Online TA\n85                     4      0        3      0   BB     PRT         Direct\n86                     3      0        2      0   BB     BEL      Online TA\n87                     4      0        2      0   BB     NLD      Online TA\n88                     4      0        3      0   BB     FRA         Direct\n89                     3      0        2      0   BB     ESP         Direct\n90                     2      0        2      0   BB     NLD      Online TA\n91                     3      0        2      0   BB     MAR      Online TA\n92                     1      0        2      0   BB     PRT      Online TA\n93                     3      0        2      0   BB     DEU      Online TA\n94                     3      0        2      0   BB     POL      Online TA\n95                     5      0        0      0   BB     PRT      Online TA\n96                     3      0        2      0   BB     ITA      Online TA\n97                     1      0        2      0   BB     RUS      Online TA\n98                     3      0        2      0   BB     ITA      Online TA\n99                     5      0        2      0   BB     ESP      Online TA\n100                    2      0        2      0   BB     USA      Online TA\n101                    5      0        2      0   BB     DEU      Online TA\n102                    2      0        2      0   BB     NLD      Online TA\n103                    4      0        2      0   BB     SAU      Online TA\n104                    2      0        2      0   BB     TZA      Online TA\n105                    3      0        2      0   BB     IRL      Online TA\n106                    2      0        2      0   BB     GBR      Online TA\n107                    5      0        2      0   BB     TUR      Online TA\n108                   11      0        0      0   SC    NULL      Corporate\n109                    1      0        2      0   BB     PRT      Online TA\n110                    4      0        2      0   BB     MAR      Online TA\n111                    4      0        2      0   BB     ARG      Online TA\n112                    4      0        2      0   BB     ROU      Online TA\n113                    2      0        2      0   BB     FRA      Online TA\n114                    0      0        0      0   SC    NULL  Complementary\n115                    0      0        0      0   SC    NULL  Complementary\n116                    0      0        0      0   SC    NULL  Complementary\n117                    1      0        2      0   BB     PRT  Complementary\n118                    2      0        2      0   BB     DEU      Online TA\n119                    3      0        2      0   BB     ESP      Online TA\n120                    9      0        2      0   BB     CHE      Online TA\n121                    3      0        2      0   BB     RUS      Online TA\n122                    4      0        2      0   BB     RUS      Online TA\n123                    2      0        0      0   SC     PRT  Complementary\n124                    5      0        2      0   BB     FRA      Online TA\n125                    3      0        0      0   SC     PRT         Groups\n126                    4      0        2      0   BB     BRA      Online TA\n127                    2      0        2      0   BB     BRA      Online TA\n128                    1      0        2      0   BB     BRA      Online TA\n129                    3      0        0      0   BB     PRT  Offline TA/TO\n130                    6      0        2      0   BB     EGY      Online TA\n131                    2      0        2      0   BB     HUN      Online TA\n132                    2      0        2      0   BB     HUN      Online TA\n133                    6      0        2      0   BB     ARE      Online TA\n134                    3      0        2      0   BB     ROU      Online TA\n135                    3      0        2      0   BB     IRL      Online TA\n136                    2      0        2      0   BB     ESP      Online TA\n137                    2      0        2      0   BB     ISR      Online TA\n138                    0      0        0      0   SC    NULL  Complementary\n139                    0      0        0      0   SC    NULL  Complementary\n140                    0      0        0      0   SC    NULL  Complementary\n141                    0      0        0      0   SC    NULL  Complementary\n142                    4      0        2      0   BB     SWE         Direct\n143                    1      0        2      0   BB     NLD      Online TA\n144                    3      0        2      0   BB     POL      Online TA\n145                    3      0        2      0   BB     AUT      Online TA\n146                    4      0        2      0   BB     DNK      Online TA\n147                    3      0        2      0   BB     AUT      Online TA\n148                    4      0        2      0   BB     DNK      Online TA\n149                    4      0        2      0   BB     DNK      Online TA\n150                    4      0        2      0   BB     DNK      Online TA\n151                    1      0        0      0   HB     PRT         Groups\n152                    5      0        2      0   BB     FRA      Online TA\n153                    3      0        2      0   BB     BRA      Online TA\n154                    1      0        2      0   BB     USA      Online TA\n155                    1      0        2      0   BB     USA      Online TA\n156                    1      0        2      0   BB     USA      Online TA\n157                    2      0        2      0   BB     NOR      Online TA\n158                    0      0        0      0   SC     PRT      Online TA\n159                    0      0        0      0   SC     PRT      Online TA\n160                    4      0        2      0   BB     ITA      Online TA\n161                    3      0        2      0   BB     FRA      Online TA\n162                    2      0        0      0   HB     ITA  Offline TA/TO\n163                   10      0        0      0   SC     ITA      Corporate\n164                    1      0        0      0   BB     PRT      Online TA\n165                    1      0        0      0   BB     PRT      Online TA\n166                    0      0        0      0   BB     PRT         Direct\n167                    4      0        0      0   SC     ITA         Groups\n168                    2      0        0      0   SC     PRT         Groups\n169                    2      0        0      0   SC     PRT      Corporate\n170                    1      0        0      0   BB     PRT         Direct\n171                    3      0        0      0   SC     FRA  Offline TA/TO\n172                    4      0        2      0   BB     GBR      Online TA\n173                    4      0        0      0   SC     PRT  Offline TA/TO\n174                    4      0        0      0   SC     PRT  Offline TA/TO\n175                    0      0        0      0   SC     PRT         Direct\n176                    2      0        2      0   BB     ESP         Direct\n177                    4      0        2      1   BB     PRT         Direct\n178                    1      0        0      0   BB     PRT  Complementary\n179                    5      0        2      0   BB     BRA         Direct\n180                    0      0        0      0   BB     FRA  Offline TA/TO\n181                    0      0        0      0   BB     FRA  Offline TA/TO\n182                    0      0        0      0   BB     ESP  Offline TA/TO\n183                    0      0        0      0   BB     FRA  Offline TA/TO\n184                    1      0        2      0   BB     PRT         Direct\n185                    5      0        2      0   BB     PRT  Complementary\n186                    3      0        2      0   BB     PRT      Online TA\n187                    2      0        3      0   BB     PRT         Direct\n188                    3      0        2      0   BB     PRT      Online TA\n189                    1      0        0      0   BB     PRT  Complementary\n190                    2      0        2      0   BB     PRT      Online TA\n191                    2      0        0      0   BB     ESP         Direct\n192                    3      0        2      0   BB     DNK      Online TA\n193                    5      0        0      0   BB     GBR      Online TA\n194                    4      0        2      0   BB     FIN      Online TA\n195                    3      0        2      0   BB     ESP      Online TA\n196                    0      0        0      0   BB     PRT         Direct\n197                    0      0        0      0   SC     DEU      Online TA\n198                    3      0        0      0   SC     SVN      Online TA\n199                    6      0        0      0   SC     NLD         Groups\n200                    1      0        2      0   BB     ITA  Complementary\n201                    1      0        2      0   BB     FRA      Online TA\n202                    4      0        3      0   BB     PRT  Offline TA/TO\n203                   35      0        0      0   BB     PRT      Corporate\n204                    5      0        0      0   SC     ESP         Direct\n205                    1      0        0      0   SC     PRT  Offline TA/TO\n206                    3      0        2      0   BB     NLD      Online TA\n207                    4      0        0      0   SC     SWE         Direct\n208                    2      0        2      0   BB     FRA      Online TA\n209                    1      0        2      0   BB     PRT      Online TA\n210                    0      0        0      0   BB     PRT  Offline TA/TO\n211                    0      0        0      0   BB     PRT  Offline TA/TO\n212                    2      0        2      0   BB     PRT         Direct\n213                    2      0        2      0   BB     PRT      Online TA\n214                    2      0        2      0   BB     PRT      Online TA\n215                    3      0        2      0   BB     ITA      Online TA\n216                    3      0        2      1   BB     ITA      Online TA\n217                    7      0        0      0   SC     FRA         Groups\n218                    1      0        2      0   BB     USA      Online TA\n219                    1      0        2      0   BB     FRA      Online TA\n220                    0      0        0      0   BB     PRT      Online TA\n221                    3      0        2      0   BB     PRT      Online TA\n222                    2      0        2      0   BB     FIN      Online TA\n223                    5      0        0      0   SC     PRT      Online TA\n224                    0      0        0      0   SC     PRT      Corporate\n225                   10      0        0      0   BB     FIN      Online TA\n226                    1      0        0      0   BB     SWE         Direct\n227                    1      0        0      0   SC     PRT         Direct\n228                    0      0        2      0   BB     BEL         Direct\n229                    1      0        2      0   BB     BEL      Online TA\n230                    0      0        0      0   SC     PRT  Complementary\n231                    0      0        0      0   SC     PRT  Complementary\n232                    1      0        2      0   BB     BRA      Online TA\n233                    3      0        2      0   BB     CHE      Online TA\n234                    9      0        2      0   BB     FRA         Direct\n235                    2      0        2      0   BB     BEL         Direct\n236                    1      0        2      0   BB     GBR      Online TA\n237                    2      0        2      0   BB     BEL         Direct\n238                    2      0        0      0   SC     RUS      Online TA\n239                    5      0        0      0   SC     FRA      Online TA\n240                    1      0        2      0   BB     FRA      Online TA\n241                    1      0        2      0   BB     NLD      Online TA\n242                    1      0        2      0   BB     BEL         Direct\n243                    1      0        2      0   BB     BEL      Online TA\n244                    2      0        2      0   BB     FRA      Online TA\n245                    1      0        2      0   BB     FRA      Online TA\n246                    1      0        2      0   BB     BEL      Online TA\n247                    4      0        0      0   BB     PRT      Online TA\n248                    1      0        2      0   BB     GBR      Online TA\n249                    5      0        3      0   HB     FRA         Direct\n250                    2      0        2      0   BB     GBR      Online TA\n251                    1      0        2      0   BB     GBR      Online TA\n252                    3      0        2      0   BB     IRL      Online TA\n253                    1      0        2      0   BB     DEU      Online TA\n254                    3      0        2      0   BB     BEL      Online TA\n255                    4      0        0      0   BB     ESP  Offline TA/TO\n256                    6      0        0      0   SC     ESP      Online TA\n257                    2      0        2      0   BB     DEU      Online TA\n258                   10      0        0      0   SC     PRT         Direct\n259                    0      0        2      0   BB     CHL      Online TA\n260                    3      0        2      0   BB     PRT      Online TA\n261                    4      0        0      0   SC     PRT         Direct\n262                   20      0        0      0   BB     USA      Online TA\n263                    3      0        2      0   BB     DEU      Online TA\n264                    9      0        0      0   SC     PRT      Online TA\n265                    1      0        2      0   BB     CHE      Online TA\n266                    3      0        0      0   BB     FRA      Corporate\n267                    2      0        2      0   BB     CHE      Online TA\n268                    3      0        0      0   BB     FRA  Offline TA/TO\n269                    2      0        2      0   BB     DEU      Online TA\n270                    4      0        0      0   BB     USA         Direct\n271                    1      0        0      0   BB     DEU  Offline TA/TO\n272                    3      0        2      0   BB     DEU      Online TA\n273                    2      0        2      0   BB     RUS      Online TA\n274                    2      0        2      0   BB     IRL      Online TA\n275                    7      0        0      0   BB     PRT  Offline TA/TO\n276                    2      0        2      0   BB     GBR      Online TA\n277                    3      0        2      0   BB     GBR      Online TA\n278                    0      0        0      0   SC     PRT      Online TA\n279                   16      0        0      0   SC     POL      Corporate\n280                    3      0        2      0   HB     FRA      Online TA\n281                    1      0        2      0   BB     ESP      Online TA\n282                    0      0        0      0   SC     PRT         Groups\n283                    2      0        2      0   BB     SWE      Online TA\n284                    2      0        2      0   BB     CHE      Online TA\n285                    3      0        2      0   BB     GBR      Online TA\n286                    0      0        0      0   SC     PRT      Online TA\n287                    2      0        0      0   SC     SGP         Direct\n288                    3      0        0      0   SC     LUX      Online TA\n289                   41      0        0      0   SC     GBR      Online TA\n290                    0      0        0      0   BB     PRT      Online TA\n291                    0      0        0      0   BB     PRT      Online TA\n292                    3      0        0      0   SC     BRA      Online TA\n293                    2      0        0      0   SC     FRA      Online TA\n294                    0      0        0      0   BB     PRT      Online TA\n295                    0      0        0      0   BB     PRT      Corporate\n296                    1      0        0      0   BB     PRT         Groups\n297                    3      0        0      0   BB     CHE  Offline TA/TO\n298                    0      0        0      0   BB     PRT      Online TA\n299                    0      0        0      0   HB     PRT  Offline TA/TO\n300                    7      0        0      0   SC     USA      Online TA\n301                    0      0        0      0   BB     PRT  Offline TA/TO\n302                    0      0        0      0   SC     PRT      Online TA\n303                    8      0        0      0   SC     ITA  Offline TA/TO\n304                    0      0        0      0   BB     PRT      Online TA\n305                    4      0        2      0   BB     ROU      Online TA\n306                    0      0        0      0   BB     PRT  Offline TA/TO\n307                    2      0        2      0   BB     ESP      Online TA\n308                    0      0        0      0   SC     PRT      Online TA\n309                    0      0        0      0   BB     PRT      Online TA\n310                    3      0        2      0   BB     ITA      Online TA\n311                    2      0        2      0   BB     PRT      Online TA\n312                    3      0        2      0   BB     ESP      Online TA\n313                    0      0        0      0   BB     PRT      Online TA\n314                    0      0        0      0   BB     PRT  Offline TA/TO\n315                    3      0        2      0   BB     BRA      Online TA\n316                    4      0        3      0   HB     FRA  Offline TA/TO\n317                    1      0        2      0   BB     PRT      Online TA\n318                    1      0        2      0   BB     FRA      Online TA\n319                    3      0        2      0   BB     PRT      Online TA\n320                    3      0        0      0   BB     PRT  Offline TA/TO\n321                    0      0        0      0   SC     PRT      Online TA\n322                    2      0        2      0   BB     ESP      Online TA\n323                    5      0        0      0   BB     PRT         Direct\n324                    0      0        0      0   SC     PRT      Online TA\n325                    2      0        2      0   BB     THA      Online TA\n326                   10      0        0      0   SC     PRT      Online TA\n327                    2      0        2      0   BB     BEL      Online TA\n328                    4      0        2      0   BB     BRA      Online TA\n329                    2      0        2      0   BB     PRT         Direct\n330                    0      0        0      0   SC     PRT      Online TA\n331                    0      0        0      0   SC     PRT      Online TA\n332                    1      0        2      0   BB     PRT         Direct\n333                    0      0        0      0   BB     PRT      Online TA\n334                    0      0        0      0   SC     PRT      Online TA\n335                    0      0        0      0   SC     PRT      Online TA\n336                    0      0        0      0   BB     PRT      Online TA\n337                   30      0        0      0   SC     USA      Online TA\n338                   10      0        0      0   SC     GBR      Online TA\n339                    0      0        0      0   BB     PRT      Online TA\n340                    0      0        0      0   SC     PRT      Online TA\n341                    0      0        0      0   BB     PRT      Online TA\n342                    3      0        2      0   BB     NLD      Online TA\n343                    3      0        0      0   BB     IRN      Online TA\n344                    9      0        0      0   BB     ITA  Offline TA/TO\n345                    0      0        0      0   SC     PRT      Online TA\n346                    0      0        0      0   BB     PRT  Offline TA/TO\n347                    0      0        0      0   BB     PRT  Offline TA/TO\n348                    0      0        0      0   BB     PRT  Offline TA/TO\n349                    7      0        0      0   SC     FRA      Online TA\n350                    0      0        0      0   SC     PRT  Offline TA/TO\n351                    7      0        0      0   SC     PRT      Online TA\n352                    0      0        0      0   SC     PRT      Online TA\n353                    0      0        0      0   SC     GBR      Online TA\n354                   10      0        0      0   SC     PRT         Direct\n355                    0      0        0      0   BB     PRT      Online TA\n356                    0      0        0      0   BB     PRT      Online TA\n357                    0      0        0      0   BB     PRT      Online TA\n358                    5      0        1      0   BB     BLR         Direct\n359                    1      0        2      0   BB     FRA         Direct\n360                   19      0        0      0   SC     PRT         Direct\n361                    2      0        2      0   BB     LUX      Online TA\n362                    1      0        2      0   BB     USA      Online TA\n363                    4      0        2      0   BB     BEL      Online TA\n364                    4      0        2      0   BB     DEU      Online TA\n365                    3      0        0      0   SC     ESP         Direct\n366                    6      0        2      0   BB     USA      Online TA\n367                    0      0        0      0   BB     FRA      Online TA\n368                    3      0        0      0   SC     PRT      Corporate\n369                    2      0        2      0   BB     CHE      Online TA\n370                    0      0        0      0   SC     PRT  Complementary\n371                    6      0        0      0   SC     ESP  Offline TA/TO\n372                    1      0        0      0   SC     PRT  Offline TA/TO\n373                    2      0        0      0   BB     IRL         Groups\n374                    0      0        0      0   BB     PRT         Groups\n375                    0      0        0      0   BB     PRT         Groups\n376                    1      0        0      0   SC     IND         Direct\n377                    2      0        0      0   SC     GBR         Groups\n378                    2      0        0      0   SC     PRT         Direct\n379                    3      0        2      0   BB     FRA      Online TA\n380                    4      0        3      0   HB     FRA      Online TA\n381                    0      0        0      0   HB     PRT         Groups\n382                    5      0        1      0   BB     FRA      Online TA\n383                    1      0        2      0   SC     DEU      Online TA\n384                    1      0        2      0   SC     DEU      Online TA\n385                    4      0        0      0   SC     GBR      Online TA\n386                    0      0        0      0   BB     PRT       Aviation\n387                    0      0        0      0   BB     PRT       Aviation\n388                    3      0        0      0   BB     CHE      Online TA\n389                    1      0        0      0   SC     PRT  Complementary\n390                    3      0        2      0   BB     JOR      Online TA\n391                    5      0        2      0   BB     FRA      Online TA\n392                    1      0        0      0   SC     SWE      Online TA\n393                    5      0        0      0   SC     RUS      Online TA\n394                    2      0        2      0   BB     ZAF         Direct\n395                    1      0        3      0   BB     PRT         Direct\n396                    2      0        0      0   BB     BRA  Offline TA/TO\n397                    3      0        2      0   BB     CHE      Online TA\n398                    5      0        2      0   BB     GBR      Online TA\n399                    3      0        2      0   BB     GBR      Online TA\n400                    1      0        2      0   BB     DEU      Online TA\n401                    2      0        2      0   BB     PRT      Online TA\n402                    3      0        2      0   SC     FRA      Online TA\n403                    2      0        3      0   BB     MAR         Direct\n    distribution_channel is_repeated_guest previous_cancellations\n1              Corporate                 0                      0\n2              Corporate                 0                      0\n3                  TA/TO                 0                      0\n4                  TA/TO                 0                      0\n5                  TA/TO                 0                      0\n6                  TA/TO                 0                      0\n7                  TA/TO                 0                      0\n8                 Direct                 0                      0\n9                  TA/TO                 0                      0\n10                Direct                 0                      0\n11             Corporate                 0                      0\n12             Corporate                 0                      0\n13                Direct                 0                      0\n14                 TA/TO                 0                      0\n15                 TA/TO                 0                      0\n16                Direct                 0                      0\n17                Direct                 0                      0\n18                 TA/TO                 0                      0\n19                Direct                 0                      0\n20                Direct                 0                      0\n21                Direct                 0                      0\n22                 TA/TO                 0                      0\n23                 TA/TO                 0                      0\n24             Corporate                 0                      0\n25                Direct                 0                      0\n26                 TA/TO                 0                      0\n27                 TA/TO                 0                      0\n28                 TA/TO                 0                      0\n29                 TA/TO                 0                      0\n30                 TA/TO                 0                      0\n31                 TA/TO                 0                      0\n32                 TA/TO                 0                      0\n33                 TA/TO                 0                      0\n34                 TA/TO                 0                      0\n35                 TA/TO                 0                      0\n36                 TA/TO                 0                      0\n37                 TA/TO                 0                      0\n38                 TA/TO                 0                      0\n39                 TA/TO                 0                      0\n40                 TA/TO                 0                      0\n41                 TA/TO                 0                      0\n42                Direct                 0                      0\n43                 TA/TO                 0                      0\n44                 TA/TO                 0                      0\n45                 TA/TO                 0                      0\n46                Direct                 0                      0\n47             Corporate                 0                      0\n48                 TA/TO                 0                      0\n49                 TA/TO                 0                      0\n50                Direct                 0                      0\n51                 TA/TO                 0                      0\n52                 TA/TO                 0                      0\n53                 TA/TO                 0                      0\n54                 TA/TO                 0                      0\n55                Direct                 0                      0\n56                 TA/TO                 0                      0\n57                 TA/TO                 0                      0\n58                 TA/TO                 0                      0\n59                 TA/TO                 0                      0\n60                 TA/TO                 0                      0\n61                 TA/TO                 0                      0\n62                 TA/TO                 0                      0\n63                 TA/TO                 0                      0\n64                 TA/TO                 0                      0\n65                 TA/TO                 0                      0\n66                 TA/TO                 0                      0\n67                 TA/TO                 0                      0\n68                 TA/TO                 0                      0\n69                 TA/TO                 0                      0\n70                 TA/TO                 0                      0\n71                Direct                 0                      0\n72                 TA/TO                 0                      0\n73                 TA/TO                 0                      0\n74                 TA/TO                 0                      0\n75                 TA/TO                 0                      0\n76                 TA/TO                 0                      0\n77                 TA/TO                 0                      0\n78                 TA/TO                 0                      0\n79                 TA/TO                 0                      0\n80                 TA/TO                 0                      0\n81                 TA/TO                 0                      0\n82                 TA/TO                 0                      0\n83                 TA/TO                 0                      0\n84                 TA/TO                 0                      0\n85                Direct                 0                      0\n86                 TA/TO                 0                      0\n87                 TA/TO                 0                      0\n88                Direct                 0                      0\n89                Direct                 0                      0\n90                 TA/TO                 0                      0\n91                 TA/TO                 0                      0\n92                 TA/TO                 0                      0\n93                 TA/TO                 0                      0\n94                 TA/TO                 0                      0\n95                 TA/TO                 0                      0\n96                 TA/TO                 0                      0\n97                 TA/TO                 0                      0\n98                 TA/TO                 0                      0\n99                 TA/TO                 0                      0\n100                TA/TO                 0                      0\n101                TA/TO                 0                      0\n102                TA/TO                 0                      0\n103                TA/TO                 0                      0\n104                TA/TO                 0                      0\n105                TA/TO                 0                      0\n106                TA/TO                 0                      0\n107                TA/TO                 0                      0\n108               Direct                 0                      0\n109                TA/TO                 0                      0\n110                TA/TO                 0                      0\n111                TA/TO                 0                      0\n112                TA/TO                 0                      0\n113                TA/TO                 0                      0\n114            Corporate                 0                      0\n115            Corporate                 0                      0\n116            Corporate                 0                      0\n117               Direct                 0                      0\n118                TA/TO                 0                      0\n119                TA/TO                 0                      0\n120                TA/TO                 0                      0\n121                TA/TO                 0                      0\n122                TA/TO                 0                      0\n123               Direct                 0                      0\n124                TA/TO                 0                      0\n125                TA/TO                 0                      0\n126                TA/TO                 0                      0\n127                TA/TO                 0                      0\n128                TA/TO                 0                      0\n129                TA/TO                 0                      0\n130                TA/TO                 0                      0\n131                TA/TO                 0                      0\n132                TA/TO                 0                      0\n133                TA/TO                 0                      0\n134                TA/TO                 0                      0\n135                TA/TO                 0                      0\n136                TA/TO                 0                      0\n137                TA/TO                 0                      0\n138            Corporate                 0                      0\n139            Corporate                 0                      0\n140            Corporate                 0                      0\n141            Corporate                 0                      0\n142               Direct                 0                      0\n143                TA/TO                 0                      0\n144                TA/TO                 0                      0\n145                TA/TO                 0                      0\n146                TA/TO                 0                      0\n147                TA/TO                 0                      0\n148                TA/TO                 0                      0\n149                TA/TO                 0                      0\n150                TA/TO                 0                      0\n151                TA/TO                 0                      0\n152                TA/TO                 0                      0\n153                TA/TO                 0                      0\n154                TA/TO                 0                      0\n155                TA/TO                 0                      0\n156                TA/TO                 0                      0\n157                TA/TO                 0                      0\n158                TA/TO                 0                      0\n159                TA/TO                 0                      0\n160                TA/TO                 0                      0\n161                TA/TO                 0                      0\n162                TA/TO                 0                      0\n163            Corporate                 0                      0\n164                TA/TO                 0                      0\n165               Direct                 1                      0\n166               Direct                 1                      0\n167                TA/TO                 0                      0\n168                TA/TO                 0                      0\n169            Corporate                 1                      0\n170            Corporate                 1                      0\n171                TA/TO                 0                      0\n172                TA/TO                 0                      0\n173                TA/TO                 1                      1\n174                TA/TO                 1                      1\n175               Direct                 1                      0\n176               Direct                 0                      0\n177               Direct                 0                      0\n178               Direct                 0                      0\n179               Direct                 0                      1\n180                TA/TO                 1                      0\n181                TA/TO                 1                      0\n182                TA/TO                 1                      0\n183                TA/TO                 1                      0\n184               Direct                 1                      0\n185               Direct                 0                      0\n186                TA/TO                 0                      1\n187               Direct                 0                      0\n188                TA/TO                 0                      0\n189               Direct                 0                      0\n190                TA/TO                 0                      0\n191               Direct                 0                      0\n192                TA/TO                 0                      0\n193                TA/TO                 0                      0\n194                TA/TO                 0                      0\n195                TA/TO                 0                      0\n196               Direct                 1                      0\n197                TA/TO                 1                      0\n198                TA/TO                 0                      0\n199                TA/TO                 0                      0\n200               Direct                 0                      0\n201                TA/TO                 0                      0\n202                TA/TO                 0                      0\n203                TA/TO                 0                      0\n204               Direct                 0                      0\n205                TA/TO                 0                      0\n206                TA/TO                 0                      0\n207               Direct                 0                      0\n208                TA/TO                 0                      0\n209                TA/TO                 0                      0\n210                TA/TO                 1                      2\n211                TA/TO                 1                      2\n212               Direct                 0                      0\n213                TA/TO                 0                      0\n214                TA/TO                 0                      0\n215                TA/TO                 0                      0\n216                TA/TO                 0                      0\n217                TA/TO                 0                      0\n218                TA/TO                 0                      0\n219                TA/TO                 0                      0\n220                TA/TO                 1                      0\n221                TA/TO                 0                      0\n222                TA/TO                 0                      0\n223                TA/TO                 0                      0\n224            Corporate                 1                      1\n225                TA/TO                 0                      0\n226               Direct                 0                      0\n227               Direct                 0                      0\n228               Direct                 0                      0\n229                TA/TO                 0                      0\n230               Direct                 1                      0\n231               Direct                 1                      0\n232                TA/TO                 0                      0\n233                TA/TO                 0                      0\n234               Direct                 0                      0\n235               Direct                 0                      0\n236                TA/TO                 0                      0\n237               Direct                 0                      0\n238                TA/TO                 0                      0\n239                TA/TO                 0                      0\n240                TA/TO                 0                      0\n241                TA/TO                 0                      0\n242               Direct                 0                      0\n243                TA/TO                 0                      0\n244                TA/TO                 0                      0\n245                TA/TO                 0                      0\n246                TA/TO                 0                      0\n247                TA/TO                 0                      0\n248                TA/TO                 0                      0\n249               Direct                 0                      0\n250                TA/TO                 0                      0\n251                TA/TO                 0                      0\n252                TA/TO                 0                      0\n253                TA/TO                 0                      0\n254                TA/TO                 0                      0\n255                TA/TO                 0                      0\n256                TA/TO                 0                      0\n257                TA/TO                 0                      0\n258               Direct                 0                      0\n259                TA/TO                 0                      0\n260                TA/TO                 0                      0\n261               Direct                 0                      0\n262                TA/TO                 0                      0\n263                TA/TO                 0                      0\n264                TA/TO                 0                      0\n265                TA/TO                 0                      0\n266            Corporate                 0                      0\n267                TA/TO                 0                      0\n268                TA/TO                 0                      0\n269                TA/TO                 0                      0\n270               Direct                 0                      0\n271                TA/TO                 0                      0\n272                TA/TO                 0                      0\n273                TA/TO                 0                      0\n274                TA/TO                 0                      0\n275                TA/TO                 0                      0\n276                TA/TO                 0                      0\n277                TA/TO                 0                      0\n278                TA/TO                 1                      0\n279            Corporate                 0                      0\n280                TA/TO                 0                      0\n281                TA/TO                 0                      0\n282                TA/TO                 1                      0\n283                TA/TO                 0                      0\n284                TA/TO                 0                      0\n285                TA/TO                 0                      0\n286                TA/TO                 1                      0\n287               Direct                 0                      0\n288                TA/TO                 0                      0\n289                TA/TO                 0                      0\n290                TA/TO                 1                      0\n291                TA/TO                 1                      0\n292                TA/TO                 0                      0\n293                TA/TO                 0                      0\n294                TA/TO                 1                      0\n295            Corporate                 1                      0\n296            Corporate                 0                      0\n297                TA/TO                 0                      0\n298                TA/TO                 1                      0\n299                TA/TO                 1                      0\n300                TA/TO                 0                      0\n301                TA/TO                 1                      0\n302                TA/TO                 1                      0\n303                TA/TO                 0                      0\n304                TA/TO                 1                      0\n305                TA/TO                 0                      0\n306                TA/TO                 1                      0\n307                TA/TO                 0                      0\n308                TA/TO                 1                      0\n309                TA/TO                 1                      0\n310                TA/TO                 0                      0\n311                TA/TO                 0                      0\n312                TA/TO                 0                      0\n313                TA/TO                 1                      0\n314                TA/TO                 1                      0\n315                TA/TO                 0                      0\n316                TA/TO                 0                      0\n317                TA/TO                 0                      0\n318                TA/TO                 0                      0\n319                TA/TO                 0                      0\n320                TA/TO                 0                      0\n321                TA/TO                 1                      0\n322                TA/TO                 0                      0\n323               Direct                 0                      0\n324                TA/TO                 1                      0\n325                TA/TO                 0                      0\n326                TA/TO                 0                      0\n327                TA/TO                 0                      0\n328                TA/TO                 0                      0\n329               Direct                 0                      0\n330                TA/TO                 1                      0\n331                TA/TO                 0                      0\n332               Direct                 0                      0\n333                TA/TO                 0                      0\n334                TA/TO                 1                      0\n335                TA/TO                 1                      0\n336                TA/TO                 1                      0\n337                TA/TO                 0                      0\n338                TA/TO                 0                      0\n339                TA/TO                 1                      0\n340                TA/TO                 1                      0\n341                TA/TO                 1                      0\n342                TA/TO                 0                      0\n343                TA/TO                 0                      0\n344                TA/TO                 0                      0\n345                TA/TO                 0                      0\n346                TA/TO                 1                      0\n347                TA/TO                 0                      0\n348                TA/TO                 1                      0\n349                TA/TO                 0                      0\n350                TA/TO                 1                      0\n351                TA/TO                 0                      0\n352                TA/TO                 1                      0\n353                TA/TO                 1                      0\n354               Direct                 0                      0\n355                TA/TO                 1                      0\n356                TA/TO                 1                      0\n357                TA/TO                 1                      0\n358               Direct                 0                      0\n359               Direct                 0                      0\n360               Direct                 0                      0\n361                TA/TO                 0                      0\n362                TA/TO                 0                      0\n363                TA/TO                 0                      0\n364                TA/TO                 0                      0\n365               Direct                 0                      0\n366                TA/TO                 0                      0\n367                TA/TO                 0                      0\n368            Corporate                 0                      0\n369                TA/TO                 0                      0\n370               Direct                 1                      0\n371                TA/TO                 0                      0\n372                TA/TO                 0                      0\n373                TA/TO                 0                      0\n374            Corporate                 0                      0\n375            Corporate                 0                      0\n376               Direct                 0                      0\n377                TA/TO                 0                      0\n378               Direct                 0                      0\n379                TA/TO                 0                      0\n380                TA/TO                 0                      0\n381                TA/TO                 1                      0\n382                TA/TO                 0                      0\n383                TA/TO                 0                      0\n384                TA/TO                 0                      0\n385                TA/TO                 0                      0\n386            Corporate                 0                      0\n387            Corporate                 0                      0\n388                TA/TO                 0                      0\n389               Direct                 0                      0\n390                TA/TO                 0                      0\n391                TA/TO                 0                      0\n392                TA/TO                 0                      0\n393                TA/TO                 0                      0\n394               Direct                 0                      0\n395               Direct                 0                      0\n396                TA/TO                 0                      0\n397                TA/TO                 0                      0\n398                TA/TO                 0                      0\n399                TA/TO                 0                      0\n400                TA/TO                 0                      0\n401                TA/TO                 0                      0\n402                TA/TO                 0                      0\n403               Direct                 0                      0\n    previous_bookings_not_canceled reserved_room_type assigned_room_type\n1                                0                  A                  I\n2                                0                  A                  I\n3                                0                  A                  C\n4                                0                  A                  A\n5                                0                  A                  C\n6                                0                  P                  P\n7                                0                  P                  P\n8                                0                  D                  D\n9                                0                  A                  G\n10                               0                  A                  I\n11                               0                  A                  A\n12                               0                  A                  A\n13                               0                  A                  I\n14                               0                  A                  A\n15                               0                  F                  B\n16                               0                  B                  B\n17                               0                  E                  K\n18                               0                  B                  B\n19                               0                  A                  K\n20                               0                  B                  B\n21                               0                  B                  B\n22                               0                  A                  A\n23                               0                  A                  K\n24                               0                  A                  K\n25                               0                  A                  K\n26                               0                  B                  B\n27                               0                  B                  B\n28                               0                  B                  B\n29                               0                  B                  B\n30                               0                  B                  B\n31                               0                  B                  B\n32                               0                  B                  B\n33                               0                  B                  B\n34                               0                  B                  B\n35                               0                  B                  B\n36                               0                  B                  B\n37                               0                  B                  B\n38                               0                  B                  B\n39                               0                  B                  B\n40                               0                  B                  B\n41                               0                  B                  B\n42                               0                  B                  B\n43                               0                  B                  B\n44                               0                  B                  B\n45                               0                  B                  B\n46                               0                  B                  B\n47                               0                  D                  A\n48                               0                  B                  B\n49                               0                  B                  B\n50                               0                  B                  K\n51                               0                  B                  B\n52                               0                  B                  B\n53                               0                  B                  B\n54                               0                  B                  B\n55                               0                  B                  B\n56                               0                  B                  B\n57                               0                  B                  B\n58                               0                  B                  B\n59                               0                  B                  B\n60                               0                  B                  B\n61                               0                  B                  B\n62                               0                  B                  B\n63                               0                  A                  A\n64                               0                  B                  B\n65                               0                  A                  A\n66                               0                  E                  K\n67                               0                  E                  K\n68                               0                  B                  B\n69                               0                  B                  B\n70                               0                  B                  B\n71                               0                  B                  B\n72                               0                  B                  B\n73                               0                  B                  B\n74                               0                  A                  K\n75                               0                  E                  E\n76                               0                  E                  E\n77                               0                  E                  E\n78                               0                  B                  A\n79                               0                  B                  B\n80                               0                  B                  B\n81                               0                  B                  B\n82                               0                  B                  A\n83                               0                  B                  B\n84                               0                  B                  A\n85                               0                  B                  B\n86                               0                  B                  B\n87                               0                  B                  B\n88                               0                  B                  B\n89                               0                  B                  B\n90                               0                  B                  B\n91                               0                  B                  B\n92                               0                  G                  G\n93                               0                  B                  B\n94                               0                  B                  B\n95                               0                  A                  K\n96                               0                  B                  B\n97                               0                  B                  B\n98                               0                  B                  B\n99                               0                  B                  B\n100                              0                  B                  B\n101                              0                  B                  B\n102                              0                  B                  B\n103                              0                  B                  B\n104                              0                  B                  B\n105                              0                  B                  B\n106                              0                  B                  B\n107                              0                  B                  B\n108                              0                  P                  P\n109                              0                  B                  B\n110                              0                  B                  B\n111                              0                  B                  B\n112                              0                  B                  B\n113                              0                  B                  B\n114                              0                  P                  P\n115                              0                  P                  P\n116                              0                  P                  P\n117                              0                  B                  A\n118                              0                  B                  B\n119                              0                  B                  B\n120                              0                  B                  B\n121                              0                  B                  B\n122                              0                  B                  B\n123                              0                  E                  E\n124                              0                  B                  B\n125                              0                  A                  A\n126                              0                  B                  B\n127                              0                  B                  B\n128                              0                  B                  B\n129                              0                  A                  A\n130                              0                  B                  B\n131                              0                  B                  B\n132                              0                  B                  B\n133                              0                  B                  B\n134                              0                  B                  B\n135                              0                  B                  B\n136                              0                  B                  B\n137                              0                  B                  B\n138                              0                  P                  P\n139                              0                  P                  P\n140                              0                  P                  P\n141                              0                  P                  P\n142                              0                  B                  B\n143                              0                  B                  B\n144                              0                  B                  B\n145                              0                  B                  B\n146                              0                  B                  B\n147                              0                  B                  B\n148                              0                  B                  B\n149                              0                  B                  B\n150                              0                  B                  B\n151                              0                  A                  A\n152                              0                  B                  B\n153                              0                  B                  B\n154                              0                  B                  B\n155                              0                  B                  B\n156                              0                  B                  B\n157                              0                  B                  B\n158                              0                  P                  P\n159                              0                  P                  P\n160                              0                  B                  B\n161                              0                  B                  B\n162                              0                  A                  A\n163                              0                  A                  K\n164                              0                  F                  B\n165                              0                  F                  B\n166                              1                  A                  E\n167                              0                  A                  D\n168                              0                  A                  K\n169                              1                  A                  A\n170                              0                  A                  B\n171                              0                  A                  K\n172                              0                  B                  B\n173                              0                  E                  E\n174                              0                  E                  E\n175                              0                  D                  D\n176                              0                  B                  B\n177                              0                  B                  B\n178                              0                  B                  B\n179                              0                  B                  B\n180                              0                  D                  K\n181                              0                  D                  K\n182                              0                  A                  K\n183                              0                  D                  K\n184                              1                  A                  A\n185                              0                  B                  B\n186                              0                  B                  B\n187                              0                  B                  B\n188                              0                  B                  B\n189                              0                  D                  D\n190                              0                  B                  B\n191                              0                  A                  D\n192                              0                  B                  B\n193                              0                  A                  D\n194                              0                  B                  B\n195                              0                  B                  B\n196                              0                  A                  A\n197                              0                  D                  K\n198                              0                  A                  K\n199                              0                  A                  K\n200                              0                  A                  D\n201                              0                  F                  B\n202                              0                  B                  B\n203                              0                  A                  K\n204                              0                  A                  K\n205                              0                  A                  K\n206                              0                  B                  B\n207                              0                  E                  K\n208                              0                  B                  B\n209                              0                  B                  B\n210                              6                  A                  A\n211                              6                  A                  A\n212                              0                  B                  B\n213                              0                  B                  B\n214                              0                  B                  B\n215                              0                  B                  B\n216                              0                  B                  B\n217                              0                  A                  K\n218                              0                  B                  B\n219                              0                  B                  B\n220                              0                  D                  C\n221                              0                  B                  B\n222                              0                  B                  B\n223                              0                  A                  K\n224                              9                  A                  K\n225                              0                  A                  K\n226                              0                  G                  B\n227                              0                  E                  B\n228                              0                  B                  B\n229                              0                  B                  B\n230                              0                  A                  B\n231                              0                  A                  A\n232                              0                  B                  B\n233                              0                  B                  B\n234                              0                  E                  A\n235                              0                  B                  B\n236                              0                  B                  B\n237                              0                  B                  B\n238                              0                  A                  K\n239                              0                  A                  K\n240                              0                  B                  B\n241                              0                  B                  B\n242                              0                  B                  B\n243                              0                  B                  B\n244                              0                  B                  B\n245                              0                  B                  B\n246                              0                  B                  B\n247                              0                  D                  K\n248                              0                  B                  B\n249                              0                  D                  D\n250                              0                  B                  B\n251                              0                  B                  B\n252                              0                  B                  B\n253                              0                  B                  B\n254                              0                  B                  B\n255                              0                  A                  K\n256                              0                  A                  K\n257                              0                  B                  B\n258                              0                  A                  K\n259                              0                  B                  B\n260                              0                  B                  B\n261                              0                  A                  K\n262                              0                  D                  K\n263                              0                  B                  D\n264                              0                  D                  K\n265                              0                  B                  B\n266                              0                  A                  K\n267                              0                  B                  B\n268                              0                  A                  K\n269                              0                  B                  B\n270                              0                  D                  D\n271                              1                  A                  K\n272                              0                  B                  A\n273                              0                  B                  B\n274                              0                  B                  B\n275                              0                  A                  K\n276                              0                  B                  B\n277                              0                  B                  B\n278                              0                  A                  K\n279                              0                  A                  K\n280                              0                  F                  B\n281                              0                  B                  B\n282                              0                  A                  K\n283                              0                  B                  B\n284                              0                  B                  B\n285                              0                  B                  B\n286                              0                  A                  A\n287                              0                  A                  K\n288                              0                  A                  K\n289                              0                  A                  K\n290                              0                  A                  A\n291                              0                  A                  A\n292                              0                  A                  K\n293                              0                  A                  K\n294                              0                  A                  A\n295                              0                  A                  A\n296                              0                  A                  A\n297                              0                  A                  K\n298                              0                  A                  A\n299                              0                  A                  D\n300                              1                  A                  K\n301                              0                  A                  D\n302                              0                  A                  A\n303                              0                  A                  K\n304                              0                  A                  A\n305                              0                  B                  A\n306                              0                  A                  K\n307                              0                  B                  A\n308                              0                  A                  K\n309                              0                  A                  K\n310                              0                  B                  A\n311                              0                  B                  B\n312                              0                  B                  A\n313                              0                  D                  K\n314                              0                  A                  D\n315                              0                  B                  B\n316                              0                  A                  A\n317                              0                  B                  A\n318                              0                  B                  A\n319                              0                  B                  A\n320                              0                  A                  K\n321                              0                  A                  K\n322                              0                  B                  A\n323                              0                  E                  K\n324                              0                  A                  K\n325                              0                  B                  A\n326                              0                  D                  K\n327                              0                  B                  B\n328                              0                  B                  A\n329                              0                  B                  B\n330                              0                  A                  K\n331                              0                  E                  E\n332                              0                  A                  A\n333                              0                  D                  D\n334                              0                  A                  K\n335                              0                  A                  K\n336                              0                  A                  K\n337                              0                  A                  K\n338                              0                  D                  K\n339                              0                  A                  K\n340                              0                  A                  K\n341                              0                  A                  K\n342                              0                  B                  A\n343                              0                  A                  A\n344                              0                  A                  K\n345                              0                  D                  K\n346                              0                  A                  K\n347                              0                  A                  D\n348                              0                  A                  K\n349                              0                  E                  K\n350                              0                  A                  K\n351                              0                  A                  K\n352                              0                  A                  K\n353                              0                  A                  A\n354                              0                  A                  K\n355                              0                  A                  C\n356                              0                  A                  G\n357                              0                  A                  G\n358                              0                  B                  A\n359                              0                  B                  B\n360                              0                  G                  K\n361                              0                  B                  A\n362                              0                  B                  B\n363                              0                  B                  A\n364                              0                  B                  A\n365                              0                  D                  K\n366                              0                  B                  B\n367                              0                  D                  K\n368                              0                  A                  K\n369                              0                  B                  A\n370                              0                  C                  C\n371                              0                  D                  K\n372                              0                  E                  G\n373                              0                  A                  A\n374                              0                  A                  D\n375                              0                  A                  A\n376                              0                  A                  K\n377                              0                  A                  K\n378                              0                  A                  K\n379                              0                  B                  A\n380                              0                  A                  A\n381                              0                  A                  K\n382                              0                  B                  B\n383                              0                  A                  A\n384                              0                  A                  A\n385                              0                  D                  K\n386                              0                  A                  A\n387                              0                  A                  A\n388                              0                  A                  A\n389                              0                  E                  K\n390                              0                  B                  B\n391                              0                  B                  A\n392                              0                  A                  K\n393                              0                  A                  K\n394                              0                  B                  B\n395                              0                  A                  A\n396                              0                  A                  A\n397                              0                  B                  A\n398                              0                  B                  B\n399                              0                  B                  A\n400                              0                  B                  B\n401                              0                  B                  A\n402                              0                  A                  A\n403                              0                  B                  A\n    booking_changes deposit_type agent company days_in_waiting_list\n1                 1   No Deposit  NULL     174                    0\n2                 0   No Deposit  NULL     174                    0\n3                 0   No Deposit    38    NULL                    0\n4                 1   No Deposit   308    NULL                  122\n5                 1   No Deposit   308    NULL                  122\n6                 0   No Deposit  NULL     383                    0\n7                 0   No Deposit  NULL     386                    0\n8                 3   No Deposit  NULL    NULL                    0\n9                 0   No Deposit   168    NULL                    0\n10                8   No Deposit  NULL    NULL                    0\n11                0   No Deposit  NULL     523                    0\n12                0   No Deposit  NULL     523                    0\n13                6   No Deposit  NULL    NULL                    0\n14                4   No Deposit     1    NULL                    0\n15                0   No Deposit     9    NULL                    0\n16                1   No Deposit  NULL    NULL                    0\n17               20   No Deposit  NULL      47                    0\n18                1   No Deposit     7    NULL                    0\n19                4   No Deposit  NULL      49                    0\n20                1   No Deposit  NULL    NULL                    0\n21                0   No Deposit  NULL      45                    0\n22                0   No Deposit     1    NULL                    0\n23                2   No Deposit    11    NULL                    0\n24                1   No Deposit  NULL    NULL                    0\n25                0   No Deposit  NULL    NULL                    0\n26                0   No Deposit    13    NULL                    0\n27                0   No Deposit     8    NULL                    0\n28                0   No Deposit     9    NULL                    0\n29                0   No Deposit     9    NULL                    0\n30                0   No Deposit     9    NULL                    0\n31                0   No Deposit     9    NULL                    0\n32                2   No Deposit     9    NULL                    0\n33                0   No Deposit     9    NULL                    0\n34                0   No Deposit     9    NULL                    0\n35                1   No Deposit     9    NULL                    0\n36                0   No Deposit     9    NULL                    0\n37                0   No Deposit     9    NULL                    0\n38                0   No Deposit     9    NULL                    0\n39                1   No Deposit     9    NULL                    0\n40                0   No Deposit     9    NULL                    0\n41                0   No Deposit     9    NULL                    0\n42                0   No Deposit    14    NULL                    0\n43                0   No Deposit     9    NULL                    0\n44                0   No Deposit     9    NULL                    0\n45                0   No Deposit     9    NULL                    0\n46                0   No Deposit    14    NULL                    0\n47                2   No Deposit  NULL      45                    0\n48                0   No Deposit     9    NULL                    0\n49                1   No Deposit     9    NULL                    0\n50                2   No Deposit    14    NULL                    0\n51                0   No Deposit     9    NULL                    0\n52                0   No Deposit     9    NULL                    0\n53                1   No Deposit     9    NULL                    0\n54                0   No Deposit     9    NULL                    0\n55                0   No Deposit    14    NULL                    0\n56                0   No Deposit     8    NULL                    0\n57                0   No Deposit     9    NULL                    0\n58                0   No Deposit     9    NULL                    0\n59                0   No Deposit     9    NULL                    0\n60                1   No Deposit     9    NULL                    0\n61                2   No Deposit     9    NULL                    0\n62                0   No Deposit     9    NULL                    0\n63                0   No Deposit    21    NULL                    0\n64                0   No Deposit     9    NULL                    0\n65                1   No Deposit     9    NULL                    0\n66                3   No Deposit    11    NULL                    0\n67                3   No Deposit    11    NULL                    0\n68                0   No Deposit     9    NULL                    0\n69                0   No Deposit     9    NULL                    0\n70                0   No Deposit     9    NULL                    0\n71                0   No Deposit  NULL    NULL                    0\n72                0   No Deposit     9    NULL                    0\n73                0   No Deposit     9    NULL                    0\n74                0   No Deposit     9    NULL                    0\n75                0   No Deposit    19    NULL                    0\n76                0   No Deposit    19    NULL                    0\n77                0   No Deposit    19    NULL                    0\n78                0   No Deposit     9    NULL                    0\n79                0   No Deposit     9    NULL                    0\n80                0   No Deposit     9    NULL                    0\n81                1   No Deposit     9    NULL                    0\n82                0   No Deposit     9    NULL                    0\n83                0   No Deposit     9    NULL                    0\n84                0   No Deposit     9    NULL                    0\n85                0   No Deposit  NULL    NULL                    0\n86                0   No Deposit     9    NULL                    0\n87                1   No Deposit     9    NULL                    0\n88                0   No Deposit    14    NULL                    0\n89                0   No Deposit    14    NULL                    0\n90                0   No Deposit     9    NULL                    0\n91                0   No Deposit     9    NULL                    0\n92                0   No Deposit     7    NULL                    0\n93                0   No Deposit     9    NULL                    0\n94                0   No Deposit     9    NULL                    0\n95                1   No Deposit    11    NULL                    0\n96                0   No Deposit     9    NULL                    0\n97                0   No Deposit     9    NULL                    0\n98                1   No Deposit     9    NULL                    0\n99                3   No Deposit     9    NULL                    0\n100               0   No Deposit     9    NULL                    0\n101               0   No Deposit     9    NULL                    0\n102               0   No Deposit     9    NULL                    0\n103               0   No Deposit     9    NULL                    0\n104               0   No Deposit     9    NULL                    0\n105               0   No Deposit     9    NULL                    0\n106               0   No Deposit     9    NULL                    0\n107               0   No Deposit     9    NULL                    0\n108               0   No Deposit  NULL     279                    0\n109               0   No Deposit     9    NULL                    0\n110               0   No Deposit     9    NULL                    0\n111               0   No Deposit     9    NULL                    0\n112               0   No Deposit     9    NULL                    0\n113               0   No Deposit     9    NULL                    0\n114               0   No Deposit  NULL     279                    0\n115               0   No Deposit  NULL     279                    0\n116               0   No Deposit  NULL     279                    0\n117               0   No Deposit  NULL      45                    0\n118               0   No Deposit     9    NULL                    0\n119               0   No Deposit     9    NULL                    0\n120               1   No Deposit     9    NULL                    0\n121               0   No Deposit     9    NULL                    0\n122               0   No Deposit     9    NULL                    0\n123               1   No Deposit  NULL    NULL                    0\n124               0   No Deposit     9    NULL                    0\n125               0   No Deposit  NULL    NULL                    0\n126               0   No Deposit     9    NULL                    0\n127               1   No Deposit     9    NULL                    0\n128               0   No Deposit     9    NULL                    0\n129               0   No Deposit  NULL    NULL                    0\n130               0   No Deposit     9    NULL                    0\n131               0   No Deposit     9    NULL                    0\n132               0   No Deposit     9    NULL                    0\n133               0   No Deposit     9    NULL                    0\n134               0   No Deposit     9    NULL                    0\n135               0   No Deposit     9    NULL                    0\n136               0   No Deposit     9    NULL                    0\n137               0   No Deposit     9    NULL                    0\n138               0   No Deposit  NULL     279                    0\n139               0   No Deposit  NULL     279                    0\n140               0   No Deposit  NULL     279                    0\n141               0   No Deposit  NULL     279                    0\n142               0   No Deposit    14    NULL                    0\n143               0   No Deposit     9    NULL                    0\n144               0   No Deposit     9    NULL                    0\n145               0   No Deposit     9    NULL                    0\n146               0   No Deposit     9    NULL                    0\n147               0   No Deposit     9    NULL                    0\n148               0   No Deposit     9    NULL                    0\n149               2   No Deposit     9    NULL                    0\n150               0   No Deposit     9    NULL                    0\n151               1   No Deposit   229    NULL                    0\n152               0   No Deposit     9    NULL                    0\n153               0   No Deposit     9    NULL                    0\n154               0   No Deposit     9    NULL                    0\n155               0   No Deposit     9    NULL                    0\n156               2   No Deposit     9    NULL                    0\n157               0   No Deposit     9    NULL                    0\n158               0   No Deposit  NULL    NULL                    0\n159               0   No Deposit  NULL    NULL                    0\n160               0   No Deposit     9    NULL                    0\n161               0   No Deposit     9    NULL                    0\n162               2   No Deposit    17    NULL                    0\n163              11   No Deposit  NULL      38                    0\n164               0   No Deposit  NULL    NULL                    0\n165               0   No Deposit  NULL    NULL                    0\n166               0   No Deposit  NULL    NULL                    0\n167               0   No Deposit     1    NULL                    0\n168               1   No Deposit     1    NULL                    0\n169               2   No Deposit  NULL      40                    0\n170               1   No Deposit  NULL    NULL                    0\n171               2   No Deposit    50    NULL                    0\n172               0   No Deposit     7    NULL                    0\n173               0   No Deposit    19    NULL                    0\n174               0   No Deposit    19    NULL                    0\n175               0   No Deposit  NULL    NULL                    0\n176               0   No Deposit    14    NULL                    0\n177               1   No Deposit    14    NULL                    0\n178               1   No Deposit    45    NULL                    0\n179               1   No Deposit  NULL    NULL                    0\n180               1   No Deposit    28    NULL                    0\n181               0   No Deposit    28    NULL                    0\n182               0   No Deposit    28    NULL                    0\n183               0   No Deposit    28    NULL                    0\n184               1   No Deposit  NULL    NULL                    0\n185               1   No Deposit  NULL    NULL                    0\n186               0   No Deposit     8    NULL                    0\n187               1   No Deposit  NULL    NULL                    0\n188               0   No Deposit     9    NULL                    0\n189               0   No Deposit    45    NULL                    0\n190               0   No Deposit     9    NULL                    0\n191               4   No Deposit  NULL    NULL                    0\n192               1   No Deposit     7    NULL                    0\n193               1   No Deposit     9    NULL                    0\n194               1   No Deposit     9    NULL                    0\n195               1   No Deposit     9    NULL                    0\n196               0   No Deposit  NULL    NULL                    0\n197               0   No Deposit  NULL    NULL                    0\n198               1   No Deposit  NULL    NULL                    0\n199               2   No Deposit    29    NULL                    0\n200               1   No Deposit  NULL    NULL                    0\n201               0   No Deposit     9    NULL                    0\n202               0   No Deposit    20    NULL                    0\n203              21   No Deposit  NULL     215                    0\n204               4   No Deposit  NULL    NULL                    0\n205               3   No Deposit    52    NULL                    0\n206               0   No Deposit     9    NULL                    0\n207               2   No Deposit    14    NULL                    0\n208               0   No Deposit     9    NULL                    0\n209               1   No Deposit     9    NULL                    0\n210               1   No Deposit    94    NULL                    0\n211               0   No Deposit    94    NULL                    0\n212               1   No Deposit    14    NULL                    0\n213               0   No Deposit     9    NULL                    0\n214               0   No Deposit     9    NULL                    0\n215               0   No Deposit     9    NULL                    0\n216               2   No Deposit     9    NULL                    0\n217               3   No Deposit    37    NULL                    0\n218               0   No Deposit     9    NULL                    0\n219               0   No Deposit     9    NULL                    0\n220               1   No Deposit     9    NULL                    0\n221               0   No Deposit     9    NULL                    0\n222               0   No Deposit     9    NULL                    0\n223               0   No Deposit    89    NULL                    0\n224               2   No Deposit  NULL      40                    0\n225               2   No Deposit    11    NULL                    0\n226               2   No Deposit    14    NULL                    0\n227               1   No Deposit    14    NULL                    0\n228               0   No Deposit    14    NULL                    0\n229               0   No Deposit     9    NULL                    0\n230               0   No Deposit  NULL    NULL                    0\n231               0   No Deposit  NULL    NULL                    0\n232               0   No Deposit     9    NULL                    0\n233               0   No Deposit     9    NULL                    0\n234               0   No Deposit    14    NULL                    0\n235               0   No Deposit    14    NULL                    0\n236               0   No Deposit     9    NULL                    0\n237               0   No Deposit    14    NULL                    0\n238               2   No Deposit     9    NULL                    0\n239               4   No Deposit  NULL    NULL                    0\n240               0   No Deposit     9    NULL                    0\n241               0   No Deposit     9    NULL                    0\n242               0   No Deposit    14    NULL                    0\n243               0   No Deposit     9    NULL                    0\n244               1   No Deposit     9    NULL                    0\n245               0   No Deposit     9    NULL                    0\n246               0   No Deposit     9    NULL                    0\n247               5   No Deposit     9    NULL                    0\n248               0   No Deposit     9    NULL                    0\n249               3   No Deposit  NULL    NULL                    0\n250               0   No Deposit     9    NULL                    0\n251               0   No Deposit     9    NULL                    0\n252               0   No Deposit     9    NULL                    0\n253               0   No Deposit     9    NULL                    0\n254               2   No Deposit     9    NULL                    0\n255               1   No Deposit    52    NULL                    0\n256               2   No Deposit     9    NULL                    0\n257               1   No Deposit     9    NULL                    0\n258               4   No Deposit  NULL    NULL                    0\n259               1   No Deposit     9    NULL                    0\n260               0   No Deposit     9    NULL                    0\n261               5   No Deposit  NULL     288                    6\n262               2   No Deposit     7    NULL                    0\n263               0   No Deposit     9    NULL                    0\n264               2   No Deposit     9    NULL                    0\n265               1   No Deposit     9    NULL                    0\n266               2   No Deposit  NULL     314                    0\n267               0   No Deposit     9    NULL                    0\n268               3   No Deposit    12    NULL                   68\n269               0   No Deposit     9    NULL                    0\n270               1   No Deposit    14    NULL                    0\n271               3   No Deposit    21    NULL                   92\n272               0   No Deposit     9    NULL                    0\n273               0   No Deposit     9    NULL                    0\n274               0   No Deposit     9    NULL                    0\n275               2   No Deposit    39    NULL                    0\n276               3   No Deposit     9    NULL                    0\n277               0   No Deposit     9    NULL                    0\n278               0   No Deposit     9    NULL                    0\n279               7   No Deposit   193    NULL                    0\n280               3   No Deposit     9    NULL                    0\n281               0   No Deposit     9    NULL                    0\n282               1   No Deposit    37    NULL                    0\n283               0   No Deposit     9    NULL                    0\n284               1   No Deposit     9    NULL                    0\n285               0   No Deposit     9    NULL                    0\n286               0   No Deposit     9    NULL                    0\n287               0   No Deposit    14    NULL                    0\n288               3   No Deposit     9    NULL                    0\n289              12   No Deposit     9    NULL                    0\n290               0   No Deposit     9    NULL                    0\n291               0   No Deposit     9    NULL                    0\n292               1   No Deposit     9    NULL                    0\n293               1   No Deposit     9    NULL                    0\n294               0   No Deposit     9    NULL                    0\n295               0   No Deposit  NULL      40                    0\n296               2   No Deposit  NULL     233                    0\n297               3   No Deposit    25    NULL                    0\n298               0   No Deposit     9    NULL                    0\n299               0   No Deposit    28    NULL                    0\n300               4   No Deposit    11    NULL                    0\n301               0   No Deposit    28    NULL                    0\n302               0   No Deposit     9    NULL                    0\n303               6   No Deposit   371    NULL                    0\n304               0   No Deposit     9    NULL                    0\n305               0   No Deposit     9    NULL                    0\n306               0   No Deposit    27    NULL                    0\n307               0   No Deposit     9    NULL                    0\n308               0   No Deposit     9    NULL                    0\n309               0   No Deposit     7    NULL                    0\n310               1   No Deposit     9    NULL                    0\n311               0   No Deposit     9    NULL                    0\n312               0   No Deposit     9    NULL                    0\n313               1   No Deposit     7    NULL                    0\n314               0   No Deposit    28    NULL                    0\n315               0   No Deposit     9    NULL                    0\n316               7   No Deposit     3    NULL                    0\n317               1   No Deposit     9    NULL                    0\n318               0   No Deposit     9    NULL                    0\n319               0   No Deposit     9    NULL                    0\n320               3   No Deposit    79    NULL                    0\n321               0   No Deposit     9    NULL                    0\n322               0   No Deposit     9    NULL                    0\n323               2   No Deposit  NULL    NULL                    0\n324               0   No Deposit     9    NULL                    0\n325               0   No Deposit     9    NULL                    0\n326               2   No Deposit     9    NULL                    0\n327               0   No Deposit     9    NULL                    0\n328               0   No Deposit     9    NULL                    0\n329               1   No Deposit  NULL    NULL                    0\n330               0   No Deposit     9    NULL                    0\n331               0   No Deposit     9    NULL                    0\n332               1   No Deposit  NULL    NULL                    0\n333               1   No Deposit     7    NULL                    0\n334               0   No Deposit     7    NULL                    0\n335               0   No Deposit     7    NULL                    0\n336               0   No Deposit    85    NULL                    0\n337              14   No Deposit     9    NULL                    0\n338               8   No Deposit     9    NULL                    0\n339               0   No Deposit     9    NULL                    0\n340               0   No Deposit     9    NULL                    0\n341               0   No Deposit     9    NULL                    0\n342               0   No Deposit     9    NULL                    0\n343               1   No Deposit    86    NULL                    0\n344               4   No Deposit    28    NULL                    0\n345               0   No Deposit     9    NULL                    0\n346               1   No Deposit    22    NULL                    0\n347               1   No Deposit    28    NULL                    0\n348               0   No Deposit    15    NULL                    0\n349               1   No Deposit   354    NULL                    0\n350               0   No Deposit   132    NULL                    0\n351               5   No Deposit     9    NULL                    0\n352               0   No Deposit     9    NULL                    0\n353               2   No Deposit     9    NULL                    0\n354               8   No Deposit  NULL    NULL                    0\n355               0   No Deposit   155    NULL                    0\n356               0   No Deposit    86    NULL                    0\n357               0   No Deposit    86    NULL                    0\n358               1   No Deposit    14    NULL                    0\n359               0   No Deposit    14    NULL                    0\n360              14   No Deposit  NULL    NULL                    0\n361               1   No Deposit     9    NULL                    0\n362               0   No Deposit     9    NULL                    0\n363               0   No Deposit     9    NULL                    0\n364               1   No Deposit     9    NULL                    0\n365               1   No Deposit    14    NULL                    0\n366               1   No Deposit     9    NULL                    0\n367               1   No Deposit     9    NULL                    0\n368               1   No Deposit  NULL     460                    0\n369               0   No Deposit     9    NULL                    0\n370               0   No Deposit  NULL    NULL                    0\n371               7   No Deposit   290    NULL                    0\n372               3   No Deposit    87    NULL                    0\n373               3   No Deposit    39    NULL                    0\n374               2   No Deposit   459    NULL                    0\n375               1   No Deposit  NULL     169                    0\n376               4   No Deposit  NULL    NULL                    0\n377               3   No Deposit   229    NULL                    0\n378               2   No Deposit  NULL    NULL                    0\n379               0   No Deposit     9    NULL                    0\n380               5   No Deposit    83    NULL                    0\n381               0   No Deposit   229    NULL                    0\n382               1   No Deposit     9    NULL                    0\n383               1   No Deposit     9    NULL                    0\n384               1   No Deposit     9    NULL                    0\n385               2   No Deposit     9    NULL                    0\n386               1   No Deposit  NULL     153                    0\n387               1   No Deposit  NULL     153                    0\n388               1   No Deposit     7    NULL                    0\n389               0   No Deposit  NULL    NULL                    0\n390               0   No Deposit     9    NULL                    0\n391               0   No Deposit     9    NULL                    0\n392               2   No Deposit   425    NULL                    0\n393               1   No Deposit     9    NULL                    0\n394               0   No Deposit  NULL    NULL                    0\n395               1   No Deposit  NULL    NULL                    0\n396               0   No Deposit    52    NULL                    0\n397               0   No Deposit     9    NULL                    0\n398               0   No Deposit     9    NULL                    0\n399               0   No Deposit     9    NULL                    0\n400               1   No Deposit     9    NULL                    0\n401               0   No Deposit     9    NULL                    0\n402               1   No Deposit     9    NULL                    0\n403               1   No Deposit  NULL    NULL                    0\n      customer_type    adr required_car_parking_spaces\n1   Transient-Party   0.00                           0\n2         Transient   0.00                           0\n3   Transient-Party   0.00                           0\n4   Transient-Party   0.00                           0\n5   Transient-Party   0.00                           0\n6         Transient   0.00                           0\n7             Group   0.00                           0\n8         Transient  28.00                           1\n9   Transient-Party   0.00                           0\n10        Transient   0.00                           0\n11  Transient-Party   0.00                           0\n12  Transient-Party   0.00                           0\n13        Transient   0.00                           0\n14  Transient-Party   0.00                           0\n15         Contract   0.00                           0\n16  Transient-Party   9.00                           0\n17        Transient   0.00                           0\n18  Transient-Party   6.00                           0\n19  Transient-Party   0.00                           0\n20  Transient-Party   6.00                           0\n21        Transient   0.00                           0\n22  Transient-Party   0.00                           0\n23        Transient   0.00                           0\n24        Transient   0.00                           0\n25        Transient   0.00                           0\n26  Transient-Party   6.00                           0\n27  Transient-Party 104.25                           0\n28  Transient-Party  77.00                           0\n29  Transient-Party   6.00                           0\n30  Transient-Party  77.25                           0\n31  Transient-Party  83.58                           0\n32  Transient-Party  15.00                           0\n33  Transient-Party   6.00                           0\n34  Transient-Party   6.00                           0\n35        Transient   6.00                           0\n36        Transient   6.00                           0\n37        Transient   6.67                           0\n38  Transient-Party  77.25                           0\n39  Transient-Party  77.25                           0\n40        Transient  69.53                           0\n41  Transient-Party  69.50                           0\n42        Transient  63.12                           0\n43  Transient-Party  66.50                           0\n44  Transient-Party  69.50                           0\n45        Transient   0.00                           0\n46  Transient-Party  67.15                           0\n47        Transient   0.00                           0\n48        Transient  69.50                           0\n49  Transient-Party  69.53                           0\n50  Transient-Party  56.27                           0\n51        Transient  65.66                           0\n52  Transient-Party  85.96                           0\n53  Transient-Party  78.00                           0\n54        Transient  78.00                           0\n55  Transient-Party  75.00                           0\n56  Transient-Party 120.00                           0\n57        Transient  79.80                           0\n58  Transient-Party  89.92                           0\n59  Transient-Party  78.00                           0\n60        Transient  73.74                           0\n61  Transient-Party   6.00                           0\n62  Transient-Party  73.74                           0\n63  Transient-Party   0.00                           0\n64  Transient-Party  81.50                           0\n65        Transient 108.38                           0\n66        Transient  50.87                           0\n67        Transient  50.87                           0\n68        Transient 119.50                           0\n69        Transient 128.49                           0\n70  Transient-Party  95.82                           0\n71  Transient-Party   6.00                           0\n72        Transient  92.12                           0\n73        Transient  81.82                           0\n74        Transient   0.00                           0\n75        Transient   0.00                           0\n76        Transient   0.00                           0\n77        Transient   0.00                           0\n78  Transient-Party  86.50                           0\n79  Transient-Party  86.50                           0\n80  Transient-Party  86.50                           0\n81        Transient  86.63                           0\n82  Transient-Party  86.50                           0\n83  Transient-Party  86.50                           0\n84  Transient-Party  86.50                           0\n85        Transient   9.00                           0\n86  Transient-Party  86.50                           0\n87  Transient-Party   6.00                           0\n88        Transient  79.27                           0\n89        Transient 118.15                           0\n90  Transient-Party  86.50                           0\n91  Transient-Party  86.50                           0\n92        Transient   0.00                           0\n93  Transient-Party  81.50                           0\n94        Transient  99.30                           0\n95        Transient  57.30                           0\n96  Transient-Party  94.62                           0\n97        Transient  86.63                           0\n98        Transient 108.66                           0\n99  Transient-Party 121.01                           0\n100       Transient  96.23                           0\n101       Transient 106.90                           0\n102       Transient 109.45                           0\n103       Transient 124.25                           0\n104       Transient 124.25                           0\n105       Transient 127.38                           0\n106       Transient  83.50                           0\n107 Transient-Party  72.00                           0\n108       Transient   0.00                           0\n109       Transient  82.44                           0\n110       Transient  77.86                           0\n111       Transient  76.16                           0\n112       Transient  67.26                           0\n113 Transient-Party  71.22                           0\n114       Transient   0.00                           0\n115       Transient   0.00                           0\n116       Transient   0.00                           0\n117 Transient-Party   0.00                           0\n118       Transient  69.50                           0\n119 Transient-Party 101.34                           0\n120       Transient  74.29                           0\n121       Transient  91.60                           0\n122       Transient 141.14                           0\n123       Transient   0.00                           0\n124       Transient  91.60                           0\n125 Transient-Party   0.00                           0\n126       Transient  82.44                           0\n127       Transient  91.60                           0\n128       Transient  91.60                           0\n129       Transient   0.00                           0\n130       Transient  89.88                           0\n131       Transient  77.86                           0\n132       Transient  77.86                           0\n133       Transient  84.73                           0\n134       Transient  90.46                           0\n135       Transient 101.25                           0\n136       Transient 110.68                           0\n137       Transient 103.05                           0\n138       Transient   0.00                           0\n139       Transient   0.00                           0\n140       Transient   0.00                           0\n141       Transient   0.00                           0\n142       Transient 101.25                           0\n143       Transient 103.05                           0\n144       Transient 103.05                           0\n145       Transient 108.95                           0\n146       Transient 110.75                           0\n147       Transient 108.95                           0\n148       Transient  99.45                           0\n149       Transient  99.45                           0\n150       Transient  99.45                           0\n151 Transient-Party   0.00                           0\n152       Transient 106.35                           0\n153       Transient  93.65                           0\n154       Transient  91.85                           0\n155       Transient  91.85                           0\n156       Transient  91.85                           0\n157       Transient  90.05                           0\n158       Transient   0.00                           0\n159       Transient   0.00                           0\n160       Transient  96.25                           0\n161       Transient 104.05                           0\n162 Transient-Party   0.00                           0\n163           Group   0.00                           0\n164        Contract   0.00                           0\n165        Contract   0.00                           0\n166       Transient   0.00                           0\n167 Transient-Party   0.00                           0\n168 Transient-Party   0.00                           0\n169       Transient   6.00                           0\n170       Transient   0.00                           0\n171 Transient-Party   0.00                           0\n172 Transient-Party  65.29                           0\n173       Transient   0.00                           0\n174       Transient   0.00                           0\n175       Transient   0.00                           0\n176       Transient  87.08                           0\n177       Transient  80.75                           0\n178       Transient   0.00                           0\n179 Transient-Party   7.80                           0\n180       Transient   0.00                           0\n181       Transient   0.00                           0\n182        Contract   0.00                           0\n183       Transient   0.00                           0\n184       Transient   6.00                           0\n185       Transient   0.00                           0\n186 Transient-Party   6.00                           0\n187 Transient-Party   9.00                           0\n188 Transient-Party  77.25                           0\n189       Transient   0.00                           0\n190 Transient-Party  73.39                           0\n191       Transient   0.00                           0\n192           Group  58.91                           0\n193       Transient   0.00                           0\n194       Transient  68.76                           0\n195 Transient-Party  71.00                           0\n196       Transient   0.00                           0\n197       Transient   0.00                           0\n198       Transient   0.00                           0\n199       Transient   3.38                           1\n200        Contract   6.00                           0\n201 Transient-Party   6.00                           1\n202 Transient-Party   9.00                           0\n203 Transient-Party   0.00                           0\n204 Transient-Party   0.00                           0\n205       Transient   0.00                           0\n206       Transient 127.83                           0\n207 Transient-Party 136.46                           0\n208       Transient 127.39                           0\n209       Transient 127.60                           0\n210       Transient   0.00                           0\n211       Transient   0.00                           0\n212       Transient 128.78                           0\n213 Transient-Party 112.01                           0\n214       Transient 112.01                           0\n215       Transient 116.49                           0\n216       Transient 116.49                           0\n217 Transient-Party   0.00                           0\n218       Transient 119.50                           0\n219       Transient 131.78                           0\n220       Transient   0.00                           0\n221       Transient  83.83                           0\n222 Transient-Party  97.55                           0\n223       Transient  77.53                           0\n224       Transient   1.00                           1\n225       Transient   0.00                           0\n226       Transient   0.00                           0\n227       Transient   0.00                           0\n228 Transient-Party   6.00                           0\n229       Transient  97.54                           0\n230       Transient   0.00                           0\n231       Transient   0.00                           0\n232       Transient  97.54                           0\n233       Transient 106.54                           0\n234       Transient   0.00                           0\n235 Transient-Party   6.00                           0\n236 Transient-Party 127.38                           0\n237       Transient  90.25                           0\n238       Transient  69.50                           0\n239       Transient   0.00                           0\n240       Transient 109.83                           0\n241 Transient-Party 138.05                           0\n242 Transient-Party 120.63                           0\n243       Transient 109.55                           0\n244       Transient 106.32                           0\n245       Transient  97.54                           0\n246       Transient 127.60                           0\n247       Transient  32.85                           0\n248       Transient  97.54                           0\n249 Transient-Party  53.40                           0\n250       Transient 115.68                           0\n251 Transient-Party 109.83                           0\n252       Transient  95.79                           0\n253 Transient-Party  88.77                           0\n254       Transient  84.82                           0\n255       Transient   0.00                           0\n256       Transient  89.20                           0\n257 Transient-Party 112.01                           0\n258 Transient-Party   0.00                           0\n259 Transient-Party 124.25                           0\n260       Transient 155.68                           0\n261 Transient-Party   0.00                           0\n262       Transient   0.00                           0\n263       Transient 127.38                           0\n264       Transient  16.92                           0\n265       Transient 108.00                           0\n266       Transient   0.00                           0\n267       Transient 108.00                           0\n268 Transient-Party   0.00                           0\n269       Transient 110.70                           0\n270       Transient   0.00                           0\n271 Transient-Party   0.00                           0\n272       Transient 110.36                           0\n273 Transient-Party 108.00                           0\n274       Transient 110.70                           0\n275 Transient-Party   0.00                           0\n276 Transient-Party  88.77                           0\n277       Transient  83.83                           0\n278       Transient   0.00                           0\n279 Transient-Party   0.00                           0\n280       Transient  18.60                           0\n281       Transient  81.50                           0\n282       Transient   0.00                           0\n283 Transient-Party  81.76                           0\n284       Transient  82.92                           0\n285       Transient  83.50                           0\n286       Transient   0.00                           0\n287       Transient 165.38                           0\n288       Transient 168.30                           0\n289       Transient   8.34                           0\n290       Transient   0.00                           0\n291       Transient   0.00                           0\n292       Transient  97.40                           1\n293       Transient  92.67                           0\n294       Transient   0.00                           0\n295       Transient   0.00                           0\n296       Transient   0.00                           0\n297 Transient-Party   0.00                           0\n298       Transient   0.00                           0\n299       Transient   0.00                           0\n300           Group   6.40                           0\n301       Transient   0.00                           0\n302       Transient   0.00                           0\n303 Transient-Party   0.00                           0\n304       Transient   0.00                           0\n305       Transient  77.86                           0\n306       Transient   0.00                           0\n307       Transient  77.86                           0\n308       Transient   0.00                           0\n309       Transient   0.00                           0\n310       Transient  82.44                           0\n311 Transient-Party  91.60                           0\n312       Transient  82.44                           0\n313       Transient   0.00                           0\n314       Transient   0.00                           0\n315       Transient  77.86                           0\n316 Transient-Party  43.80                           0\n317       Transient  91.60                           0\n318       Transient  91.60                           0\n319       Transient  67.26                           0\n320       Transient  20.88                           0\n321       Transient   0.00                           0\n322       Transient  91.60                           0\n323       Transient   1.29                           0\n324       Transient   0.00                           0\n325       Transient  82.44                           0\n326       Transient  27.69                           0\n327       Transient  86.86                           0\n328       Transient  77.86                           0\n329 Transient-Party   6.00                           1\n330       Transient   0.00                           0\n331       Transient   0.00                           0\n332 Transient-Party   6.00                           0\n333       Transient   0.00                           0\n334       Transient   0.00                           0\n335       Transient   0.00                           0\n336       Transient   0.00                           0\n337       Transient   0.00                           0\n338       Transient  35.81                           0\n339       Transient   0.00                           0\n340       Transient   0.00                           0\n341       Transient   0.00                           0\n342       Transient  76.16                           0\n343       Transient  86.40                           0\n344       Transient   0.00                           0\n345       Transient   0.00                           0\n346       Transient   0.00                           0\n347       Transient   0.00                           0\n348       Transient   0.00                           0\n349       Transient   0.00                           0\n350       Transient   0.00                           0\n351       Transient   8.00                           0\n352       Transient   0.00                           0\n353       Transient   0.00                           1\n354       Transient   0.00                           0\n355       Transient   0.00                           0\n356       Transient   0.00                           0\n357       Transient   0.00                           0\n358       Transient  93.82                           0\n359 Transient-Party  98.55                           0\n360       Transient   0.00                           0\n361       Transient 103.05                           0\n362       Transient 103.05                           0\n363       Transient 103.05                           0\n364       Transient 103.05                           0\n365       Transient   0.00                           0\n366       Transient 101.70                           0\n367       Transient   0.00                           0\n368       Transient   0.00                           0\n369       Transient 114.50                           0\n370       Transient   0.00                           0\n371       Transient   0.00                           0\n372       Transient 200.00                           0\n373 Transient-Party   0.00                           0\n374 Transient-Party   0.00                           0\n375 Transient-Party   0.00                           0\n376       Transient   0.00                           0\n377 Transient-Party   0.00                           0\n378       Transient   0.00                           1\n379       Transient 123.05                           0\n380       Transient 129.60                           0\n381 Transient-Party   0.00                           0\n382 Transient-Party 107.45                           0\n383 Transient-Party 109.00                           0\n384 Transient-Party 109.00                           0\n385       Transient  90.00                           0\n386       Transient   0.00                           0\n387       Transient   0.00                           0\n388       Transient 100.80                           0\n389       Transient   0.00                           1\n390       Transient  93.65                           0\n391       Transient  91.85                           0\n392       Transient  73.80                           0\n393 Transient-Party  22.86                           0\n394 Transient-Party  91.40                           0\n395 Transient-Party   0.00                           0\n396       Transient   0.00                           0\n397       Transient  90.05                           0\n398       Transient  91.85                           0\n399       Transient  98.85                           0\n400       Transient  93.64                           0\n401       Transient  98.85                           0\n402       Transient 121.88                           0\n403 Transient-Party   6.00                           0\n    total_of_special_requests reservation_status reservation_status_date\n1                           0          Check-Out              2015-10-06\n2                           0          Check-Out              2015-10-12\n3                           0          Check-Out              2015-11-23\n4                           0          Check-Out              2016-01-04\n5                           0          Check-Out              2016-01-05\n6                           0           Canceled              2016-02-15\n7                           0           Canceled              2016-11-21\n8                           0          Check-Out              2017-01-06\n9                           0          Check-Out              2017-01-15\n10                          0          Check-Out              2017-02-06\n11                          0          Check-Out              2017-04-07\n12                          0          Check-Out              2017-04-07\n13                          0          Check-Out              2017-06-25\n14                          0          Check-Out              2015-07-30\n15                          0          Check-Out              2015-08-11\n16                          0          Check-Out              2015-08-12\n17                          0          Check-Out              2015-09-08\n18                          2          Check-Out              2015-08-14\n19                          1          Check-Out              2015-08-15\n20                          1          Check-Out              2015-08-18\n21                          1          Check-Out              2015-08-29\n22                          0           Canceled              2015-09-05\n23                          1          Check-Out              2015-09-12\n24                          0          Check-Out              2015-09-13\n25                          0          Check-Out              2015-09-12\n26                          1           Canceled              2015-09-02\n27                          1           Canceled              2015-12-10\n28                          2          Check-Out              2015-12-19\n29                          0          Check-Out              2015-12-26\n30                          1           Canceled              2015-12-19\n31                          0          Check-Out              2015-12-29\n32                          1          Check-Out              2016-01-04\n33                          1          Check-Out              2016-01-08\n34                          0          Check-Out              2016-01-08\n35                          1          Check-Out              2016-01-17\n36                          1          Check-Out              2016-01-17\n37                          1          Check-Out              2016-01-17\n38                          0           Canceled              2016-01-11\n39                          0           Canceled              2016-01-04\n40                          0           Canceled              2016-01-18\n41                          1          Check-Out              2016-02-08\n42                          0          Check-Out              2016-02-12\n43                          1           Canceled              2016-01-26\n44                          1          Check-Out              2016-02-12\n45                          1            No-Show              2016-02-11\n46                          0          Check-Out              2016-02-15\n47                          2          Check-Out              2016-02-13\n48                          1          Check-Out              2016-02-15\n49                          1          Check-Out              2016-02-24\n50                          0          Check-Out              2016-02-27\n51                          0           Canceled              2016-02-20\n52                          1           Canceled              2016-03-01\n53                          0           Canceled              2016-01-11\n54                          0           Canceled              2016-03-18\n55                          1          Check-Out              2016-03-30\n56                          1          Check-Out              2016-03-29\n57                          2          Check-Out              2016-03-29\n58                          0           Canceled              2016-01-13\n59                          1          Check-Out              2016-04-03\n60                          1          Check-Out              2016-04-07\n61                          1           Canceled              2016-03-28\n62                          0          Check-Out              2016-04-08\n63                          0           Canceled              2016-03-29\n64                          1          Check-Out              2016-04-17\n65                          1           Canceled              2016-02-17\n66                          0          Check-Out              2016-05-11\n67                          0          Check-Out              2016-05-11\n68                          0           Canceled              2016-05-17\n69                          0           Canceled              2016-05-05\n70                          1          Check-Out              2016-06-03\n71                          1           Canceled              2016-05-04\n72                          0           Canceled              2016-06-23\n73                          2          Check-Out              2016-07-17\n74                          1          Check-Out              2016-07-10\n75                          0           Canceled              2016-02-23\n76                          0           Canceled              2016-02-23\n77                          0           Canceled              2016-02-23\n78                          1          Check-Out              2016-07-09\n79                          0           Canceled              2016-02-20\n80                          1          Check-Out              2016-07-09\n81                          0           Canceled              2015-12-30\n82                          0          Check-Out              2016-07-16\n83                          0          Check-Out              2016-07-18\n84                          0          Check-Out              2016-07-22\n85                          2           Canceled              2016-06-02\n86                          1           Canceled              2016-01-26\n87                          1          Check-Out              2016-08-03\n88                          1           Canceled              2016-06-04\n89                          0           Canceled              2016-06-29\n90                          1           Canceled              2016-07-14\n91                          0           Canceled              2016-06-27\n92                          1           Canceled              2016-07-30\n93                          1           Canceled              2016-07-21\n94                          1           Canceled              2016-05-10\n95                          0          Check-Out              2016-08-29\n96                          1           Canceled              2016-02-27\n97                          1           Canceled              2016-01-13\n98                          0           Canceled              2016-08-11\n99                          0           Canceled              2016-09-01\n100                         0           Canceled              2016-07-04\n101                         0           Canceled              2016-08-02\n102                         2           Canceled              2016-09-09\n103                         0            No-Show              2016-09-12\n104                         2            No-Show              2016-09-16\n105                         3           Canceled              2016-07-08\n106                         2           Canceled              2016-09-05\n107                         0           Canceled              2016-03-10\n108                         0           Canceled              2016-11-17\n109                         2           Canceled              2016-10-27\n110                         1           Canceled              2016-11-11\n111                         0           Canceled              2016-08-03\n112                         0           Canceled              2016-08-03\n113                         0           Canceled              2016-06-24\n114                         0           Canceled              2016-12-06\n115                         0           Canceled              2016-12-06\n116                         0           Canceled              2016-12-06\n117                         1           Canceled              2016-12-09\n118                         1           Canceled              2015-12-23\n119                         0           Canceled              2016-09-22\n120                         2           Canceled              2016-10-13\n121                         2           Canceled              2016-11-22\n122                         1           Canceled              2016-06-13\n123                         0           Canceled              2017-01-25\n124                         2           Canceled              2016-09-13\n125                         0           Canceled              2017-02-17\n126                         1           Canceled              2017-02-02\n127                         1           Canceled              2017-02-03\n128                         1           Canceled              2016-11-22\n129                         0           Canceled              2017-03-16\n130                         1           Canceled              2016-11-22\n131                         2           Canceled              2016-09-17\n132                         2           Canceled              2016-09-17\n133                         0           Canceled              2017-02-02\n134                         2           Canceled              2016-10-31\n135                         0           Canceled              2017-01-19\n136                         1           Canceled              2017-01-25\n137                         3           Canceled              2016-11-14\n138                         0           Canceled              2017-04-10\n139                         0           Canceled              2017-04-10\n140                         0           Canceled              2017-04-10\n141                         0           Canceled              2017-04-10\n142                         0           Canceled              2016-09-15\n143                         0           Canceled              2017-03-24\n144                         3           Canceled              2017-04-07\n145                         2           Canceled              2017-01-06\n146                         1           Canceled              2017-01-17\n147                         2           Canceled              2017-01-06\n148                         0           Canceled              2016-08-15\n149                         0           Canceled              2016-08-14\n150                         0           Canceled              2016-08-15\n151                         1           Canceled              2017-06-03\n152                         1           Canceled              2016-11-14\n153                         1           Canceled              2016-12-12\n154                         2           Canceled              2016-09-19\n155                         2           Canceled              2016-09-19\n156                         2           Canceled              2016-09-19\n157                         2           Canceled              2016-09-17\n158                         0           Canceled              2017-08-11\n159                         0           Canceled              2017-08-11\n160                         0           Canceled              2017-01-20\n161                         3           Canceled              2016-12-09\n162                         0          Check-Out              2015-07-29\n163                         0          Check-Out              2015-08-23\n164                         2          Check-Out              2015-08-27\n165                         2          Check-Out              2015-08-28\n166                         0          Check-Out              2015-10-15\n167                         0          Check-Out              2015-09-30\n168                         0          Check-Out              2015-10-07\n169                         0          Check-Out              2015-12-03\n170                         0          Check-Out              2015-10-15\n171                         0          Check-Out              2015-10-21\n172                         0          Check-Out              2015-10-30\n173                         0           Canceled              2015-11-05\n174                         0           Canceled              2015-11-05\n175                         0          Check-Out              2015-11-11\n176                         1          Check-Out              2015-12-07\n177                         1            No-Show              2015-12-09\n178                         1          Check-Out              2015-12-12\n179                         1           Canceled              2015-12-23\n180                         0          Check-Out              2015-12-24\n181                         1          Check-Out              2015-12-24\n182                         0          Check-Out              2015-12-24\n183                         0          Check-Out              2015-12-24\n184                         1          Check-Out              2017-01-22\n185                         0          Check-Out              2015-12-27\n186                         1           Canceled              2016-01-04\n187                         1          Check-Out              2016-01-10\n188                         0          Check-Out              2016-01-24\n189                         0          Check-Out              2016-02-05\n190                         1          Check-Out              2016-02-09\n191                         0          Check-Out              2016-02-13\n192                         2          Check-Out              2016-02-19\n193                         1          Check-Out              2016-02-22\n194                         0          Check-Out              2016-02-27\n195                         0          Check-Out              2016-02-28\n196                         0          Check-Out              2016-03-16\n197                         1          Check-Out              2016-03-16\n198                         0          Check-Out              2016-03-17\n199                         0          Check-Out              2016-03-24\n200                         2          Check-Out              2016-03-26\n201                         0          Check-Out              2016-04-09\n202                         1          Check-Out              2016-04-16\n203                         0          Check-Out              2016-04-29\n204                         0          Check-Out              2016-05-04\n205                         0          Check-Out              2016-05-04\n206                         1          Check-Out              2016-05-05\n207                         1          Check-Out              2016-05-06\n208                         0          Check-Out              2016-05-16\n209                         0          Check-Out              2016-05-18\n210                         2          Check-Out              2016-12-21\n211                         2          Check-Out              2016-12-21\n212                         0          Check-Out              2016-05-28\n213                         1          Check-Out              2016-06-02\n214                         1          Check-Out              2016-06-02\n215                         0          Check-Out              2016-06-12\n216                         2          Check-Out              2016-06-12\n217                         0          Check-Out              2016-06-13\n218                         1          Check-Out              2016-06-15\n219                         0          Check-Out              2016-06-17\n220                         1          Check-Out              2016-06-21\n221                         1          Check-Out              2016-07-01\n222                         0          Check-Out              2016-07-02\n223                         0          Check-Out              2016-07-12\n224                         0          Check-Out              2017-03-07\n225                         1          Check-Out              2016-07-18\n226                         1          Check-Out              2016-07-20\n227                         0          Check-Out              2016-07-21\n228                         1          Check-Out              2016-07-25\n229                         1          Check-Out              2016-07-26\n230                         0          Check-Out              2016-07-26\n231                         0          Check-Out              2016-07-26\n232                         0          Check-Out              2016-07-28\n233                         1          Check-Out              2016-07-29\n234                         1          Check-Out              2016-07-29\n235                         1          Check-Out              2016-07-31\n236                         1          Check-Out              2016-08-05\n237                         1          Check-Out              2016-08-06\n238                         2          Check-Out              2016-08-09\n239                         0          Check-Out              2016-08-10\n240                         1          Check-Out              2016-08-14\n241                         3          Check-Out              2016-08-15\n242                         0          Check-Out              2016-08-17\n243                         1          Check-Out              2016-08-18\n244                         0          Check-Out              2016-08-18\n245                         1          Check-Out              2016-08-19\n246                         1          Check-Out              2016-08-20\n247                         2          Check-Out              2016-08-20\n248                         1          Check-Out              2016-08-21\n249                         3          Check-Out              2016-08-22\n250                         1          Check-Out              2016-08-22\n251                         2          Check-Out              2016-08-23\n252                         1          Check-Out              2016-08-26\n253                         0          Check-Out              2016-08-29\n254                         0          Check-Out              2016-08-30\n255                         0          Check-Out              2016-09-05\n256                         2          Check-Out              2016-09-05\n257                         1          Check-Out              2016-09-08\n258                         1          Check-Out              2016-09-08\n259                         1          Check-Out              2016-09-20\n260                         4          Check-Out              2016-09-23\n261                         0          Check-Out              2016-09-24\n262                         1          Check-Out              2016-09-24\n263                         3          Check-Out              2016-09-29\n264                         1          Check-Out              2016-09-30\n265                         1          Check-Out              2016-10-08\n266                         0          Check-Out              2016-10-11\n267                         3          Check-Out              2016-10-12\n268                         1          Check-Out              2016-10-12\n269                         4          Check-Out              2016-10-13\n270                         0          Check-Out              2016-10-17\n271                         0          Check-Out              2016-10-29\n272                         2          Check-Out              2016-10-20\n273                         1          Check-Out              2016-10-22\n274                         1          Check-Out              2016-10-23\n275                         0          Check-Out              2016-10-25\n276                         2          Check-Out              2016-10-27\n277                         3          Check-Out              2016-10-28\n278                         1          Check-Out              2016-10-28\n279                         1          Check-Out              2016-10-29\n280                         2          Check-Out              2016-10-30\n281                         2          Check-Out              2016-10-31\n282                         0          Check-Out              2016-11-01\n283                         1          Check-Out              2016-11-02\n284                         3          Check-Out              2016-11-03\n285                         1          Check-Out              2016-11-04\n286                         1          Check-Out              2016-11-11\n287                         2          Check-Out              2016-11-11\n288                         1          Check-Out              2016-11-11\n289                         2          Check-Out              2016-11-18\n290                         1          Check-Out              2016-11-19\n291                         1          Check-Out              2016-11-19\n292                         1          Check-Out              2016-11-21\n293                         2          Check-Out              2016-11-21\n294                         1          Check-Out              2016-11-23\n295                         0          Check-Out              2016-11-23\n296                         0          Check-Out              2016-11-23\n297                         0          Check-Out              2016-11-23\n298                         2          Check-Out              2016-11-24\n299                         0          Check-Out              2016-11-24\n300                         0          Check-Out              2016-12-07\n301                         0          Check-Out              2016-11-26\n302                         1          Check-Out              2016-11-26\n303                         0          Check-Out              2016-11-26\n304                         0          Check-Out              2016-12-03\n305                         2          Check-Out              2016-12-03\n306                         0          Check-Out              2016-12-05\n307                         1          Check-Out              2016-12-06\n308                         1          Check-Out              2016-12-06\n309                         1          Check-Out              2016-12-07\n310                         1          Check-Out              2016-12-10\n311                         0          Check-Out              2016-12-10\n312                         1          Check-Out              2016-12-11\n313                         1          Check-Out              2016-12-18\n314                         0          Check-Out              2016-12-20\n315                         2          Check-Out              2016-12-21\n316                         0          Check-Out              2016-12-26\n317                         2          Check-Out              2016-12-28\n318                         2          Check-Out              2016-12-28\n319                         1          Check-Out              2016-12-29\n320                         0          Check-Out              2016-12-30\n321                         0          Check-Out              2017-01-03\n322                         1          Check-Out              2017-01-05\n323                         2          Check-Out              2017-01-06\n324                         2          Check-Out              2017-01-09\n325                         0          Check-Out              2017-01-09\n326                         2          Check-Out              2017-01-15\n327                         0          Check-Out              2017-01-17\n328                         2          Check-Out              2017-01-21\n329                         1          Check-Out              2017-01-24\n330                         0          Check-Out              2017-01-30\n331                         3          Check-Out              2017-02-02\n332                         0          Check-Out              2017-02-05\n333                         1          Check-Out              2017-02-13\n334                         1          Check-Out              2017-02-15\n335                         1          Check-Out              2017-02-15\n336                         0          Check-Out              2017-02-27\n337                         0          Check-Out              2017-02-27\n338                         0          Check-Out              2017-02-27\n339                         3          Check-Out              2017-02-27\n340                         2          Check-Out              2017-03-01\n341                         2          Check-Out              2017-03-01\n342                         2          Check-Out              2017-03-03\n343                         1          Check-Out              2017-03-04\n344                         0          Check-Out              2017-03-06\n345                         0          Check-Out              2017-03-06\n346                         0          Check-Out              2017-03-07\n347                         0          Check-Out              2017-03-08\n348                         0          Check-Out              2017-03-09\n349                         0          Check-Out              2017-03-13\n350                         0          Check-Out              2017-03-15\n351                         0          Check-Out              2017-03-15\n352                         0          Check-Out              2017-03-17\n353                         1          Check-Out              2017-03-21\n354                         1          Check-Out              2017-03-21\n355                         1          Check-Out              2017-03-24\n356                         0          Check-Out              2017-03-24\n357                         0          Check-Out              2017-03-24\n358                         0          Check-Out              2017-03-28\n359                         0          Check-Out              2017-04-04\n360                         0          Check-Out              2017-04-05\n361                         0          Check-Out              2017-04-11\n362                         1          Check-Out              2017-04-12\n363                         2          Check-Out              2017-04-15\n364                         2          Check-Out              2017-04-15\n365                         1          Check-Out              2017-04-19\n366                         1          Check-Out              2017-04-21\n367                         3           Canceled              2017-04-25\n368                         1          Check-Out              2017-04-25\n369                         3          Check-Out              2017-04-27\n370                         0          Check-Out              2017-05-05\n371                         0          Check-Out              2017-05-05\n372                         0          Check-Out              2017-05-10\n373                         0          Check-Out              2017-05-12\n374                         0          Check-Out              2017-05-14\n375                         0          Check-Out              2017-05-14\n376                         0          Check-Out              2017-05-18\n377                         1          Check-Out              2017-05-24\n378                         0          Check-Out              2017-05-25\n379                         2          Check-Out              2017-05-28\n380                         0          Check-Out              2017-05-28\n381                         1          Check-Out              2017-06-04\n382                         1          Check-Out              2017-06-11\n383                         2          Check-Out              2017-06-14\n384                         2          Check-Out              2017-06-14\n385                         1          Check-Out              2017-06-24\n386                         0          Check-Out              2017-06-28\n387                         0          Check-Out              2017-06-28\n388                         0          Check-Out              2017-06-30\n389                         1          Check-Out              2017-07-01\n390                         1          Check-Out              2017-07-13\n391                         3          Check-Out              2017-07-16\n392                         0          Check-Out              2017-07-17\n393                         1          Check-Out              2017-07-22\n394                         1          Check-Out              2017-07-22\n395                         3          Check-Out              2017-07-26\n396                         0          Check-Out              2017-07-29\n397                         1          Check-Out              2017-07-30\n398                         3          Check-Out              2017-07-30\n399                         1          Check-Out              2017-07-31\n400                         2          Check-Out              2017-08-02\n401                         1          Check-Out              2017-08-02\n402                         1          Check-Out              2017-08-04\n403                         1          Check-Out              2017-08-16\n\n\nCode\nalot_adults\n\n\n          hotel is_canceled lead_time arrival_date_year arrival_date_month\n1  Resort Hotel           1       304              2015          September\n2  Resort Hotel           1       333              2015          September\n3  Resort Hotel           1       336              2015          September\n4  Resort Hotel           1       340              2015          September\n5  Resort Hotel           1       347              2015          September\n6  Resort Hotel           1       349              2015          September\n7  Resort Hotel           1       352              2015          September\n8  Resort Hotel           1       354              2015          September\n9  Resort Hotel           1       361              2015            October\n10 Resort Hotel           1       338              2015            October\n   arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights\n1                        36                         3                       0\n2                        36                         5                       2\n3                        37                         7                       1\n4                        37                        12                       2\n5                        38                        19                       2\n6                        39                        21                       1\n7                        39                        24                       1\n8                        39                        26                       2\n9                        40                         3                       2\n10                       41                         4                       2\n   stays_in_week_nights adults children babies meal country market_segment\n1                     3     40        0      0   BB     PRT         Direct\n2                     5     26        0      0   BB     PRT  Offline TA/TO\n3                     2     50        0      0   BB     PRT         Direct\n4                     5     26        0      0   BB     PRT  Offline TA/TO\n5                     5     26        0      0   BB     PRT  Offline TA/TO\n6                     3     27        0      0   HB     PRT         Direct\n7                     3     27        0      0   HB     PRT         Direct\n8                     5     26        0      0   BB     PRT  Offline TA/TO\n9                     5     26        0      0   BB     PRT  Offline TA/TO\n10                    0     55        0      0   HB     PRT         Direct\n   distribution_channel is_repeated_guest previous_cancellations\n1                Direct                 0                      0\n2                 TA/TO                 0                      0\n3                Direct                 0                      0\n4                 TA/TO                 0                      0\n5                 TA/TO                 0                      0\n6                Direct                 0                      0\n7                Direct                 0                      0\n8                 TA/TO                 0                      0\n9                 TA/TO                 0                      0\n10               Direct                 0                      0\n   previous_bookings_not_canceled reserved_room_type assigned_room_type\n1                               0                  A                  A\n2                               0                  A                  A\n3                               0                  A                  A\n4                               0                  A                  A\n5                               0                  A                  A\n6                               0                  A                  A\n7                               0                  A                  A\n8                               0                  A                  A\n9                               0                  A                  A\n10                              0                  A                  A\n   booking_changes deposit_type agent company days_in_waiting_list\n1                0   No Deposit  NULL    NULL                    0\n2                0   No Deposit    96    NULL                    0\n3                0   No Deposit  NULL    NULL                    0\n4                0   No Deposit    96    NULL                    0\n5                0   No Deposit    96    NULL                    0\n6                0   No Deposit  NULL    NULL                    0\n7                0   No Deposit  NULL    NULL                    0\n8                0   No Deposit    96    NULL                    0\n9                0   No Deposit    96    NULL                    0\n10               0   No Deposit  NULL    NULL                    0\n   customer_type adr required_car_parking_spaces total_of_special_requests\n1          Group   0                           0                         0\n2          Group   0                           0                         0\n3          Group   0                           0                         0\n4          Group   0                           0                         0\n5          Group   0                           0                         0\n6          Group   0                           0                         0\n7          Group   0                           0                         0\n8          Group   0                           0                         0\n9          Group   0                           0                         0\n10         Group   0                           0                         0\n   reservation_status reservation_status_date\n1            Canceled              2015-01-02\n2            Canceled              2015-01-02\n3            Canceled              2015-01-18\n4            Canceled              2015-01-02\n5            Canceled              2015-01-02\n6            Canceled              2015-01-02\n7            Canceled              2015-01-02\n8            Canceled              2015-01-02\n9            Canceled              2015-01-02\n10           Canceled              2015-01-02\n\n\nIn this function, I’m trying to figure out if the hotels are international based off the countries the bookings come from. Here I will admit I’m a bit stuck as to where to go with the interpretation. It appears that both of them have a robust array of international guests.\n\n\nCode\ninternationalhotel <- h_bookings %>%\n select(\"country\") %>%\n distinct()\n\ninternationalhotel\n\n\n    country\n1       PRT\n2       GBR\n3       USA\n4       ESP\n5       IRL\n6       FRA\n7      NULL\n8       ROU\n9       NOR\n10      OMN\n11      ARG\n12      POL\n13      DEU\n14      BEL\n15      CHE\n16       CN\n17      GRC\n18      ITA\n19      NLD\n20      DNK\n21      RUS\n22      SWE\n23      AUS\n24      EST\n25      CZE\n26      BRA\n27      FIN\n28      MOZ\n29      BWA\n30      LUX\n31      SVN\n32      ALB\n33      IND\n34      CHN\n35      MEX\n36      MAR\n37      UKR\n38      SMR\n39      LVA\n40      PRI\n41      SRB\n42      CHL\n43      AUT\n44      BLR\n45      LTU\n46      TUR\n47      ZAF\n48      AGO\n49      ISR\n50      CYM\n51      ZMB\n52      CPV\n53      ZWE\n54      DZA\n55      KOR\n56      CRI\n57      HUN\n58      ARE\n59      TUN\n60      JAM\n61      HRV\n62      HKG\n63      IRN\n64      GEO\n65      AND\n66      GIB\n67      URY\n68      JEY\n69      CAF\n70      CYP\n71      COL\n72      GGY\n73      KWT\n74      NGA\n75      MDV\n76      VEN\n77      SVK\n78      FJI\n79      KAZ\n80      PAK\n81      IDN\n82      LBN\n83      PHL\n84      SEN\n85      SYC\n86      AZE\n87      BHR\n88      NZL\n89      THA\n90      DOM\n91      MKD\n92      MYS\n93      ARM\n94      JPN\n95      LKA\n96      CUB\n97      CMR\n98      BIH\n99      MUS\n100     COM\n101     SUR\n102     UGA\n103     BGR\n104     CIV\n105     JOR\n106     SYR\n107     SGP\n108     BDI\n109     SAU\n110     VNM\n111     PLW\n112     QAT\n113     EGY\n114     PER\n115     MLT\n116     MWI\n117     ECU\n118     MDG\n119     ISL\n120     UZB\n121     NPL\n122     BHS\n123     MAC\n124     TGO\n125     TWN\n126     DJI\n127     STP\n128     KNA\n129     ETH\n130     IRQ\n131     HND\n132     RWA\n133     KHM\n134     MCO\n135     BGD\n136     IMN\n137     TJK\n138     NIC\n139     BEN\n140     VGB\n141     TZA\n142     GAB\n143     GHA\n144     TMP\n145     GLP\n146     KEN\n147     LIE\n148     GNB\n149     MNE\n150     UMI\n151     MYT\n152     FRO\n153     MMR\n154     PAN\n155     BFA\n156     LBY\n157     MLI\n158     NAM\n159     BOL\n160     PRY\n161     BRB\n162     ABW\n163     AIA\n164     SLV\n165     DMA\n166     PYF\n167     GUY\n168     LCA\n169     ATA\n170     GTM\n171     ASM\n172     MRT\n173     NCL\n174     KIR\n175     SDN\n176     ATF\n177     SLE\n178     LAO\n\n\nWith this function, I want to see at which hotel do the guests usually stay in on week nights. I would almost expect people to stay in less in a city hotel when compared to a resort hotel, but it may also depend on what time of year a guest is at the hotel.\n\n\nCode\nh_bookings %>% \n  group_by(hotel) %>%\n  summarise(mean = mean(stays_in_weekend_nights), sd = sd(stays_in_weekend_nights), \n  mode = mode(stays_in_weekend_nights))\n\n\n# A tibble: 2 × 4\n  hotel         mean    sd mode   \n  <chr>        <dbl> <dbl> <chr>  \n1 City Hotel   0.795 0.885 numeric\n2 Resort Hotel 1.19  1.15  numeric\n\n\nCode\navg_lead_time <- h_bookings %>% \n  group_by(hotel) %>%\n  summarise(mean = mean(lead_time), sd = sd(lead_time), median = median(lead_time))\n\navg_lead_time\n\n\n# A tibble: 2 × 4\n  hotel         mean    sd median\n  <chr>        <dbl> <dbl>  <dbl>\n1 City Hotel   110.  111.      74\n2 Resort Hotel  92.7  97.3     57\n\n\nAbove the variable “avg_lead_time” is created to see what the average time between when someone makes a reservation to the time they actually arrive. Guests typically book 17 days more in advance at City Hotel than at Resort Hotel. By looking at the summary from the beginning, it shows there was a max lead time of 737, so someone booked a little over 2 years in advance.\n\nExplain and Interpret\nThe variables “no_adults” and “alot_adults” was interesting to see. I assume “alot_adults” is for conventions of some sort, but all these reservations occurred in September 2015, except for 2 in October of the same year. They were also all cancelled.\nWhat I find even odder is “no_adults”. These are bookings with only children or even babies with no adults present. Now this could have a simple explanation, like parents are in the next room over, but I don’t think that is always the case with 403 instances within the variable.\nI think the most interesting data to look at is the lead time between hotels and maybe even between guests. I just find it interesting how far in advance people may plan a trip, but then what is the story with the walk-ins with 0 days lead time? I look forward to exploring this data set more in other challenges."
  },
  {
    "objectID": "posts/challenge5_ManiShankerKamarapu.html",
    "href": "posts/challenge5_ManiShankerKamarapu.html",
    "title": "Challenge 5",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_ManiShankerKamarapu.html#challenge-overview",
    "href": "posts/challenge5_ManiShankerKamarapu.html#challenge-overview",
    "title": "Challenge 5",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least two univariate visualizations\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\n\nCreate at least one bivariate visualization\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge5_ManiShankerKamarapu.html#read-in-data",
    "href": "posts/challenge5_ManiShankerKamarapu.html#read-in-data",
    "title": "Challenge 5",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\ncereal ⭐\npathogen cost ⭐\nAustralian Marriage ⭐⭐\nAB_NYC_2019.csv ⭐⭐⭐\nrailroads ⭐⭐⭐\nPublic School Characteristics ⭐⭐⭐⭐\nUSA Households ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge5_ManiShankerKamarapu.html#tidy-data-as-needed",
    "href": "posts/challenge5_ManiShankerKamarapu.html#tidy-data-as-needed",
    "title": "Challenge 5",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge5_ManiShankerKamarapu.html#univariate-visualizations",
    "href": "posts/challenge5_ManiShankerKamarapu.html#univariate-visualizations",
    "title": "Challenge 5",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations"
  },
  {
    "objectID": "posts/challenge5_ManiShankerKamarapu.html#bivariate-visualizations",
    "href": "posts/challenge5_ManiShankerKamarapu.html#bivariate-visualizations",
    "title": "Challenge 5",
    "section": "Bivariate Visualization(s)",
    "text": "Bivariate Visualization(s)\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge4-youngsoo choi.html",
    "href": "posts/challenge4-youngsoo choi.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4-youngsoo choi.html#challenge-overview",
    "href": "posts/challenge4-youngsoo choi.html#challenge-overview",
    "title": "Challenge 4",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4-youngsoo choi.html#read-in-data",
    "href": "posts/challenge4-youngsoo choi.html#read-in-data",
    "title": "Challenge 4",
    "section": "Read in data",
    "text": "Read in data\nRead Fed Funds Rate\n\n\nCode\nffr<-read_csv(\"_data/FedFundsRate.csv\")\nffr\n\n\n# A tibble: 904 × 10\n    Year Month   Day Federal F…¹ Feder…² Feder…³ Effec…⁴ Real …⁵ Unemp…⁶ Infla…⁷\n   <dbl> <dbl> <dbl>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  1954     7     1          NA      NA      NA    0.8      4.6     5.8      NA\n 2  1954     8     1          NA      NA      NA    1.22    NA       6        NA\n 3  1954     9     1          NA      NA      NA    1.06    NA       6.1      NA\n 4  1954    10     1          NA      NA      NA    0.85     8       5.7      NA\n 5  1954    11     1          NA      NA      NA    0.83    NA       5.3      NA\n 6  1954    12     1          NA      NA      NA    1.28    NA       5        NA\n 7  1955     1     1          NA      NA      NA    1.39    11.9     4.9      NA\n 8  1955     2     1          NA      NA      NA    1.29    NA       4.7      NA\n 9  1955     3     1          NA      NA      NA    1.35    NA       4.6      NA\n10  1955     4     1          NA      NA      NA    1.43     6.7     4.7      NA\n# … with 894 more rows, and abbreviated variable names\n#   ¹​`Federal Funds Target Rate`, ²​`Federal Funds Upper Target`,\n#   ³​`Federal Funds Lower Target`, ⁴​`Effective Federal Funds Rate`,\n#   ⁵​`Real GDP (Percent Change)`, ⁶​`Unemployment Rate`, ⁷​`Inflation Rate`\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nBriefly describe the data\nThis data set is regarding various national economic variables from July of 1954 to March of 2017. It has 904 rows and 10 columns. But it also has many missing values and the recorded date is not even constant."
  },
  {
    "objectID": "posts/challenge4-youngsoo choi.html#tidy-data-as-needed",
    "href": "posts/challenge4-youngsoo choi.html#tidy-data-as-needed",
    "title": "Challenge 4",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nI Selected data of January 1st and July 1st each year to tidy data. First of all I filtered January 1st data and then filtered July 1st data. And I merged those two data.\n\n\nCode\njan<-filter(ffr, Month==1, Day==1)\njul<-filter(ffr, Month==7, Day==1)\nsffr<-rbind(jan,jul)\nsffr<-arrange(sffr, Year, Month)\nsffr\n\n\n# A tibble: 126 × 10\n    Year Month   Day Federal F…¹ Feder…² Feder…³ Effec…⁴ Real …⁵ Unemp…⁶ Infla…⁷\n   <dbl> <dbl> <dbl>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  1954     7     1          NA      NA      NA    0.8      4.6     5.8    NA  \n 2  1955     1     1          NA      NA      NA    1.39    11.9     4.9    NA  \n 3  1955     7     1          NA      NA      NA    1.68     5.5     4      NA  \n 4  1956     1     1          NA      NA      NA    2.45    -1.5     4      NA  \n 5  1956     7     1          NA      NA      NA    2.75    -0.3     4.4    NA  \n 6  1957     1     1          NA      NA      NA    2.84     2.6     4.2    NA  \n 7  1957     7     1          NA      NA      NA    2.99     4       4.2    NA  \n 8  1958     1     1          NA      NA      NA    2.72   -10       5.8     3.2\n 9  1958     7     1          NA      NA      NA    0.68     9.6     7.5     2.4\n10  1959     1     1          NA      NA      NA    2.48     7.7     6       1.7\n# … with 116 more rows, and abbreviated variable names\n#   ¹​`Federal Funds Target Rate`, ²​`Federal Funds Upper Target`,\n#   ³​`Federal Funds Lower Target`, ⁴​`Effective Federal Funds Rate`,\n#   ⁵​`Real GDP (Percent Change)`, ⁶​`Unemployment Rate`, ⁷​`Inflation Rate`\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge5_EmmaRasmussen.html",
    "href": "posts/challenge5_EmmaRasmussen.html",
    "title": "Challenge 5 Emma Rasmussen",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n###Read in data Reading in the data and saving a original copy of the dataset"
  },
  {
    "objectID": "posts/challenge5_EmmaRasmussen.html#tidy-data-as-needed",
    "href": "posts/challenge5_EmmaRasmussen.html#tidy-data-as-needed",
    "title": "Challenge 5 Emma Rasmussen",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nI think the data is tidy? Each case has a row, each column is a variable, and each value has it’s own cell.\nLooking at summary of variables/dimensions of data/distribution of certain variables\n\nprint(summarytools::dfSummary(AB_NYC_2019,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nAB_NYC_2019\nDimensions: 48895 x 16\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      id\n[numeric]\n      Mean (sd) : 19017143 (10983108)min ≤ med ≤ max:2539 ≤ 19677284 ≤ 36487245IQR (CV) : 19680234 (0.6)\n      48895 distinct values\n      \n      0\n(0.0%)\n    \n    \n      name\n[character]\n      1. Hillside Hotel2. Home away from home3. New york Multi-unit build4. Brooklyn Apartment5. Loft Suite @ The Box Hous6. Private Room7. Artsy Private BR in Fort 8. Private room9. Beautiful Brooklyn Browns10. Cozy Brooklyn Apartment[ 47884 others ]\n      18(0.0%)17(0.0%)16(0.0%)12(0.0%)11(0.0%)11(0.0%)10(0.0%)10(0.0%)8(0.0%)8(0.0%)48758(99.8%)\n      \n      16\n(0.0%)\n    \n    \n      host_id\n[numeric]\n      Mean (sd) : 67620011 (78610967)min ≤ med ≤ max:2438 ≤ 30793816 ≤ 274321313IQR (CV) : 99612390 (1.2)\n      37457 distinct values\n      \n      0\n(0.0%)\n    \n    \n      host_name\n[character]\n      1. Michael2. David3. Sonder (NYC)4. John5. Alex6. Blueground7. Sarah8. Daniel9. Jessica10. Maria[ 11442 others ]\n      417(0.9%)403(0.8%)327(0.7%)294(0.6%)279(0.6%)232(0.5%)227(0.5%)226(0.5%)205(0.4%)204(0.4%)46060(94.2%)\n      \n      21\n(0.0%)\n    \n    \n      neighbourhood_group\n[character]\n      1. Bronx2. Brooklyn3. Manhattan4. Queens5. Staten Island\n      1091(2.2%)20104(41.1%)21661(44.3%)5666(11.6%)373(0.8%)\n      \n      0\n(0.0%)\n    \n    \n      neighbourhood\n[character]\n      1. Williamsburg2. Bedford-Stuyvesant3. Harlem4. Bushwick5. Upper West Side6. Hell's Kitchen7. East Village8. Upper East Side9. Crown Heights10. Midtown[ 211 others ]\n      3920(8.0%)3714(7.6%)2658(5.4%)2465(5.0%)1971(4.0%)1958(4.0%)1853(3.8%)1798(3.7%)1564(3.2%)1545(3.2%)25449(52.0%)\n      \n      0\n(0.0%)\n    \n    \n      latitude\n[numeric]\n      Mean (sd) : 40.7 (0.1)min ≤ med ≤ max:40.5 ≤ 40.7 ≤ 40.9IQR (CV) : 0.1 (0)\n      19048 distinct values\n      \n      0\n(0.0%)\n    \n    \n      longitude\n[numeric]\n      Mean (sd) : -74 (0)min ≤ med ≤ max:-74.2 ≤ -74 ≤ -73.7IQR (CV) : 0 (0)\n      14718 distinct values\n      \n      0\n(0.0%)\n    \n    \n      room_type\n[character]\n      1. Entire home/apt2. Private room3. Shared room\n      25409(52.0%)22326(45.7%)1160(2.4%)\n      \n      0\n(0.0%)\n    \n    \n      price\n[numeric]\n      Mean (sd) : 152.7 (240.2)min ≤ med ≤ max:0 ≤ 106 ≤ 10000IQR (CV) : 106 (1.6)\n      674 distinct values\n      \n      0\n(0.0%)\n    \n    \n      minimum_nights\n[numeric]\n      Mean (sd) : 7 (20.5)min ≤ med ≤ max:1 ≤ 3 ≤ 1250IQR (CV) : 4 (2.9)\n      109 distinct values\n      \n      0\n(0.0%)\n    \n    \n      number_of_reviews\n[numeric]\n      Mean (sd) : 23.3 (44.6)min ≤ med ≤ max:0 ≤ 5 ≤ 629IQR (CV) : 23 (1.9)\n      394 distinct values\n      \n      0\n(0.0%)\n    \n    \n      last_review\n[Date]\n      min : 2011-03-28med : 2019-05-19max : 2019-07-08range : 8y 3m 10d\n      1764 distinct values\n      \n      10052\n(20.6%)\n    \n    \n      reviews_per_month\n[numeric]\n      Mean (sd) : 1.4 (1.7)min ≤ med ≤ max:0 ≤ 0.7 ≤ 58.5IQR (CV) : 1.8 (1.2)\n      937 distinct values\n      \n      10052\n(20.6%)\n    \n    \n      calculated_host_listings_count\n[numeric]\n      Mean (sd) : 7.1 (33)min ≤ med ≤ max:1 ≤ 1 ≤ 327IQR (CV) : 1 (4.6)\n      47 distinct values\n      \n      0\n(0.0%)\n    \n    \n      availability_365\n[numeric]\n      Mean (sd) : 112.8 (131.6)min ≤ med ≤ max:0 ≤ 45 ≤ 365IQR (CV) : 227 (1.2)\n      366 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\ndim(AB_NYC_2019)\n\n[1] 48895    16\n\nprop.table(table(AB_NYC_2019$neighbourhood_group))\n\n\n        Bronx      Brooklyn     Manhattan        Queens Staten Island \n  0.022313120   0.411166786   0.443010533   0.115880969   0.007628592 \n\n\nVariables appear to be in usable form (ex- dates correct) for visualizations. Variable names and values make sense. While completing this assignment there were some variables I wanted to mutate/create a new column to display other subsets of data but it’s getting too late for that at this point and I already have a bunch of visuals\nI am going to work with price and neighborhood group in the next graphs so I am looking at the distribution of values in these columns to figure out the best way to display the data. The first price histogram I made was not super readable (just a couple large columns at beginning of data set so I am filtering the data before graphing)\n\nsummarize(AB_NYC_2019, max(price), min(price), median(price), mean(price))\n\n# A tibble: 1 × 4\n  `max(price)` `min(price)` `median(price)` `mean(price)`\n         <dbl>        <dbl>           <dbl>         <dbl>\n1        10000            0             106          153.\n\nfilter(AB_NYC_2019, price==0)#there are 11 listings where the cost is zero!\n\n# A tibble: 11 × 16\n         id name   host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n      <dbl> <chr>    <dbl> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <dbl>\n 1 18750597 Huge …  8.99e6 Kimber… Brookl… Bedfor…    40.7   -74.0 Privat…     0\n 2 20333471 ★Host…  1.32e8 Anisha  Bronx   East M…    40.8   -73.9 Privat…     0\n 3 20523843 MARTI…  1.58e7 Martia… Brookl… Bushwi…    40.7   -73.9 Privat…     0\n 4 20608117 Sunny…  1.64e6 Lauren  Brookl… Greenp…    40.7   -73.9 Privat…     0\n 5 20624541 Moder…  1.01e7 Aymeric Brookl… Willia…    40.7   -73.9 Entire…     0\n 6 20639628 Spaci…  8.63e7 Adeyemi Brookl… Bedfor…    40.7   -73.9 Privat…     0\n 7 20639792 Conte…  8.63e7 Adeyemi Brookl… Bedfor…    40.7   -73.9 Privat…     0\n 8 20639914 Cozy …  8.63e7 Adeyemi Brookl… Bedfor…    40.7   -73.9 Privat…     0\n 9 20933849 the b…  1.37e7 Qiuchi  Manhat… Murray…    40.8   -74.0 Entire…     0\n10 21291569 Coliv…  1.02e8 Sergii  Brookl… Bushwi…    40.7   -73.9 Shared…     0\n11 21304320 Best …  1.02e8 Sergii  Brookl… Bushwi…    40.7   -73.9 Shared…     0\n# … with 6 more variables: minimum_nights <dbl>, number_of_reviews <dbl>,\n#   last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>, and\n#   abbreviated variable names ¹​host_name, ²​neighbourhood_group,\n#   ³​neighbourhood, ⁴​latitude, ⁵​longitude, ⁶​room_type\n# ℹ Use `colnames()` to see all variable names\n\nfilter(AB_NYC_2019, price>1000)#only 239 listings over $1000\n\n# A tibble: 239 × 16\n        id name    host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n     <dbl> <chr>     <dbl> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <dbl>\n 1  174966 Luxury…  836168 Henry   Manhat… Upper …    40.8   -74.0 Entire…  2000\n 2  273190 6 Bedr…  605463 West V… Manhat… West V…    40.7   -74.0 Entire…  1300\n 3  363673 Beauti…  256239 Tracey  Manhat… Upper …    40.8   -74.0 Privat…  3000\n 4  468613 $ (Pho… 2325861 Cynthia Manhat… Lower …    40.7   -74.0 Privat…  1300\n 5  664047 Lux 2B…  836168 Henry   Manhat… Upper …    40.8   -74.0 Entire…  2000\n 6  826690 Sunny,… 4289240 Lucy    Brookl… Prospe…    40.7   -74.0 Entire…  4000\n 7  893413 Archit… 4751930 Martin  Manhat… East V…    40.7   -74.0 Entire…  2500\n 8 1056256 Beauti…  462379 Loretta Brookl… Carrol…    40.7   -74.0 Entire…  1395\n 9 1300097 Marcel… 4069241 Shannon Brookl… Brookl…    40.7   -74.0 Privat…  1500\n10 1301321 West V… 2214774 Ben An… Manhat… West V…    40.7   -74.0 Entire…  1899\n# … with 229 more rows, 6 more variables: minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>, and\n#   abbreviated variable names ¹​host_name, ²​neighbourhood_group,\n#   ³​neighbourhood, ⁴​latitude, ⁵​longitude, ⁶​room_type\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nfilter(AB_NYC_2019, price>0 & price<1000) #most listings fall in this range, there are 48895 rows in total, and 48586 in this range\n\n# A tibble: 48,586 × 16\n      id name      host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n   <dbl> <chr>       <dbl> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <dbl>\n 1  2539 Clean & …    2787 John    Brookl… Kensin…    40.6   -74.0 Privat…   149\n 2  2595 Skylit M…    2845 Jennif… Manhat… Midtown    40.8   -74.0 Entire…   225\n 3  3647 THE VILL…    4632 Elisab… Manhat… Harlem     40.8   -73.9 Privat…   150\n 4  3831 Cozy Ent…    4869 LisaRo… Brookl… Clinto…    40.7   -74.0 Entire…    89\n 5  5022 Entire A…    7192 Laura   Manhat… East H…    40.8   -73.9 Entire…    80\n 6  5099 Large Co…    7322 Chris   Manhat… Murray…    40.7   -74.0 Entire…   200\n 7  5121 BlissArt…    7356 Garon   Brookl… Bedfor…    40.7   -74.0 Privat…    60\n 8  5178 Large Fu…    8967 Shunic… Manhat… Hell's…    40.8   -74.0 Privat…    79\n 9  5203 Cozy Cle…    7490 MaryEl… Manhat… Upper …    40.8   -74.0 Privat…    79\n10  5238 Cute & C…    7549 Ben     Manhat… Chinat…    40.7   -74.0 Entire…   150\n# … with 48,576 more rows, 6 more variables: minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>, and\n#   abbreviated variable names ¹​host_name, ²​neighbourhood_group,\n#   ³​neighbourhood, ⁴​latitude, ⁵​longitude, ⁶​room_type\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n#creating a new df with above filter\nAB_NYC_2019_filter<-filter(AB_NYC_2019, price>0 & price<1000)"
  },
  {
    "objectID": "posts/challenge5_EmmaRasmussen.html#univariate-visualizations",
    "href": "posts/challenge5_EmmaRasmussen.html#univariate-visualizations",
    "title": "Challenge 5 Emma Rasmussen",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations\n\n#first histogram\nggplot(AB_NYC_2019, aes(price))+geom_histogram()\n\n\n\n#second histogram\nfilter(AB_NYC_2019, price>0 & price<1000) %>% \n  ggplot(aes(price))+\n  geom_histogram(binwidth=10, alpha=0.6, color = 1)+\n  labs(title=\"Cost of NYC Airbnbs (2019)*\", caption=\"*Histogram excludes prices equal to $0 and greater than $1,000\", x=\"Price in $\", y=\"Frequency\")\n\n\n\n#pie chart\npie(table(AB_NYC_2019$neighbourhood_group), main= \"Distribution of NYC Airbnbs by Neighborhood (%) (2019)\", cex=0.8, labels=(prop.table(table(AB_NYC_2019$neighbourhood_group)))*100)\nlegend(\"left\", c(\"Bronx\", \"Brooklyn\", \"Manhattan\", \"Queens\", \"Staten Island\"))\n\n\n\n#I give up on adding values to this legend/fixing decimal points, everywhere says pie charts aren't super useful anyway so I will switch to a bar graph\n\n#bar graph\nggplot(AB_NYC_2019, aes(neighbourhood_group))+\n  geom_bar(alpha=0.85)+\n  ggtitle(\"NYC Airbnbs by Neighborhood (2019)\")+\n  xlab(\"Neighborhood\")+\n  ylab(\"Number of Airbnbs\")\n\n\n\n\n#First Histogram: Ignore this. Shows why I filtered the data before creating my edited histogram #Second Histogram: Here, I excluded prices of 0 dollars and over 1000. Surprisingly there were 11 listings with a cost=0. After a quick google I learned that Airbnb listers may set their price to 0 when renting to family/friends instead of marking it as unavailable. I figured it made sense to exclude this data from the histogram. The max listing cost was 10,000 dollars which stretched out the distribution making it nearly impossible to visualize the data so this histogram uses the filtered data set #PieChart: I tried. Everywhere I looked up to help said pie charts are criticized anyway. Did not want to spend any more time on it. #Bar Graph: Generally better for visualizing distribution of categorical variables than pie chart. Also took 5 min compared to maybe an 1 hour trying to format pie chart (pie() is not a ggplot function)"
  },
  {
    "objectID": "posts/challenge5_EmmaRasmussen.html#bivariate-visualizations",
    "href": "posts/challenge5_EmmaRasmussen.html#bivariate-visualizations",
    "title": "Challenge 5 Emma Rasmussen",
    "section": "Bivariate Visualization(s)",
    "text": "Bivariate Visualization(s)\n\n#First boxplot (with unfiltered data)\nggplot(AB_NYC_2019, aes(neighbourhood_group, price))+\n  geom_boxplot()\n\n\n\n#Second boxplot (with filtered data)\nggplot(AB_NYC_2019_filter, aes(neighbourhood_group, price))+\n  geom_boxplot()+\n  labs(title=\"Cost of NYC Airbnbs by Neighborhood (2019)\", x= \"Neighborhood\", y=\"Price ($)\", caption=\"*Histogram excludes prices equal to $0 and greater than $1,000\")\n\n\n\n#Violin plot with filtered data\nggplot(AB_NYC_2019_filter, aes(room_type, price))+geom_violin()+\n  labs(title=\"NYC Airbnb Price Based on Room Type (2019)*\", x= \"Room Type\", y= \"Price ($)\", caption= \"*Excludes listings where price is equal to $0 or greater than $1,000\")\n\n\n\n\n#First boxplot: Ignore this. Shows boxpplot distribution without filter. Not helpful. #Second boxplot: Gives us a better visual of the average price of Airbnbs by neighborhood. Brooklyn and Manhattan appear to have the most expensive Airbnbs including some costing 1000 or more. Bronx, Queens, and Staten Island Airbnbs don’t get quite as expensive. #Violin Plot Entire home/apt is on average most expensive (makes sense) but there is a lot of variability in the average. Interestingly, according to the chart, private rooms can cost 1000 dollars or more! I wonder what you get for that money. Shared rooms appear to have the lowest average price (also makes sense) and there seems to be a smaller IQR around the mean (if I am using that terminology correctly)."
  },
  {
    "objectID": "posts/challenge1_jerinjacob.html",
    "href": "posts/challenge1_jerinjacob.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_jerinjacob.html#reading-railroad-employees-dataset",
    "href": "posts/challenge1_jerinjacob.html#reading-railroad-employees-dataset",
    "title": "Challenge 1",
    "section": "Reading Railroad Employees Dataset",
    "text": "Reading Railroad Employees Dataset\n\n\nCode\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nThis is a data set of the rail road employees working in 2930 counties of the states in US in the year of 2012. There are 3 variables in the dataset; state, county and total number of employees."
  },
  {
    "objectID": "posts/challenge1_jerinjacob.html#describing-railroad-data",
    "href": "posts/challenge1_jerinjacob.html#describing-railroad-data",
    "title": "Challenge 1",
    "section": "Describing Railroad Data",
    "text": "Describing Railroad Data\n\n\nCode\nview(railroad)\nrailroad%>%\n  select(state)%>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  distinct()\n\n\n# A tibble: 53 × 1\n   state\n   <chr>\n 1 AE   \n 2 AK   \n 3 AL   \n 4 AP   \n 5 AR   \n 6 AZ   \n 7 CA   \n 8 CO   \n 9 CT   \n10 DC   \n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThere are 53 distinct values in the variable column named state. This means that there are certain additional values other than the name of the states. The variable ‘state’ contains all the states along with armed forces, DC etc. To find what values are included other than the name of the states, the distinct values of the variable ‘state’ is taken."
  },
  {
    "objectID": "posts/challenge4_nickboonstra.html",
    "href": "posts/challenge4_nickboonstra.html",
    "title": "Challenge 4 Submission",
    "section": "",
    "text": "Today’s challenge introduces the mutate() function as an additional tool in the data-wrangling process. I will be using the “FedFundsRate” data set.\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_nickboonstra.html#challenge-overview",
    "href": "posts/challenge4_nickboonstra.html#challenge-overview",
    "title": "Challenge 4 Submission",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_nickboonstra.html#read-in-data",
    "href": "posts/challenge4_nickboonstra.html#read-in-data",
    "title": "Challenge 4 Submission",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\nfed <- read_csv(\n  file = \"_data/FedFundsRate.csv\",\n  skip = 1,\n  col_names = c(\"year\",\"month\",\"day\",\"fedfunds_target\",\"fedfunds_upper\",\"fedfunds_lower\",\n                \"fedfunds_eff\",\"gdp_change\",\"unemploy_rate\",\"inflation_rate\")\n  )\n\nfed\n\n\n# A tibble: 904 × 10\n    year month   day fedfunds_…¹ fedfu…² fedfu…³ fedfu…⁴ gdp_c…⁵ unemp…⁶ infla…⁷\n   <dbl> <dbl> <dbl>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  1954     7     1          NA      NA      NA    0.8      4.6     5.8      NA\n 2  1954     8     1          NA      NA      NA    1.22    NA       6        NA\n 3  1954     9     1          NA      NA      NA    1.06    NA       6.1      NA\n 4  1954    10     1          NA      NA      NA    0.85     8       5.7      NA\n 5  1954    11     1          NA      NA      NA    0.83    NA       5.3      NA\n 6  1954    12     1          NA      NA      NA    1.28    NA       5        NA\n 7  1955     1     1          NA      NA      NA    1.39    11.9     4.9      NA\n 8  1955     2     1          NA      NA      NA    1.29    NA       4.7      NA\n 9  1955     3     1          NA      NA      NA    1.35    NA       4.6      NA\n10  1955     4     1          NA      NA      NA    1.43     6.7     4.7      NA\n# … with 894 more rows, and abbreviated variable names ¹​fedfunds_target,\n#   ²​fedfunds_upper, ³​fedfunds_lower, ⁴​fedfunds_eff, ⁵​gdp_change,\n#   ⁶​unemploy_rate, ⁷​inflation_rate\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nBriefly describe the data\nThis data set is not far from being tidy. Once the data set is tidy, there will be four columns: one for the date of the observation, one for the specific rate in question, one to describe what it is about that rate that is being observed, and one to provide the value of the given rate. When I read in the data, I renamed the columns with this tidying in mind."
  },
  {
    "objectID": "posts/challenge4_nickboonstra.html#tidying-and-wrangling",
    "href": "posts/challenge4_nickboonstra.html#tidying-and-wrangling",
    "title": "Challenge 4 Submission",
    "section": "Tidying and Wrangling",
    "text": "Tidying and Wrangling\nOne inconvenience about this data set is that the year, month, and date of each observation each exist in their own columns, rather than there being one single column for the date. Fortunately, the lubridate package provides a simple work-around for this with the make_date() function.\nWhen we pivot_longer() the data, we are going to take every column but the date (we will select() out the individual year/month/day columns once we’ve generated our date column) and pivot them into two columns: one for rate_type, and one for value. Then, we will take one further step and use the separate() function to split rate_type into two separate columns, one for each variable.\n\n\nCode\nfed <- fed %>% \n  mutate(date=make_date(year,month,day)) %>% \n  select(-c(year,month,day)) %>% \n  pivot_longer(\n    cols=!date,\n    names_to=\"rate_type\",\n    values_to=\"val\"\n  ) %>% \n  separate(col=rate_type,into=c(\"rate\",\"type\")) \n\nfed\n\n\n# A tibble: 6,328 × 4\n   date       rate      type     val\n   <date>     <chr>     <chr>  <dbl>\n 1 1954-07-01 fedfunds  target  NA  \n 2 1954-07-01 fedfunds  upper   NA  \n 3 1954-07-01 fedfunds  lower   NA  \n 4 1954-07-01 fedfunds  eff      0.8\n 5 1954-07-01 gdp       change   4.6\n 6 1954-07-01 unemploy  rate     5.8\n 7 1954-07-01 inflation rate    NA  \n 8 1954-08-01 fedfunds  target  NA  \n 9 1954-08-01 fedfunds  upper   NA  \n10 1954-08-01 fedfunds  lower   NA  \n# … with 6,318 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/Yakub_challenge2_.html",
    "href": "posts/Yakub_challenge2_.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Yakub_challenge2_.html#challenge-overview",
    "href": "posts/Yakub_challenge2_.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/Yakub_challenge2_.html#read-in-the-data",
    "href": "posts/Yakub_challenge2_.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\nCode\nlibrary(readr)\nFAOstat <- read_csv(\"_data/FAOSTAT_livestock.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/Yakub_challenge2_.html#describe-the-data",
    "href": "posts/Yakub_challenge2_.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nDoing a Head of the Dataset to get a view of what the Data Looks Like\n\n\nCode\nhead(FAOstat)\n\n\n# A tibble: 6 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1961  1961 Head \n2 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1962  1962 Head \n3 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1963  1963 Head \n4 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1964  1964 Head \n5 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1965  1965 Head \n6 QA      Live …       2 Afgh…    5111 Stocks     1107 Asses    1966  1966 Head \n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names\n\n\n\n\nCode\nFAO.sm <- FAOstat %>%\n  select(-contains(\"Code\"))\nFAO.sm\n\n\n# A tibble: 82,116 × 9\n   Domain       Area        Element Item   Year Unit    Value Flag  Flag Descr…¹\n   <chr>        <chr>       <chr>   <chr> <dbl> <chr>   <dbl> <chr> <chr>       \n 1 Live Animals Afghanistan Stocks  Asses  1961 Head  1300000 <NA>  Official da…\n 2 Live Animals Afghanistan Stocks  Asses  1962 Head   851850 <NA>  Official da…\n 3 Live Animals Afghanistan Stocks  Asses  1963 Head  1001112 <NA>  Official da…\n 4 Live Animals Afghanistan Stocks  Asses  1964 Head  1150000 F     FAO estimate\n 5 Live Animals Afghanistan Stocks  Asses  1965 Head  1300000 <NA>  Official da…\n 6 Live Animals Afghanistan Stocks  Asses  1966 Head  1200000 <NA>  Official da…\n 7 Live Animals Afghanistan Stocks  Asses  1967 Head  1200000 <NA>  Official da…\n 8 Live Animals Afghanistan Stocks  Asses  1968 Head  1328000 <NA>  Official da…\n 9 Live Animals Afghanistan Stocks  Asses  1969 Head  1250000 <NA>  Official da…\n10 Live Animals Afghanistan Stocks  Asses  1970 Head  1300000 <NA>  Official da…\n# … with 82,106 more rows, and abbreviated variable name ¹​`Flag Description`\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\nprint(dfSummary(FAO.sm, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nFAO.sm\nDimensions: 82116 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Live Animals\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Africa2. Asia3. China, mainland4. Eastern Africa5. Eastern Asia6. Eastern Europe7. Egypt8. Europe9. India10. Northern Africa[ 243 others ]\n      522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)76896(93.6%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Stocks\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Asses2. Buffaloes3. Camels4. Cattle5. Goats6. Horses7. Mules8. Pigs9. Sheep\n      8571(10.4%)3505(4.3%)3265(4.0%)13086(15.9%)12498(15.2%)11104(13.5%)6153(7.5%)12015(14.6%)11919(14.5%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.4 (16.8)min ≤ med ≤ max:1961 ≤ 1991 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. Head\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 11625569 (64779790)min ≤ med ≤ max:0 ≤ 224667 ≤ 1489744504IQR (CV) : 2364200 (5.6)\n      43667 distinct values\n      \n      1301\n(1.6%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Im5. M\n      2667(6.1%)12567(28.7%)24550(56.0%)2877(6.6%)1185(2.7%)\n      \n      38270\n(46.6%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Data not available3. FAO data based on imputat4. FAO estimate5. Official data6. Unofficial figure\n      12567(15.3%)1185(1.4%)2877(3.5%)24550(29.9%)38270(46.6%)2667(3.2%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-22\n\n\n\nFind out information about flags to see which one that I would pick.\n\n\nCode\nflag_description <- FAO.sm%>%\n  select(Flag,`Flag Description`)\nunique(flag_description)\n\n\n# A tibble: 6 × 2\n  Flag  `Flag Description`                                                      \n  <chr> <chr>                                                                   \n1 <NA>  Official data                                                           \n2 F     FAO estimate                                                            \n3 *     Unofficial figure                                                       \n4 Im    FAO data based on imputation methodology                                \n5 M     Data not available                                                      \n6 A     Aggregate, may include official, semi-official, estimated or calculated…\n\n\n\n\nCode\nlifestocktypes<- FAO.sm%>%\n  select(Flag,`Flag Description`)\nunique(flag_description)\n\n\n# A tibble: 6 × 2\n  Flag  `Flag Description`                                                      \n  <chr> <chr>                                                                   \n1 <NA>  Official data                                                           \n2 F     FAO estimate                                                            \n3 *     Unofficial figure                                                       \n4 Im    FAO data based on imputation methodology                                \n5 M     Data not available                                                      \n6 A     Aggregate, may include official, semi-official, estimated or calculated…"
  },
  {
    "objectID": "posts/Yakub_challenge2_.html#provide-grouped-summary-statistics",
    "href": "posts/Yakub_challenge2_.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nFAO.sm %>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n# A tibble: 28 × 2\n   Area                          n\n   <chr>                     <int>\n 1 Africa                      522\n 2 Americas                    464\n 3 Asia                        522\n 4 Australia and New Zealand   376\n 5 Caribbean                   464\n 6 Central America             406\n 7 Central Asia                243\n 8 Eastern Africa              522\n 9 Eastern Asia                522\n10 Eastern Europe              522\n# … with 18 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\nFAO.sm %>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n# A tibble: 28 × 2\n   Area                          n\n   <chr>                     <int>\n 1 Africa                      522\n 2 Americas                    464\n 3 Asia                        522\n 4 Australia and New Zealand   376\n 5 Caribbean                   464\n 6 Central America             406\n 7 Central Asia                243\n 8 Eastern Africa              522\n 9 Eastern Asia                522\n10 Eastern Europe              522\n# … with 18 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included.\nHere is where I filtered by area\n\n\nCode\narea_filter<-FAO.sm %>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\nI then wanted to see the averages by proportions.\n\n\nCode\narea_filter%>%\nmutate(prop = prop.table(n))\n\n\n# A tibble: 28 × 3\n   Area                          n   prop\n   <chr>                     <int>  <dbl>\n 1 Africa                      522 0.0415\n 2 Americas                    464 0.0369\n 3 Asia                        522 0.0415\n 4 Australia and New Zealand   376 0.0299\n 5 Caribbean                   464 0.0369\n 6 Central America             406 0.0323\n 7 Central Asia                243 0.0193\n 8 Eastern Africa              522 0.0415\n 9 Eastern Asia                522 0.0415\n10 Eastern Europe              522 0.0415\n# … with 18 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nI did a filter for Pigs and also wanted to see number of Pigs in Iran.\n\n\nCode\npig_analysis <- FAO.sm %>%\n  filter(Item==\"Pigs\",Area==\"Iran (Islamic Republic of)\")\npig_analysis\n\n\n# A tibble: 50 × 9\n   Domain       Area               Element Item   Year Unit  Value Flag  Flag …¹\n   <chr>        <chr>              <chr>   <chr> <dbl> <chr> <dbl> <chr> <chr>  \n 1 Live Animals Iran (Islamic Rep… Stocks  Pigs   1961 Head  55000 <NA>  Offici…\n 2 Live Animals Iran (Islamic Rep… Stocks  Pigs   1962 Head  58000 <NA>  Offici…\n 3 Live Animals Iran (Islamic Rep… Stocks  Pigs   1963 Head  52000 <NA>  Offici…\n 4 Live Animals Iran (Islamic Rep… Stocks  Pigs   1964 Head  50000 <NA>  Offici…\n 5 Live Animals Iran (Islamic Rep… Stocks  Pigs   1965 Head  50000 <NA>  Offici…\n 6 Live Animals Iran (Islamic Rep… Stocks  Pigs   1966 Head  50000 <NA>  Offici…\n 7 Live Animals Iran (Islamic Rep… Stocks  Pigs   1967 Head  52000 <NA>  Offici…\n 8 Live Animals Iran (Islamic Rep… Stocks  Pigs   1968 Head  52000 <NA>  Offici…\n 9 Live Animals Iran (Islamic Rep… Stocks  Pigs   1969 Head  50000 F     FAO es…\n10 Live Animals Iran (Islamic Rep… Stocks  Pigs   1970 Head  48000 F     FAO es…\n# … with 40 more rows, and abbreviated variable name ¹​`Flag Description`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nI then did a data visualization by year and found something interesting. There was a massive drop in the number of Pigs around the year 1980, which is when the Islamic Revolution happened and Iran became a theocracy which made Pork banned to eat.\n\n\nCode\nggplot(data = pig_analysis, aes(x = Year, y = Value)) +\n     geom_line()\n\n\n\n\n\nUsing the Table function, I found that the data for Iran stopped being available after 1994.\n\n\nCode\n(table(pig_analysis$`Flag Description`,pig_analysis$Year))\n\n\n                    \n                     1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971\n  Data not available    0    0    0    0    0    0    0    0    0    0    0\n  FAO estimate          0    0    0    0    0    0    0    0    1    1    1\n  Official data         1    1    1    1    1    1    1    1    0    0    0\n                    \n                     1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1990\n  Data not available    0    0    0    0    0    0    0    0    0    0    0\n  FAO estimate          1    0    0    1    1    1    1    1    1    1    0\n  Official data         0    1    1    0    0    0    0    0    0    0    1\n                    \n                     1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001\n  Data not available    0    0    1    1    1    1    1    1    1    1    1\n  FAO estimate          0    0    0    0    0    0    0    0    0    0    0\n  Official data         1    1    0    0    0    0    0    0    0    0    0\n                    \n                     2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012\n  Data not available    1    1    1    1    1    1    1    1    1    1    1\n  FAO estimate          0    0    0    0    0    0    0    0    0    0    0\n  Official data         0    0    0    0    0    0    0    0    0    0    0\n                    \n                     2013 2014 2015 2016 2017 2018\n  Data not available    1    1    1    1    1    1\n  FAO estimate          0    0    0    0    0    0\n  Official data         0    0    0    0    0    0"
  },
  {
    "objectID": "posts/challenge2_instructions.html",
    "href": "posts/challenge2_instructions.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions.html#challenge-overview",
    "href": "posts/challenge2_instructions.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions.html#read-in-the-data",
    "href": "posts/challenge2_instructions.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_instructions.html#describe-the-data",
    "href": "posts/challenge2_instructions.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included."
  },
  {
    "objectID": "posts/challenge1_KatiePopiela.html",
    "href": "posts/challenge1_KatiePopiela.html",
    "title": "Challenge1_KatiePopiela",
    "section": "",
    "text": "#For my purposes I'll be reading in the birds.csv dataset. The data appears to record the number of various birds (ducks, geese, chickens, etc.) in different countries during the mid 20th-early 21st century. \n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ dplyr   1.0.9\n✔ tibble  3.1.8     ✔ stringr 1.4.0\n✔ tidyr   1.2.0     ✔ forcats 0.5.1\n✔ purrr   0.3.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(ggplot2)\n\nbirds_1<-read.csv(\"_data/birds.csv\")\nhead(birds_1)\n\n  Domain.Code       Domain Area.Code        Area Element.Code Element Item.Code\n1          QA Live Animals         2 Afghanistan         5112  Stocks      1057\n2          QA Live Animals         2 Afghanistan         5112  Stocks      1057\n3          QA Live Animals         2 Afghanistan         5112  Stocks      1057\n4          QA Live Animals         2 Afghanistan         5112  Stocks      1057\n5          QA Live Animals         2 Afghanistan         5112  Stocks      1057\n6          QA Live Animals         2 Afghanistan         5112  Stocks      1057\n      Item Year.Code Year      Unit Value Flag Flag.Description\n1 Chickens      1961 1961 1000 Head  4700    F     FAO estimate\n2 Chickens      1962 1962 1000 Head  4900    F     FAO estimate\n3 Chickens      1963 1963 1000 Head  5000    F     FAO estimate\n4 Chickens      1964 1964 1000 Head  5300    F     FAO estimate\n5 Chickens      1965 1965 1000 Head  5500    F     FAO estimate\n6 Chickens      1966 1966 1000 Head  5800    F     FAO estimate\n\n#Below is the head of the dataset. At first glance it looks ok, but the dimensions, as I will show, require the data to be trimmed down. \n\n\ndim(birds_1)\n\n[1] 30977    14\n\n#There are 30,977 rows and 14 columns in this dataset (too many), so I will use the select function(s) to isolate specific variables; I want to look at the record of duck numbers in Czechoslovakia.\n\n\ncolnames(birds_1)\n\n [1] \"Domain.Code\"      \"Domain\"           \"Area.Code\"        \"Area\"            \n [5] \"Element.Code\"     \"Element\"          \"Item.Code\"        \"Item\"            \n [9] \"Year.Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag.Description\"\n\n\n\nbirds_refined<- select(birds_1,Area,Item,Value,Year)\n\nbirds_ref2 <- birds_refined %>%\n  filter(Area==\"Czechoslovakia\") %>%\n  filter(Item==\"Ducks\")\n\nhead(birds_ref2)\n\n            Area  Item Value Year\n1 Czechoslovakia Ducks   434 1961\n2 Czechoslovakia Ducks   344 1962\n3 Czechoslovakia Ducks   368 1963\n4 Czechoslovakia Ducks   431 1964\n5 Czechoslovakia Ducks   344 1965\n6 Czechoslovakia Ducks   324 1966\n\n\n\n#The recorded number of ducks in Czechoslovakia fluctuates between being in the 300,000s and the 700,000s. Lets visualize it!\n\nggplot(birds_ref2, aes(x=Year,y=Value)) + geom_jitter() + labs(y=\"Number of Ducks by 1000\")"
  },
  {
    "objectID": "posts/challenge1_ShoshanaBuck.html",
    "href": "posts/challenge1_ShoshanaBuck.html",
    "title": "challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_ShoshanaBuck.html#challenge-overview",
    "href": "posts/challenge1_ShoshanaBuck.html#challenge-overview",
    "title": "challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview"
  },
  {
    "objectID": "posts/challenge1_ShoshanaBuck.html#read-in-the-data",
    "href": "posts/challenge1_ShoshanaBuck.html#read-in-the-data",
    "title": "challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")\nrailroad\n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThis data is separated into three columns: state, county, and total employees and 2,930 rows."
  },
  {
    "objectID": "posts/challenge1_ShoshanaBuck.html#describe-the-data",
    "href": "posts/challenge1_ShoshanaBuck.html#describe-the-data",
    "title": "challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nI imported the data set of railroad_2012_clean_county.csv and renamed it as Railroad. I then used the function colnames() to breakdown the three column names of “state” “county” and “total_employees.” From there I used the spec() function to extract the column names. I then used a pipe function in order to filter and select to see the total amount of employees in each state.\n\n\nCode\ncolnames(railroad)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nCode\nspec(railroad)\n\n\ncols(\n  state = col_character(),\n  county = col_character(),\n  total_employees = col_double()\n)\n\n\nCode\nhead(railroad)\n\n\n# A tibble: 6 × 3\n  state county               total_employees\n  <chr> <chr>                          <dbl>\n1 AE    APO                                2\n2 AK    ANCHORAGE                          7\n3 AK    FAIRBANKS NORTH STAR               2\n4 AK    JUNEAU                             3\n5 AK    MATANUSKA-SUSITNA                  2\n6 AK    SITKA                              1\n\n\nCode\nrailroad %>% \n  group_by(state) %>% \n  summarise(total_employees2=sum(total_employees))\n\n\n# A tibble: 53 × 2\n   state total_employees2\n   <chr>            <dbl>\n 1 AE                   2\n 2 AK                 103\n 3 AL                4257\n 4 AP                   1\n 5 AR                3871\n 6 AZ                3153\n 7 CA               13137\n 8 CO                3650\n 9 CT                2592\n10 DC                 279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge6_instructions.html",
    "href": "posts/challenge6_instructions.html",
    "title": "Challenge 6 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge6_instructions.html#challenge-overview",
    "href": "posts/challenge6_instructions.html#challenge-overview",
    "title": "Challenge 6 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least one graph including time (evolution)\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\n\nCreate at least one graph depicting part-whole or flow relationships\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge6_instructions.html#read-in-data",
    "href": "posts/challenge6_instructions.html#read-in-data",
    "title": "Challenge 6 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\ndebt ⭐\nfed_rate ⭐⭐\nabc_poll ⭐⭐⭐\nusa_hh ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\nair_bnb ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge6_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge6_instructions.html#tidy-data-as-needed",
    "title": "Challenge 6 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge6_instructions.html#time-dependent-visualization",
    "href": "posts/challenge6_instructions.html#time-dependent-visualization",
    "title": "Challenge 6 Instructions",
    "section": "Time Dependent Visualization",
    "text": "Time Dependent Visualization"
  },
  {
    "objectID": "posts/challenge6_instructions.html#visualizing-part-whole-relationships",
    "href": "posts/challenge6_instructions.html#visualizing-part-whole-relationships",
    "title": "Challenge 6 Instructions",
    "section": "Visualizing Part-Whole Relationships",
    "text": "Visualizing Part-Whole Relationships"
  },
  {
    "objectID": "posts/challenge4_EmmaRasmussen.html",
    "href": "posts/challenge4_EmmaRasmussen.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_EmmaRasmussen.html#read-in-data",
    "href": "posts/challenge4_EmmaRasmussen.html#read-in-data",
    "title": "Challenge 4",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\nFedFundsRate<-read_csv(\"_data/FedFundsRate.csv\",\n                        show_col_types = FALSE)\n#Saving an unchanged copy of the dataset\nFedFundsRateOrig<-FedFundsRate\n\n#checking variables\ncolnames(FedFundsRate)\n\n\n [1] \"Year\"                         \"Month\"                       \n [3] \"Day\"                          \"Federal Funds Target Rate\"   \n [5] \"Federal Funds Upper Target\"   \"Federal Funds Lower Target\"  \n [7] \"Effective Federal Funds Rate\" \"Real GDP (Percent Change)\"   \n [9] \"Unemployment Rate\"            \"Inflation Rate\"              \n\n\nCode\n#Checking columns with a lot of NA values\nFedFundsRate%>%\n  select(\"Federal Funds Target Rate\", \"Federal Funds Upper Target\", \"Federal Funds Lower Target\") %>% \n  distinct()\n\n\n# A tibble: 68 × 3\n   `Federal Funds Target Rate` `Federal Funds Upper Target` Federal Funds Lowe…¹\n                         <dbl>                        <dbl>                <dbl>\n 1                       NA                              NA                   NA\n 2                       10.2                            NA                   NA\n 3                       10                              NA                   NA\n 4                        9.5                            NA                   NA\n 5                        9                              NA                   NA\n 6                        8.5                            NA                   NA\n 7                        8.62                           NA                   NA\n 8                        8.75                           NA                   NA\n 9                        9.25                           NA                   NA\n10                        9.44                           NA                   NA\n# … with 58 more rows, and abbreviated variable name\n#   ¹​`Federal Funds Lower Target`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n#renaming columns prior to pivot\nFedFundsRate<-rename(FedFundsRate, \"TargetRate\"=\"Federal Funds Target Rate\", \"UpperTargetRate\"=\"Federal Funds Upper Target\", \"LowerTargetRate\"=\"Federal Funds Lower Target\", \"EffectiveRate\"=\"Effective Federal Funds Rate\", \"GDP(PercentChange)\"=\"Real GDP (Percent Change)\", \"UnemploymentRate\"=\"Unemployment Rate\", \"InflationRate\"=\"Inflation Rate\")\n\nFedFundsRate\n\n\n# A tibble: 904 × 10\n    Year Month   Day TargetRate UpperT…¹ Lower…² Effec…³ GDP(P…⁴ Unemp…⁵ Infla…⁶\n   <dbl> <dbl> <dbl>      <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  1954     7     1         NA       NA      NA    0.8      4.6     5.8      NA\n 2  1954     8     1         NA       NA      NA    1.22    NA       6        NA\n 3  1954     9     1         NA       NA      NA    1.06    NA       6.1      NA\n 4  1954    10     1         NA       NA      NA    0.85     8       5.7      NA\n 5  1954    11     1         NA       NA      NA    0.83    NA       5.3      NA\n 6  1954    12     1         NA       NA      NA    1.28    NA       5        NA\n 7  1955     1     1         NA       NA      NA    1.39    11.9     4.9      NA\n 8  1955     2     1         NA       NA      NA    1.29    NA       4.7      NA\n 9  1955     3     1         NA       NA      NA    1.35    NA       4.6      NA\n10  1955     4     1         NA       NA      NA    1.43     6.7     4.7      NA\n# … with 894 more rows, and abbreviated variable names ¹​UpperTargetRate,\n#   ²​LowerTargetRate, ³​EffectiveRate, ⁴​`GDP(PercentChange)`, ⁵​UnemploymentRate,\n#   ⁶​InflationRate\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nBriefly describe the data\nI know nothing about economics but the Federal Funds Rate appears to be set by the government to regulate lending by banks. The Federal Open Markets Committee (FOMC) sets an upper and lower limit target for the Federal Funds Rate. This data set includes a target rate, these upper and lower target limits, and the effective rate (I am assuming these are independent variables). So one observation includes: date, target rate, upper and lower limits, and effective rate). The data set also includes the GDP, Unemployment Rate, and inflation which I am assuming to be dependent variables because they are effected by the federal funds rate."
  },
  {
    "objectID": "posts/challenge4_EmmaRasmussen.html#tidy-data-as-needed",
    "href": "posts/challenge4_EmmaRasmussen.html#tidy-data-as-needed",
    "title": "Challenge 4",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\n#Pivoting federal fund rates into a single column\n\n\nCode\nFedFundsRateLonger<-pivot_longer(FedFundsRate, col=c(\"TargetRate\", \"UpperTargetRate\", \"LowerTargetRate\", \"EffectiveRate\"),\n                                 names_to=\"FederalFundsRate\",\n                                 values_to=\"Value\")\nFedFundsRateLonger\n\n\n# A tibble: 3,616 × 8\n    Year Month   Day `GDP(PercentChange)` UnemploymentRate Infla…¹ Feder…² Value\n   <dbl> <dbl> <dbl>                <dbl>            <dbl>   <dbl> <chr>   <dbl>\n 1  1954     7     1                  4.6              5.8      NA Target… NA   \n 2  1954     7     1                  4.6              5.8      NA UpperT… NA   \n 3  1954     7     1                  4.6              5.8      NA LowerT… NA   \n 4  1954     7     1                  4.6              5.8      NA Effect…  0.8 \n 5  1954     8     1                 NA                6        NA Target… NA   \n 6  1954     8     1                 NA                6        NA UpperT… NA   \n 7  1954     8     1                 NA                6        NA LowerT… NA   \n 8  1954     8     1                 NA                6        NA Effect…  1.22\n 9  1954     9     1                 NA                6.1      NA Target… NA   \n10  1954     9     1                 NA                6.1      NA UpperT… NA   \n# … with 3,606 more rows, and abbreviated variable names ¹​InflationRate,\n#   ²​FederalFundsRate\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAny additional comments?\nIf I knew how the data was being used/type of analyses we were doing I might pivot differently"
  },
  {
    "objectID": "posts/challenge4_EmmaRasmussen.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_EmmaRasmussen.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\n\n\nCode\n#reformatting date\nFedFundsRateLonger$Date<- paste(FedFundsRateLonger$Year, FedFundsRateLonger$Month, FedFundsRateLonger$Day, sep=\"-\") %>% \n  ymd() %>% \n  as.Date()\n\n#removing date columns and reordering columns for readability: date-rate-value-Dependent Variables\nselect(FedFundsRateLonger, 9,7,8,6,5,4)\n\n\n# A tibble: 3,616 × 6\n   Date       FederalFundsRate Value InflationRate UnemploymentRate GDP(Percen…¹\n   <date>     <chr>            <dbl>         <dbl>            <dbl>        <dbl>\n 1 1954-07-01 TargetRate       NA               NA              5.8          4.6\n 2 1954-07-01 UpperTargetRate  NA               NA              5.8          4.6\n 3 1954-07-01 LowerTargetRate  NA               NA              5.8          4.6\n 4 1954-07-01 EffectiveRate     0.8             NA              5.8          4.6\n 5 1954-08-01 TargetRate       NA               NA              6           NA  \n 6 1954-08-01 UpperTargetRate  NA               NA              6           NA  \n 7 1954-08-01 LowerTargetRate  NA               NA              6           NA  \n 8 1954-08-01 EffectiveRate     1.22            NA              6           NA  \n 9 1954-09-01 TargetRate       NA               NA              6.1         NA  \n10 1954-09-01 UpperTargetRate  NA               NA              6.1         NA  \n# … with 3,606 more rows, and abbreviated variable name ¹​`GDP(PercentChange)`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAny additional comments?\nI left the unemployment rate, inflation rate, and GDP in their own columns so analyses on these data could more easily compare Federal Funds Rate against these dependent variables over time (i.e. filtering effective rate and comparing to one of these variables). Condensing the data into one rate and variable might be easier to answer specific questions, but I did not want to remove any data."
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html",
    "href": "posts/challenge2_AdithyaParupudi.html",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html#challenge-overview",
    "href": "posts/challenge2_AdithyaParupudi.html#challenge-overview",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html#read-in-the-data",
    "href": "posts/challenge2_AdithyaParupudi.html#read-in-the-data",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "Read in the Data",
    "text": "Read in the Data\nFocusing on FAOSTAT_cattle_dairy.csv\n\n\nCode\ncattle <- read_csv(\"_data/FAOSTAT_cattle_dairy.csv\")\ncattle\n\n\n\n\n  \n\n\n\nLooks like this is an extensive data gathered by faostat regarding the cattle’s products from different countries between 1961 and 2018. A quick glance at the last column tells whether this data is estimated or calculated by faostat. I am interested to see whether over the years, did faostat collect more data or did it unofficially made entries in this data set."
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html#describe-the-data",
    "href": "posts/challenge2_AdithyaParupudi.html#describe-the-data",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\ndim(cattle)\n\n\n[1] 36449    14\n\n\nCode\ncolnames(cattle)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\n\n\nCode\ncattle %>% \n  group_by(`Flag Description`) %>% \n  summarise()\n\n\n\n\n  \n\n\n\nCode\ncattle %>% \n  group_by(Area) %>% \n  summarise()\n\n\n\n\n  \n\n\n\nBy filtering the unique values of the Flag Description column, I notice there are some unofficial figures and “Data not available” entries. I want to see how many entries of these were since 1961 and did their count reduce. Maybe this will tell the data gathering techniques have improved over the years.\nThere are 232 unique countries from the data set with their domain = “Primary Livestock” common to all. (FAOSTAT focused on collecting livestock information from all the countries)"
  },
  {
    "objectID": "posts/challenge2_AdithyaParupudi.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_AdithyaParupudi.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 - Adithya Parupudi",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\nCode\nall_countries <- cattle %>%\n  group_by(Area,`Flag Description`, Year) %>%\n  summarise(median_val=median(Value,na.rm = TRUE), .groups = 'drop') %>% \n  arrange(Year, `Flag Description`) %>% \n  select(Year, Area, `Flag Description`)\nall_countries\n\n\n\n\n  \n\n\n\nFor analysis purpose, I place my focus on these columns (Year, Area, Flag Description) and arranged the list in ascending order with respect to Year column\n\n\nCode\nall_grouped <- all_countries %>%\n  group_by(`Flag Description`, Year) %>%\n  summarise(count = n(), ) %>% \n  arrange(desc(Year)) %>% \n  select(Year, `Flag Description`, count)\nall_grouped\n\n\n\n\n  \n\n\n\nWe have grouped the data by year and flag-description to see the number of flag-description entries in each year. Now lets pick first and last years of the data set and compare the results.\n\n\nCode\nyr_2018 <- all_grouped %>% \n  filter(Year == '2018') %>% \n  arrange(desc(count))\nyr_2000 <- all_grouped %>% \n  filter(Year == '2000') %>% \n  arrange(desc(count))\nyr_2018\n\n\n\n\n  \n\n\n\nCode\nyr_2000\n\n\n\n\n  \n\n\n\nI wanted to compare the entries of “unofficial figure” of two different years. Looks like there were efficient data gathering methods in place, when we look at “Calculated Data” and “Official Data”. There is a rise in “FAO data based on imputation methodology” from 45 to 125 in 18 years.\n\n\nCode\nall_grouped %>% \n  filter(`Flag Description` == 'Unofficial figure') %>% \n  arrange(desc(count))\n\n\n\n\n  \n\n\n\nUnofficial figures were high in 1981. This observation didnt have a steady decline in number, but it varied was rather varied each year.\n\n\nCode\nindia <- cattle %>%\n  filter(Area == 'India') %>% \n  select(Area, Year, Item, Element, `Element Code` ,`Flag Description`) %>% \n  group_by(`Flag Description`) %>%\n  summarise(count = n())\nindia\n\n\n\n\n  \n\n\n\nCode\ncattle %>%\n  filter(Area == 'Afghanistan') %>% \n  select(Area, Year, Item, Element, `Element Code` ,`Flag Description`) %>% \n  group_by(`Flag Description`) %>%\n  summarise(count = n())\n\n\n\n\n  \n\n\n\nWanted to see India’s contribution to FAOSTAT since 1961. I’ve compared the details with Afghanistan and noticed India’s overall contribution to the faostat list is less, and unofficial figure numbers are higher. In India, there was no use of “FAO data based on imputation methodology” as well.\n\nExplain and Interpret\nI began this analysis, think to group Area, Flag Description and Year and probably tell that over the years (1961 to 2018) the numbers will tell a different story in terms of types of entries made. Though I didn’t uncover groundbreaking observations, it was interesting to see that not all countries have contributed to this FAOSTAT list equally in terms of how data was added."
  },
  {
    "objectID": "posts/challenge1_ManiShankerKamarapu.html",
    "href": "posts/challenge1_ManiShankerKamarapu.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_ManiShankerKamarapu.html#challenge-overview",
    "href": "posts/challenge1_ManiShankerKamarapu.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_ManiShankerKamarapu.html#read-in-the-data",
    "href": "posts/challenge1_ManiShankerKamarapu.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\n\nI will be working on the “wild_bird_data” dataset.\n\n\nCode\n# Loading `readxl` package\nlibrary(readxl)\nwild_bird <- read_xlsx(\"_data/wild_bird_data.xlsx\")\n\n# View the dataset\nwild_bird\n\n\n# A tibble: 147 × 2\n   Reference           `Taken from Figure 1 of Nee et al.`\n   <chr>               <chr>                              \n 1 Wet body weight [g] Population size                    \n 2 5.45887180052624    532194.395145161                   \n 3 7.76456810683605    3165107.44544653                   \n 4 8.63858738018464    2592996.86778979                   \n 5 10.6897349302105    3524193.2266336                    \n 6 7.41722577905587    389806.168891807                   \n 7 9.1169347252776     604765.97978904                    \n 8 8.03684333000353    192360.511579436                   \n 9 8.70473119796067    250452.449623033                   \n10 8.89032317828959    16997.4156415239                   \n# … with 137 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge1_ManiShankerKamarapu.html#describe-the-data",
    "href": "posts/challenge1_ManiShankerKamarapu.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\n# Use dim() to get dimensions of dataset\ndim(wild_bird)\n\n\n[1] 147   2\n\n\nThere are 147 cases in 2 columns(Reference and Taken from Figure 1 of Nee et al). Actually the second row has the real column names so we will now make second row as column names and remove the first row.\n\n\nCode\n#Rename the column names\ncolnames(wild_bird) <- wild_bird[1,]\n#Removing the first row\nwild_bird <- wild_bird[-1,]\n#New dimensions of dataset\ndim(wild_bird)\n\n\n[1] 146   2\n\n\nCode\n#View the dataset\nwild_bird\n\n\n# A tibble: 146 × 2\n   `Wet body weight [g]` `Population size`\n   <chr>                 <chr>            \n 1 5.45887180052624      532194.395145161 \n 2 7.76456810683605      3165107.44544653 \n 3 8.63858738018464      2592996.86778979 \n 4 10.6897349302105      3524193.2266336  \n 5 7.41722577905587      389806.168891807 \n 6 9.1169347252776       604765.97978904  \n 7 8.03684333000353      192360.511579436 \n 8 8.70473119796067      250452.449623033 \n 9 8.89032317828959      16997.4156415239 \n10 9.51590845877281      595.09393677964  \n# … with 136 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n#Summary of dataset\nsummary(wild_bird)\n\n\n Wet body weight [g] Population size   \n Length:146          Length:146        \n Class :character    Class :character  \n Mode  :character    Mode  :character  \n\n\nThe dataset is in character class so first we need to convert character class to numeric and then get the summary.\n\n\nCode\n#Converting datset to numeric\nwild_bird$`Wet body weight [g]` <- as.numeric(wild_bird$`Wet body weight [g]`)\nwild_bird$`Population size` <- as.numeric(wild_bird$`Population size`)\n#Summary of the converted dataset\nsummary(wild_bird)\n\n\n Wet body weight [g] Population size  \n Min.   :   5.459    Min.   :      5  \n 1st Qu.:  18.620    1st Qu.:   1821  \n Median :  69.232    Median :  24353  \n Mean   : 363.694    Mean   : 382874  \n 3rd Qu.: 309.826    3rd Qu.: 198515  \n Max.   :9639.845    Max.   :5093378  \n\n\nThis is the brief summary of the wild_bird dataset."
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html",
    "href": "posts/challenge2_MirandaManka.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html#challenge-overview",
    "href": "posts/challenge2_MirandaManka.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html#read-in-the-data",
    "href": "posts/challenge2_MirandaManka.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\nhotel_bookings = read_csv(\"_data/hotel_bookings.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html#describe-the-data",
    "href": "posts/challenge2_MirandaManka.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\n\n\nCode\n#Looking at the data\nview(hotel_bookings)\n\n#Dimensions of the data\ndim(hotel_bookings)\n\n\n[1] 119390     32\n\n\nCode\n#Summary of the variables in the dataset\nsummary(hotel_bookings)\n\n\n    hotel            is_canceled       lead_time   arrival_date_year\n Length:119390      Min.   :0.0000   Min.   :  0   Min.   :2015     \n Class :character   1st Qu.:0.0000   1st Qu.: 18   1st Qu.:2016     \n Mode  :character   Median :0.0000   Median : 69   Median :2016     \n                    Mean   :0.3704   Mean   :104   Mean   :2016     \n                    3rd Qu.:1.0000   3rd Qu.:160   3rd Qu.:2017     \n                    Max.   :1.0000   Max.   :737   Max.   :2017     \n                                                                    \n arrival_date_month arrival_date_week_number arrival_date_day_of_month\n Length:119390      Min.   : 1.00            Min.   : 1.0             \n Class :character   1st Qu.:16.00            1st Qu.: 8.0             \n Mode  :character   Median :28.00            Median :16.0             \n                    Mean   :27.17            Mean   :15.8             \n                    3rd Qu.:38.00            3rd Qu.:23.0             \n                    Max.   :53.00            Max.   :31.0             \n                                                                      \n stays_in_weekend_nights stays_in_week_nights     adults      \n Min.   : 0.0000         Min.   : 0.0         Min.   : 0.000  \n 1st Qu.: 0.0000         1st Qu.: 1.0         1st Qu.: 2.000  \n Median : 1.0000         Median : 2.0         Median : 2.000  \n Mean   : 0.9276         Mean   : 2.5         Mean   : 1.856  \n 3rd Qu.: 2.0000         3rd Qu.: 3.0         3rd Qu.: 2.000  \n Max.   :19.0000         Max.   :50.0         Max.   :55.000  \n                                                              \n    children           babies              meal             country         \n Min.   : 0.0000   Min.   : 0.000000   Length:119390      Length:119390     \n 1st Qu.: 0.0000   1st Qu.: 0.000000   Class :character   Class :character  \n Median : 0.0000   Median : 0.000000   Mode  :character   Mode  :character  \n Mean   : 0.1039   Mean   : 0.007949                                        \n 3rd Qu.: 0.0000   3rd Qu.: 0.000000                                        \n Max.   :10.0000   Max.   :10.000000                                        \n NA's   :4                                                                  \n market_segment     distribution_channel is_repeated_guest\n Length:119390      Length:119390        Min.   :0.00000  \n Class :character   Class :character     1st Qu.:0.00000  \n Mode  :character   Mode  :character     Median :0.00000  \n                                         Mean   :0.03191  \n                                         3rd Qu.:0.00000  \n                                         Max.   :1.00000  \n                                                          \n previous_cancellations previous_bookings_not_canceled reserved_room_type\n Min.   : 0.00000       Min.   : 0.0000                Length:119390     \n 1st Qu.: 0.00000       1st Qu.: 0.0000                Class :character  \n Median : 0.00000       Median : 0.0000                Mode  :character  \n Mean   : 0.08712       Mean   : 0.1371                                  \n 3rd Qu.: 0.00000       3rd Qu.: 0.0000                                  \n Max.   :26.00000       Max.   :72.0000                                  \n                                                                         \n assigned_room_type booking_changes   deposit_type          agent          \n Length:119390      Min.   : 0.0000   Length:119390      Length:119390     \n Class :character   1st Qu.: 0.0000   Class :character   Class :character  \n Mode  :character   Median : 0.0000   Mode  :character   Mode  :character  \n                    Mean   : 0.2211                                        \n                    3rd Qu.: 0.0000                                        \n                    Max.   :21.0000                                        \n                                                                           \n   company          days_in_waiting_list customer_type           adr         \n Length:119390      Min.   :  0.000      Length:119390      Min.   :  -6.38  \n Class :character   1st Qu.:  0.000      Class :character   1st Qu.:  69.29  \n Mode  :character   Median :  0.000      Mode  :character   Median :  94.58  \n                    Mean   :  2.321                         Mean   : 101.83  \n                    3rd Qu.:  0.000                         3rd Qu.: 126.00  \n                    Max.   :391.000                         Max.   :5400.00  \n                                                                             \n required_car_parking_spaces total_of_special_requests reservation_status\n Min.   :0.00000             Min.   :0.0000            Length:119390     \n 1st Qu.:0.00000             1st Qu.:0.0000            Class :character  \n Median :0.00000             Median :0.0000            Mode  :character  \n Mean   :0.06252             Mean   :0.5714                              \n 3rd Qu.:0.00000             3rd Qu.:1.0000                              \n Max.   :8.00000             Max.   :5.0000                              \n                                                                         \n reservation_status_date\n Min.   :2014-10-17     \n 1st Qu.:2016-02-01     \n Median :2016-08-07     \n Mean   :2016-07-30     \n 3rd Qu.:2017-02-08     \n Max.   :2017-09-14     \n                        \n\n\nThis dataset has 32 variables with 119,390 observations. The variables include information about hotel bookings, while each observation/case is a different hotel booking. Some variables include hotel type (city hotel vs resort hotel), if the booking was canceled, arrival date, number of nights stayed (week and weekend), number of people and kids and babies, the market segment, if the guest is a repeat guest, and room type (there are more, this just points out a few). Some of the variables have categories (city vs resort hotel, for the type of hotel), some are numeric and continuous (lead time, in days for example 14) and some are numerical but binary (is canceled, 0 or 1). This data likely came from a hotel chain with different locations and/or multiple hotels, as the country variable shows that these are hotels in different countries."
  },
  {
    "objectID": "posts/challenge2_MirandaManka.html#provide-grouped-summary-statistics-explain-and-interpret",
    "href": "posts/challenge2_MirandaManka.html#provide-grouped-summary-statistics-explain-and-interpret",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics & Explain and Interpret",
    "text": "Provide Grouped Summary Statistics & Explain and Interpret\n\n\nCode\n#Find mean and sd for number of stays in week nights grouped by hotel type\nhotel_bookings %>%\n  group_by(hotel) %>%\n  summarise(mean = mean(stays_in_week_nights), sd = sd(stays_in_week_nights))\n\n\n# A tibble: 2 × 3\n  hotel         mean    sd\n  <chr>        <dbl> <dbl>\n1 City Hotel    2.18  1.46\n2 Resort Hotel  3.13  2.46\n\n\nCode\n#Find mean and sd for number of stays in weekend nights grouped by hotel type\nhotel_bookings %>%\n  group_by(hotel) %>%\n  summarise(mean = mean(stays_in_weekend_nights), sd = sd(stays_in_weekend_nights))\n\n\n# A tibble: 2 × 3\n  hotel         mean    sd\n  <chr>        <dbl> <dbl>\n1 City Hotel   0.795 0.885\n2 Resort Hotel 1.19  1.15 \n\n\nCode\n#Find mean and sd for number of stays in week nights grouped by whether the guest is a repeat guest\nhotel_bookings %>%\n  group_by(is_repeated_guest) %>%\n  summarise(mean = mean(stays_in_week_nights), sd = sd(stays_in_week_nights))\n\n\n# A tibble: 2 × 3\n  is_repeated_guest  mean    sd\n              <dbl> <dbl> <dbl>\n1                 0  2.53  1.91\n2                 1  1.48  1.62\n\n\nI started by picking out a few interesting variables and looking at them. First, I grouped by hotel and looked at number of night stayed during the week to see if there was any difference. The resort hotels had a higher mean (3.1 compared to 2.2 for the city hotels) which was interesting, I thought maybe people staying at resorts plan an extra day more of their trip during the week. I also looked the same hotel grouping for weekend nights and the mean for resort hotels was still higher (1.2 vs 0.8 for city), so maybe people staying at resort hotels simply stay longer. This could be explored more in the future. I also looked grouped by whether someone is a repeated guest (0 for no, 1 for yes), then examined the mean for how many week nights they stayed. Repeat guests tend to stay for shorter amount of nights (1.48 vs 2.53 for non repeat guests). I thought this was interesting because people who tend to stay again aren’t staying as long (maybe more business people for a night rather than a family vacation).\n\n\nCode\n#Find summary statistics for lead time for booking grouped by hotel type\nhotel_bookings %>%\n  group_by(hotel) %>%\n  select(lead_time, hotel) %>%\n  summarize_all(list(mean=mean, median = median, min = min, max = max, sd = sd, var = var, IQR = IQR), na.rm = TRUE)\n\n\n# A tibble: 2 × 8\n  hotel         mean median   min   max    sd    var   IQR\n  <chr>        <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>\n1 City Hotel   110.      74     0   629 111.  12310.   140\n2 Resort Hotel  92.7     57     0   737  97.3  9464.   145\n\n\nI wanted to look at the lead time for each type of hotel. Lead time is how many days ahead of their stay someone booked, for example 7 would mean they booked their hotel a week bfore they showed up. The mean lead time for city hotels is 109.7 days (about 3.5 months ahead of time), while the mean lead time for the resort hotels is 92.7 days (about 3 months ahead of time). That is interesting but is only different by a few weeks. The medians for both groups were much lower than the mean, which indicates the data are skewed (positively, or towards the right), meaning more of the lead times were lower values. The maximums were still high though, with 629 days for city hotels and 737 days for resort hotels, although they both had minimums of 0 (same day or walk-in). The standard deviation and other measures of dispersion were fairly large, indicating the data are spread out (looking at the maximums and minimums, this makes sense).\n\n\nCode\n#Creating variable to see if people got the room they booked\ndifferent_room = ifelse(hotel_bookings$reserved_room_type != hotel_bookings$assigned_room_type, 1, 0)\n\n#Looking at the results\nprop.table(table(different_room))\n\n\ndifferent_room\n        0         1 \n0.8750565 0.1249435 \n\n\nFinally, I thought it would be interesting to look at how many people got the room they booked. I made a binary indicator variable to do this. If someone got a different room they were assigned a 1 for different_room, otherwise a 0 indicating they got the room they booked. The proportion table shows that 87.5% of people got the room they wanted, and 12.5% of people did not."
  },
  {
    "objectID": "posts/challenge1_LindsayJones.html",
    "href": "posts/challenge1_LindsayJones.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_LindsayJones.html#describe-the-data",
    "href": "posts/challenge1_LindsayJones.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nData set contains the number of railroad employees in the United States in 2012, organized by county. Data was likely gathered reported to labor bureau either by counties or by each railroad station. Each row represents a county. Columns indicate state (or territory), county, and number of employees. There are 2930 counties as shown below:\n\n\nCode\ndim(Railroad)\n\n\n[1] 2930    3\n\n\nThe 10 counties with the most railroad employees are:\n\n\nCode\nRailroad %>%\n  arrange(desc(total_employees)) %>%\n  select(state,county,total_employees)%>%\n  group_by(total_employees) %>%\n  slice(1)%>%\n  ungroup()%>%\n  arrange(desc(total_employees))%>%\n  slice(1:10)\n\n\n\n\n  \n\n\n\nState_Railroad_Props illustrates the percentage of railroad workers located in each state or territory.\n\n\nCode\nState <- select(Railroad, state)\nState_Railroad_Props <- prop.table(table(State))*100\nState_Railroad_Props\n\n\nstate\n        AE         AK         AL         AP         AR         AZ         CA \n0.03412969 0.20477816 2.28668942 0.03412969 2.45733788 0.51194539 1.87713311 \n        CO         CT         DC         DE         FL         GA         HI \n1.94539249 0.27303754 0.03412969 0.10238908 2.28668942 5.18771331 0.10238908 \n        IA         ID         IL         IN         KS         KY         LA \n3.37883959 1.22866894 3.51535836 3.13993174 3.24232082 4.06143345 2.15017065 \n        MA         MD         ME         MI         MN         MO         MS \n0.40955631 0.81911263 0.54607509 2.66211604 2.93515358 3.92491468 2.66211604 \n        MT         NC         ND         NE         NH         NJ         NM \n1.80887372 3.20819113 1.67235495 3.03754266 0.34129693 0.71672355 0.98976109 \n        NV         NY         OH         OK         OR         PA         RI \n0.40955631 2.08191126 3.00341297 2.49146758 1.12627986 2.21843003 0.17064846 \n        SC         SD         TN         TX         UT         VA         VT \n1.56996587 1.77474403 3.10580205 7.54266212 0.85324232 3.13993174 0.47781570 \n        WA         WI         WV         WY \n1.33105802 2.35494881 1.80887372 0.75085324"
  },
  {
    "objectID": "posts/challenge3_Mekhala Kumar.html",
    "href": "posts/challenge3_Mekhala Kumar.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_Mekhala Kumar.html#cleaning-data-and-the-reasoning-for-using-pivot",
    "href": "posts/challenge3_Mekhala Kumar.html#cleaning-data-and-the-reasoning-for-using-pivot",
    "title": "Challenge 3",
    "section": "Cleaning data and the reasoning for using pivot",
    "text": "Cleaning data and the reasoning for using pivot\nThe dataset used was organicpoultry. It contains information about the quantity of different poultry types for all months across the years 2004 to 2013. Currently the month and year data all fall under a single column. In order to make the data easy to interpret, first, the column with the data of the month and year need to be separated into two columns.\nFollowing which, the data needs to be pivoted in such a manner that the years become columns and the types of poultry become rows. This format will make it easier to select a subgroup within the types of poultry and compare the changes across years.\n\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\neggpoul <- read_excel(\"_data/organiceggpoultry.xls\",skip=4)\nView(eggpoul)\ncolnames(eggpoul)\n\n\n [1] \"...1\"                            \"Extra Large \\nDozen\"            \n [3] \"Extra Large 1/2 Doz.\\n1/2 Dozen\" \"Large \\nDozen\"                  \n [5] \"Large \\n1/2 Doz.\"                \"...6\"                           \n [7] \"Whole\"                           \"B/S Breast\"                     \n [9] \"Bone-in Breast\"                  \"Whole Legs\"                     \n[11] \"Thighs\"                         \n\n\nCode\neggpoul=subset(eggpoul,select=-c(...6))\ntail(eggpoul, 10)\n\n\n# A tibble: 10 × 10\n   ...1     Extra…¹ Extra…² Large…³ Large…⁴ Whole B/S B…⁵ Bone-…⁶ Whole…⁷ Thighs\n   <chr>      <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl> <chr>     <dbl> <chr> \n 1 March        290    188.    268.     178  238.    704. 390.5      204. 216.25\n 2 April        290    188.    268.     178  238.    704. 390.5      204. 216.25\n 3 May          290    188.    268.     178  238.    704. 390.5      204. 216.25\n 4 June         290    188.    268.     178  238.    704. 390.5      204. 216.25\n 5 July         290    188.    268.     178  238.    704. 390.5      204. 216.25\n 6 August       290    188.    268.     178  238.    704. 390.5      204. 216.25\n 7 Septemb…     290    188.    268.     178  238.    704. 390.5      204. 216.25\n 8 October      290    188.    268.     178  238.    704. 390.5      204. 216.25\n 9 November     290    188.    268.     178  238.    704. 390.5      204. 216.25\n10 December     290    188.    268.     178  238.    704. 390.5      204. 216.25\n# … with abbreviated variable names ¹​`Extra Large \\nDozen`,\n#   ²​`Extra Large 1/2 Doz.\\n1/2 Dozen`, ³​`Large \\nDozen`, ⁴​`Large \\n1/2 Doz.`,\n#   ⁵​`B/S Breast`, ⁶​`Bone-in Breast`, ⁷​`Whole Legs`\n\n\nCode\nhead(eggpoul)\n\n\n# A tibble: 6 × 10\n  ...1     Extra …¹ Extra…² Large…³ Large…⁴ Whole B/S B…⁵ Bone-…⁶ Whole…⁷ Thighs\n  <chr>       <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl> <chr>     <dbl> <chr> \n1 Jan 2004     230     132     230     126   198.    646. too few    194. too f…\n2 February     230     134.    226.    128.  198.    642. too few    194. 203   \n3 March        230     137     225     131   209     642. too few    194. 203   \n4 April        234.    137     225     131   212     642. too few    194. 203   \n5 May          236     137     225     131   214.    642. too few    194. 203   \n6 June         241     137     231.    134.  216.    641  too few    202. 200.3…\n# … with abbreviated variable names ¹​`Extra Large \\nDozen`,\n#   ²​`Extra Large 1/2 Doz.\\n1/2 Dozen`, ³​`Large \\nDozen`, ⁴​`Large \\n1/2 Doz.`,\n#   ⁵​`B/S Breast`, ⁶​`Bone-in Breast`, ⁷​`Whole Legs`\n\n\nCode\nstr(eggpoul)\n\n\ntibble [120 × 10] (S3: tbl_df/tbl/data.frame)\n $ ...1                           : chr [1:120] \"Jan 2004\" \"February\" \"March\" \"April\" ...\n $ Extra Large \nDozen            : num [1:120] 230 230 230 234 236 ...\n $ Extra Large 1/2 Doz.\n1/2 Dozen: num [1:120] 132 134 137 137 137 ...\n $ Large \nDozen                  : num [1:120] 230 226 225 225 225 ...\n $ Large \n1/2 Doz.               : num [1:120] 126 128 131 131 131 ...\n $ Whole                          : num [1:120] 198 198 209 212 214 ...\n $ B/S Breast                     : num [1:120] 646 642 642 642 642 ...\n $ Bone-in Breast                 : chr [1:120] \"too few\" \"too few\" \"too few\" \"too few\" ...\n $ Whole Legs                     : num [1:120] 194 194 194 194 194 ...\n $ Thighs                         : chr [1:120] \"too few\" \"203\" \"203\" \"203\" ...\n\n\nCode\neggpoul<-eggpoul%>%\n   mutate(`Bone-in Breast` = parse_number(na_if(`Bone-in Breast`, \"too few\")),\n           Thighs = parse_number(na_if(Thighs, \"too few\")))\neggpoul<-eggpoul %>% separate(1, c(\"Month\", \"Year\"), extra = \"drop\", fill = \"right\")\nvec<-rep(c(1,2,3,4,5,6,7,8,9,10),each=12)\neggpoul$Year[vec==1] <- 2004\neggpoul$Year[vec==2] <- 2005\neggpoul$Year[vec==3] <- 2006\neggpoul$Year[vec==4] <- 2007\neggpoul$Year[vec==5] <- 2008\neggpoul$Year[vec==6] <- 2009\neggpoul$Year[vec==7] <- 2010\neggpoul$Year[vec==8] <- 2011\neggpoul$Year[vec==9] <- 2012\neggpoul$Year[vec==10] <- 2013\ndim(eggpoul)\n\n\n[1] 120  11\n\n\n\nChallenge: Describe the final dimensions\nThe original dataset has 120 rows and 11 columns. 2 of the variables are being used to identify a case. Hence,after pivoting, we expect to have 1080 rows and 4 columns. It is anticipated that the data will be long (taller).\n\n\nCode\n#existing rows/cases\nnrow(eggpoul)\n\n\n[1] 120\n\n\nCode\n#existing columns/cases\nncol(eggpoul)\n\n\n[1] 11\n\n\nCode\n#expected rows/cases\nnrow(eggpoul) * (ncol(eggpoul)-2)\n\n\n[1] 1080\n\n\nCode\n# expected columns \n(11-9)+2\n\n\n[1] 4\n\n\n\n\nChallenge: Pivot the Chosen Data\nAfter pivoting, the data has become taller. Pivoting has also ensured that all the variables of poultry types have been kept in a single column and the values corresponding to them are easy to access.\n\n\nCode\neggpoul<-pivot_longer(eggpoul, 3:11, names_to = \"Type of Poultry\", values_to = \"Amount\")\ndim(eggpoul)\n\n\n[1] 1080    4\n\n\nCode\nView(eggpoul)"
  },
  {
    "objectID": "posts/challenge1_AdithyaParupudi.html",
    "href": "posts/challenge1_AdithyaParupudi.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(dplyr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_AdithyaParupudi.html#challenge-overview",
    "href": "posts/challenge1_AdithyaParupudi.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_AdithyaParupudi.html#read-in-the-data",
    "href": "posts/challenge1_AdithyaParupudi.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nbirds_data <- read_csv(\"_data/birds.csv\",show_col_types = FALSE)\n#spec(birds_data) -> full column specification\n\n\nAfter importing the csv file, and I notice that out of the 14 columns, 8 of them are of character type and 6 columns are double. Total rows -> 30977!"
  },
  {
    "objectID": "posts/challenge1_AdithyaParupudi.html#describe-the-data",
    "href": "posts/challenge1_AdithyaParupudi.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nnames(birds_data)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nColumn names at a glance\n\n\nCode\nhead(birds_data)\n\n\n# A tibble: 6 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1961  1961 1000…\n2 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1962  1962 1000…\n3 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1963  1963 1000…\n4 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1964  1964 1000…\n5 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1965  1965 1000…\n6 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1966  1966 1000…\n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names\n\n\n\n\nCode\nstr(birds_data)\n\n\nspec_tbl_df [30,977 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Domain Code     : chr [1:30977] \"QA\" \"QA\" \"QA\" \"QA\" ...\n $ Domain          : chr [1:30977] \"Live Animals\" \"Live Animals\" \"Live Animals\" \"Live Animals\" ...\n $ Area Code       : num [1:30977] 2 2 2 2 2 2 2 2 2 2 ...\n $ Area            : chr [1:30977] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ Element Code    : num [1:30977] 5112 5112 5112 5112 5112 ...\n $ Element         : chr [1:30977] \"Stocks\" \"Stocks\" \"Stocks\" \"Stocks\" ...\n $ Item Code       : num [1:30977] 1057 1057 1057 1057 1057 ...\n $ Item            : chr [1:30977] \"Chickens\" \"Chickens\" \"Chickens\" \"Chickens\" ...\n $ Year Code       : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Year            : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Unit            : chr [1:30977] \"1000 Head\" \"1000 Head\" \"1000 Head\" \"1000 Head\" ...\n $ Value           : num [1:30977] 4700 4900 5000 5300 5500 5800 6600 6290 6300 6000 ...\n $ Flag            : chr [1:30977] \"F\" \"F\" \"F\" \"F\" ...\n $ Flag Description: chr [1:30977] \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Domain Code` = col_character(),\n  ..   Domain = col_character(),\n  ..   `Area Code` = col_double(),\n  ..   Area = col_character(),\n  ..   `Element Code` = col_double(),\n  ..   Element = col_character(),\n  ..   `Item Code` = col_double(),\n  ..   Item = col_character(),\n  ..   `Year Code` = col_double(),\n  ..   Year = col_double(),\n  ..   Unit = col_character(),\n  ..   Value = col_double(),\n  ..   Flag = col_character(),\n  ..   `Flag Description` = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nWe get to see a get a high level view of the column names and its entries.\n\n\nCode\nhist(birds_data$`Item Code`)\n\n\n\n\n\nCode\nhist(birds_data$`Area Code`)\n\n\n\n\n\nUsing the histogram functions, observed that the frequency for item code and area codes respectively."
  },
  {
    "objectID": "posts/Challenge_1_Yakub.html",
    "href": "posts/Challenge_1_Yakub.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Challenge_1_Yakub.html#challenge-overview",
    "href": "posts/Challenge_1_Yakub.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/Challenge_1_Yakub.html#read-in-the-data",
    "href": "posts/Challenge_1_Yakub.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nlibrary(readxl)\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/Challenge_1_Yakub.html#describe-the-data",
    "href": "posts/Challenge_1_Yakub.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nstatevcounty <- read_xls(\"_data/StateCounty2012.xls\", skip = 2)\n\n\n\n\nCode\nhead(statevcounty)\n\n\n# A tibble: 6 × 5\n  STATE     ...2  COUNTY               ...4  TOTAL\n  <chr>     <lgl> <chr>                <lgl> <dbl>\n1 AE        NA    APO                  NA        2\n2 AE Total1 NA    <NA>                 NA        2\n3 AK        NA    ANCHORAGE            NA        7\n4 AK        NA    FAIRBANKS NORTH STAR NA        2\n5 AK        NA    JUNEAU               NA        3\n6 AK        NA    MATANUSKA-SUSITNA    NA        2\n\n\n\n\nCode\nglimpse(statevcounty)\n\n\nRows: 2,990\nColumns: 5\n$ STATE  <chr> \"AE\", \"AE Total1\", \"AK\", \"AK\", \"AK\", \"AK\", \"AK\", \"AK\", \"AK Tota…\n$ ...2   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ COUNTY <chr> \"APO\", NA, \"ANCHORAGE\", \"FAIRBANKS NORTH STAR\", \"JUNEAU\", \"MATA…\n$ ...4   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ TOTAL  <dbl> 2, 2, 7, 2, 3, 2, 1, 88, 103, 102, 143, 1, 25, 154, 13, 29, 45,…\n\n\nPicked Relevant Columns\n\n\nCode\nstatevcounty <- statevcounty %>% select(STATE,COUNTY,TOTAL)"
  },
  {
    "objectID": "posts/challenge4_stevenoneill.html",
    "href": "posts/challenge4_stevenoneill.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(zoo)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_stevenoneill.html#read-in-data",
    "href": "posts/challenge4_stevenoneill.html#read-in-data",
    "title": "Challenge 4",
    "section": "Read in data",
    "text": "Read in data\n\ndebt_in_trillions ⭐⭐⭐⭐⭐\n\nI chose the debt_in_trillions dataset. I am going to use the .name_repair function to remove spaces from column names and replace them with periods.\n\n\nCode\ndebt <- read_excel(\"_data/debt_in_trillions.xlsx\",\n                  .name_repair = \"universal\")\ndebt\n\n\n\n\n  \n\n\n\nThis dataset contains debt in trillions based on fiscal year and quarter. It comes with a precalculated total for each of the 6 categories.\nThe column names are self-explanatory, except for HE.Revolving - that refers to home equity revolving lines of credit, also known as HELOC debt.\n\nSanity check\nDo the totals for each category of debt add up according to the “Total” column? Let’s see:\n\n\nCode\ndebt %>% rowwise() %>% mutate(calculated_total = sum(c_across(\"Mortgage\":\"Other\"), na.rm = T))\n\n\n\n\n  \n\n\n\nYes - it looks like Total and calculated_total are matching up, so I can rely on the original values.\n\n\nBriefly describe the data\nThe data describes total household debt balance (in trillions) and its composition. It was published by the Federal Reserve Bank of New York and its source can be found here."
  },
  {
    "objectID": "posts/challenge4_stevenoneill.html#tidy-ness",
    "href": "posts/challenge4_stevenoneill.html#tidy-ness",
    "title": "Challenge 4",
    "section": "Tidy-ness",
    "text": "Tidy-ness\nThe data seems tidy, but fiscal year is not the best format to do time-series analysis based on.\nThe lubridate package has helpful tools to get the quarter from each date… if you already have the full date. Here we just have the year and quarters, so I need to use the zoo package to get the first day of the quarter for that specific year.\nSo, using the zoo package, I can interpret the quarters as dates the following way:\n\n\nCode\ndebt2 <- debt %>% rowwise() %>% mutate(quarter_beginning_date = as.Date(as.yearqtr(Year.and.Quarter, format = \"%y:Q%q\")))\n\n\nThis is preferable to using complicated lookup tables or regular expressions.I don’t mind the original Year.And.Quarter column so I am going to keep it rather than replacing it with date.\nIf preferred, I can also get the last date of the quarter:\n\n\nCode\ndebt2 <- debt2 %>% rowwise() %>% mutate(quarter_ending_date = as.Date(as.yearqtr(Year.and.Quarter, format = \"%y:Q%q\"), frac = 1))\n\n\n\nTime intervals\nLubridate supports time intervals, which is maybe a better idea:\n\n\nCode\ndebt2 <- debt2 %>% rowwise() %>% mutate(quarter_interval = interval(quarter_beginning_date, quarter_ending_date))"
  },
  {
    "objectID": "posts/challenge4_stevenoneill.html#more-observations",
    "href": "posts/challenge4_stevenoneill.html#more-observations",
    "title": "Challenge 4",
    "section": "More observations",
    "text": "More observations\nThe percentage of student loan debt in proportion to total debt (per year) increased from 3.32% in Q1 of 2003 to 10.49% in Q1 of 2021. Ow!\n\n\nCode\ndebt2 %>% group_by(quarter_interval) %>% mutate(Student.Loan.Percent = 100 * Student.Loan/Total)"
  },
  {
    "objectID": "posts/challenge4_AnanyaPujary.html",
    "href": "posts/challenge4_AnanyaPujary.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(skimr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_AnanyaPujary.html#read-in-data",
    "href": "posts/challenge4_AnanyaPujary.html#read-in-data",
    "title": "Challenge 4",
    "section": "Read in data",
    "text": "Read in data\nI’ll be reading in the ‘FedFundsRate.csv’ dataset.\n\n\nCode\nfed_funds_rate<-read_csv(\"_data/FedFundsRate.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data\nGenerating an overview of the data.\n\n\nCode\nskim(fed_funds_rate)\n\n\n\nData summary\n\n\nName\nfed_funds_rate\n\n\nNumber of rows\n904\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1.00\n1986.68\n17.17\n1954.00\n1973.00\n1987.50\n2001.00\n2017.00\n▅▆▇▇▆\n\n\nMonth\n0\n1.00\n6.60\n3.47\n1.00\n4.00\n7.00\n10.00\n12.00\n▇▅▅▅▇\n\n\nDay\n0\n1.00\n3.60\n6.79\n1.00\n1.00\n1.00\n1.00\n31.00\n▇▁▁▁▁\n\n\nFederal Funds Target Rate\n442\n0.51\n5.66\n2.55\n1.00\n3.75\n5.50\n7.75\n11.50\n▅▅▇▅▂\n\n\nFederal Funds Upper Target\n801\n0.11\n0.31\n0.14\n0.25\n0.25\n0.25\n0.25\n1.00\n▇▁▁▁▁\n\n\nFederal Funds Lower Target\n801\n0.11\n0.06\n0.14\n0.00\n0.00\n0.00\n0.00\n0.75\n▇▁▁▁▁\n\n\nEffective Federal Funds Rate\n152\n0.83\n4.91\n3.61\n0.07\n2.43\n4.70\n6.58\n19.10\n▇▇▃▁▁\n\n\nReal GDP (Percent Change)\n654\n0.28\n3.14\n3.60\n-10.00\n1.40\n3.10\n4.88\n16.50\n▁▂▇▂▁\n\n\nUnemployment Rate\n152\n0.83\n5.98\n1.57\n3.40\n4.90\n5.70\n7.00\n10.80\n▅▇▅▁▁\n\n\nInflation Rate\n194\n0.79\n3.73\n2.57\n0.60\n2.00\n2.80\n4.70\n13.60\n▇▃▁▁▁\n\n\n\n\n\nThere are 904 rows and 10 columns in this dataset, all of which are numeric. It includes information on federal fund targets, unemployment rate, real GDP, and inflation rate between the years of 1954 and 2017. There seem to be a lot of missing values.\nThe ‘Federal Funds Target Rate’ varies between 1 and 11.5 percent, and the ‘Effective Federal Funds Target Rate’ varies between 0.07 and 19.1 percent. ‘Real GDP (Percent Change)’ lies between -10 and 16.5 percent, ‘Unemployment Rate’ between 3.4 and 10.8 percent, and ‘Inflation Rate’ between 0.6 and 13.6 percent."
  },
  {
    "objectID": "posts/challenge4_AnanyaPujary.html#tidy-data-and-mutate-variables",
    "href": "posts/challenge4_AnanyaPujary.html#tidy-data-and-mutate-variables",
    "title": "Challenge 4",
    "section": "Tidy Data and Mutate Variables",
    "text": "Tidy Data and Mutate Variables\nI think that converting the missing values to a numeric value like ‘0.0’ would hold some weight and not make much sense, since some of the existing column values are ‘0.0’. The columns ‘Year’, ‘Month’, and ‘Day’ can be combined to give a comprehensive date for each row.\n\n\nCode\nfed_funds_rate$Date <- str_c(fed_funds_rate$Year,\"-\", fed_funds_rate$Month,\"-\",fed_funds_rate$Day)\n\nstr(fed_funds_rate$Date) # the data type is character\n\n\n chr [1:904] \"1954-7-1\" \"1954-8-1\" \"1954-9-1\" \"1954-10-1\" \"1954-11-1\" ...\n\n\nCode\nDates <- as.Date(fed_funds_rate$Date)\n\nfed_funds_rate$Dates <- as.Date(fed_funds_rate$Date, format=\"%Y-%m-%d\")\n\nstr(fed_funds_rate$Dates) # the data type is date\n\n\n Date[1:904], format: \"1954-07-01\" \"1954-08-01\" \"1954-09-01\" \"1954-10-01\" \"1954-11-01\" ...\n\n\nCode\nfed_funds_rate_final <- fed_funds_rate %>%\n  select(-Date, -Year, -Month, -Day,-'Federal Funds Upper Target', -'Federal Funds Lower Target')\n\n\nAfter combining the three columns (‘Year’,‘Month’,‘Day’) into a new column ‘Date’, its data type was still ‘character’. I used the as.Date() function to convert the values to the date type and stored them in a new column, ‘Dates’. I removed the ‘Year’,‘Month’,‘Day’, and (old) ‘Date’ columns as well.\nWe could remove the ‘Federal Funds Upper Target’ and ‘Federal Funds Lower Target’ columns because they have a lot of missing values (801 of 904 total rows) and the ‘Federal Funds Target Rate’ seems to be providing the same information. I’m also reordering the columns such that ‘Dates’ is the first one.\n\n\nCode\nfed_funds_rate_final <- fed_funds_rate_final[, c(6,1,2,3,4,5)]\n\n\nBesides this, I think that the dataset is tidy enough to work with."
  },
  {
    "objectID": "posts/challenge5_MirandaManka.html",
    "href": "posts/challenge5_MirandaManka.html",
    "title": "Challenge 5",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_MirandaManka.html#challenge-overview",
    "href": "posts/challenge5_MirandaManka.html#challenge-overview",
    "title": "Challenge 5",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least two univariate visualizations\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\n\nCreate at least one bivariate visualization\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type"
  },
  {
    "objectID": "posts/challenge5_MirandaManka.html#read-in-data",
    "href": "posts/challenge5_MirandaManka.html#read-in-data",
    "title": "Challenge 5",
    "section": "Read in data",
    "text": "Read in data\n\nair_bnb = read_csv(\"_data/AB_NYC_2019.csv\", show_col_types = FALSE)\n\n\nBriefly describe the data\nThis is a dataset containing information about Airbnb listings in New York City. The variables include id and name of the listing, id and name of the host, neighborhood (including neighborhood group), latitude and longitude, room type, price, the minimum number of nights someone must book, number of reviews, last review date, reviews per month, the calculated count of host listings, and the availability (number out of 365/a year). Each row is a different listing.\n\nair_bnb\n\n# A tibble: 48,895 × 16\n      id name      host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n   <dbl> <chr>       <dbl> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <dbl>\n 1  2539 Clean & …    2787 John    Brookl… Kensin…    40.6   -74.0 Privat…   149\n 2  2595 Skylit M…    2845 Jennif… Manhat… Midtown    40.8   -74.0 Entire…   225\n 3  3647 THE VILL…    4632 Elisab… Manhat… Harlem     40.8   -73.9 Privat…   150\n 4  3831 Cozy Ent…    4869 LisaRo… Brookl… Clinto…    40.7   -74.0 Entire…    89\n 5  5022 Entire A…    7192 Laura   Manhat… East H…    40.8   -73.9 Entire…    80\n 6  5099 Large Co…    7322 Chris   Manhat… Murray…    40.7   -74.0 Entire…   200\n 7  5121 BlissArt…    7356 Garon   Brookl… Bedfor…    40.7   -74.0 Privat…    60\n 8  5178 Large Fu…    8967 Shunic… Manhat… Hell's…    40.8   -74.0 Privat…    79\n 9  5203 Cozy Cle…    7490 MaryEl… Manhat… Upper …    40.8   -74.0 Privat…    79\n10  5238 Cute & C…    7549 Ben     Manhat… Chinat…    40.7   -74.0 Entire…   150\n# … with 48,885 more rows, 6 more variables: minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>, and\n#   abbreviated variable names ¹​host_name, ²​neighbourhood_group,\n#   ³​neighbourhood, ⁴​latitude, ⁵​longitude, ⁶​room_type\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nsummary(air_bnb)\n\n       id               name              host_id           host_name        \n Min.   :    2539   Length:48895       Min.   :     2438   Length:48895      \n 1st Qu.: 9471945   Class :character   1st Qu.:  7822033   Class :character  \n Median :19677284   Mode  :character   Median : 30793816   Mode  :character  \n Mean   :19017143                      Mean   : 67620011                     \n 3rd Qu.:29152178                      3rd Qu.:107434423                     \n Max.   :36487245                      Max.   :274321313                     \n                                                                             \n neighbourhood_group neighbourhood         latitude       longitude     \n Length:48895        Length:48895       Min.   :40.50   Min.   :-74.24  \n Class :character    Class :character   1st Qu.:40.69   1st Qu.:-73.98  \n Mode  :character    Mode  :character   Median :40.72   Median :-73.96  \n                                        Mean   :40.73   Mean   :-73.95  \n                                        3rd Qu.:40.76   3rd Qu.:-73.94  \n                                        Max.   :40.91   Max.   :-73.71  \n                                                                        \n  room_type             price         minimum_nights    number_of_reviews\n Length:48895       Min.   :    0.0   Min.   :   1.00   Min.   :  0.00   \n Class :character   1st Qu.:   69.0   1st Qu.:   1.00   1st Qu.:  1.00   \n Mode  :character   Median :  106.0   Median :   3.00   Median :  5.00   \n                    Mean   :  152.7   Mean   :   7.03   Mean   : 23.27   \n                    3rd Qu.:  175.0   3rd Qu.:   5.00   3rd Qu.: 24.00   \n                    Max.   :10000.0   Max.   :1250.00   Max.   :629.00   \n                                                                         \n  last_review         reviews_per_month calculated_host_listings_count\n Min.   :2011-03-28   Min.   : 0.010    Min.   :  1.000               \n 1st Qu.:2018-07-08   1st Qu.: 0.190    1st Qu.:  1.000               \n Median :2019-05-19   Median : 0.720    Median :  1.000               \n Mean   :2018-10-04   Mean   : 1.373    Mean   :  7.144               \n 3rd Qu.:2019-06-23   3rd Qu.: 2.020    3rd Qu.:  2.000               \n Max.   :2019-07-08   Max.   :58.500    Max.   :327.000               \n NA's   :10052        NA's   :10052                                   \n availability_365\n Min.   :  0.0   \n 1st Qu.:  0.0   \n Median : 45.0   \n Mean   :112.8   \n 3rd Qu.:227.0   \n Max.   :365.0"
  },
  {
    "objectID": "posts/challenge5_MirandaManka.html#tidy-data",
    "href": "posts/challenge5_MirandaManka.html#tidy-data",
    "title": "Challenge 5",
    "section": "Tidy Data",
    "text": "Tidy Data\nI decided to take a few steps to tidy the data. First, I replaced the NAs in the reviews_per_month column because I could see in the number_of_reviews column there were some rows that had 0 (since you can’t divide 0 by a number of you get NA). I also removed some columns I knew I wouldn’t use for this analysis (id, name, host_id, host_name, latitude, and longitude). I thought about maybe dropping listings that had 0 availability_365 but I didn’t because I wasn’t sure if 0 meant the listing was inactive or fully booked.\n\nair_bnb = air_bnb %>% \n  mutate(reviews_per_month = replace_na(reviews_per_month, 0))\n\nair_bnb = air_bnb %>% \n  select(-c(\"id\", \"name\", \"host_id\", \"host_name\", \"latitude\", \"longitude\"))"
  },
  {
    "objectID": "posts/challenge5_MirandaManka.html#univariate-visualizations",
    "href": "posts/challenge5_MirandaManka.html#univariate-visualizations",
    "title": "Challenge 5",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations\nThe first variable I wanted to look at is room_type, because I thought it would be really interesting and could give some insight into the dataset as a whole. I chose a bar graph for room_type because it is a discrete variable and I think a bar graph makes it easy to visualize. The graph shows that entire home/apt is the most common room type (25409 listings), private room is close behind with the second most listings (22326 listings), while there are very few listings with a shared room (1160 listings).\n\nggplot(air_bnb, aes(x = room_type)) + \n  geom_bar(fill = \"lightgrey\") + \n  geom_text(aes(label = ..count..), stat = \"count\", size = 3, vjust = 1, \n     color = \"black\") +\n  labs(title = \"Room Type for Airbnb Listings\", x = \"Room Type\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\nI also wanted to examine neighbourhood_group because it provides a good idea of how many listings are in each area. I also chose a bar graph for neighbourhood_group because it is a discrete variable. The graph shows that there are many listings in Manhattan (21661 listings) and Brooklyn (20104 listings), much less in Queens (5666), and few in Bronx (1091 listings) and Staten Island (373 listings).\n\nggplot(air_bnb, aes(x = neighbourhood_group)) + \n  geom_bar(fill = \"lightgrey\") + \n  geom_text(aes(label = ..count..), stat = \"count\", size = 3, vjust = 1, \n     color = \"black\") +\n  labs(title = \"Neighborhood Group for Airbnb Listings\", x = \"Neighborhood Group\", \n     y = \"Frequency\") + \n  theme_bw()\n\n\n\n\nThe next variable I wanted to look at is number_of_reviews. Since it is a continuous variable, I used a histogram. The histogram shows that many listings have few reviews, and as the number of reviews increases the frequency decreases.\n\nggplot(air_bnb, aes(x = number_of_reviews)) + \n  geom_histogram(fill = \"lightgrey\") + \n  xlim(0, 100) + \n  labs(title = \"Number of Reviews for Airbnb Listings\", \n     x = \"Number of Reviews\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\nFor the final univariate visualization, I looked at last_review. This variable contains the date that the last review for the listing was given. I chose a density plot because it is a continuous variable and it looks smoother than a histogram. The plot shows that most of the listings had recent reviews (as of 2019), but a few had their last review in 2016 (these may be listings no longer active).\n\nggplot(air_bnb, aes(x = last_review)) +\n  geom_density(fill = \"lightgrey\") + \n  labs(title = \"Last Review Date for Airbnb Listings\", x = \"Date of Last Review\", \n     y = \"Density\") + \n  theme_bw()"
  },
  {
    "objectID": "posts/challenge5_MirandaManka.html#bivariate-visualizations",
    "href": "posts/challenge5_MirandaManka.html#bivariate-visualizations",
    "title": "Challenge 5",
    "section": "Bivariate Visualization(s)",
    "text": "Bivariate Visualization(s)\nThis visualization is a histogram of price (continuous) with room_type (discrete) as the fill. I chose this combination because I thought it showed these variables well together and gave some interesting insights. As expected, the shared rooms generally cost the least, while private rooms go for slightly more than shared rooms (however they have a fairly large range and some go for more), and the entire home/apt listings generally cost the most (but also have a wide range of prices).\n\nggplot(air_bnb, aes(x = price, fill = room_type)) + \n  geom_histogram(fill = \"lightgrey\") + \n  xlim(0, 600) + \n  labs(title = \"Airbnb Price and Room Type\", x = \"Price\", y = \"Frequency\", \n     fill = guide_legend(\"Room Type\")) + \n  theme_bw()"
  },
  {
    "objectID": "posts/challenge5_instructions_NJ.html",
    "href": "posts/challenge5_instructions_NJ.html",
    "title": "Challenge 5",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_instructions_NJ.html#read-in-data",
    "href": "posts/challenge5_instructions_NJ.html#read-in-data",
    "title": "Challenge 5",
    "section": "Read in data",
    "text": "Read in data\n\nAB <- read_csv(\"_data/AB_NYC_2019.csv\", col_names = c(\"del\", \"name\", \"del\", \"host_name\",\"neighbourhood_group\", \"neighbourhood\", \"latitude\", \"longitude\", \"room_type\", \"price\", \"minimum_nights\", \"number_of_reviews\", \"last_review\", \"reviews_per_month\", \"calculated_host_listings_count\", \"availability_365\" ), skip=1) %>% \n  select(!starts_with(\"del\")) %>%\n  drop_na(reviews_per_month)\n  \n  \n\nAB\n\n\n\n  \n\n\nprint(dfSummary(AB, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nAB\nDimensions: 38843 x 14\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      name\n[character]\n      1. Home away from home2. Loft Suite @ The Box Hous3. Private Room4. Brooklyn Apartment5. Cozy Brooklyn Apartment6. New york Multi-unit build7. Private room8. Beautiful Brooklyn Browns9. Harlem Gem10. Hillside Hotel[ 38253 others ]\n      12(0.0%)11(0.0%)10(0.0%)9(0.0%)8(0.0%)8(0.0%)8(0.0%)7(0.0%)7(0.0%)7(0.0%)38750(99.8%)\n      \n      6\n(0.0%)\n    \n    \n      host_name\n[character]\n      1. Michael2. David3. John4. Alex5. Sonder (NYC)6. Sarah7. Maria8. Daniel9. Jessica10. Anna[ 9876 others ]\n      335(0.9%)309(0.8%)250(0.6%)229(0.6%)207(0.5%)179(0.5%)174(0.4%)170(0.4%)170(0.4%)160(0.4%)36644(94.4%)\n      \n      16\n(0.0%)\n    \n    \n      neighbourhood_group\n[character]\n      1. Bronx2. Brooklyn3. Manhattan4. Queens5. Staten Island\n      876(2.3%)16447(42.3%)16632(42.8%)4574(11.8%)314(0.8%)\n      \n      0\n(0.0%)\n    \n    \n      neighbourhood\n[character]\n      1. Williamsburg2. Bedford-Stuyvesant3. Harlem4. Bushwick5. Hell's Kitchen6. East Village7. Upper West Side8. Upper East Side9. Crown Heights10. Midtown[ 208 others ]\n      3163(8.1%)3141(8.1%)2206(5.7%)1944(5.0%)1532(3.9%)1490(3.8%)1482(3.8%)1405(3.6%)1265(3.3%)986(2.5%)20229(52.1%)\n      \n      0\n(0.0%)\n    \n    \n      latitude\n[numeric]\n      Mean (sd) : 40.7 (0.1)min ≤ med ≤ max:40.5 ≤ 40.7 ≤ 40.9IQR (CV) : 0.1 (0)\n      17443 distinct values\n      \n      0\n(0.0%)\n    \n    \n      longitude\n[numeric]\n      Mean (sd) : -74 (0)min ≤ med ≤ max:-74.2 ≤ -74 ≤ -73.7IQR (CV) : 0 (0)\n      13641 distinct values\n      \n      0\n(0.0%)\n    \n    \n      room_type\n[character]\n      1. Entire home/apt2. Private room3. Shared room\n      20332(52.3%)17665(45.5%)846(2.2%)\n      \n      0\n(0.0%)\n    \n    \n      price\n[numeric]\n      Mean (sd) : 142.3 (196.9)min ≤ med ≤ max:0 ≤ 101 ≤ 10000IQR (CV) : 101 (1.4)\n      581 distinct values\n      \n      0\n(0.0%)\n    \n    \n      minimum_nights\n[numeric]\n      Mean (sd) : 5.9 (17.4)min ≤ med ≤ max:1 ≤ 2 ≤ 1250IQR (CV) : 3 (3)\n      89 distinct values\n      \n      0\n(0.0%)\n    \n    \n      number_of_reviews\n[numeric]\n      Mean (sd) : 29.3 (48.2)min ≤ med ≤ max:1 ≤ 9 ≤ 629IQR (CV) : 30 (1.6)\n      393 distinct values\n      \n      0\n(0.0%)\n    \n    \n      last_review\n[Date]\n      min : 2011-03-28med : 2019-05-19max : 2019-07-08range : 8y 3m 10d\n      1764 distinct values\n      \n      0\n(0.0%)\n    \n    \n      reviews_per_month\n[numeric]\n      Mean (sd) : 1.4 (1.7)min ≤ med ≤ max:0 ≤ 0.7 ≤ 58.5IQR (CV) : 1.8 (1.2)\n      937 distinct values\n      \n      0\n(0.0%)\n    \n    \n      calculated_host_listings_count\n[numeric]\n      Mean (sd) : 5.2 (26.3)min ≤ med ≤ max:1 ≤ 1 ≤ 327IQR (CV) : 1 (5.1)\n      47 distinct values\n      \n      0\n(0.0%)\n    \n    \n      availability_365\n[numeric]\n      Mean (sd) : 114.9 (129.5)min ≤ med ≤ max:0 ≤ 55 ≤ 365IQR (CV) : 229 (1.1)\n      366 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\n\nDescribe the data and Tidy\nAfter reading in the data I can tell I am dealing with NYC Air BnB data from 2019. To Tidy the data, I removed the id columns and rows with a value of 0 in them for number_of_reviews. The reason for this is that number_of_reivews will be a focal point of the analysis for this dataset. I want to compare number of reviews with other variables to see if reviews have any effect whether it is the independent or dependent variable. By looking at the summary table I could also investigate the number of reviews for each neighborhood_group and neighborhood."
  },
  {
    "objectID": "posts/challenge5_instructions_NJ.html#univariate-visualizations",
    "href": "posts/challenge5_instructions_NJ.html#univariate-visualizations",
    "title": "Challenge 5",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations\n\nggplot(data = AB) + \n  geom_bar(mapping = aes(x = neighbourhood_group, fill = room_type))\n\n\n\nAB_n <- AB %>%\n  group_by(neighbourhood_group) %>%\n  summarise(number_of_reviews = sum(number_of_reviews))\n\nAB_n\n\n\n\n  \n\n\n\nHere I created a Bar chart that counts the number of instances in a neighborhood group. I used the fill function to see which neighborhoods offer different types of rooms. Based on the results I can see that Brooklyn and Manhattan take up majority of the Air Bnbs in NYC. Also, within those two neighborhoods I can see that most of the Air BnBs are either a entire home/apt or a Private room. The Bronx and Staten Island do not offer any shared rooms."
  },
  {
    "objectID": "posts/challenge5_instructions_NJ.html#bivariate-visualizations",
    "href": "posts/challenge5_instructions_NJ.html#bivariate-visualizations",
    "title": "Challenge 5",
    "section": "Bivariate Visualization(s)",
    "text": "Bivariate Visualization(s)\n\nggplot(data = AB) + \n  geom_point(mapping = aes(x = price, y = number_of_reviews, color =room_type))+\n   facet_wrap(~ room_type, nrow = 2)\n\n\n\n\nHere I created subplots that each display price ad the X axis and each room_type as the Y axis. Based on the results I can see that around the price of 500, the amount of reviews starts to decline for all room types. One problem with these graphs is that they are out of scop making it hard to visualize what is going on."
  },
  {
    "objectID": "posts/challenge2_KatiePopiela.html#provide-summary-statistics-for-different-interesting-groups-within-the-data-and-interpret-those-statistics.",
    "href": "posts/challenge2_KatiePopiela.html#provide-summary-statistics-for-different-interesting-groups-within-the-data-and-interpret-those-statistics.",
    "title": "Challenge2_KatiePopiela",
    "section": "2) Provide summary statistics for different interesting groups within the data, and interpret those statistics.",
    "text": "2) Provide summary statistics for different interesting groups within the data, and interpret those statistics.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(readr)\nlibrary(dplyr)\n\n#For this challenge, I will read in \"railroad_2012_clean_county.csv.\" This dataset shows the number of railroad employees in each county in each state.\n\n\ncountyrr_data <- read.csv(\"_data/railroad_2012_clean_county.csv\")\nhead(countyrr_data)\n\n  state               county total_employees\n1    AE                  APO               2\n2    AK            ANCHORAGE               7\n3    AK FAIRBANKS NORTH STAR               2\n4    AK               JUNEAU               3\n5    AK    MATANUSKA-SUSITNA               2\n6    AK                SITKA               1\n\n\n\ndim(countyrr_data)\n\n[1] 2930    3\n\n\n\n#For confirmation, I got the dimensions of the table. With 2,930 rows, it must be filtered down and grouped in some way in order to present specific information. As an example, I will filter the data down to one state and put in for the most and fewest number of RR employees in that state.\n\ncountyrr_NY <- countyrr_data %>%\n  filter(state==\"NY\")\nmin(countyrr_NY$total_employees)\n\n[1] 5\n\nmax(countyrr_NY$total_employees)\n\n[1] 3685\n\n\n\ncountyrr_NY %>%\n  group_by(total_employees) %>%\n  slice_min(order_by = county)\n\n# A tibble: 54 × 3\n# Groups:   total_employees [54]\n   state county    total_employees\n   <chr> <chr>               <int>\n 1 NY    LEWIS                   5\n 2 NY    YATES                   6\n 3 NY    SCHUYLER                7\n 4 NY    TOMPKINS                8\n 5 NY    CORTLAND               11\n 6 NY    SENECA                 13\n 7 NY    SULLIVAN               14\n 8 NY    JEFFERSON              19\n 9 NY    FULTON                 20\n10 NY    CHEMUNG                21\n# … with 44 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n#There is a massive difference between the NY counties with the most and fewest RR employees - Suffolk County (3685) and Lewis (5). Furthermore, despite the large disparity between these two numbers, the average number of RR employees in NY is relatively low at 279.508. The SD (standard deviation) is 590.779. The variance came out as 349,019.9 which doesn't really make sense to me, though.\n\n\ncountyrr_NY %>%\n  summarize(mean = mean(total_employees, na.rm = TRUE), sd = sd(total_employees, na.rm = TRUE), var = var(total_employees, na.rm = TRUE)) \n\n      mean      sd      var\n1 279.5082 590.779 349019.9"
  },
  {
    "objectID": "posts/challenge1_instructions.html",
    "href": "posts/challenge1_instructions.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#challenge-overview",
    "href": "posts/challenge1_instructions.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#read-in-the-data",
    "href": "posts/challenge1_instructions.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions.html#describe-the-data",
    "href": "posts/challenge1_instructions.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge4_instructions_NJ.html",
    "href": "posts/challenge4_instructions_NJ.html",
    "title": "Challenge 4 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\nlibrary(lubridate)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_instructions_NJ.html#read-in-data",
    "href": "posts/challenge4_instructions_NJ.html#read-in-data",
    "title": "Challenge 4 Instructions",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\nFED <-read_csv(\"_data/FedFundsRate.csv\",\n                        show_col_types = FALSE, col_names = c(\"Year\", \"Month\",\n                               \"Day\", \"fed_funds_target_rate\",\n                              \"fed_funds_upper_target\",\"fed_funds_lower_target\",\"effective_fed_funds_rate\",\n                              \"real_gdp_percent_change\", \"unemployment_rate\", \"inflation_rate\"), skip =1)\nFED\n\n\n\n\n  \n\n\n\nCode\ncolnames(FED)\n\n\n [1] \"Year\"                     \"Month\"                   \n [3] \"Day\"                      \"fed_funds_target_rate\"   \n [5] \"fed_funds_upper_target\"   \"fed_funds_lower_target\"  \n [7] \"effective_fed_funds_rate\" \"real_gdp_percent_change\" \n [9] \"unemployment_rate\"        \"inflation_rate\"          \n\n\nCode\nprint(dfSummary(FED, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nFED\nDimensions: 904 x 10\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Year\n[numeric]\n      Mean (sd) : 1986.7 (17.2)min ≤ med ≤ max:1954 ≤ 1987.5 ≤ 2017IQR (CV) : 28 (0)\n      64 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Month\n[numeric]\n      Mean (sd) : 6.6 (3.5)min ≤ med ≤ max:1 ≤ 7 ≤ 12IQR (CV) : 6 (0.5)\n      12 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Day\n[numeric]\n      Mean (sd) : 3.6 (6.8)min ≤ med ≤ max:1 ≤ 1 ≤ 31IQR (CV) : 0 (1.9)\n      29 distinct values\n      \n      0\n(0.0%)\n    \n    \n      fed_funds_target_rate\n[numeric]\n      Mean (sd) : 5.7 (2.6)min ≤ med ≤ max:1 ≤ 5.5 ≤ 11.5IQR (CV) : 4 (0.5)\n      63 distinct values\n      \n      442\n(48.9%)\n    \n    \n      fed_funds_upper_target\n[numeric]\n      Mean (sd) : 0.3 (0.1)min ≤ med ≤ max:0.2 ≤ 0.2 ≤ 1IQR (CV) : 0 (0.5)\n      4 distinct values\n      \n      801\n(88.6%)\n    \n    \n      fed_funds_lower_target\n[numeric]\n      Mean (sd) : 0.1 (0.1)min ≤ med ≤ max:0 ≤ 0 ≤ 0.8IQR (CV) : 0 (2.4)\n      4 distinct values\n      \n      801\n(88.6%)\n    \n    \n      effective_fed_funds_rate\n[numeric]\n      Mean (sd) : 4.9 (3.6)min ≤ med ≤ max:0.1 ≤ 4.7 ≤ 19.1IQR (CV) : 4.2 (0.7)\n      466 distinct values\n      \n      152\n(16.8%)\n    \n    \n      real_gdp_percent_change\n[numeric]\n      Mean (sd) : 3.1 (3.6)min ≤ med ≤ max:-10 ≤ 3.1 ≤ 16.5IQR (CV) : 3.5 (1.1)\n      113 distinct values\n      \n      654\n(72.3%)\n    \n    \n      unemployment_rate\n[numeric]\n      Mean (sd) : 6 (1.6)min ≤ med ≤ max:3.4 ≤ 5.7 ≤ 10.8IQR (CV) : 2.1 (0.3)\n      71 distinct values\n      \n      152\n(16.8%)\n    \n    \n      inflation_rate\n[numeric]\n      Mean (sd) : 3.7 (2.6)min ≤ med ≤ max:0.6 ≤ 2.8 ≤ 13.6IQR (CV) : 2.7 (0.7)\n      106 distinct values\n      \n      194\n(21.5%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\n\nBriefly describe the data\nAfter reading in the data, I can see that each case is YMD rate value. The rate represents the Fund Rate type that is being assigned. The Value is the actual rate with the unit being percent. I noticed that the names of the columns with multiple words in them were not formatted correctly so that I can pivot them. I renamed the columns using underscores so that they are in the correct form to pivot. Once I created the new names for the columns I realized that the new column names were inserted into the first row. To remove them, I used the skip parameter and set it to 1 so that my columns names do not repeat. Using summary tools, I can see that all columns that include a rate value have missing values. The columns with most missing values are fed_funds_upper_target and fed_funds_lower_target , which suggests that for those months there was no fund intervals for the fed_funds_target_rate. In fact, there was no intervals created until filter(FED,Year == 2008, Month ==12, Day==16). This means that there is a lot of unfilled data that I will have to handle accordingly when tidying."
  },
  {
    "objectID": "posts/challenge4_instructions_NJ.html#tidy-data",
    "href": "posts/challenge4_instructions_NJ.html#tidy-data",
    "title": "Challenge 4 Instructions",
    "section": "Tidy Data",
    "text": "Tidy Data\nHere I create a new variable FED_long which is a long pivot of all the columns that include a rate value. I need to set up the data in the form YMD rate value, where each data instance has a Date, rate type and value. I use the parameter values_drop_na to remove any missing value from the dataset.\n\n\nCode\nFED_long <- FED%>%\n  pivot_longer( col = c(fed_funds_target_rate,\n                              fed_funds_upper_target,fed_funds_lower_target,effective_fed_funds_rate,\n                              real_gdp_percent_change, unemployment_rate, inflation_rate),\n                 names_to=\"type\",\n                 values_to = \"value\",\n                 values_drop_na = TRUE)\n\n\nFED_long \n\n\n\n\n  \n\n\n\nOne issue I see is that not all the data is being shown in the tibble because here the dataset ends at 1980 when it should continue to 2017. I believe it is something to do with the dimensions of the code chunk since I can still filter out the missing years."
  },
  {
    "objectID": "posts/challenge4_instructions_NJ.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_instructions_NJ.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Instructions",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nNow I have to mutate the dates so that they are in the correct form. I now want one column that includes Year, month, Day in form YYYY-MM-DD. To do this, I used mutate() to create the new column called Date, and within that I use the function make_datetime to format the date. I then select the three columns that we need to create our case: Date, type and value.\n\n\nCode\nnrow(FED_long)\n\n\n[1] 3132\n\n\nCode\nFED_long%>%\n  count(Year)\n\n\n\n\n  \n\n\n\nCode\nFED_long%>%\n  filter(Year > 1980)\n\n\n\n\n  \n\n\n\nCode\nFED_long%>%\n  filter(Year > 1998)\n\n\n\n\n  \n\n\n\nCode\nFED_t <- FED_long %>% \n  mutate(Date = make_datetime(Year, Month, Day)) %>%\n  select(Date,type,value)\n\n\n\nFED_t"
  },
  {
    "objectID": "posts/challenge3_AnanyaPujary.html",
    "href": "posts/challenge3_AnanyaPujary.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nlibrary(skimr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_AnanyaPujary.html#read-in-data",
    "href": "posts/challenge3_AnanyaPujary.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\nI’ll be reading in the ‘animal_weight’ dataset.\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\n\n\n\nBriefly describe the data\n\n\nCode\ndim(animal_weight)\n\n\n[1]  9 17\n\n\nCode\ncolnames(animal_weight) \n\n\n [1] \"IPCC Area\"          \"Cattle - dairy\"     \"Cattle - non-dairy\"\n [4] \"Buffaloes\"          \"Swine - market\"     \"Swine - breeding\"  \n [7] \"Chicken - Broilers\" \"Chicken - Layers\"   \"Ducks\"             \n[10] \"Turkeys\"            \"Sheep\"              \"Goats\"             \n[13] \"Horses\"             \"Asses\"              \"Mules\"             \n[16] \"Camels\"             \"Llamas\"            \n\n\nThe data chosen seems to describe the average weights of different animals (dairy and non-dairy cattle, chickens, ducks, etc.) across global regions (Africa, Latin America, Middle East, etc.). It has 9 rows and 17 columns, of which the \\(n=9\\) rows indicate the region name and \\(k=16\\) columns the type of animal.\n\n\nCode\nskim(animal_weight)\n\n\n\nData summary\n\n\nName\nanimal_weight\n\n\nNumber of rows\n9\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nIPCC Area\n0\n1\n4\n19\n0\n9\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCattle - dairy\n0\n1\n425.44\n140.39\n275.0\n275.0\n400.0\n550.0\n604.0\n▇▅▁▂▇\n\n\nCattle - non-dairy\n0\n1\n298.00\n116.26\n110.0\n173.0\n330.0\n391.0\n420.0\n▂▃▁▃▇\n\n\nBuffaloes\n0\n1\n370.56\n28.33\n295.0\n380.0\n380.0\n380.0\n380.0\n▁▁▁▁▇\n\n\nSwine - market\n0\n1\n39.22\n10.79\n28.0\n28.0\n45.0\n50.0\n50.0\n▇▁▁▂▇\n\n\nSwine - breeding\n0\n1\n116.44\n84.19\n28.0\n28.0\n180.0\n180.0\n198.0\n▆▁▁▁▇\n\n\nChicken - Broilers\n0\n1\n0.90\n0.00\n0.9\n0.9\n0.9\n0.9\n0.9\n▁▁▇▁▁\n\n\nChicken - Layers\n0\n1\n1.80\n0.00\n1.8\n1.8\n1.8\n1.8\n1.8\n▁▁▇▁▁\n\n\nDucks\n0\n1\n2.70\n0.00\n2.7\n2.7\n2.7\n2.7\n2.7\n▁▁▇▁▁\n\n\nTurkeys\n0\n1\n6.80\n0.00\n6.8\n6.8\n6.8\n6.8\n6.8\n▁▁▇▁▁\n\n\nSheep\n0\n1\n39.39\n10.80\n28.0\n28.0\n48.5\n48.5\n48.5\n▆▁▁▁▇\n\n\nGoats\n0\n1\n34.72\n4.48\n30.0\n30.0\n38.5\n38.5\n38.5\n▆▁▁▁▇\n\n\nHorses\n0\n1\n315.22\n73.26\n238.0\n238.0\n377.0\n377.0\n377.0\n▆▁▁▁▇\n\n\nAsses\n0\n1\n130.00\n0.00\n130.0\n130.0\n130.0\n130.0\n130.0\n▁▁▇▁▁\n\n\nMules\n0\n1\n130.00\n0.00\n130.0\n130.0\n130.0\n130.0\n130.0\n▁▁▇▁▁\n\n\nCamels\n0\n1\n217.00\n0.00\n217.0\n217.0\n217.0\n217.0\n217.0\n▁▁▇▁▁\n\n\nLlamas\n0\n1\n217.00\n0.00\n217.0\n217.0\n217.0\n217.0\n217.0\n▁▁▇▁▁\n\n\n\n\n\nThere are no missing values in this dataset. Overall, dairy cattle seem to have the highest average weight (425.44) and broiler chickens have the lowest (0.9).\nI plan to pivot it because it seems that the selected animals are recurring categories in all of the regions. \\(k-3\\) variables will be pivoted and put in a new column.\nHence, the pivoted dataset would have 144 rows and 3 columns (‘IPCC Area’,‘Animal Type’, ‘Weight’).\n\n\nChallenge: Describe the final dimensions\n\n\nCode\n# existing rows/cases\nnrow(animal_weight)\n\n\n[1] 9\n\n\nCode\n# existing columns/cases\nncol(animal_weight)\n\n\n[1] 17\n\n\nCode\n#expected rows/cases\nnrow(animal_weight) * (ncol(animal_weight)-1)\n\n\n[1] 144\n\n\nCode\n# expected columns \n1 + 2\n\n\n[1] 3\n\n\nThere are 9 existing rows and 17 existing columns. The expected rows are 144 and expected columns are 3.\n\n\nChallenge: Pivot the Chosen Data\n\n\nCode\nanimal_weight_pivoted <- pivot_longer(animal_weight,\n                         col = c('Cattle - dairy', 'Cattle - non-dairy', 'Buffaloes', 'Swine - market', 'Swine - breeding', 'Chicken - Broilers', 'Chicken - Layers', 'Ducks', 'Turkeys', 'Sheep', 'Goats', 'Horses', 'Asses', 'Mules', 'Camels', 'Llamas'), names_to = 'Animal Type', values_to = 'Weight')\nanimal_weight_pivoted\n\n\n# A tibble: 144 × 3\n   `IPCC Area`         `Animal Type`      Weight\n   <chr>               <chr>               <dbl>\n 1 Indian Subcontinent Cattle - dairy      275  \n 2 Indian Subcontinent Cattle - non-dairy  110  \n 3 Indian Subcontinent Buffaloes           295  \n 4 Indian Subcontinent Swine - market       28  \n 5 Indian Subcontinent Swine - breeding     28  \n 6 Indian Subcontinent Chicken - Broilers    0.9\n 7 Indian Subcontinent Chicken - Layers      1.8\n 8 Indian Subcontinent Ducks                 2.7\n 9 Indian Subcontinent Turkeys               6.8\n10 Indian Subcontinent Sheep                28  \n# … with 134 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\ndim(animal_weight_pivoted)\n\n\n[1] 144   3\n\n\nThe dimensions of the pivoted data, as predicted, are 144 rows and 3 columns. The new case created is ‘Animal Type’. Overall, pivoting made the data easier to understand since we can now find the weight of a certain animal from a particular region."
  },
  {
    "objectID": "posts/challenge1_instructions-Youngsoo Choi.html",
    "href": "posts/challenge1_instructions-Youngsoo Choi.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions-Youngsoo Choi.html#challenge-overview",
    "href": "posts/challenge1_instructions-Youngsoo Choi.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions-Youngsoo Choi.html#read-in-the-data",
    "href": "posts/challenge1_instructions-Youngsoo Choi.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nI choose data about birds.\n\n\nCode\nlibrary(readr)\nbirds <- read_csv(\"_data/birds.csv\")\nbirds\n\n\n# A tibble: 30,977 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1961  1961\n 2 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1962  1962\n 3 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1963  1963\n 4 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1964  1964\n 5 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1965  1965\n 6 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1966  1966\n 7 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1967  1967\n 8 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1968  1968\n 9 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1969  1969\n10 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    1970  1970\n# … with 30,967 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThis dataset contains over 30000 birds’ characters include area, years, etc."
  },
  {
    "objectID": "posts/challenge1_instructions-Youngsoo Choi.html#describe-the-data",
    "href": "posts/challenge1_instructions-Youngsoo Choi.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nThis data set I read contains data from 1961. I wonder how many birds of that year code is after year 2000 in this data set.\n\n\nCode\na2000<-filter(birds, Year>=2000)\na2000\n\n\n# A tibble: 10,945 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2000  2000\n 2 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2001  2001\n 3 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2002  2002\n 4 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2003  2003\n 5 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2004  2004\n 6 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2005  2005\n 7 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2006  2006\n 8 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2007  2007\n 9 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2008  2008\n10 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2009  2009\n# … with 10,935 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nCode\ncount(a2000)\n\n\n# A tibble: 1 × 1\n      n\n  <int>\n1 10945\n\n\nAnd the number of birds with year 2000 after is 10945."
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html",
    "href": "posts/challenge2_LindsayJones.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#challenge-overview",
    "href": "posts/challenge2_LindsayJones.html#challenge-overview",
    "title": "Challenge 2",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#read-in-the-data",
    "href": "posts/challenge2_LindsayJones.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nhotel_bookings ⭐⭐⭐\nFAOstat*.csv ⭐⭐⭐⭐⭐ (join FAOSTAT_country_groups)\n\n\n\nCode\nlibrary(readxl)\nRailroad <- read_xls(\"_data/StateCounty2012.xls\", \n    skip =3)\nView(Railroad)"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#describe-the-data",
    "href": "posts/challenge2_LindsayJones.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nTotal railroad employment by state and county for calendar year 2012.\n\n\nCode\nlibrary(readxl)\nRailroad <- read_xls(\"_data/StateCounty2012.xls\", \n    skip =3)\nView(Railroad)"
  },
  {
    "objectID": "posts/challenge2_LindsayJones.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_LindsayJones.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\nThe total number of railroad employees for each state/territory are shown below.\n\n\nCode\nstate_totals = Railroad %>% \n  filter(grepl(\"Total\", STATE)) %>%\n  filter(!grepl(\"Grand\", STATE))\nprint(state_totals)\n\n\n# A tibble: 53 × 5\n   STATE     ...2  COUNTY ...4  TOTAL\n   <chr>     <lgl> <chr>  <lgl> <dbl>\n 1 AE Total1 NA    <NA>   NA        2\n 2 AK Total  NA    <NA>   NA      103\n 3 AL Total  NA    <NA>   NA     4257\n 4 AP Total1 NA    <NA>   NA        1\n 5 AR Total  NA    <NA>   NA     3871\n 6 AZ Total  NA    <NA>   NA     3153\n 7 CA Total  NA    <NA>   NA    13137\n 8 CO Total  NA    <NA>   NA     3650\n 9 CT Total  NA    <NA>   NA     2592\n10 DC Total  NA    <NA>   NA      279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nHere is the median number of railroad employees per state:\n\n\nCode\nsummarize(state_totals,median(TOTAL))\n\n\n# A tibble: 1 × 1\n  `median(TOTAL)`\n            <dbl>\n1            3379\n\n\nThe mean number railroad employees in each state is:\n\n\nCode\nsummarize(state_totals,mean(TOTAL))\n\n\n# A tibble: 1 × 1\n  `mean(TOTAL)`\n          <dbl>\n1         4819.\n\n\nThe Armed Forces Pacific (AP) is the territory with the fewest employees:\n\n\nCode\nsummarize(state_totals,min(TOTAL))\n\n\n# A tibble: 1 × 1\n  `min(TOTAL)`\n         <dbl>\n1            1\n\n\nAnd Texas is the territory with the most employees:\n\n\nCode\nsummarize(state_totals,max(TOTAL))\n\n\n# A tibble: 1 × 1\n  `max(TOTAL)`\n         <dbl>\n1        19839\n\n\nStandard deviation of employees is:\n\n\nCode\nsd(state_totals$TOTAL)\n\n\n[1] 4781.829\n\n\n\nExplain and Interpret\nI chose the states because I found some of the totals for each state surprising, based on what I know about the size of those states’ populations."
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html",
    "href": "posts/challenge1_RoyYoon.html",
    "title": "Challenge 1 Roy Yoon",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#reading-data",
    "href": "posts/challenge1_RoyYoon.html#reading-data",
    "title": "Challenge 1 Roy Yoon",
    "section": "Reading Data",
    "text": "Reading Data\n\n\nCode\nlibrary(readr)\nbirds <- read_csv(\"_data/birds.csv\")\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#a-quick-look-at-birds",
    "href": "posts/challenge1_RoyYoon.html#a-quick-look-at-birds",
    "title": "Challenge 1 Roy Yoon",
    "section": "A Quick Look at ‘birds’",
    "text": "A Quick Look at ‘birds’\n\n\nCode\nhead(birds)\n\n\n# A tibble: 6 × 14\n  Domai…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year Unit \n  <chr>   <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl> <chr>\n1 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1961  1961 1000…\n2 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1962  1962 1000…\n3 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1963  1963 1000…\n4 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1964  1964 1000…\n5 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1965  1965 1000…\n6 QA      Live …       2 Afgh…    5112 Stocks     1057 Chic…    1966  1966 1000…\n# … with 3 more variables: Value <dbl>, Flag <chr>, `Flag Description` <chr>,\n#   and abbreviated variable names ¹​`Domain Code`, ²​`Area Code`,\n#   ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `colnames()` to see all variable names"
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#dimensions",
    "href": "posts/challenge1_RoyYoon.html#dimensions",
    "title": "Challenge 1 Roy Yoon",
    "section": "Dimensions",
    "text": "Dimensions\n\n\nCode\n#understanding the dimensions of data set 'birds'\ndim(birds)\n\n\n[1] 30977    14"
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#column-names",
    "href": "posts/challenge1_RoyYoon.html#column-names",
    "title": "Challenge 1 Roy Yoon",
    "section": "Column Names",
    "text": "Column Names\nThere are 30977 rows and 14 column in the data set\n\n\nCode\n#column names in  data set 'birds'\n\ncolnames(birds)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\""
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#cases-when-birds-data-value-is-greater-then-10000",
    "href": "posts/challenge1_RoyYoon.html#cases-when-birds-data-value-is-greater-then-10000",
    "title": "Challenge 1 Roy Yoon",
    "section": "Cases when ‘birds’ data ‘Value’ is greater then 10000",
    "text": "Cases when ‘birds’ data ‘Value’ is greater then 10000\n\n\nCode\n#looking at 'birds' data set that has 'Value' column value greater than 10000\nmore_than_10000 <- filter(birds, Value > 10000)\n\nmore_than_10000 \n\n\n# A tibble: 8,991 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2002  2002\n 2 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2003  2003\n 3 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2004  2004\n 4 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2005  2005\n 5 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2006  2006\n 6 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2008  2008\n 7 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2009  2009\n 8 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2010  2010\n 9 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2011  2011\n10 QA           Live …       2 Afgh…    5112 Stocks     1057 Chic…    2012  2012\n# … with 8,981 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nCode\narrange(more_than_10000, Value)\n\n\n# A tibble: 8,991 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …     211 Swit…    5112 Stocks     1057 Chic…    2013  2013\n 2 QA           Live …     171 Phil…    5112 Stocks     1068 Ducks    2012  2012\n 3 QA           Live …     138 Mexi…    5112 Stocks     1079 Turk…    1982  1982\n 4 QA           Live …     115 Camb…    5112 Stocks     1057 Chic…    1994  1994\n 5 QA           Live …     222 Tuni…    5112 Stocks     1079 Turk…    2012  2012\n 6 QA           Live …      26 Brun…    5112 Stocks     1057 Chic…    2002  2002\n 7 QA           Live …      49 Cuba     5112 Stocks     1057 Chic…    1965  1965\n 8 QA           Live …     158 Niger    5112 Stocks     1057 Chic…    1990  1990\n 9 QA           Live …     133 Mali     5112 Stocks     1057 Chic…    1961  1961\n10 QA           Live …     225 Unit…    5112 Stocks     1057 Chic…    1994  1994\n# … with 8,981 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe data above shows ‘birds’ for if the Value column was greater than 10000. The data is arranged by the ‘Value’ column values."
  },
  {
    "objectID": "posts/challenge1_RoyYoon.html#data-for-algeria-in-bird",
    "href": "posts/challenge1_RoyYoon.html#data-for-algeria-in-bird",
    "title": "Challenge 1 Roy Yoon",
    "section": "Data for Algeria in ‘bird’",
    "text": "Data for Algeria in ‘bird’\n\n\nCode\n#looking at 'birds' data set that has 'Value' column value greater than 10000 specifically for Algeria \nmore_than_10000_ALG <- filter(more_than_10000, Area == \"Algeria\")\n\nmore_than_10000_ALG \n\n\n# A tibble: 54 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1965  1965\n 2 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1966  1966\n 3 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1967  1967\n 4 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1968  1968\n 5 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1969  1969\n 6 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1970  1970\n 7 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1971  1971\n 8 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1972  1972\n 9 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1973  1973\n10 QA           Live …       4 Alge…    5112 Stocks     1057 Chic…    1974  1974\n# … with 44 more rows, 4 more variables: Unit <chr>, Value <dbl>, Flag <chr>,\n#   `Flag Description` <chr>, and abbreviated variable names ¹​`Domain Code`,\n#   ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe data above examines the data for Algeria by looking at ‘birds’ data set that haa values greater than 10000."
  },
  {
    "objectID": "posts/challenge3_ManiShankerKamarapu.html",
    "href": "posts/challenge3_ManiShankerKamarapu.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_ManiShankerKamarapu.html#read-in-data",
    "href": "posts/challenge3_ManiShankerKamarapu.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\nvote_response <- read_excel(\"_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\", sheet = \"Table 2\", skip = 7, col_names = c(\"Area\", \"Yes\", \"Yes_P\", \"No\", \"No_P\", \"Totalclear\", \"Totalclear_P\", \"Empty\", \"clear\", \"clear_P\", \"Not_Eligible\", \"Not_Eligible_P\", \"NoResponse\", \"NoResponse_P\", \"Total\", \"Total_P\")) %>%\n  select(\"Area\", \"Yes\", \"No\", \"Not_Eligible\", \"NoResponse\", \"Total\") %>%\n  drop_na(\"Area\") %>%\n  filter(!grepl(\"Total\", Area))\nR <- nrow(vote_response)-7\nvote_response <- slice(vote_response, 1:R)\nvote_response\n\n\n# A tibble: 158 × 6\n   Area                        Yes    No Not_Eligible NoResponse  Total\n   <chr>                     <dbl> <dbl>        <dbl>      <dbl>  <dbl>\n 1 New South Wales Divisions    NA    NA           NA         NA     NA\n 2 Banks                     37736 46343          247      20928 105254\n 3 Barton                    37153 47984          226      24008 109371\n 4 Bennelong                 42943 43215          244      19973 106375\n 5 Berowra                   48471 40369          212      16038 105090\n 6 Blaxland                  20406 57926          220      25883 104435\n 7 Bradfield                 53681 34927          202      17261 106071\n 8 Calare                    54091 35779          285      25342 115497\n 9 Chifley                   32871 46702          263      28180 108016\n10 Cook                      47505 38804          229      18713 105251\n# … with 148 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\ndim(vote_response)\n\n\n[1] 158   6\n\n\nThe dimensions of the data set are 158 rows and 6 columns.\nThe data is on the postal survey of Australian Electoral Roll. It contains the data of the eligible participants and responses of the participants in federal election as at 24 August 2017. It has data based on different federal electoral divisions survey by the Australian Election Commission. It is basically the total number of response we got in each division during the federal election. The data set is untidy and variables are not defined correctly and there are a lot of extra variables which are not required and a bunch of NA values. So using different R functions to remove unwanted variables and drop some NA values and also remove extra rows."
  },
  {
    "objectID": "posts/challenge3_ManiShankerKamarapu.html#separating-different-divisions-and-areas",
    "href": "posts/challenge3_ManiShankerKamarapu.html#separating-different-divisions-and-areas",
    "title": "Challenge 3",
    "section": "Separating different divisions and areas",
    "text": "Separating different divisions and areas\n\n\nCode\nvote_response <- vote_response %>%\n  mutate(Division = case_when(\n    str_ends(Area, \"Divisions\") ~ Area,\n    TRUE ~ NA_character_ )) %>%\n  fill(Division) %>%\n  drop_na(\"Yes\")\nvote_response\n\n\n# A tibble: 150 × 7\n   Area        Yes    No Not_Eligible NoResponse  Total Division                \n   <chr>     <dbl> <dbl>        <dbl>      <dbl>  <dbl> <chr>                   \n 1 Banks     37736 46343          247      20928 105254 New South Wales Divisio…\n 2 Barton    37153 47984          226      24008 109371 New South Wales Divisio…\n 3 Bennelong 42943 43215          244      19973 106375 New South Wales Divisio…\n 4 Berowra   48471 40369          212      16038 105090 New South Wales Divisio…\n 5 Blaxland  20406 57926          220      25883 104435 New South Wales Divisio…\n 6 Bradfield 53681 34927          202      17261 106071 New South Wales Divisio…\n 7 Calare    54091 35779          285      25342 115497 New South Wales Divisio…\n 8 Chifley   32871 46702          263      28180 108016 New South Wales Divisio…\n 9 Cook      47505 38804          229      18713 105251 New South Wales Divisio…\n10 Cowper    57493 38317          315      25197 121322 New South Wales Divisio…\n# … with 140 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\ndim(vote_response)\n\n\n[1] 150   7\n\n\nAfter we read the data and did some hard coding along with read, now we have separated the divisions row from the area column and formed a new column so we can use it as a grouping variable."
  },
  {
    "objectID": "posts/challenge3_ManiShankerKamarapu.html#using-pivot-to-tidy-data",
    "href": "posts/challenge3_ManiShankerKamarapu.html#using-pivot-to-tidy-data",
    "title": "Challenge 3",
    "section": "Using Pivot to tidy data",
    "text": "Using Pivot to tidy data\n\n\nCode\nvote_response <- pivot_longer(vote_response, Yes:Total, names_to = \"Response\", values_to = \"Count\")\nvote_response\n\n\n# A tibble: 750 × 4\n   Area   Division                  Response      Count\n   <chr>  <chr>                     <chr>         <dbl>\n 1 Banks  New South Wales Divisions Yes           37736\n 2 Banks  New South Wales Divisions No            46343\n 3 Banks  New South Wales Divisions Not_Eligible    247\n 4 Banks  New South Wales Divisions NoResponse    20928\n 5 Banks  New South Wales Divisions Total        105254\n 6 Barton New South Wales Divisions Yes           37153\n 7 Barton New South Wales Divisions No            47984\n 8 Barton New South Wales Divisions Not_Eligible    226\n 9 Barton New South Wales Divisions NoResponse    24008\n10 Barton New South Wales Divisions Total        109371\n# … with 740 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\ndim(vote_response)\n\n\n[1] 750   4\n\n\nNow the data set has become longer and final dimensions are 750 rows and 4 columns.\nI have used pivot_longer to tidy the data set where I have collapse the responses(Yes, No, Not_Eligible, NoResponse and Total) into one column so we can vary them easily and really make sense. So now we can get the divisional level count or area level count easily and we can also plot it easily now in different ways."
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html",
    "href": "posts/challenge1_instructions_NJ.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html#challenge-overview",
    "href": "posts/challenge1_instructions_NJ.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html#read-in-the-data",
    "href": "posts/challenge1_instructions_NJ.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")\nrailroad\n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions_NJ.html#describe-the-data",
    "href": "posts/challenge1_instructions_NJ.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nspec(railroad)\n\n\ncols(\n  state = col_character(),\n  county = col_character(),\n  total_employees = col_double()\n)\n\n\nCode\nrailroad %>%\n  filter(state == \"AK\")\n\n\n# A tibble: 6 × 3\n  state county               total_employees\n  <chr> <chr>                          <dbl>\n1 AK    ANCHORAGE                          7\n2 AK    FAIRBANKS NORTH STAR               2\n3 AK    JUNEAU                             3\n4 AK    MATANUSKA-SUSITNA                  2\n5 AK    SITKA                              1\n6 AK    SKAGWAY MUNICIPALITY              88\n\n\nCode\nrailroad %>%\n  group_by(state) %>%\n  summarise(total_employees2 = sum(total_employees))\n\n\n# A tibble: 53 × 2\n   state total_employees2\n   <chr>            <dbl>\n 1 AE                   2\n 2 AK                 103\n 3 AL                4257\n 4 AP                   1\n 5 AR                3871\n 6 AZ                3153\n 7 CA               13137\n 8 CO                3650\n 9 CT                2592\n10 DC                 279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWhen I used the spec() function, it returned the variable types of the columns. We know that state is type col_character(), county is type col_character() and total_employees is type col_double(). When I filtered through just the state of AK it returns the list of the total employees for each county in that state. I created a pipe that groups all the states together an than is summarized by the total amount of employees in each state. This pipe is useful because I now know how many total employees are in each state. This data was likely gathered in 2012 from the most used railroad stations in the USA. This dataset is also long in the sense that the column length is much greater than the amount of columns present."
  },
  {
    "objectID": "posts/challenge5_instructions.html",
    "href": "posts/challenge5_instructions.html",
    "title": "Challenge 5 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_instructions.html#challenge-overview",
    "href": "posts/challenge5_instructions.html#challenge-overview",
    "title": "Challenge 5 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least two univariate visualizations\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\n\nCreate at least one bivariate visualization\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge5_instructions.html#read-in-data",
    "href": "posts/challenge5_instructions.html#read-in-data",
    "title": "Challenge 5 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\ncereal ⭐\npathogen cost ⭐\nAustralian Marriage ⭐⭐\nAB_NYC_2019.csv ⭐⭐⭐\nrailroads ⭐⭐⭐\nPublic School Characteristics ⭐⭐⭐⭐\nUSA Households ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge5_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge5_instructions.html#tidy-data-as-needed",
    "title": "Challenge 5 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge5_instructions.html#univariate-visualizations",
    "href": "posts/challenge5_instructions.html#univariate-visualizations",
    "title": "Challenge 5 Instructions",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations"
  },
  {
    "objectID": "posts/challenge5_instructions.html#bivariate-visualizations",
    "href": "posts/challenge5_instructions.html#bivariate-visualizations",
    "title": "Challenge 5 Instructions",
    "section": "Bivariate Visualization(s)",
    "text": "Bivariate Visualization(s)\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html",
    "href": "posts/challenge2_instructions_NJ.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html#read-in-data",
    "href": "posts/challenge2_instructions_NJ.html#read-in-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in Data",
    "text": "Read in Data\n\n\nCode\nFAO <- read_csv(\"_data/FAOSTAT_livestock.csv\")\nFAO\n\n\n# A tibble: 82,116 × 14\n   Domain Cod…¹ Domain Area …² Area  Eleme…³ Element Item …⁴ Item  Year …⁵  Year\n   <chr>        <chr>    <dbl> <chr>   <dbl> <chr>     <dbl> <chr>   <dbl> <dbl>\n 1 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1961  1961\n 2 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1962  1962\n 3 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1963  1963\n 4 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1964  1964\n 5 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1965  1965\n 6 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1966  1966\n 7 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1967  1967\n 8 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1968  1968\n 9 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1969  1969\n10 QA           Live …       2 Afgh…    5111 Stocks     1107 Asses    1970  1970\n# … with 82,106 more rows, 4 more variables: Unit <chr>, Value <dbl>,\n#   Flag <chr>, `Flag Description` <chr>, and abbreviated variable names\n#   ¹​`Domain Code`, ²​`Area Code`, ³​`Element Code`, ⁴​`Item Code`, ⁵​`Year Code`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThis dataset comes from the Food and Agriculture Association of the United Nations. They publish country-level data regularly and I am going to be looking at country-level estimates of the number of animals that are raised for livestock. We can see that there are 82116 rows in the livestock data."
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html#describe-the-data",
    "href": "posts/challenge2_instructions_NJ.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nspec(FAO)\n\n\ncols(\n  `Domain Code` = col_character(),\n  Domain = col_character(),\n  `Area Code` = col_double(),\n  Area = col_character(),\n  `Element Code` = col_double(),\n  Element = col_character(),\n  `Item Code` = col_double(),\n  Item = col_character(),\n  `Year Code` = col_double(),\n  Year = col_double(),\n  Unit = col_character(),\n  Value = col_double(),\n  Flag = col_character(),\n  `Flag Description` = col_character()\n)\n\n\nCode\nFAO.sm <- FAO %>%\n  select(-contains(\"Code\"))\nFAO.sm\n\n\n# A tibble: 82,116 × 9\n   Domain       Area        Element Item   Year Unit    Value Flag  Flag Descr…¹\n   <chr>        <chr>       <chr>   <chr> <dbl> <chr>   <dbl> <chr> <chr>       \n 1 Live Animals Afghanistan Stocks  Asses  1961 Head  1300000 <NA>  Official da…\n 2 Live Animals Afghanistan Stocks  Asses  1962 Head   851850 <NA>  Official da…\n 3 Live Animals Afghanistan Stocks  Asses  1963 Head  1001112 <NA>  Official da…\n 4 Live Animals Afghanistan Stocks  Asses  1964 Head  1150000 F     FAO estimate\n 5 Live Animals Afghanistan Stocks  Asses  1965 Head  1300000 <NA>  Official da…\n 6 Live Animals Afghanistan Stocks  Asses  1966 Head  1200000 <NA>  Official da…\n 7 Live Animals Afghanistan Stocks  Asses  1967 Head  1200000 <NA>  Official da…\n 8 Live Animals Afghanistan Stocks  Asses  1968 Head  1328000 <NA>  Official da…\n 9 Live Animals Afghanistan Stocks  Asses  1969 Head  1250000 <NA>  Official da…\n10 Live Animals Afghanistan Stocks  Asses  1970 Head  1300000 <NA>  Official da…\n# … with 82,106 more rows, and abbreviated variable name ¹​`Flag Description`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nprint(dfSummary(FAO.sm, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nFAO.sm\nDimensions: 82116 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Live Animals\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Africa2. Asia3. China, mainland4. Eastern Africa5. Eastern Asia6. Eastern Europe7. Egypt8. Europe9. India10. Northern Africa[ 243 others ]\n      522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)522(0.6%)76896(93.6%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Stocks\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Asses2. Buffaloes3. Camels4. Cattle5. Goats6. Horses7. Mules8. Pigs9. Sheep\n      8571(10.4%)3505(4.3%)3265(4.0%)13086(15.9%)12498(15.2%)11104(13.5%)6153(7.5%)12015(14.6%)11919(14.5%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.4 (16.8)min ≤ med ≤ max:1961 ≤ 1991 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. Head\n      82116(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 11625569 (64779790)min ≤ med ≤ max:0 ≤ 224667 ≤ 1489744504IQR (CV) : 2364200 (5.6)\n      43667 distinct values\n      \n      1301\n(1.6%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Im5. M\n      2667(6.1%)12567(28.7%)24550(56.0%)2877(6.6%)1185(2.7%)\n      \n      38270\n(46.6%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Data not available3. FAO data based on imputat4. FAO estimate5. Official data6. Unofficial figure\n      12567(15.3%)1185(1.4%)2877(3.5%)24550(29.9%)38270(46.6%)2667(3.2%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\nBased on the results of the spec() function, I can see that there are six variables that are type double and eight that are type character. Out of the six double() variables, Area Code, Year and Item Code are all good grouping variables because they do not have values that vary across rows. I dropped the double() variables that contain code because they are just numeric codes for database management purposes. Using summarytools(), I can say that the records in this dataset are the number of Live Animal Stocks and the units of the values is Head. Each case in this dataset consists of an animal record based on the country and year that tries to estimate the number of live animals which is represented by Value. In total, I have estimates of the stock of nine different types of livestock (Asses, Buffaloes, Camels, Cattle, Goats, Horses, Mules, Pigs, Sheep ) in 253 areas for 58 years. The flags correspond to what type of estimate is being used."
  },
  {
    "objectID": "posts/challenge2_instructions_NJ.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions_NJ.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\n\n\nCode\nFAO.sm %>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n# A tibble: 28 × 2\n   Area                          n\n   <chr>                     <int>\n 1 Africa                      522\n 2 Americas                    464\n 3 Asia                        522\n 4 Australia and New Zealand   376\n 5 Caribbean                   464\n 6 Central America             406\n 7 Central Asia                243\n 8 Eastern Africa              522\n 9 Eastern Asia                522\n10 Eastern Europe              522\n# … with 18 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nFAO_clc <- FAO.sm %>%\n  filter(Flag!=\"A\")\n\nFAO_clc  \n\n\n# A tibble: 31,279 × 9\n   Domain       Area        Element Item   Year Unit    Value Flag  Flag Descr…¹\n   <chr>        <chr>       <chr>   <chr> <dbl> <chr>   <dbl> <chr> <chr>       \n 1 Live Animals Afghanistan Stocks  Asses  1964 Head  1150000 F     FAO estimate\n 2 Live Animals Afghanistan Stocks  Asses  1973 Head  1250000 F     FAO estimate\n 3 Live Animals Afghanistan Stocks  Asses  1974 Head  1250000 F     FAO estimate\n 4 Live Animals Afghanistan Stocks  Asses  1975 Head  1250000 F     FAO estimate\n 5 Live Animals Afghanistan Stocks  Asses  1976 Head  1250000 F     FAO estimate\n 6 Live Animals Afghanistan Stocks  Asses  1978 Head  1300000 *     Unofficial …\n 7 Live Animals Afghanistan Stocks  Asses  1979 Head  1300000 *     Unofficial …\n 8 Live Animals Afghanistan Stocks  Asses  1980 Head  1295000 *     Unofficial …\n 9 Live Animals Afghanistan Stocks  Asses  1981 Head  1315000 *     Unofficial …\n10 Live Animals Afghanistan Stocks  Asses  1982 Head  1315000 *     Unofficial …\n# … with 31,269 more rows, and abbreviated variable name ¹​`Flag Description`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nFAO_clc %>%\n  group_by(Item) %>%\n  summarize(avg=mean(Value, na.rm = TRUE),\n            mode = n(),\n            median = median(Value, na.rm = TRUE),\n            stdev= sd(Value, na.rm = TRUE),\n            min = min(Value, na.rm = TRUE),\n            max = max(Value, na.rm = TRUE))\n\n\n# A tibble: 9 × 7\n  Item           avg  mode median     stdev   min       max\n  <chr>        <dbl> <int>  <dbl>     <dbl> <dbl>     <dbl>\n1 Asses      196051.  4899  14300   615866.     0   8793747\n2 Buffaloes 5901247.   756   6550 19546207.    20 114151770\n3 Camels     499737.  1075  85350  1311815.    45   7762545\n4 Cattle    4380953.  3554  47650 21361967.    15 203634000\n5 Goats     2577844.  4711  57625 11790005.     0 139467008\n6 Horses     276368.  5046  11500   999297.     0  10479246\n7 Mules       80414.  3357   4400   357196.     0   3287449\n8 Pigs       746710.  4107  29000  9429158.     0 345754816\n9 Sheep     2463044.  3774  40000  8206951.     0 111238000\n\n\n\nExplain and Interpret\nHere we can confirm that not all cases are countries. Flag Value A corresponds to Areas that are actually regional aggregations. These should be filtered out if I want to keep the same type of case as a country-level case. The second filter statement removes all cases with Flag Value A so that our dataset is at a country-level case. It seems like the distribution of cases for regional aggregations is even except for Areas Melanesia and Micronesia. FAO_clc is more specific version of the dataset that only includes the cases that are type country-level. I have conducted exploartory analysis on FAO_clc on the group Item and my first impression was how vastly different the mean and median were for each Item. This implies that our data is skewed in one direction. I also see that each Item stdev is really high which indicates that the data observed is quite spread out. The min and max values tell little about the dataset"
  },
  {
    "objectID": "posts/challenge1_MirandaManka.html",
    "href": "posts/challenge1_MirandaManka.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_MirandaManka.html#challenge-overview",
    "href": "posts/challenge1_MirandaManka.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_MirandaManka.html#read-in-the-data",
    "href": "posts/challenge1_MirandaManka.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\n#Read in the excel file\nwild_bird_data = read_excel(\"_data/wild_bird_data.xlsx\")"
  },
  {
    "objectID": "posts/challenge1_MirandaManka.html#describe-the-data",
    "href": "posts/challenge1_MirandaManka.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, provide a high level description of the data.\n\n\nCode\n#View the data\nview(wild_bird_data)\n\n#Find the dimensions of the data\ndim(wild_bird_data)\n\n\n[1] 147   2\n\n\nCode\n#Look at the first few observations\nhead(wild_bird_data)\n\n\n# A tibble: 6 × 2\n  Reference           `Taken from Figure 1 of Nee et al.`\n  <chr>               <chr>                              \n1 Wet body weight [g] Population size                    \n2 5.45887180052624    532194.395145161                   \n3 7.76456810683605    3165107.44544653                   \n4 8.63858738018464    2592996.86778979                   \n5 10.6897349302105    3524193.2266336                    \n6 7.41722577905587    389806.168891807                   \n\n\nCode\n#Get column names\ncolnames(wild_bird_data)\n\n\n[1] \"Reference\"                         \"Taken from Figure 1 of Nee et al.\"\n\n\nThe data appear to be about 146 wild birds, detailing two pieces of information for each - their wet body weight (in grams) and the size of the population they are in. It is hard to tell more about this data without other information, such as the types of birds or how/when/where the data were collected, although if I had to guess, I would say that it was probably collected in forests or other outdoor areas with many birds because the title of the dataset indicates they are wild birds.\nInterestingly, the actual variable names seem to be in the next row instead of the labels the data currently has (the variables should be “Wet body weight [g]” for the first variable/column instead of “Reference”, and “Population size” for the second instead of “Taken from Figure 1 of Nee et al.”)\n\n\nCode\n#Work to correct variable/column names\n#Remove first row\nwild_bird_data <- wild_bird_data[-c(1), ]\n#Rename columns/variables\nwild_bird_data = rename(wild_bird_data, wet_body_weight_g = Reference)\nwild_bird_data = rename(wild_bird_data, population_size = `Taken from Figure 1 of Nee et al.`)\n#Create value to use from each column\nwet_body_weight_g = wild_bird_data[, 1]\npopulation_size = wild_bird_data[, 2]\n\n#Change type\nwet_body_weight_g = as.numeric(unlist(wet_body_weight_g))\npopulation_size = as.numeric(unlist(population_size))\n\n#Summary of variables\nsummary(wet_body_weight_g)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   5.459   18.620   69.232  363.694  309.826 9639.845 \n\n\nCode\nsummary(population_size)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      5    1821   24353  382874  198515 5093378 \n\n\nThe data are very spread out, with a lot of variation in values. This indicates that there is likely a large variety in the types of birds and/or the geographical location of the birds in these measurements."
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html",
    "href": "posts/challenge3_instructions_NJ.html",
    "title": "Challenge 3 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html#challenge-overview",
    "href": "posts/challenge3_instructions_NJ.html#challenge-overview",
    "title": "Challenge 3 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html#read-in-data",
    "href": "posts/challenge3_instructions_NJ.html#read-in-data",
    "title": "Challenge 3 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\nCode\neggs <-read_csv(\"_data/eggs_tidy.csv\",\n                        show_col_types = FALSE)\neggs\n\n\n# A tibble: 120 × 6\n   month      year large_half_dozen large_dozen extra_large_half_dozen extra_l…¹\n   <chr>     <dbl>            <dbl>       <dbl>                  <dbl>     <dbl>\n 1 January    2004             126         230                    132       230 \n 2 February   2004             128.        226.                   134.      230 \n 3 March      2004             131         225                    137       230 \n 4 April      2004             131         225                    137       234.\n 5 May        2004             131         225                    137       236 \n 6 June       2004             134.        231.                   137       241 \n 7 July       2004             134.        234.                   137       241 \n 8 August     2004             134.        234.                   137       241 \n 9 September  2004             130.        234.                   136.      241 \n10 October    2004             128.        234.                   136.      241 \n# … with 110 more rows, and abbreviated variable name ¹​extra_large_dozen\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nprint(dfSummary(eggs, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\neggs\nDimensions: 120 x 6\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      month\n[character]\n      1. April2. August3. December4. February5. January6. July7. June8. March9. May10. November[ 2 others ]\n      10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)10(8.3%)20(16.7%)\n      \n      0\n(0.0%)\n    \n    \n      year\n[numeric]\n      Mean (sd) : 2008.5 (2.9)min ≤ med ≤ max:2004 ≤ 2008.5 ≤ 2013IQR (CV) : 5 (0)\n      2004:12(10.0%)2005:12(10.0%)2006:12(10.0%)2007:12(10.0%)2008:12(10.0%)2009:12(10.0%)2010:12(10.0%)2011:12(10.0%)2012:12(10.0%)2013:12(10.0%)\n      \n      0\n(0.0%)\n    \n    \n      large_half_dozen\n[numeric]\n      Mean (sd) : 155.2 (22.6)min ≤ med ≤ max:126 ≤ 174.5 ≤ 178IQR (CV) : 45.1 (0.1)\n      126.00  :1(0.8%)128.50  :29(24.2%)129.75  :1(0.8%)131.00  :3(2.5%)131.12 !:1(0.8%)132.00  :15(12.5%)133.50  :3(2.5%)173.25  :6(5.0%)174.50  :47(39.2%)178.00  :14(11.7%)! rounded\n      \n\n\n      0\n(0.0%)\n    \n    \n      large_dozen\n[numeric]\n      Mean (sd) : 254.2 (18.5)min ≤ med ≤ max:225 ≤ 267.5 ≤ 277.5IQR (CV) : 34.5 (0.1)\n      12 distinct values\n      \n      0\n(0.0%)\n    \n    \n      extra_large_half_dozen\n[numeric]\n      Mean (sd) : 164.2 (24.7)min ≤ med ≤ max:132 ≤ 185.5 ≤ 188.1IQR (CV) : 49.7 (0.2)\n      132.00  :1(0.8%)134.50  :1(0.8%)135.50  :28(23.3%)135.88 !:1(0.8%)137.00  :6(5.0%)138.12 !:1(0.8%)139.00  :15(12.5%)185.50  :53(44.2%)188.13  :14(11.7%)! rounded\n      \n\n\n      0\n(0.0%)\n    \n    \n      extra_large_dozen\n[numeric]\n      Mean (sd) : 266.8 (22.8)min ≤ med ≤ max:230 ≤ 285.5 ≤ 290IQR (CV) : 44 (0.1)\n      11 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\n\nBriefly describe the data\nThis dataset is comprised of 120 rows with 6 variables. One variable is a character (month) and the rest are doubles. Based off the summary statistics of the egg variables, I can see that the variance of sales for each dozen of eggs are all high meaning that most cases are not near there average. ## Anticipate the End Result\nThe column names large_half_dozen, large_dozen, extra_large_half_dozen and extra_large_dozen represent values of the type variable, the values in the columns represent values of the sales variable, and each row represents multiple observations, not one.\n\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA    1344.     989.\n2 USA      1990 NAFTA    1135.    1108.\n3 France   1980 EU        271.     580.\n4 Mexico   1990 NAFTA    1678.    1778.\n5 USA      1980 NAFTA     490.     703.\n6 France   1990 EU        799.    1183.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\nCode\nnrow(eggs)\n\n\n[1] 120\n\n\nCode\nncol(eggs)\n\n\n[1] 6\n\n\nWhat needs to changed is that the column names that include the values for dozens needs to become its own columns called type and the corresponding values for that type will be stored in a column names sales"
  },
  {
    "objectID": "posts/challenge3_instructions_NJ.html#pivot-the-data",
    "href": "posts/challenge3_instructions_NJ.html#pivot-the-data",
    "title": "Challenge 3 Instructions",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1344.\n 2 Mexico   1980 NAFTA incoming               989.\n 3 USA      1990 NAFTA outgoing              1135.\n 4 USA      1990 NAFTA incoming              1108.\n 5 France   1980 EU    outgoing               271.\n 6 France   1980 EU    incoming               580.\n 7 Mexico   1990 NAFTA outgoing              1678.\n 8 Mexico   1990 NAFTA incoming              1778.\n 9 USA      1980 NAFTA outgoing               490.\n10 USA      1980 NAFTA incoming               703.\n11 France   1990 EU    outgoing               799.\n12 France   1990 EU    incoming              1183.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\nCode\neggs <- pivot_longer(eggs, col = c(large_half_dozen, large_dozen, extra_large_half_dozen, extra_large_dozen),\n                 names_to=\"type\",\n                 values_to = \"sales\")\neggs\n\n\n# A tibble: 480 × 4\n   month     year type                   sales\n   <chr>    <dbl> <chr>                  <dbl>\n 1 January   2004 large_half_dozen        126 \n 2 January   2004 large_dozen             230 \n 3 January   2004 extra_large_half_dozen  132 \n 4 January   2004 extra_large_dozen       230 \n 5 February  2004 large_half_dozen        128.\n 6 February  2004 large_dozen             226.\n 7 February  2004 extra_large_half_dozen  134.\n 8 February  2004 extra_large_dozen       230 \n 9 March     2004 large_half_dozen        131 \n10 March     2004 large_dozen             225 \n# … with 470 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow this dataset is in tidy form because each row is now one observation. I used pivot_longer because we needed more rows so that each observation could be individual in the dataset. When you go to read the dataset U can see each case only has one value attached to it."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge5.html",
    "href": "posts/SaaradhaaM_Challenge5.html",
    "title": "Challenge 5",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge5.html#read-in-data",
    "href": "posts/SaaradhaaM_Challenge5.html#read-in-data",
    "title": "Challenge 5",
    "section": "Read in data",
    "text": "Read in data\nI’ve previously worked with the households dataset, so I’ll be working with the Public School Characteristics dataset for this challenge. Using complete(), I’ll set all missing values to NA.\n\n# read in dataset.\npsc <- read_csv(\"_data/Public_School_Characteristics_2017-18.csv\")\n\n# fill in NA.\ncomplete(psc)\n\n\n\n  \n\n\n# view dataset.\npsc\n\n\n\n  \n\n\n\n\nBriefly describe data\nThe dataset has 100,729 rows and 79 columns. I found a codebook here. This dataset describes the characteristics of public schools from 2017 to 2018 by the National Center for Education Statistics. There is a mix of numeric, character and logical columns.\nThe data definitely needs to be tidied! Due to time constraints and how massive the dataset is, I’ll first identify the visualisations I want to produce using the codebook. I’ll then work on a subset of the original dataset. I specifically want to look at the following:\n\nUnivariate: American Indian\nUnivariate: Black\nBivariate: state, American Indian"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge5.html#tidy-data",
    "href": "posts/SaaradhaaM_Challenge5.html#tidy-data",
    "title": "Challenge 5",
    "section": "Tidy data",
    "text": "Tidy data\nThe variables I need are AM, BL and LSTATE.\n\n# creating subset.\nsubset <- psc %>% select(AM, BL, LSTATE)\n\n# sanity check.\nsubset\n\n\n\n  \n\n\n\nWe need to change STATE to a factor.\n\n# change type to factor.\nsubset$LSTATE <- as.factor(subset$LSTATE)\n\n# view dataset using summarytools.\nprint(summarytools::dfSummary(subset, varnumbers = FALSE, plain.ascii = FALSE, graph.magnif = 0.50, style = \"grid\", valid.col = FALSE), \n      method = 'render', table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nsubset\nDimensions: 100729 x 3\n  Duplicates: 62955\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      AM\n[numeric]\n      Mean (sd) : 6.7 (30.3)min ≤ med ≤ max:0 ≤ 1 ≤ 1395IQR (CV) : 4 (4.5)\n      424 distinct values\n      \n      20609\n(20.5%)\n    \n    \n      BL\n[numeric]\n      Mean (sd) : 83 (151.4)min ≤ med ≤ max:0 ≤ 19 ≤ 5088IQR (CV) : 90 (1.8)\n      1166 distinct values\n      \n      8325\n(8.3%)\n    \n    \n      LSTATE\n[factor]\n      1. AK2. AL3. AR4. AS5. AZ6. CA7. CO8. CT9. DC10. DE[ 45 others ]\n      513(0.5%)1478(1.5%)1087(1.1%)28(0.0%)2465(2.4%)10325(10.3%)1900(1.9%)1031(1.0%)227(0.2%)229(0.2%)81446(80.9%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\nNormally, I would pivot ethnicity into 1 column, but we don’t want to do that so that we can run the visualisations. We also have some missing values. This is expected, as not all students may wish to report their ethnicity, or some school districts may have opted not to share this data, etc."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge5.html#univariate-visualizations",
    "href": "posts/SaaradhaaM_Challenge5.html#univariate-visualizations",
    "title": "Challenge 5",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations\nThe univariate visualizations I want to work on are the distributions of students of American Indian and Black ethnicity. These are numeric variables, so we should use histograms.\n\nsubset %>% filter(! is.na(AM)) %>% ggplot(aes(AM)) + geom_histogram() + theme_minimal() + labs(title = \"American Indian Students in Public Schools (2017-2018)\")\n\n\n\nsubset %>% filter(! is.na(BL)) %>% ggplot(aes(BL)) + geom_histogram() + theme_minimal() + labs(title = \"Black Students in Public Schools (2017-2018)\")"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge5.html#bivariate-visualization",
    "href": "posts/SaaradhaaM_Challenge5.html#bivariate-visualization",
    "title": "Challenge 5",
    "section": "Bivariate Visualization",
    "text": "Bivariate Visualization\nI want to plot American Indian (numeric) against state (categorical).\n\nsubset %>% filter(! is.na(LSTATE)) %>% ggplot(aes(LSTATE, AM)) + geom_boxplot() + theme_minimal() + labs(title = \"American Indian Students in Public Schools By State (2017-2018)\", x = \"Number of American Indian Students\", y = \"State\") + theme(axis.text.x=element_text(angle=90,hjust=1))\n\n\n\n\nI notice an error in my boxplot - I should have summed up the number of American Indian students by state before doing this! It’s close to 11pm so I’m turning this in first - let me try to fix this within the week."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge4.html",
    "href": "posts/SaaradhaaM_Challenge4.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge4.html#reading-in-and-describing-data",
    "href": "posts/SaaradhaaM_Challenge4.html#reading-in-and-describing-data",
    "title": "Challenge 4",
    "section": "Reading in and describing data",
    "text": "Reading in and describing data\n\n\n\n\n\n\nJust trying out pop-up text for fun!\n\n\n\nI will be working with the debt in trillions dataset.\n\n\n\n\nCode\n# read in data.\ndebt <- read_excel(\"_data/debt_in_trillions.xlsx\")\n\n# dimensions, column names and basic descriptives.\ndim(debt)\n\n\n[1] 74  8\n\n\nCode\ncolnames(debt)\n\n\n[1] \"Year and Quarter\" \"Mortgage\"         \"HE Revolving\"     \"Auto Loan\"       \n[5] \"Credit Card\"      \"Student Loan\"     \"Other\"            \"Total\"           \n\n\nCode\nsummary(debt)\n\n\n Year and Quarter      Mortgage       HE Revolving      Auto Loan     \n Length:74          Min.   : 4.942   Min.   :0.2420   Min.   :0.6220  \n Class :character   1st Qu.: 8.036   1st Qu.:0.4275   1st Qu.:0.7430  \n Mode  :character   Median : 8.412   Median :0.5165   Median :0.8145  \n                    Mean   : 8.274   Mean   :0.5161   Mean   :0.9309  \n                    3rd Qu.: 9.047   3rd Qu.:0.6172   3rd Qu.:1.1515  \n                    Max.   :10.442   Max.   :0.7140   Max.   :1.4150  \n  Credit Card      Student Loan        Other            Total       \n Min.   :0.6590   Min.   :0.2407   Min.   :0.2960   Min.   : 7.231  \n 1st Qu.:0.6966   1st Qu.:0.5333   1st Qu.:0.3414   1st Qu.:11.311  \n Median :0.7375   Median :0.9088   Median :0.3921   Median :11.852  \n Mean   :0.7565   Mean   :0.9189   Mean   :0.3831   Mean   :11.779  \n 3rd Qu.:0.8165   3rd Qu.:1.3022   3rd Qu.:0.4154   3rd Qu.:12.674  \n Max.   :0.9270   Max.   :1.5840   Max.   :0.4860   Max.   :14.957  \n\n\nCode\n# check for missing data.\napply(debt, 2, anyNA)\n\n\nYear and Quarter         Mortgage     HE Revolving        Auto Loan \n           FALSE            FALSE            FALSE            FALSE \n     Credit Card     Student Loan            Other            Total \n           FALSE            FALSE            FALSE            FALSE \n\n\nThis dataset has 74 rows and 8 columns. Except for Year and Quarter, all other columns are in numeric format. A quick Google search shows that each row represents different types of debt in the US for that particular year and quarter. Looking at the median values for each column, mortgage debt seems to be the greatest in value (8.412 trillion!), while other debt has the lowest median value (0.3921). There also seems to be no missing data."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge4.html#tidy-and-mutate-data",
    "href": "posts/SaaradhaaM_Challenge4.html#tidy-and-mutate-data",
    "title": "Challenge 4",
    "section": "Tidy and Mutate Data",
    "text": "Tidy and Mutate Data\nThe data needs to be tidied:\n\nYear and Quarter should be split into two separate columns.\nFor consistency, all debt-type columns should have the same number of decimal places.\nThe data would be easier to read if we pivoted the debt-type columns (N = 7) such that we only have to look at Year, Quarter, Debt Type and Value.\n\nAt the end of the tidying process, we should have 74 * 7 = 514 rows, 4 columns and all decimals to 3 decimal places.\n\n\nCode\n# separate the two columns.\ndebt <- separate(debt, \"Year and Quarter\", into=c(\"year\", \"quarter\"), sep=\":Q\")\n\n# mutate for consistent decimal places.\ndebt <- mutate(debt, across(where(is.numeric), ~ round(., 3)))\n\n# pivot.\ndebt <- debt %>% pivot_longer(cols = where(is.numeric), names_to = \"debt_type\", values_to = \"amount\")\n\n# check new dimensions.\ndim(debt)\n\n\n[1] 518   4\n\n\nGreat! Now we have tidy data, where each case is represented by a particular year, quarter, debt_type and associated amount. Moving on to changing column types - we can change quarter to numeric format, debt_type into a factor and year into date format.\n\n\nCode\n# change \"quarter\" to numeric.\ndebt$quarter <- as.numeric(debt$quarter)\n\n# change \"debt_type\" to categories.\ndebt$debt_type <- as.factor(debt$debt_type)\n\n# change \"year\" to date.\ndebt$year <- as.Date(debt$year, format = \"%y\")\ndebt$year <- year(debt$year)\n\n# check new dataset.\nprint(summarytools::dfSummary(debt, varnumbers = FALSE, plain.ascii = FALSE, graph.magnif = 0.50, style = \"grid\", valid.col = FALSE), \n      method = 'render', table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\ndebt\nDimensions: 518 x 4\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      year\n[numeric]\n      Mean (sd) : 2011.8 (5.3)min ≤ med ≤ max:2003 ≤ 2012 ≤ 2021IQR (CV) : 9 (0)\n      19 distinct values\n      \n      0\n(0.0%)\n    \n    \n      quarter\n[numeric]\n      Mean (sd) : 2.5 (1.1)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 2 (0.5)\n      1:133(25.7%)2:133(25.7%)3:126(24.3%)4:126(24.3%)\n      \n      0\n(0.0%)\n    \n    \n      debt_type\n[factor]\n      1. Auto Loan2. Credit Card3. HE Revolving4. Mortgage5. Other6. Student Loan7. Total\n      74(14.3%)74(14.3%)74(14.3%)74(14.3%)74(14.3%)74(14.3%)74(14.3%)\n      \n      0\n(0.0%)\n    \n    \n      amount\n[numeric]\n      Mean (sd) : 3.4 (4.4)min ≤ med ≤ max:0.2 ≤ 0.8 ≤ 15IQR (CV) : 7.4 (1.3)\n      449 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-22\n\n\n\nOk - we’ve changed our column types as required, and checked that this was done correctly. From the summary table, we can see that 6 values for quarters 3 and 4 have not been keyed in. I want to look at year and quarter to see which values these are - so that for future analyses using year and quarter, we know which year(s) might not have all the information we need.\n\n\nCode\ndebt %>% select(year, quarter) %>% arrange(desc(quarter), desc(year))\n\n\n\n\n  \n\n\n\nLooking through the table shows us that the missing values are for 2021 (understandable - the data might not yet be available). This means that our dataset ranges from Q1 of 2003 to Q2 of 2021. When looking at total debt for each year, it might be useful just to look at 2003 to 2020 for completeness."
  },
  {
    "objectID": "posts/challenge4_MirandaManka.html",
    "href": "posts/challenge4_MirandaManka.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_MirandaManka.html#challenge-overview",
    "href": "posts/challenge4_MirandaManka.html#challenge-overview",
    "title": "Challenge 4",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_MirandaManka.html#read-in-data",
    "href": "posts/challenge4_MirandaManka.html#read-in-data",
    "title": "Challenge 4",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\ndebt = read_excel(\"_data/debt_in_trillions.xlsx\")\n\n\n\nBriefly describe the data\nThis is a dataset containing information about different types of debt over time. The first column contains year and quarter (for example 03:Q1 seems to be the first quarter of 2003), and it goes from 03:Q1 to 21:Q2, so probably the first quarter of 2003 to the second quarter of 2021. There are 74 rows (18.5 years) with 4 observations for each year (4 quarters) except the last which has 2. Based on the name of the dataset, each column is the amount of that type of debt in millions. There are also columns for Mortgage, HE Revolving, Auto Loan, Credit Card, Student Loan, Other, and Total (which is the sum of all the others in the row)."
  },
  {
    "objectID": "posts/challenge4_MirandaManka.html#tidy-data-identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_MirandaManka.html#tidy-data-identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4",
    "section": "Tidy data & Identify variables that need to be mutated",
    "text": "Tidy data & Identify variables that need to be mutated\nTidy data: Each variable has it’s own column. Each observation has it’s own row. Each value has it’s own cell.\nThe dataset needs some work to make it fully tidy. First, I want to separate the first column into 2 columns and rename them to be “year” and “quarter” separately. Then change the year column from 03 04 etc. to 2003 2004 etc. and change it to numeric. This is important to change before the dataset can be used because otherwise I can’t work with it, for example filter by a specific year or quarter separately.\n\n\nCode\ndebt = debt %>% \n  separate(\"Year and Quarter\", into = c(\"Year1\", \"Quarter\"), sep = \":\")\n\ndebt = debt %>% \n  mutate(Quarter = str_remove(Quarter, \"Q\"))\n\ndebt = debt %>% \n  mutate(new_col = rep(c(20), times = 74), .before = Quarter)\n\ndebt = debt %>%\n  unite(\"Year\", new_col:Year1, sep = \"\")\n\ndebt = debt %>% \n  mutate(Year = parse_number(Year))\n\n\nAfter this, I decided to pivot the data so that the columns are Year, Quarter, Type, and Amount. This way, there are more rows, but less columns and information in each row. Type contains the different debt types, while Amount is just the amount of debt (in millions).\n\n\nCode\ndebt = debt %>% \n  pivot_longer(cols = contains(c(\"Mortgage\", \"HE Revolving\", \"Auto Loan\", \"Credit Card\", \"Student Loan\", \"Other\", \"Total\")), names_to = c(\"Type\"), values_to = \"Amount\")\n\n\nPotentially one further step could be to multiply the Amount column by 1,000,000 to get the actual value since it is are currently in millions (for example, a mortgage of 4.942 in this dataset would actually be 4,942,000). I could also remove the trailing 0 after the decimal. I’m not sure if these would be helpful/necessary at this point, though."
  },
  {
    "objectID": "posts/challenge2_MekhalaKumar.html",
    "href": "posts/challenge2_MekhalaKumar.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_MekhalaKumar.html#reading-in-data",
    "href": "posts/challenge2_MekhalaKumar.html#reading-in-data",
    "title": "Challenge 2",
    "section": "Reading in Data",
    "text": "Reading in Data\nThe dataset used is State County (the excel file). While reading in the data, it was also cleaned by removing the first few rows and last few rows which had unnecessary text. Since the country Canada was also present in the last row of the dataset but is a different country, it has been removed as well.\n\n\nCode\nlibrary(readr)\nlibrary(readxl)\nlibrary(stringr)\nrailroad_df <- read_excel(\"_data/StateCounty2012.xls\",skip=3,col_names= c(\"state\", \"delete\", \"county\", \"delete\", \"total_employees\"))%>%\nselect(!contains(\"delete\"))%>%\n  filter(!str_detect(state, \"Total\"))\ntail(railroad_df, 10)\n\n\n# A tibble: 10 × 3\n   state                                               county     total_employ…¹\n   <chr>                                               <chr>      <chr>         \n 1 WY                                                  PLATTE     129           \n 2 WY                                                  SHERIDAN   252           \n 3 WY                                                  SUBLETTE   3             \n 4 WY                                                  SWEETWATER 196           \n 5 WY                                                  UINTA      49            \n 6 WY                                                  WASHAKIE   10            \n 7 WY                                                  WESTON     37            \n 8 CANADA                                              <NA>       662           \n 9 1  Military designation.                            <NA>       <NA>          \n10 NOTE:  Excludes 2,896 employees without an address. <NA>       <NA>          \n# … with abbreviated variable name ¹​total_employees\n\n\nCode\nrailroad_df<-head(railroad_df, -2)\ntail(railroad_df, 10)\n\n\n# A tibble: 10 × 3\n   state  county     total_employees\n   <chr>  <chr>      <chr>          \n 1 WY     NIOBRARA   51             \n 2 WY     PARK       29             \n 3 WY     PLATTE     129            \n 4 WY     SHERIDAN   252            \n 5 WY     SUBLETTE   3              \n 6 WY     SWEETWATER 196            \n 7 WY     UINTA      49             \n 8 WY     WASHAKIE   10             \n 9 WY     WESTON     37             \n10 CANADA <NA>       662            \n\n\nCode\nrailroad_df<-head(railroad_df, -1)\ntail(railroad_df, 10)\n\n\n# A tibble: 10 × 3\n   state county     total_employees\n   <chr> <chr>      <chr>          \n 1 WY    NATRONA    92             \n 2 WY    NIOBRARA   51             \n 3 WY    PARK       29             \n 4 WY    PLATTE     129            \n 5 WY    SHERIDAN   252            \n 6 WY    SUBLETTE   3              \n 7 WY    SWEETWATER 196            \n 8 WY    UINTA      49             \n 9 WY    WASHAKIE   10             \n10 WY    WESTON     37             \n\n\nCode\nrailroad_df = railroad_df[-1,] \nrailroad_df <- transform(railroad_df,employees = as.numeric(total_employees))\n View(railroad_df)"
  },
  {
    "objectID": "posts/challenge2_MekhalaKumar.html#data-description",
    "href": "posts/challenge2_MekhalaKumar.html#data-description",
    "title": "Challenge 2",
    "section": "Data description",
    "text": "Data description\nThe data consists of 3 columns- state, county and employees. Since the employees column contained character datatype, total_employees was created with the double type of variable. There are 53 states included in the data and 1709 counties. The number of employees by state can also be seen in the third table.\n\n\nCode\nhead(railroad_df)\n\n\n  state               county total_employees employees\n1    AE                  APO               2         2\n2    AK            ANCHORAGE               7         7\n3    AK FAIRBANKS NORTH STAR               2         2\n4    AK               JUNEAU               3         3\n5    AK    MATANUSKA-SUSITNA               2         2\n6    AK                SITKA               1         1\n\n\nCode\nrailroad_df%>%\n  select(state)%>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad_df%>%\n  select(county)%>%\n  n_distinct(.)\n\n\n[1] 1709\n\n\nCode\nrailroad_df %>%\n  group_by(state) %>%\n  summarise(employees2 = sum(employees))\n\n\n# A tibble: 53 × 2\n   state employees2\n   <chr>      <dbl>\n 1 AE             2\n 2 AK           103\n 3 AL          4257\n 4 AP             1\n 5 AR          3871\n 6 AZ          3153\n 7 CA         13137\n 8 CO          3650\n 9 CT          2592\n10 DC           279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge2_MekhalaKumar.html#grouped-summary-statistics-and-interpretation",
    "href": "posts/challenge2_MekhalaKumar.html#grouped-summary-statistics-and-interpretation",
    "title": "Challenge 2",
    "section": "Grouped Summary Statistics and Interpretation",
    "text": "Grouped Summary Statistics and Interpretation\nFirst, the summary statistics were checked for a country wide comparison. Then a few states were selected in such a way that a few states had many employees (around 13000) and a few states selected had a small number of employees (100-200). This was done in order to check how the central tendency and dispersion varied across counties in a particular state.\nBoth California and Nebraska, which had a larger number of employees, had a higher mean for the total employees among the counties compared to the mean number of employees across states. This is possibly due to the fact that they had more employees. Similarly, it was observed that in the case where there were a lower number of employees, the mean for the total employees across counties were lower and the standard deviation across counties was lower (in Vermont and Alaska). As can be seen below, the District of Columbia cannot be analysed in this way as it consists of a single county.\nFinally, the total number of employees in each county of a state were arranged in descending order because a county with a higher number of employees indicates that there are more job opportunities or perhaps a higher population in the area.\n\n\nCode\nlibrary(dplyr)\nlibrary(summarytools)\n\nrailroad_df%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1       87.17816               21             1          8207     283.6359\n  var.employees IQR.employees\n1      80449.32            58\n\n\nCode\nrailroad_df%>% filter(state == \"CA\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1       238.8545               61             1          2888     549.4692\n  var.employees IQR.employees\n1      301916.3           188\n\n\nCode\nrailroad_df%>% filter(state == \"CA\")%>%group_by(county)%>%arrange(county, desc(employees))\n\n\n# A tibble: 55 × 4\n# Groups:   county [55]\n   state county       total_employees employees\n   <chr> <chr>        <chr>               <dbl>\n 1 CA    ALAMEDA      346                   346\n 2 CA    AMADOR       9                       9\n 3 CA    BUTTE        69                     69\n 4 CA    CALAVERAS    30                     30\n 5 CA    COLUSA       2                       2\n 6 CA    CONTRA COSTA 348                   348\n 7 CA    EL DORADO    103                   103\n 8 CA    FRESNO       341                   341\n 9 CA    GLENN        4                       4\n10 CA    HUMBOLDT     2                       2\n# … with 45 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nrailroad_df%>% filter(state == \"NE\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1       148.0449               15             1          3797     511.5816\n  var.employees IQR.employees\n1      261715.7            66\n\n\nCode\nrailroad_df%>% filter(state == \"NE\")%>%group_by(county)%>%arrange(county, desc(employees))\n\n\n# A tibble: 89 × 4\n# Groups:   county [89]\n   state county    total_employees employees\n   <chr> <chr>     <chr>               <dbl>\n 1 NE    ADAMS     77                     77\n 2 NE    ANTELOPE  2                       2\n 3 NE    ARTHUR    2                       2\n 4 NE    BANNER    8                       8\n 5 NE    BLAINE    2                       2\n 6 NE    BOONE     2                       2\n 7 NE    BOX BUTTE 1168                 1168\n 8 NE    BROWN     1                       1\n 9 NE    BUFFALO   107                   107\n10 NE    BURT      9                       9\n# … with 79 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nrailroad_df%>% filter(state == \"DC\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1            279              279           279           279           NA\n  var.employees IQR.employees\n1            NA             0\n\n\nCode\nrailroad_df%>% filter(state == \"VT\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1           18.5              8.5             3            83     24.54431\n  var.employees IQR.employees\n1      602.4231             8\n\n\nCode\nrailroad_df%>% filter(state == \"VT\")%>%group_by(county)%>%arrange(county, desc(employees))\n\n\n# A tibble: 14 × 4\n# Groups:   county [14]\n   state county     total_employees employees\n   <chr> <chr>      <chr>               <dbl>\n 1 VT    ADDISON    8                       8\n 2 VT    BENNINGTON 8                       8\n 3 VT    CALEDONIA  3                       3\n 4 VT    CHITTENDEN 40                     40\n 5 VT    ESSEX      4                       4\n 6 VT    FRANKLIN   83                     83\n 7 VT    GRAND ISLE 5                       5\n 8 VT    LAMOILLE   4                       4\n 9 VT    ORANGE     9                       9\n10 VT    ORLEANS    13                     13\n11 VT    RUTLAND    59                     59\n12 VT    WASHINGTON 3                       3\n13 VT    WINDHAM    10                     10\n14 VT    WINDSOR    10                     10\n\n\nCode\nrailroad_df%>% filter(state == \"AK\")%>% summarise (mean.employees = mean(`employees`), median.employees = median(`employees`), min.employees = min(`employees`), max.employees = max(`employees`), sd.employees = sd(`employees`), var.employees = var(`employees`), IQR.employees = IQR(`employees`))\n\n\n  mean.employees median.employees min.employees max.employees sd.employees\n1       17.16667              2.5             1            88     34.76445\n  var.employees IQR.employees\n1      1208.567             4\n\n\nCode\nrailroad_df%>% filter(state == \"AK\")%>%group_by(county)%>%arrange(county, desc(employees))\n\n\n# A tibble: 6 × 4\n# Groups:   county [6]\n  state county               total_employees employees\n  <chr> <chr>                <chr>               <dbl>\n1 AK    ANCHORAGE            7                       7\n2 AK    FAIRBANKS NORTH STAR 2                       2\n3 AK    JUNEAU               3                       3\n4 AK    MATANUSKA-SUSITNA    2                       2\n5 AK    SITKA                1                       1\n6 AK    SKAGWAY MUNICIPALITY 88                     88"
  },
  {
    "objectID": "posts/challenge3_solutions.html",
    "href": "posts/challenge3_solutions.html",
    "title": "Challenge 3 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE, cache=TRUE)"
  },
  {
    "objectID": "posts/challenge3_solutions.html#challenge-overview",
    "href": "posts/challenge3_solutions.html#challenge-overview",
    "title": "Challenge 3 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nanticipate the shape of pivoted data, and\npivot the data into tidy format using pivot_longer\n\n\nExampleAnimal Weights ⭐Eggs ⭐⭐/⭐⭐⭐Australian Marriage Ballot ⭐⭐⭐USA Households ⭐⭐⭐⭐\n\n\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nFind current and future data dimensions\nLets see if this works with a simple example.\n\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n\n  \n\n\n#existing rows/cases\nnrow(df)\n\n[1] 6\n\n#existing columns/cases\nncol(df)\n\n[1] 5\n\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n[1] 12\n\n# expected columns \n3 + 2\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nPivot the data\n\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n\n  \n\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\n\nThe animal weights dataset contains tabular-style data, with cells representing the average live animal weight (in kg) of 16 types of livestock for each of 9 geographic areas as defined by the Intergovernmental Panel on Climate Change (IPCC. Livestock weights are a critical part of the Global Livestock Envrionmental Assessment Model used by the FAO.\n\nanimal_weight<-read_csv(\"_data/animal_weight.csv\")\nanimal_weight\n\n\n\n  \n\n\n\nBecause the animal weights data is in tabular format, it is easy to see that \\(n=9\\) regions (categories or cases) in the original data, and that there are \\(k=16\\) types of livestock (categories or columns). Therefore, we expect the pivoted dataset to have \\(9 * 16\\) = 144 rows and 3 columns (region, animal type, and animal weight.)\n\n\n\n\n\n\ninline R code\n\n\n\nIf you check out the code above, you will see that I didn’t use a calculator to figure out \\(9*16=144\\), but used inline r code like this: `r 9*16`.\n\n\n\nPivot the data\n\nanimal_weight_longer<-pivot_longer(animal_weight, \n                                    col=-`IPCC Area`,\n                                    names_to = \"Livestock\",\n                                    values_to = \"Weight\")\nanimal_weight_longer\n\n\n\n  \n\n\n\nYes, it looks like we ended up with 144 rows and 3 columns, exactly as expected!\n\n\n\n\n\n\nNote\n\n\n\n#Go further\nstringr functions, and separate from tidyr, would be useful in helping split out additional infromation from the Livestock column.\n\n\n\n\n\nThis section covers pivoting for the organic eggs data, available in both excel and (partially cleaned) .csv format. The data reports the average price per carton paid to the farmer or producer for organic eggs (and organic chicken), reported monthly from 2004 to 20013. Average price is reported by carton type, which can vary in both size (x-large or large) and quantity (half-dozen or dozen.)\nIf you are using the eggs_tidy.csv, you can skip the first section as your data is in .csv format and already partially cleaned. The first section reviews data read-in and cleaning for the organicpoultry.xls file.\n\nRead and Clean the dataPivot Type OnlyPivot Size and Quantity\n\n\nThere are three sheets in the organicpoultry.xls workbook: one titled Data, one titled “Organic egg prices, 2004-13” and one with a similar title for chicken prices. While I can tell all of this from inspection, I can also use a ask R to return the sheet names for me.\n\n\n\n\n\n\nGet sheet names with excel_sheets()\n\n\n\nBoth readxl and googlesheets4 have a function that can return sheet names as a vector. This is really useful if you need to parse and read multiple sheets in the same workbook.\n\n\n\nexcel_sheets(\"_data/organiceggpoultry.xls\")\n\n[1] \"Data\"                            \"Organic egg prices, 2004-13\"    \n[3] \"Organic poultry prices, 2004-13\"\n\n\nWhile it may seem like it would be easier to read in the individual egg prices and chicken prices, the amount of formatting introduced into the second and third sheets is pretty intimidating (see the screenshot below.) There are repeated headers to remove, a year column to shift, and other formatting issues. Ironically, it may be easier to read in the egg data from the Data sheet, with a skip of 5 (to skip the table title, etc), custom column names designed for pivoting to two categories (final section) and only reading in columns B to F.\n\n\n\nOrganic Poultry Data\n\n\n\n\n\nOrganic Poultry Egg Prices\n\n\n\n\n\n\n\n\nHard-coding Table Formats\n\n\n\nFormatted excel tables are a horrible data source, but may be the only way to get some data. If table formatting is consistent from year to year, hard-coding can be an acceptable approach. If table format is inconsistent, then more powerful tools are needed.\n\n\n\neggs_orig<-read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dozen\",\n                               \"xlarge_halfdozen\", \"large_dozen\",\n                               \"large_halfdozen\")\n                 )\neggs_orig\n\n\n\n  \n\n\n\nSometimes there are notes in the first column of tables, so lets make sure that isn’t an issue.\n\neggs_orig%>%\n  count(date)\n\n\n\n  \n\n\n\nWe need to remove the “note” indicators in two of the rows. Some characters require an escape to be included in regular expressions, but this time it is straightforward to find ” /1”.\n\neggs<-eggs_orig%>%\n  mutate(date = str_remove(date, \" /1\"))\n\nOne final step is needed to split the year variable away from the month. You will often need to separate out two variables from a single column when working with published tables, and also need to use the equivalent of dragging to fill in a normal spreadsheet. Lets look at the easiest way to fix both of these issues.\n\n\n\n\n\n\ntidyr::separate()\n\n\n\nSeparate is a fantastic function for working with strings. It will break a string column into multiple new (named) columns, at the indicated separator character (e.g., “,” or ” “). The old variable is automatically removed, but can be left.\n\n\n\n\n\n\n\n\ntidyr::fill()\n\n\n\nFill works like dragging to fill functionality in a spreadsheet. You can choose the direction to fill.\n\n\n\neggs<-eggs%>%\n  separate(date, into=c(\"month\", \"year\"), sep=\" \")%>%\n  fill(year)\neggs\n\n\n\n  \n\n\n\n\n\nLooking at the data, we can see that each of the original 120 cases consist of a year-month combination (e.g., January 2004), while the values are the average price (in cents) of four different types of eggs (e.g., large_half_dozen, large_dozen, etc) So to tidy our data, we should create a matrix with a year-month-eggType combination, with a single price value for each case.\nTo do this (and make our data easier to graph and analyze), we can pivot longer - changing our data from 120 rows with 6 variables (2 grouping and 4 values) to 480 rows of 4 variables (with 3 grouping variables and a single price value).\n\neggs_long<-eggs%>%\n  pivot_longer(cols=contains(\"large\"), \n               names_to = \"eggType\",\n               values_to = \"avgPrice\"\n  )\neggs_long\n\n\n\n  \n\n\n\nWell, that was super easy. But wait, what if you are interested in egg size - you want to know how much more expensive extra-large eggs are compared to large eggs. Right now, that will be annoying, as you will have to keep sorting out the egg quantity - whether the price is for a half_dozen or a dozen eggs.\n\n\nWouldn’t it be nice if we had two new columns - size and quantity - in place of the existing eggType categorical variable? In other words, to have fully tidy data, we would need 4 grouping variables (year, month, size, and quantity) and the same value (price). So, we want to use pivot longer, but we will be adding two new category variables (for a total of 4) and this will cut the number of rows in half (to 240).\nHow can we let R know what we want it to do?? Thankfully, we created pretty systematic column names for egg types in our original data, following the general pattern: size-quantity. Maybe we can use this to our advantage? Working with patterns in the names_sep option of the pivot functions makes it easier than you would think to pivot four existing columns into two new columns.\n\neggs_long<- eggs%>%\n  pivot_longer(cols=contains(\"large\"),\n               names_to = c(\"size\", \"quantity\"),\n               names_sep=\"_\",\n               values_to = \"price\"\n  )\neggs_long\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is another tabular data source published by the Australian Bureau of Statistics that requires a decent amount of cleaning. In 2017, Australia conducted a postal survey to gauge citizens’ opinions towards same sex marriage: “Should the law be changed to allow same-sex couples to marry?” All Australian citizens are required to vote in elections, so citizens could respond in one of four ways: vote yes, vote no, vote in an unclear way, or fail to vote. (See the “Explanatory Notes” sheet for more details.)\nThe provided table includes estimates of the proportion of citizens choosing each of the four options, aggregated by Federal Electoral District, which are nested within one of 8 overarching Electoral Divisions. Here is a quick image showing the original table format.\n ### Identify desired data structure\nInspection reveals several critical issues to address: - Typical long header (skip = 7) - No single row with variable names - Two redundant values (count and percentage - percentage is easy to recover from complete count data) - Total columns that are redundant (remove) - The sum of “Yes” and “No” votes appears to be redundant with Response Clear in columns I and J - District and Division are in the same column\nIn this example, we are going to identify the desired structure early in the process, because clever naming of variables makes it much easier to use pivot functions. We will skip reading in redundant data (proportions and “totals” columns), and then can identify four potentially distinct pieces of information. Three grouping variables: Division (in column 1), District (also in column 1), and citizen Response (yes, no, unclear, and non-response), plus one value: aggregated response Count.\nOur basic data reading and cleaning process should therefore follow these steps:\n\nRead in data, skipping unneeded columns and renaming variables\nCreate Division and District variables using separate() and fill()\npivot_longer() four response variables into 2 new Response and Count variables (double the number of rows)\n\n\nRead DataSeparate District and DivisionPivot_longer to tidy format\n\n\nIt is best to confine serious hard-coding to the initial data read in step, to make it easy to locate and make changes or replicate in the future. So, we will use a combination of tools introduced earlier to read and reformat the data: skip and col_names to read in the data, select to get rid of unneeded columns, and filter to get rid of unneeded rows. We also use the drop_na function to filter unwanted rows.\n\nvote_orig <- read_excel(\"_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\",\n           sheet=\"Table 2\",\n           skip=7,\n           col_names = c(\"District\", \"Yes\", \"del\", \"No\", rep(\"del\", 6), \"Illegible\", \"del\", \"No Response\", rep(\"del\", 3)))%>%\n  select(!starts_with(\"del\"))%>%\n  drop_na(District)%>%\n  filter(!str_detect(District, \"(Total)\"))%>%\n  filter(!str_starts(District, \"\\\\(\"))\nvote_orig\n\n\n\n  \n\n\n\n\n\nThe most glaring remaining issue is that the administrative Division is not in its own column, but is on its own row within the District column. The following code uses case_when to make a new Division variable with an entry (e.g., New South Wales Division) where there is a Division name in the District column, and otherwise it create just an empty space. After that, fill can be used to fill in empty spaces with the most recent Division name. We then filter out rows with only the title information.\n\nvote<- vote_orig%>%\n  mutate(Division = case_when(\n    str_ends(District, \"Divisions\") ~ District,\n    TRUE ~ NA_character_ ))%>%\n  fill(Division, .direction = \"down\")\nvote<- filter(vote,!str_detect(District, \"Division|Australia\"))\nvote\n\n\n\n  \n\n\n\n\n\nSupposed we wanted to create a stacked bar chart to compare the % who votes Yes to the people who either said No or didn’t vote. Or if we wanted to use division level characteristics to predict the proortion of people voting in a specific way? In both cases, we would need tidy data, which requires us to pivot longer into the original (aggregated) data format: Division, District, Response, Count. We should end up with 600 rows and 4 columns.\n\nvote_long<- vote%>%\n  pivot_longer(\n    cols = Yes:`No Response`,\n    names_to = \"Response\",\n    values_to = \"Count\"\n  )\nvote\n\n\n\n  \n\n\n\n\n\n\n\n\nThe excel workbook “USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019” is clearly a table of census-type household data (e.g., Current Population Study or CPS, American Community Study or ACS, etc.) Row 3 of the workbook provides a link to more details about the origin of the data used to produce the table.\nThe cases in this example are essentially year-identity groups, where I use the term identity to refer to the wide range of ways that the census can cluster racial and identity identity. While there are 12 categories in the data, many of these overlap and/or are not available in specific years. For example, one category is “All Races”, and it overlaps with all other categories but cannot be easily eliminated because it isn’t clear how\n\n\n\nExcel Workbook Screenshot\n\n\n\nIdentify desired data structure\nInspection of the excel workbook reveals several critical features of the data. - column names (of a sort) are in rows 4 and 5 (skip=5 and rename) - first column includes year and race/hispanic origin households - first column appears to have notes of some sort (remove notes) - there are end notes starting in row 358 (n_max = 352) - “Total” column appears to be redundant proportion info\nThe data appears to have two grouping variables (year and identityity), plus several values:\n\na count of number of households\nmedian and mean income (and associated margin of error)\nproportion of households with hhold income in one of 9 designated ranges or brackets\n\nThe final data should probably\n\nRead and clean the dataClean and separate *year” columnSanity check for identity\n\n\n\nincome_brackets <- c(i1 = \"Under $15,000\",\n                     i2 = \"$15,000 to $24,999\",\n                     i3 = \"$25,000 to $34,999\",\n                     i4= \"$35,000 to $49,999\",\n                     i5 = \"$50,000 to $74,999\",\n                     i6 = \"$75,000 to $99,999\",\n                     i7 = \"$100,000 to $149,999\",\n                     i8 = \"$150,000 to $199,999\",\n                     i9 = \"$200,000 and over\")\n\nushh_orig <- read_excel(\"_data/USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\",\n                        skip=5,\n                        n_max = 352,\n                        col_names = c(\"year\", \"hholds\", \"del\", \n                                str_c(\"income\",1:9,sep=\"_i\"),\n                               \"median_inc\", \"median_se\", \"mean_inc\",\"mean_se\"))%>%\n  select(-del)\n\n\n\nThe current year column still has identityity information on the hholds, as well as notes that need to be removed. Because identityity labels have spaces, we will need to remove those first before our typical approach to removing notes using separate is going to work.\n\n\n\n\n\n\nRegex and Regexr\n\n\n\nRegular expressions are a critical tool for messy, real world data where you will need to search, replace, and extract information from string variables. Learning regex is tough, but Regexer makes it much easier!\n\n\n\nushh_orig%>%\n  filter(str_detect(year, \"[[:alpha:]]\"))\n\n\n\n  \n\n\n\nNow that we know how to use regular expressions to find the household identityity information, we can quickly separate out the identityity information from the years, then do the standard fill prior to removing the unneeded category rows.\nOnce that is done, we can use separate to remove the notes from the year column. Removing notes from the identityity column is a bit trickier, and requires regex to find cases where there is a space then at least one numeric digit\n\nushh_id<-ushh_orig%>%\n  mutate(identity = case_when(\n    str_detect(year, \"[[:alpha:]]\") ~ year,\n    TRUE ~ NA_character_\n  ))%>%\n  fill(identity)%>%\n  filter(!str_detect(year, \"[[:alpha:]]\"))\n\nushh_id<-ushh_id%>%\n  separate(year, into=c(\"year\", \"delete\"), sep=\" \")%>%\n  mutate(identity = str_remove(identity, \" [0-9]+\"),\n         year = parse_number(year))%>%\n  select(-delete)\n\n\n\nEven from the detailed notes, it is difficult to fully understand what is going on with the identity variable, and whether all of the values are available in every year. A simple sanity check is to pick out several years mentioned in the notes and see if the number of households are available for all categories, and also check to see if there are specific categories that add up to the “all races” category.\n\nushh_id%>%\n  filter(year%in%c(1970, 1972, 1980, 2001, 2002))%>%\n  select(identity, hholds, year)%>%\n  pivot_wider(values_from=hholds, names_from=year)\n\n\n\n  \n\n\n\nBased on these examples, we can now confirm that the survey did not include a question about Hispanic background prior to 109228, that only “White” and “Black” (and not “Asian”) were systematically recorded prior to 2002, and that other mentioned dates of changes are not relevant to the categories represented in the data. Additionally, we can see from the example years that it would be reasonable to create a consistent time series that collapses the “White” and “White Alone” and “Black” and “Black A labels.\nBased on this exploratory data, one reasonable option that will streamline future analysis is to create two new variables “race” and “hispanic” as follows. :::{.callout-tip} ## Keep your original data\nOriginal data that has been carefully documented can be overly detailed and broken into categories that make systematic analysis difficult. When you simplify data categories for exploratory work, keep the original data so that you can reintroduce it at the appropriate point.\n\n\n\n\nushh <-ushh_id%>%\n  mutate(id = case_when(\n    identity %in% c(\"White\", \"White Alone\") ~ \"race_white\"\n  ))"
  },
  {
    "objectID": "posts/KaushikaPotluri_Challenge1.html",
    "href": "posts/KaushikaPotluri_Challenge1.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(dplyr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/KaushikaPotluri_Challenge1.html#challenge-overview",
    "href": "posts/KaushikaPotluri_Challenge1.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/KaushikaPotluri_Challenge1.html#read-in-the-data",
    "href": "posts/KaushikaPotluri_Challenge1.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\ndata <- read_csv(\"_data/birds.csv\", col_types = \"ccncncncnncncc\")\nspec(data)\n\n\ncols(\n  `Domain Code` = col_character(),\n  Domain = col_character(),\n  `Area Code` = col_number(),\n  Area = col_character(),\n  `Element Code` = col_number(),\n  Element = col_character(),\n  `Item Code` = col_number(),\n  Item = col_character(),\n  `Year Code` = col_number(),\n  Year = col_number(),\n  Unit = col_character(),\n  Value = col_number(),\n  Flag = col_character(),\n  `Flag Description` = col_character()\n)\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/KaushikaPotluri_Challenge1.html#describe-the-data",
    "href": "posts/KaushikaPotluri_Challenge1.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nnames(data)\n\n\n [1] \"Domain Code\"      \"Domain\"           \"Area Code\"        \"Area\"            \n [5] \"Element Code\"     \"Element\"          \"Item Code\"        \"Item\"            \n [9] \"Year Code\"        \"Year\"             \"Unit\"             \"Value\"           \n[13] \"Flag\"             \"Flag Description\"\n\n\nCode\ndim(data)\n\n\n[1] 30977    14\n\n\nCode\nstr(data)\n\n\nspec_tbl_df [30,977 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Domain Code     : chr [1:30977] \"QA\" \"QA\" \"QA\" \"QA\" ...\n $ Domain          : chr [1:30977] \"Live Animals\" \"Live Animals\" \"Live Animals\" \"Live Animals\" ...\n $ Area Code       : num [1:30977] 2 2 2 2 2 2 2 2 2 2 ...\n $ Area            : chr [1:30977] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ Element Code    : num [1:30977] 5112 5112 5112 5112 5112 ...\n $ Element         : chr [1:30977] \"Stocks\" \"Stocks\" \"Stocks\" \"Stocks\" ...\n $ Item Code       : num [1:30977] 1057 1057 1057 1057 1057 ...\n $ Item            : chr [1:30977] \"Chickens\" \"Chickens\" \"Chickens\" \"Chickens\" ...\n $ Year Code       : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Year            : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Unit            : chr [1:30977] \"1000 Head\" \"1000 Head\" \"1000 Head\" \"1000 Head\" ...\n $ Value           : num [1:30977] 4700 4900 5000 5300 5500 5800 6600 6290 6300 6000 ...\n $ Flag            : chr [1:30977] \"F\" \"F\" \"F\" \"F\" ...\n $ Flag Description: chr [1:30977] \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Domain Code` = col_character(),\n  ..   Domain = col_character(),\n  ..   `Area Code` = col_number(),\n  ..   Area = col_character(),\n  ..   `Element Code` = col_number(),\n  ..   Element = col_character(),\n  ..   `Item Code` = col_number(),\n  ..   Item = col_character(),\n  ..   `Year Code` = col_number(),\n  ..   Year = col_number(),\n  ..   Unit = col_character(),\n  ..   Value = col_number(),\n  ..   Flag = col_character(),\n  ..   `Flag Description` = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nCode\nsummary(data)\n\n\n Domain Code           Domain            Area Code        Area          \n Length:30977       Length:30977       Min.   :   1   Length:30977      \n Class :character   Class :character   1st Qu.:  79   Class :character  \n Mode  :character   Mode  :character   Median : 156   Mode  :character  \n                                       Mean   :1202                     \n                                       3rd Qu.: 231                     \n                                       Max.   :5504                     \n                                                                        \n  Element Code    Element            Item Code        Item          \n Min.   :5112   Length:30977       Min.   :1057   Length:30977      \n 1st Qu.:5112   Class :character   1st Qu.:1057   Class :character  \n Median :5112   Mode  :character   Median :1068   Mode  :character  \n Mean   :5112                      Mean   :1066                     \n 3rd Qu.:5112                      3rd Qu.:1072                     \n Max.   :5112                      Max.   :1083                     \n                                                                    \n   Year Code         Year          Unit               Value         \n Min.   :1961   Min.   :1961   Length:30977       Min.   :       0  \n 1st Qu.:1976   1st Qu.:1976   Class :character   1st Qu.:     171  \n Median :1992   Median :1992   Mode  :character   Median :    1800  \n Mean   :1991   Mean   :1991                      Mean   :   99411  \n 3rd Qu.:2005   3rd Qu.:2005                      3rd Qu.:   15404  \n Max.   :2018   Max.   :2018                      Max.   :23707134  \n                                                  NA's   :1036      \n     Flag           Flag Description  \n Length:30977       Length:30977      \n Class :character   Class :character  \n Mode  :character   Mode  :character"
  },
  {
    "objectID": "posts/challenge4_ZhiyuanZhou.html",
    "href": "posts/challenge4_ZhiyuanZhou.html",
    "title": "Challenge 4 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_ZhiyuanZhou.html#challenge-overview",
    "href": "posts/challenge4_ZhiyuanZhou.html#challenge-overview",
    "title": "Challenge 4 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_ZhiyuanZhou.html#read-in-data",
    "href": "posts/challenge4_ZhiyuanZhou.html#read-in-data",
    "title": "Challenge 4 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ⭐\npoultry_tidy.csv⭐⭐\nFedFundsRate.csv⭐⭐⭐\nhotel_bookings.csv⭐⭐⭐⭐\ndebt_in_trillions ⭐⭐⭐⭐⭐\n\n\n\nCode\npoultry <- read_csv(\"_data/poultry_tidy.csv\",\n                     show_col_types = FALSE)\n\n\n\nBriefly describe the data\nThis is price of poultry in month including whole chicken, B/S Breast, Bone-in Breast, Whole Legs, Thighs from 2004 to 2013."
  },
  {
    "objectID": "posts/challenge4_ZhiyuanZhou.html#tidy-data-as-needed",
    "href": "posts/challenge4_ZhiyuanZhou.html#tidy-data-as-needed",
    "title": "Challenge 4 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\nCode\n#chck missing\npoultry%>%\n  summarise_all(funs(sum(is.na(.))))\n\n\n# A tibble: 1 × 4\n  Product  Year Month Price_Dollar\n    <int> <int> <int>        <int>\n1       0     0     0            7\n\n\nCode\npoultry_tidy<-\n  poultry%>%\n  transmute(\n    Product,\n    YM=as.Date(ymd(paste0(Year,\"-\",Month,\"-01\"))),\n    Price_Dollar=ifelse(is.na(Price_Dollar),0,Price_Dollar))\n\npoultry_wide<-\n  poultry_tidy%>%\n  pivot_wider(\n  names_from = YM, values_from = Price_Dollar ,values_fill = 0\n  )\n#sanity check\n#chck missing\npoultry_tidy%>%\n  summarise_all(funs(sum(is.na(.))))\n\n\n# A tibble: 1 × 3\n  Product    YM Price_Dollar\n    <int> <int>        <int>\n1       0     0            0\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge4_ZhiyuanZhou.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_ZhiyuanZhou.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Instructions",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\nDates are combined to be YYYY-MM-DD\nNANs are filled with 0\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html",
    "title": "Homework 2",
    "section": "",
    "text": "Read in a dataset from the _data folder in the course blog repository, or choose your own data. If you decide to use one of the datasets we have provided, please use a challenging dataset - check with us if you are not sure.\nClean the data as needed using dplyr and related tidyverse packages.\nProvide a narrative about the data set (look it up if you aren’t sure what you have got) and the variables in your dataset, including what type of data each variable is. The goal of this step is to communicate in a visually appealing way to non-experts - not to replicate r-code.\nIdentify potential research questions that your dataset can help answer.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(Hmisc)\nlibrary(psych)\nlibrary(readxl)\nlibrary(stringr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#data-reading",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#data-reading",
    "title": "Homework 2",
    "section": "Data Reading",
    "text": "Data Reading\n\nRead dataset ‘organiceggpoultry.xls’, sheet 2, available in the posts/_data folder, using the read_excel R package and command.\n\n\nCode\norganicegg <- read_excel(\"_data/organiceggpoultry.xls\", sheet = \"Organic egg prices, 2004-13\", skip = 1)\n\nglimpse(organicegg)\n\n\nRows: 74\nColumns: 13\n$ Year <chr> \"2013\", \"Certified Organic Eggs:\", \"Extra Large \\nDozen\", \"Extra …\n$ Jan. <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"Jan.\", NA, NA, \"288.5\",…\n$ Feb. <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"Feb.\", NA, NA, \"288.5\",…\n$ Mar. <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"Mar.\", NA, NA, \"288.5\",…\n$ Apr. <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"Apr.\", NA, NA, \"288.5\",…\n$ May  <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"May\", NA, NA, \"288.5\", …\n$ June <chr> \"Cents per pound\", NA, \"290\", \"188.13\", \"267.5\", \"178\", \"June\", \"…\n$ July <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"July\", NA, NA, \"288.5\",…\n$ Aug. <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"Aug.\", NA, NA, \"288.5\",…\n$ Sep. <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"Sep.\", NA, NA, \"290\", \"…\n$ Oct. <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"Oct.\", NA, NA, \"290\", \"…\n$ Nov. <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"Nov.\", NA, NA, \"290\", \"…\n$ Dec. <chr> NA, NA, \"290\", \"188.13\", \"267.5\", \"178\", \"Dec.\", NA, NA, \"290\", \"…"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#data-wrangling",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#data-wrangling",
    "title": "Homework 2",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n- mutate function to convert column type to numeric\n\n\n- filter along with regex to remove row consisting NA,Year & Certified Organic Eggs:\n\n\n- replace to replace all NA value with 0\n\n\n- filter to remove unnecessary tail of 41, 42 43 rows\n\n\nCode\norganicegg <-organicegg%>%\n  mutate_at(vars(colnames(organicegg)[2:13]), function(x)as.numeric(x)) %>% \n  filter(!is.na(Year) & !grepl(\"^2\", Year) & !grepl(\"^Ce\", Year)) %>% \n  replace(is.na(.), 0)%>% \n  filter(!row_number() %in% c(41, 42, 43))"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#data-wrangling-1",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#data-wrangling-1",
    "title": "Homework 2",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n- changed name of first column\n\n\n- create vector of same length as the number of rows in the organicegg dataset and same distribution of years as original dataset\n\n\n- column bind to merge Year vector & organicegg dataset\n\n\n- pivot_longer on All Month Columns\n\n\nCode\nnames(organicegg)[1] = \"Certified_Organic_Eggs\"\nYear <- rep(c(2013,2012,2011,2010,2009,2008,2007,2006,2005,2004),each=4)\norganicegg<- cbind(Year, organicegg)\norganicegg <- organicegg%>% \n  pivot_longer(colnames(organicegg)[3:14], names_to = 'Month', values_to = 'Price')"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#describe-data",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#describe-data",
    "title": "Homework 2",
    "section": "Describe Data",
    "text": "Describe Data\n\n- summary of organicegg dataset using summarytools::dfSummary\n\n\nCode\nprint(summarytools::dfSummary(organicegg,\n                              varnumbers = FALSE,\n                              plain.ascii  = FALSE,\n                              style        = \"grid\",\n                              graph.magnif = 0.70,\n                              valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\norganicegg\nDimensions: 480 x 4\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Year\n[numeric]\n      Mean (sd) : 2008.5 (2.9)min ≤ med ≤ max:2004 ≤ 2008.5 ≤ 2013IQR (CV) : 5 (0)\n      2004:48(10.0%)2005:48(10.0%)2006:48(10.0%)2007:48(10.0%)2008:48(10.0%)2009:48(10.0%)2010:48(10.0%)2011:48(10.0%)2012:48(10.0%)2013:48(10.0%)\n      \n      0\n(0.0%)\n    \n    \n      Certified_Organic_Eggs\n[character]\n      1. Extra Large Dozen2. Extra Large 1/2 Doz.3. Extra Large 1/2 Doz.1/2 4. Large 1/2 Doz.5. Large Dozen\n      120(25.0%)108(22.5%)12(2.5%)120(25.0%)120(25.0%)\n      \n      0\n(0.0%)\n    \n    \n      Month\n[character]\n      1. Apr.2. Aug.3. Dec.4. Feb.5. Jan.6. July7. June8. Mar.9. May10. Nov.[ 2 others ]\n      40(8.3%)40(8.3%)40(8.3%)40(8.3%)40(8.3%)40(8.3%)40(8.3%)40(8.3%)40(8.3%)40(8.3%)80(16.7%)\n      \n      0\n(0.0%)\n    \n    \n      Price\n[numeric]\n      Mean (sd) : 210.8 (55.3)min ≤ med ≤ max:126 ≤ 206.6 ≤ 290IQR (CV) : 93 (0.3)\n      46 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\n\n\nCode\n# a <-as.data.frame(strsplit(organicegg$Certified_Organic_Eggs, split = '([a-zA-Z\\\\ -1/-2])'))\n# View(a)"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#group-summary-statistics",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#group-summary-statistics",
    "title": "Homework 2",
    "section": "Group Summary Statistics",
    "text": "Group Summary Statistics\n\n- Year wise, Certified_Organic_Eggs wise Price Distribution\n\n\nCode\norganicegg$Year<-as.factor(organicegg$Year)\norganicegg %>% \n  group_by(Year, Certified_Organic_Eggs) %>% \n  summarise(Price_min=min(Price),Price_max=max(Price),Price_mean=mean(Price), .groups = 'keep') %>%\n  arrange(desc(Certified_Organic_Eggs))\n\n\n# A tibble: 40 × 5\n# Groups:   Year, Certified_Organic_Eggs [40]\n   Year  Certified_Organic_Eggs Price_min Price_max Price_mean\n   <fct> <chr>                      <dbl>     <dbl>      <dbl>\n 1 2004  \"Large \\nDozen\"             225       234.       230.\n 2 2005  \"Large \\nDozen\"             234.      234.       234.\n 3 2006  \"Large \\nDozen\"             234.      236.       234.\n 4 2007  \"Large \\nDozen\"             234.      237        237.\n 5 2008  \"Large \\nDozen\"             237       278.       267.\n 6 2009  \"Large \\nDozen\"             272.      278.       274 \n 7 2010  \"Large \\nDozen\"             268.      268        268.\n 8 2011  \"Large \\nDozen\"             268.      270        269.\n 9 2012  \"Large \\nDozen\"             268.      268.       268.\n10 2013  \"Large \\nDozen\"             268.      268.       268.\n# … with 30 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n  print(organicegg, n=40)\n\n\n# A tibble: 480 × 4\n   Year  Certified_Organic_Eggs Month Price\n   <fct> <chr>                  <chr> <dbl>\n 1 2013  \"Extra Large \\nDozen\"  Jan.   290 \n 2 2013  \"Extra Large \\nDozen\"  Feb.   290 \n 3 2013  \"Extra Large \\nDozen\"  Mar.   290 \n 4 2013  \"Extra Large \\nDozen\"  Apr.   290 \n 5 2013  \"Extra Large \\nDozen\"  May    290 \n 6 2013  \"Extra Large \\nDozen\"  June   290 \n 7 2013  \"Extra Large \\nDozen\"  July   290 \n 8 2013  \"Extra Large \\nDozen\"  Aug.   290 \n 9 2013  \"Extra Large \\nDozen\"  Sep.   290 \n10 2013  \"Extra Large \\nDozen\"  Oct.   290 \n11 2013  \"Extra Large \\nDozen\"  Nov.   290 \n12 2013  \"Extra Large \\nDozen\"  Dec.   290 \n13 2013  \"Extra Large 1/2 Doz.\" Jan.   188.\n14 2013  \"Extra Large 1/2 Doz.\" Feb.   188.\n15 2013  \"Extra Large 1/2 Doz.\" Mar.   188.\n16 2013  \"Extra Large 1/2 Doz.\" Apr.   188.\n17 2013  \"Extra Large 1/2 Doz.\" May    188.\n18 2013  \"Extra Large 1/2 Doz.\" June   188.\n19 2013  \"Extra Large 1/2 Doz.\" July   188.\n20 2013  \"Extra Large 1/2 Doz.\" Aug.   188.\n21 2013  \"Extra Large 1/2 Doz.\" Sep.   188.\n22 2013  \"Extra Large 1/2 Doz.\" Oct.   188.\n23 2013  \"Extra Large 1/2 Doz.\" Nov.   188.\n24 2013  \"Extra Large 1/2 Doz.\" Dec.   188.\n25 2013  \"Large \\nDozen\"        Jan.   268.\n26 2013  \"Large \\nDozen\"        Feb.   268.\n27 2013  \"Large \\nDozen\"        Mar.   268.\n28 2013  \"Large \\nDozen\"        Apr.   268.\n29 2013  \"Large \\nDozen\"        May    268.\n30 2013  \"Large \\nDozen\"        June   268.\n31 2013  \"Large \\nDozen\"        July   268.\n32 2013  \"Large \\nDozen\"        Aug.   268.\n33 2013  \"Large \\nDozen\"        Sep.   268.\n34 2013  \"Large \\nDozen\"        Oct.   268.\n35 2013  \"Large \\nDozen\"        Nov.   268.\n36 2013  \"Large \\nDozen\"        Dec.   268.\n37 2013  \"Large \\n1/2 Doz.\"     Jan.   178 \n38 2013  \"Large \\n1/2 Doz.\"     Feb.   178 \n39 2013  \"Large \\n1/2 Doz.\"     Mar.   178 \n40 2013  \"Large \\n1/2 Doz.\"     Apr.   178 \n# … with 440 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#group-summary-statistics-1",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#group-summary-statistics-1",
    "title": "Homework 2",
    "section": "Group Summary Statistics",
    "text": "Group Summary Statistics\n\n- pivot_wider to expand Month wise Price value across columns\n\n\nCode\norganicegg %>% \n  pivot_wider(names_from = Month, values_from = Price) %>% \n  print(n=40)\n\n\n# A tibble: 40 × 14\n   Year  Certified…¹  Jan.  Feb.  Mar.  Apr.   May  June  July  Aug.  Sep.  Oct.\n   <fct> <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 2013  \"Extra Lar…  290   290   290   290   290   290   290   290   290   290 \n 2 2013  \"Extra Lar…  188.  188.  188.  188.  188.  188.  188.  188.  188.  188.\n 3 2013  \"Large \\nD…  268.  268.  268.  268.  268.  268.  268.  268.  268.  268.\n 4 2013  \"Large \\n1…  178   178   178   178   178   178   178   178   178   178 \n 5 2012  \"Extra Lar…  288.  288.  288.  288.  288.  288.  288.  288.  290   290 \n 6 2012  \"Extra Lar…  186.  186.  186.  186.  186.  186.  186.  186.  188.  188.\n 7 2012  \"Large \\nD…  268.  268.  268.  268.  268.  268.  268.  268.  268.  268.\n 8 2012  \"Large \\n1…  174.  174.  173.  173.  173.  173.  173.  173.  178   178 \n 9 2011  \"Extra Lar…  286.  286.  286.  286.  286.  286.  286.  286.  286.  286.\n10 2011  \"Extra Lar…  186.  186.  186.  186.  186.  186.  186.  186.  186.  186.\n11 2011  \"Large \\nD…  268.  268.  268.  270   270   270   270   270   270   270 \n12 2011  \"Large \\n1…  174.  174.  174.  174.  174.  174.  174.  174.  174.  174.\n13 2010  \"Extra Lar…  286.  286.  286.  286.  286.  286.  286.  286.  286.  286.\n14 2010  \"Extra Lar…  186.  186.  186.  186.  186.  186.  186.  186.  186.  186.\n15 2010  \"Large \\nD…  268   268   268   268   268   268   268   268.  268.  268.\n16 2010  \"Large \\n1…  174.  174.  174.  174.  174.  174.  174.  174.  174.  174.\n17 2009  \"Extra Lar…  286.  286.  286.  286.  286.  286.  286.  286.  286.  286.\n18 2009  \"Extra Lar…  186.  186.  186.  186.  186.  186.  186.  186.  186.  186.\n19 2009  \"Large \\nD…  278.  278.  278.  278.  278.  272.  272.  272.  272.  272.\n20 2009  \"Large \\n1…  174.  174.  174.  174.  174.  174.  174.  174.  174.  174.\n21 2008  \"Extra Lar…  245   245   245   286.  286.  286.  286.  286.  286.  286.\n22 2008  \"Extra Lar…  139   139   139   186.  186.  186.  186.  186.  186.  186.\n23 2008  \"Large \\nD…  237   237   237   278.  278.  278.  278.  278.  278.  278.\n24 2008  \"Large \\n1…  132   132   132   174.  174.  174.  174.  174.  174.  174.\n25 2007  \"Extra Lar…  241.  244.  245   245   245   245   245   245   245   245 \n26 2007  \"Extra Lar…  136.  138.  139   139   139   139   139   139   139   139 \n27 2007  \"Large \\nD…  234.  236.  237   237   237   237   237   237   237   237 \n28 2007  \"Large \\n1…  128.  131.  132   132   132   132   132   132   132   132 \n29 2006  \"Extra Lar…  241   240   241.  242.  242.  242.  242.  242.  242.  242.\n30 2006  \"Extra Lar…  136.  136.  136.  136.  136.  136.  136.  136.  136.  136.\n31 2006  \"Large \\nD…  234.  234.  234.  234.  234.  234.  234.  234.  234.  234.\n32 2006  \"Large \\n1…  128.  128.  128.  128.  128.  128.  128.  128.  128.  128.\n33 2005  \"Extra Lar…  241   241   241   241   241   241   241   241   241   241 \n34 2005  \"Extra Lar…  136.  136.  136.  136.  136.  136.  136.  136.  136.  136.\n35 2005  \"Large \\nD…  234.  234.  234.  234.  234.  234.  234.  234.  234.  234.\n36 2005  \"Large \\n1…  128.  128.  128.  128.  128.  128.  128.  128.  128.  128.\n37 2004  \"Extra Lar…  230   230   230   234.  236   241   241   241   241   241 \n38 2004  \"Extra Lar…  132   134.  137   137   137   137   137   137   136.  136.\n39 2004  \"Large \\nD…  230   226.  225   225   225   231.  234.  234.  234.  234.\n40 2004  \"Large \\n1…  126   128.  131   131   131   134.  134.  134.  130.  128.\n# … with 2 more variables: Nov. <dbl>, Dec. <dbl>, and abbreviated variable\n#   name ¹​Certified_Organic_Eggs\n# ℹ Use `colnames()` to see all variable names"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#data-reading-1",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#data-reading-1",
    "title": "Homework 2",
    "section": "Data Reading",
    "text": "Data Reading\n\n- read dataset ‘organiceggpoultry.xls’, sheet 3, , available in the posts/_data folder, using the read_excel R package and command.\n\n\nCode\norganicepoultry <- read_excel(\"_data/organiceggpoultry.xls\", sheet = \"Organic poultry prices, 2004-13\", skip = 1)\n\nglimpse(organicepoultry)\n\n\nRows: 99\nColumns: 13\n$ Year <chr> \"2013\", \"Organic young chicken:\", \"Whole\", \"B/S Breast\", \"Bone-in…\n$ Jan. <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"Jan.\", NA…\n$ Feb. <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"Feb.\", NA…\n$ Mar. <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"Mar.\", NA…\n$ Apr. <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"Apr.\", NA…\n$ May  <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"May\", NA,…\n$ June <chr> \"Cents per pound\", NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.…\n$ July <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"July\", NA…\n$ Aug. <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"Aug.\", NA…\n$ Sep. <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"Sep.\", NA…\n$ Oct. <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"Oct.\", NA…\n$ Nov. <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"Nov.\", NA…\n$ Dec. <chr> NA, NA, \"238.5\", \"703.75\", \"390.5\", \"203.5\", \"216.25\", \"Dec.\", NA…"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#data-wrangling-2",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#data-wrangling-2",
    "title": "Homework 2",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n- mutate function to convert column type to numeric\n\n\n- filter along with regex to remove rows consisting NA, Year & Organic young chicken:\n\n\n- replace to replace all NA value with 0\n\n\n- filter to remove unnecessary tail of 51, 52 and 53 rows\n\n\nCode\norganicepoultry <-organicepoultry%>%\n  mutate_at(vars(colnames(organicepoultry)[2:13]), function(x)as.numeric(x)) %>% \n  filter(!is.na(Year) & !grepl(\"^2\", Year) & !grepl(\"^Or\", Year)) %>% \n  replace(is.na(.), 0)%>% \n  filter(!row_number() %in% c(51, 52, 53))"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#data-wrangling-3",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#data-wrangling-3",
    "title": "Homework 2",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n- changed name of first column to Organic_Young_Chicken\n\n\n- create vector of same length as the number of rows in the organicegg dataset and same distribution of years as original dataset\n\n\n- column bind to merge, Year vector & organicegg\n\n\n- pivot_longer on All Month Columns\n\n\nCode\nnames(organicepoultry)[1] = \"Organic_Young_Chicken\"\n\nYear <- rep(c(2013,2012,2011,2010,2009,2008,2007,2006,2005,2004),each=5)\n\norganicepoultry<- cbind(Year, organicepoultry)\n\norganicepoultry<-organicepoultry%>% \n  pivot_longer(colnames(organicepoultry)[3:14], names_to = 'Month', values_to = 'Price')"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#describe-data-1",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#describe-data-1",
    "title": "Homework 2",
    "section": "Describe Data",
    "text": "Describe Data\n\n- Summary of organicepoultry Dataset, using summarytools::dfSummary\n\n\nCode\nprint(summarytools::dfSummary(organicepoultry,\n                              varnumbers = FALSE,\n                              plain.ascii  = FALSE,\n                              style        = \"grid\",\n                              graph.magnif = 0.70,\n                              valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\norganicepoultry\nDimensions: 600 x 4\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Year\n[numeric]\n      Mean (sd) : 2008.5 (2.9)min ≤ med ≤ max:2004 ≤ 2008.5 ≤ 2013IQR (CV) : 5 (0)\n      2004:60(10.0%)2005:60(10.0%)2006:60(10.0%)2007:60(10.0%)2008:60(10.0%)2009:60(10.0%)2010:60(10.0%)2011:60(10.0%)2012:60(10.0%)2013:60(10.0%)\n      \n      0\n(0.0%)\n    \n    \n      Organic_Young_Chicken\n[character]\n      1. B/S Breast2. Bone-in Breast3. Thighs4. Whole5. Whole Legs\n      120(20.0%)120(20.0%)120(20.0%)120(20.0%)120(20.0%)\n      \n      0\n(0.0%)\n    \n    \n      Month\n[character]\n      1. Apr.2. Aug.3. Dec.4. Feb.5. Jan.6. July7. June8. Mar.9. May10. Nov.[ 2 others ]\n      50(8.3%)50(8.3%)50(8.3%)50(8.3%)50(8.3%)50(8.3%)50(8.3%)50(8.3%)50(8.3%)50(8.3%)100(16.7%)\n      \n      0\n(0.0%)\n    \n    \n      Price\n[numeric]\n      Mean (sd) : 335.1 (175.9)min ≤ med ≤ max:0 ≤ 235 ≤ 703.8IQR (CV) : 175.5 (0.5)\n      33 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23"
  },
  {
    "objectID": "posts/hw_organiceggpoultry_Akhilesh.html#group-summary-statistics-2",
    "href": "posts/hw_organiceggpoultry_Akhilesh.html#group-summary-statistics-2",
    "title": "Homework 2",
    "section": "Group Summary Statistics",
    "text": "Group Summary Statistics\n\n- Year wise, Certified_Organic_Eggs wise Price Distribution\n\n\nCode\norganicepoultry$Year<-as.factor(organicepoultry$Year)\norganicepoultry %>% \n  group_by(Year, Organic_Young_Chicken) %>% \n  summarise(Price_min=min(Price),Price_max=max(Price),Price_mean=mean(Price), .groups = 'keep') %>% \n  arrange(desc(Organic_Young_Chicken)) %>%\n  print(n = 50)\n\n\n# A tibble: 50 × 5\n# Groups:   Year, Organic_Young_Chicken [50]\n   Year  Organic_Young_Chicken Price_min Price_max Price_mean\n   <fct> <chr>                     <dbl>     <dbl>      <dbl>\n 1 2004  Whole Legs                 194.      204.       199.\n 2 2005  Whole Legs                 204.      204.       204.\n 3 2006  Whole Legs                 204.      204.       204.\n 4 2007  Whole Legs                 204.      204.       204.\n 5 2008  Whole Legs                 204.      204.       204.\n 6 2009  Whole Legs                 204.      204.       204.\n 7 2010  Whole Legs                 204.      204.       204.\n 8 2011  Whole Legs                 204.      204.       204.\n 9 2012  Whole Legs                 204.      204.       204.\n10 2013  Whole Legs                 204.      204.       204.\n11 2004  Whole                      198.      217        212.\n12 2005  Whole                      217       217        217 \n13 2006  Whole                      217       220.       220.\n14 2007  Whole                      220.      220.       220.\n15 2008  Whole                      220.      248        237.\n16 2009  Whole                      248       248        248 \n17 2010  Whole                      235       248        239.\n18 2011  Whole                      235       235        235 \n19 2012  Whole                      235       238.       238.\n20 2013  Whole                      238.      238.       238.\n21 2004  Thighs                       0       203        184.\n22 2005  Thighs                     213       222        221.\n23 2006  Thighs                     222       222        222 \n24 2007  Thighs                     222       222        222 \n25 2008  Thighs                     222       222        222 \n26 2009  Thighs                     222       222        222 \n27 2010  Thighs                     215       222        219.\n28 2011  Thighs                     215       215        215 \n29 2012  Thighs                     215       216.       216.\n30 2013  Thighs                     216.      216.       216.\n31 2004  Bone-in Breast               0       390.       195.\n32 2005  Bone-in Breast             390.      390.       390.\n33 2006  Bone-in Breast             390.      390.       390.\n34 2007  Bone-in Breast             390.      390.       390.\n35 2008  Bone-in Breast             390.      390.       390.\n36 2009  Bone-in Breast             390.      390.       390.\n37 2010  Bone-in Breast             390.      390.       390.\n38 2011  Bone-in Breast             390.      390.       390.\n39 2012  Bone-in Breast             390.      390.       390.\n40 2013  Bone-in Breast             390.      390.       390.\n41 2004  B/S Breast                 641       646.       643.\n42 2005  B/S Breast                 644       646.       645.\n43 2006  B/S Breast                 646.      646.       646.\n44 2007  B/S Breast                 646.      646.       646.\n45 2008  B/S Breast                 646.      646.       646.\n46 2009  B/S Breast                 646.      646.       646.\n47 2010  B/S Breast                 638.      646.       643.\n48 2011  B/S Breast                 638.      638.       638.\n49 2012  B/S Breast                 638.      704.       695.\n50 2013  B/S Breast                 704.      704.       704."
  },
  {
    "objectID": "posts/challenge2_stevenoneill.html",
    "href": "posts/challenge2_stevenoneill.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_stevenoneill.html#introduction",
    "href": "posts/challenge2_stevenoneill.html#introduction",
    "title": "Challenge 2",
    "section": "Introduction",
    "text": "Introduction\nI’m going to try to do hotel_bookings: ⭐⭐⭐⭐\n\n\nCode\nhotel_bookings <- read_csv(\"_data/hotel_bookings.csv\")\nhotel_bookings\n\n\n\n\n  \n\n\n\nReading in the data is straightforward."
  },
  {
    "objectID": "posts/challenge2_stevenoneill.html#describe-the-data",
    "href": "posts/challenge2_stevenoneill.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nThis dataset is available on Kaggle and was originally published in the journal Hospitality Management.\nThe data describes two hotels - one ‘city’ and one ‘resort’-style.\nBefore I begin, I should establish some terminology based on research online:\nTA: Travel Agent TO: Tour Operator Distribution channel: HB (meal): Half Board (breakfast + other meal) FB (meal): Full Board (3 meals a day)"
  },
  {
    "objectID": "posts/challenge2_stevenoneill.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_stevenoneill.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nThere are just two de-identified hotels analyzed in this whole dataset:\n\n\nCode\nhotel_bookings %>% group_by(hotel) %>% summarise()\n\n\n\n\n  \n\n\n\nIt is straightforward to give basic descriptive statistics on the Average Daily Rate of both hotels:\n\n\nCode\nhotel_bookings %>% select(adr) %>% summary(adr)\n\n\n      adr         \n Min.   :  -6.38  \n 1st Qu.:  69.29  \n Median :  94.58  \n Mean   : 101.83  \n 3rd Qu.: 126.00  \n Max.   :5400.00  \n\n\nI can also demonstrate some more interesting stats based on included values like visitor nationality and lead time:\n\n\nCode\nby_hotel <- hotel_bookings %>% group_by(hotel)\n\nby_hotel <- by_hotel %>% summarise(\n  average.lead.time = mean(lead_time),\n  busiest.year = names(which.max(table(arrival_date_year))),\n  busiest.month = names(which.max(table(arrival_date_month))),\n  most.freq.nationality = names(which.max(table(country))),\n  most.infreq.nationality = names(which.min(table(country)))\n)\n\nby_hotel\n\n\n\n\n  \n\n\n\nAs we can see,\n\nThe city hotel has a longer lead time on average compared to the resort - could this be based on conferences and business travel?\nBoth hotels’ busiest year in the dataset was 2016, as well as their busiest month being August [annually].\nThe most frequent nationality of guests was that of PRT - Portugal. That makes sense because the hotels are in Portugal.\nThe least frequent visitors in the City and Resort hotels hailed from Anguilla and Burundi, respectively.\n\nTo demonstrate select(), I pull out a few values:\n\n\nCode\nvisits <- select(hotel_bookings, hotel, market_segment, children, babies, country, reservation_status, reservation_status_date, arrival_date_month, adr)\nvisits\n\n\n\n\n  \n\n\n\nUsing filter(), here are visits that had a higher-than-median amount of babies brought along as compared to other visitors in the same market segment (Direct, Online, Corporate):\n\n\nCode\nvisits_with_baby <- visits %>% group_by(market_segment) %>% filter(babies > median(babies, na.rm = TRUE))\nvisits_with_baby\n\n\n\n\n  \n\n\n\nSo let’s compare the general make-up of travelers:\n\n\nCode\ntable(visits$market_segment)\n\n\n\n     Aviation Complementary     Corporate        Direct        Groups \n          237           743          5295         12606         19811 \nOffline TA/TO     Online TA     Undefined \n        24219         56477             2 \n\n\n…with those traveling with a baby:\n\n\nCode\ntable(visits_with_baby$market_segment)\n\n\n\nComplementary     Corporate        Direct        Groups Offline TA/TO \n           22             8           281            13           177 \n    Online TA \n          416 \n\n\nSomewhat expectedly, there aren’t too many business travelers bringing their kids!"
  },
  {
    "objectID": "posts/challenge2_AnanyaPujary.html",
    "href": "posts/challenge2_AnanyaPujary.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(skimr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_AnanyaPujary.html#read-in-the-data",
    "href": "posts/challenge2_AnanyaPujary.html#read-in-the-data",
    "title": "Challenge 2",
    "section": "Read in the Data",
    "text": "Read in the Data\nI’ll be working with the ‘hotel_bookings.csv’ dataset.\n\n\nCode\nhotelbookings <- read_csv(\"_data/hotel_bookings.csv\")"
  },
  {
    "objectID": "posts/challenge2_AnanyaPujary.html#describe-the-data",
    "href": "posts/challenge2_AnanyaPujary.html#describe-the-data",
    "title": "Challenge 2",
    "section": "Describe the data",
    "text": "Describe the data\nFirst, we’ll generate a broad overview of the data.\n\n\nCode\nskim(hotelbookings)\n\n\n\nData summary\n\n\nName\nhotelbookings\n\n\nNumber of rows\n119390\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nDate\n1\n\n\nnumeric\n18\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n178\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n8\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n12\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n334\n0\n\n\ncompany\n0\n1\n1\n4\n0\n353\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nreservation_status\n0\n1\n7\n9\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2014-10-17\n2017-09-14\n2016-08-07\n926\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nis_canceled\n0\n1\n0.37\n0.48\n0.00\n0.00\n0.00\n1\n1\n▇▁▁▁▅\n\n\nlead_time\n0\n1\n104.01\n106.86\n0.00\n18.00\n69.00\n160\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.16\n0.71\n2015.00\n2016.00\n2016.00\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.17\n13.61\n1.00\n16.00\n28.00\n38\n53\n▅▇▇▇▅\n\n\narrival_date_day_of_month\n0\n1\n15.80\n8.78\n1.00\n8.00\n16.00\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n1.00\n0.00\n0.00\n1.00\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.50\n1.91\n0.00\n1.00\n2.00\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.86\n0.58\n0.00\n2.00\n2.00\n2\n55\n▇▁▁▁▁\n\n\nchildren\n4\n1\n0.10\n0.40\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nbabies\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nis_repeated_guest\n0\n1\n0.03\n0.18\n0.00\n0.00\n0.00\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.09\n0.84\n0.00\n0.00\n0.00\n0\n26\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.14\n1.50\n0.00\n0.00\n0.00\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.22\n0.65\n0.00\n0.00\n0.00\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n2.32\n17.59\n0.00\n0.00\n0.00\n0\n391\n▇▁▁▁▁\n\n\nadr\n0\n1\n101.83\n50.54\n-6.38\n69.29\n94.58\n126\n5400\n▇▁▁▁▁\n\n\nrequired_car_parking_spaces\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0\n8\n▇▁▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.57\n0.79\n0.00\n0.00\n0.00\n1\n5\n▇▁▁▁▁\n\n\n\n\n\nThis dataset has 32 variables and 119390 cases. There are 14 columns of character type, 1 of date type (‘reservation_status_date’), and 18 of numeric type. Only the ‘children’ column has missing values (4).\n\n\nCode\ndistinct(hotelbookings,arrival_date_year) \n\n\n# A tibble: 3 × 1\n  arrival_date_year\n              <dbl>\n1              2015\n2              2016\n3              2017\n\n\nCode\ndistinct(hotelbookings,is_canceled)\n\n\n# A tibble: 2 × 1\n  is_canceled\n        <dbl>\n1           0\n2           1\n\n\nFrom the variable names, this dataset seems to be logging the data of a hotel’s bookings for the years 2015-2017, such as their customers’ arrival date, reservation status, and the number of adults/children/babies checking in. The variable ‘is_canceled’ has binary values and indicates whether a booking has been canceled (0) or not (1).\n\n\nCode\ndistinct(hotelbookings,reservation_status)\n\n\n# A tibble: 3 × 1\n  reservation_status\n  <chr>             \n1 Check-Out         \n2 Canceled          \n3 No-Show           \n\n\nCode\ndistinct(hotelbookings,hotel) \n\n\n# A tibble: 2 × 1\n  hotel       \n  <chr>       \n1 Resort Hotel\n2 City Hotel  \n\n\nCode\nhotelbookings %>%\n  group_by(hotel) %>%\n  tally()\n\n\n# A tibble: 2 × 2\n  hotel            n\n  <chr>        <int>\n1 City Hotel   79330\n2 Resort Hotel 40060\n\n\nThe ‘reservation_status’ variable has three values: ‘Check-Out’, ‘Canceled’, and ‘No-Show’. There are two types of hotels from which data are collected: ‘Resort Hotel’ and ‘City Hotel’. 79330 of the rows contain data related to the City Hotel, while 40060 rows are related to the Resort Hotel."
  },
  {
    "objectID": "posts/challenge2_AnanyaPujary.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_AnanyaPujary.html#provide-grouped-summary-statistics",
    "title": "Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nThe following command returns central tendency and dispersion values for the numeric values in the dataset.\n\n\nCode\nsummary(hotelbookings)\n\n\n    hotel            is_canceled       lead_time   arrival_date_year\n Length:119390      Min.   :0.0000   Min.   :  0   Min.   :2015     \n Class :character   1st Qu.:0.0000   1st Qu.: 18   1st Qu.:2016     \n Mode  :character   Median :0.0000   Median : 69   Median :2016     \n                    Mean   :0.3704   Mean   :104   Mean   :2016     \n                    3rd Qu.:1.0000   3rd Qu.:160   3rd Qu.:2017     \n                    Max.   :1.0000   Max.   :737   Max.   :2017     \n                                                                    \n arrival_date_month arrival_date_week_number arrival_date_day_of_month\n Length:119390      Min.   : 1.00            Min.   : 1.0             \n Class :character   1st Qu.:16.00            1st Qu.: 8.0             \n Mode  :character   Median :28.00            Median :16.0             \n                    Mean   :27.17            Mean   :15.8             \n                    3rd Qu.:38.00            3rd Qu.:23.0             \n                    Max.   :53.00            Max.   :31.0             \n                                                                      \n stays_in_weekend_nights stays_in_week_nights     adults      \n Min.   : 0.0000         Min.   : 0.0         Min.   : 0.000  \n 1st Qu.: 0.0000         1st Qu.: 1.0         1st Qu.: 2.000  \n Median : 1.0000         Median : 2.0         Median : 2.000  \n Mean   : 0.9276         Mean   : 2.5         Mean   : 1.856  \n 3rd Qu.: 2.0000         3rd Qu.: 3.0         3rd Qu.: 2.000  \n Max.   :19.0000         Max.   :50.0         Max.   :55.000  \n                                                              \n    children           babies              meal             country         \n Min.   : 0.0000   Min.   : 0.000000   Length:119390      Length:119390     \n 1st Qu.: 0.0000   1st Qu.: 0.000000   Class :character   Class :character  \n Median : 0.0000   Median : 0.000000   Mode  :character   Mode  :character  \n Mean   : 0.1039   Mean   : 0.007949                                        \n 3rd Qu.: 0.0000   3rd Qu.: 0.000000                                        \n Max.   :10.0000   Max.   :10.000000                                        \n NA's   :4                                                                  \n market_segment     distribution_channel is_repeated_guest\n Length:119390      Length:119390        Min.   :0.00000  \n Class :character   Class :character     1st Qu.:0.00000  \n Mode  :character   Mode  :character     Median :0.00000  \n                                         Mean   :0.03191  \n                                         3rd Qu.:0.00000  \n                                         Max.   :1.00000  \n                                                          \n previous_cancellations previous_bookings_not_canceled reserved_room_type\n Min.   : 0.00000       Min.   : 0.0000                Length:119390     \n 1st Qu.: 0.00000       1st Qu.: 0.0000                Class :character  \n Median : 0.00000       Median : 0.0000                Mode  :character  \n Mean   : 0.08712       Mean   : 0.1371                                  \n 3rd Qu.: 0.00000       3rd Qu.: 0.0000                                  \n Max.   :26.00000       Max.   :72.0000                                  \n                                                                         \n assigned_room_type booking_changes   deposit_type          agent          \n Length:119390      Min.   : 0.0000   Length:119390      Length:119390     \n Class :character   1st Qu.: 0.0000   Class :character   Class :character  \n Mode  :character   Median : 0.0000   Mode  :character   Mode  :character  \n                    Mean   : 0.2211                                        \n                    3rd Qu.: 0.0000                                        \n                    Max.   :21.0000                                        \n                                                                           \n   company          days_in_waiting_list customer_type           adr         \n Length:119390      Min.   :  0.000      Length:119390      Min.   :  -6.38  \n Class :character   1st Qu.:  0.000      Class :character   1st Qu.:  69.29  \n Mode  :character   Median :  0.000      Mode  :character   Median :  94.58  \n                    Mean   :  2.321                         Mean   : 101.83  \n                    3rd Qu.:  0.000                         3rd Qu.: 126.00  \n                    Max.   :391.000                         Max.   :5400.00  \n                                                                             \n required_car_parking_spaces total_of_special_requests reservation_status\n Min.   :0.00000             Min.   :0.0000            Length:119390     \n 1st Qu.:0.00000             1st Qu.:0.0000            Class :character  \n Median :0.00000             Median :0.0000            Mode  :character  \n Mean   :0.06252             Mean   :0.5714                              \n 3rd Qu.:0.00000             3rd Qu.:1.0000                              \n Max.   :8.00000             Max.   :5.0000                              \n                                                                         \n reservation_status_date\n Min.   :2014-10-17     \n 1st Qu.:2016-02-01     \n Median :2016-08-07     \n Mean   :2016-07-30     \n 3rd Qu.:2017-02-08     \n Max.   :2017-09-14     \n                        \n\n\n\n\nCode\n# calculating the number of cancellations and number of repeat guests grouped by hotel\nhotel_stats<-hotelbookings %>%\n  select(hotel,is_canceled,is_repeated_guest) %>%\n  group_by(hotel)%>%\n  summarize(cancellations = sum(is_canceled),guests_repeated = sum(is_repeated_guest))\n\nhotelbookings$room_assignment = (ifelse(hotelbookings$reserved_room_type==hotelbookings$assigned_room_type, 'Same', 'Different'))\n\nrooms <- hotelbookings %>%\n  select(hotel,room_assignment) %>%\n  group_by(hotel,room_assignment) %>%\n  tally()\n\n\nCity Hotel saw 33102 cancellations and Resort Hotel had 11122 cancellations during this time period. City Hotel also had more repeat guests (2032) than the Resort Hotel (1778). Also, I created a new column in the dataset called ‘room_assignment’ that indicates whether guests got their preferred room assignment (‘Same’) or not (‘Different’). City Hotel guests were less likely to be assigned a room not of their preference (7192) than Resort Hotel guests (7725).\n\n\nCode\nhotel_reservations <- hotelbookings %>%\n  select(hotel,reservation_status)%>%\n  group_by(hotel, reservation_status)%>%\n  tally()\n\n\nThen, I compared reservation statuses (‘Canceled, ’Check-Out’,‘No Show’) across hotels. For both, the number of guests checked out were more than those that cancelled or didn’t show up.\n\n\nCode\nhotelbookings %>% \n  select(hotel,arrival_date_year,arrival_date_month) %>% \n  group_by(hotel) %>% \n  count(arrival_date_year,arrival_date_month) %>%\n  slice(which.max(n))\n\n\n# A tibble: 2 × 4\n# Groups:   hotel [2]\n  hotel        arrival_date_year arrival_date_month     n\n  <chr>                    <dbl> <chr>              <int>\n1 City Hotel                2017 May                 4556\n2 Resort Hotel              2016 October             1984\n\n\nI also wanted to look at which month-year was the busiest in terms of bookings for both hotels. The busiest month was May 2017 for City Hotel (4556), and October 2016 for Resort Hotel (1984).\n\nExplain and Interpret\nI chose certain variables that would objectively indicate which hotel (City Hotel or Resort Hotel) was more successful, such as the number of cancellations and reservation status. However, City Hotel had more entries in the dataset than Resort Hotel, which could skew the results. I also wanted to compare which hotel assigned most customers their preferred room type (reserved_room_type versus assigned_room_type) which could be contributing to customer satisfaction, and found that City Hotel was better in this aspect. It would also be interesting to look at both hotels’ statistics through the years (2015-2017) to check whether their businesses have improved or declined."
  },
  {
    "objectID": "posts/challenge2_RoyYoon.html",
    "href": "posts/challenge2_RoyYoon.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n#library(readr)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_RoyYoon.html#read-in-the-dataset-railroad_2012_clean_county.csv",
    "href": "posts/challenge2_RoyYoon.html#read-in-the-dataset-railroad_2012_clean_county.csv",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Dataset “railroad_2012_clean_county.csv”",
    "text": "Read in the Dataset “railroad_2012_clean_county.csv”\n\n\nCode\nrailroad <- read_csv(\"_data/railroad_2012_clean_county.csv\")\n\nrailroad\n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\ndim(railroad)\n\n\n[1] 2930    3\n\n\nCode\ncolnames(railroad)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nThere are three variable names: ‘state’, ‘county’, and ‘total_employees’."
  },
  {
    "objectID": "posts/challenge2_RoyYoon.html#describe-the-data",
    "href": "posts/challenge2_RoyYoon.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\ncolnames(railroad)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nCode\ndim(railroad)\n\n\n[1] 2930    3\n\n\nCode\nrailroad \n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge2_RoyYoon.html#summary-statistics",
    "href": "posts/challenge2_RoyYoon.html#summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nFirst I tried to attempt at making the data grouped by states with a single total employee count(regardless of the counties)\n\n\nCode\nstate_grouped_railroad <- railroad %>%\n    select(state, total_employees) %>%\n    group_by(state) %>%\n    tally(total_employees)\n\nstate_grouped_railroad <-rename(state_grouped_railroad, total_employees = n)\n\nstate_grouped_railroad\n\n\n# A tibble: 53 × 2\n   state total_employees\n   <chr>           <dbl>\n 1 AE                  2\n 2 AK                103\n 3 AL               4257\n 4 AP                  1\n 5 AR               3871\n 6 AZ               3153\n 7 CA              13137\n 8 CO               3650\n 9 CT               2592\n10 DC                279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/challenge2_RoyYoon.html#better-way-of-producing-results-above",
    "href": "posts/challenge2_RoyYoon.html#better-way-of-producing-results-above",
    "title": "Challenge 2 Instructions",
    "section": "Better way of producing results above",
    "text": "Better way of producing results above\n\n\nCode\nrailroad\n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\ntest_railroad <- railroad %>% group_by(state)\n\ntest_railroad\n\n\n# A tibble: 2,930 × 3\n# Groups:   state [53]\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\n#finding the total employees for each state\n\ntest_railroad %>%summarise(\n  total_employees = sum(total_employees)\n)\n\n\n# A tibble: 53 × 2\n   state total_employees\n   <chr>           <dbl>\n 1 AE                  2\n 2 AK                103\n 3 AL               4257\n 4 AP                  1\n 5 AR               3871\n 6 AZ               3153\n 7 CA              13137\n 8 CO               3650\n 9 CT               2592\n10 DC                279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n#trying to find descending order \n\n#test_railroad <- test_railroad %>%summarise(\n  #total_employees = sum(total_employees)\n#)\n\n#test_railroad %>% arrange(desc(total_employees))\n\n\n\n\nCode\n# finding the county(s) for each state that has the most number of employees\ntest_railroad %>% filter(total_employees == max(total_employees))\n\n\n# A tibble: 53 × 3\n# Groups:   state [53]\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    SKAGWAY MUNICIPALITY              88\n 3 AL    JEFFERSON                        990\n 4 AP    APO                                1\n 5 AR    PULASKI                          972\n 6 AZ    PIMA                             749\n 7 CA    SAN BERNARDINO                  2888\n 8 CO    ADAMS                            553\n 9 CT    NEW HAVEN                       1561\n10 DC    WASHINGTON DC                    279\n# … with 43 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\n#finding the county(s) for each state that has the least number of employees. In this case, there are multiple counties from a state that have the same minimum. \ntest_railroad %>% filter(total_employees == min(total_employees))\n\n\n# A tibble: 170 × 3\n# Groups:   state [53]\n   state county   total_employees\n   <chr> <chr>              <dbl>\n 1 AE    APO                    2\n 2 AK    SITKA                  1\n 3 AL    BARBOUR                1\n 4 AL    HENRY                  1\n 5 AP    APO                    1\n 6 AR    NEWTON                 1\n 7 AZ    GREENLEE               3\n 8 CA    MONO                   1\n 9 CO    BENT                   1\n10 CO    CHEYENNE               1\n# … with 160 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nExplain and Interpret\nWork is still in progress.\nThought process:\n\nexamine which state has the most/least ‘total_employees’\nidentify which county has the most/least ‘total_employees’ in the state with the most/least ‘total_employees’\nlook at overall, which county has the most/least ‘total_employees’, and how does that compare to the state values\nexamine the average, min, max across states/counties"
  },
  {
    "objectID": "posts/homework2_ManiShankerKamarapu.html",
    "href": "posts/homework2_ManiShankerKamarapu.html",
    "title": "Homework2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/homework2_ManiShankerKamarapu.html#read-in-data",
    "href": "posts/homework2_ManiShankerKamarapu.html#read-in-data",
    "title": "Homework2",
    "section": "Read in data",
    "text": "Read in data\nThe data set is about the marital status of the active duty DOD service. The data set focuses on four types of services(Air Force, Marine Corps, Navy and Army). The data set gives us the detailed description of marital status of the DOD service people and divided them on basis of gender and four types of statuses(Single without Children(SWC), Single with Children(SC), Joint Service Marriage(JSM) and Civilian Marriage(CM)). The enrolled people are also divided on the basis of pay grade into 3 groups(Enlisted, Officer and Warrant).\n\n\nCode\nAirForce_MaritalStatus <- read_excel(\"_data/ActiveDuty_MaritalStatus.xls\", \n    sheet = \"AirForce\", skip = 9, col_names = c(\"remove\", \"Pay Grade\", \"SWC_Male\", \"SWC_Female\", \"SWC_Total\", \"SC_Male\", \"SC_Female\", \"SC_Total\", \"JSM_Male\", \"JSM_Female\", \"JSM_Total\", \"CM_Male\", \"CM_Female\", \"CM_Total\", \"Male\", \"Female\", \"Total\")) %>%\n  select(\"Pay Grade\", \"SWC_Male\", \"SWC_Female\", \"SC_Male\", \"SC_Female\", \"JSM_Male\", \"JSM_Female\", \"CM_Male\", \"CM_Female\") %>%\n  drop_na(\"Pay Grade\") %>%\n  filter(!grepl(\"TOTAL\", `Pay Grade`)) %>%\n    mutate(DOD_type=\"AirForce\")\n\nMarineCorps_MaritalStatus <- read_excel(\"_data/ActiveDuty_MaritalStatus.xls\", \n    sheet = \"MarineCorps\", skip = 9, col_names = c(\"remove\", \"Pay Grade\", \"SWC_Male\", \"SWC_Female\", \"SWC_Total\", \"SC_Male\", \"SC_Female\", \"SC_Total\", \"JSM_Male\", \"JSM_Female\", \"JSM_Total\", \"CM_Male\", \"CM_Female\", \"CM_Total\", \"M_Male\", \"M_Female\", \"M_Total\")) %>%\n  select(\"Pay Grade\", \"SWC_Male\", \"SWC_Female\", \"SC_Male\", \"SC_Female\", \"JSM_Male\", \"JSM_Female\", \"CM_Male\", \"CM_Female\") %>%\n  drop_na(\"Pay Grade\") %>%\n  filter(!grepl(\"TOTAL\", `Pay Grade`)) %>%\n    mutate(DOD_type=\"MarineCorps\")\n\nNavy_MaritalStatus <- read_excel(\"_data/ActiveDuty_MaritalStatus.xls\", \n    sheet = \"Navy\", skip = 9, col_names = c(\"remove\", \"Pay Grade\", \"SWC_Male\", \"SWC_Female\", \"SWC_Total\", \"SC_Male\", \"SC_Female\", \"SC_Total\", \"JSM_Male\", \"JSM_Female\", \"JSM_Total\", \"CM_Male\", \"CM_Female\", \"CM_Total\", \"N_Male\", \"N_Female\", \"N_Total\")) %>%\n  select(\"Pay Grade\", \"SWC_Male\", \"SWC_Female\", \"SC_Male\", \"SC_Female\", \"JSM_Male\", \"JSM_Female\", \"CM_Male\", \"CM_Female\") %>%\n  drop_na(\"Pay Grade\") %>%\n  filter(!grepl(\"TOTAL\", `Pay Grade`)) %>%\n    mutate(DOD_type=\"Navy\")\n\nArmy_MaritalStatus <- read_excel(\"_data/ActiveDuty_MaritalStatus.xls\", \n    sheet = \"Army\", skip = 9, col_names = c(\"remove\", \"Pay Grade\", \"SWC_Male\", \"SWC_Female\", \"SWC_Total\", \"SC_Male\", \"SC_Female\", \"SC_Total\", \"JSM_Male\", \"JSM_Female\", \"JSM_Total\", \"CM_Male\", \"CM_Female\", \"CM_Total\", \"Army_Male\", \"Army_Female\", \"Army_Total\")) %>%\n  select(\"Pay Grade\", \"SWC_Male\", \"SWC_Female\", \"SC_Male\", \"SC_Female\", \"JSM_Male\", \"JSM_Female\", \"CM_Male\", \"CM_Female\") %>%\n  drop_na(\"Pay Grade\") %>%\n  filter(!grepl(\"TOTAL\", `Pay Grade`)) %>%\n    mutate(DOD_type=\"Army\")\n\nTotalDOD_MaritalStatus <- rbind(AirForce_MaritalStatus,MarineCorps_MaritalStatus, Navy_MaritalStatus, Army_MaritalStatus)\n\nTotalDOD_MaritalStatus\n\n\n# A tibble: 90 × 10\n   `Pay Grade` SWC_Male SWC_Fe…¹ SC_Male SC_Fe…² JSM_M…³ JSM_F…⁴ CM_Male CM_Fe…⁵\n   <chr>          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 E-1             7721     1550      27       5      49      27    1064     178\n 2 E-2             4380     1010      33       9      97     105     802     163\n 3 E-3            29725     7108     396     266    1258    1687   10436    1631\n 4 E-4            20805     4756     987     842    3036    3207   15363    1769\n 5 E-5            14623     4104    2755    2171    6154    5519   31711    2889\n 6 E-6             3660     1377    2446    1449    3654    3263   23868    2026\n 7 E-7             1441      617    1539     734    2118    1419   17290    1188\n 8 E-8              182      139     236     110     505     241    3655     228\n 9 E-9               83       48     106      39     204      73    1975     108\n10 O-1             3831     1068      45      43     182     265    1693     211\n# … with 80 more rows, 1 more variable: DOD_type <chr>, and abbreviated\n#   variable names ¹​SWC_Female, ²​SC_Female, ³​JSM_Male, ⁴​JSM_Female, ⁵​CM_Female\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\nBriefly describe the data\nI have read the data sets and there were many rows empty, so I used the skip to remove first nine rows of data and also removed some columns to reduce redundancy in data set by using select function and then I removed empty rows by drop function and filtered out the values which can calculated if wanted and added a new column using mutate function in regards to combine the data sets and at last combined multiple data sets into one data set using rbind function.\n\n\n\n\n\n\nMutate function\n\n\n\nIt is used to create new columns with the values of old columns.\n\n\n\n\n\n\n\n\nRbind function\n\n\n\nIt is used to select to combine two or more data sets into one data set vertically. Catch is that they should have same columns.\n\n\n\n\nCode\ndim(TotalDOD_MaritalStatus)\n\n\n[1] 90 10\n\n\nThese are the present dimensions of the data set after reading and transforming it, It has 90 rows and 10 columns.\n\n\nCode\nsummary(TotalDOD_MaritalStatus)\n\n\n  Pay Grade            SWC_Male          SWC_Female        SC_Male       \n Length:90          Min.   :    0.00   Min.   :   0.0   Min.   :   0.00  \n Class :character   1st Qu.:   19.25   1st Qu.:   7.0   1st Qu.:  25.25  \n Mode  :character   Median :  442.50   Median : 166.5   Median : 122.50  \n                    Mean   : 5084.57   Mean   : 931.5   Mean   : 571.42  \n                    3rd Qu.: 4434.00   3rd Qu.:1091.2   3rd Qu.: 471.00  \n                    Max.   :50758.00   Max.   :7108.0   Max.   :7010.00  \n   SC_Female          JSM_Male        JSM_Female        CM_Male       \n Min.   :   0.00   Min.   :   0.0   Min.   :   0.0   Min.   :    3.0  \n 1st Qu.:   2.25   1st Qu.:  17.0   1st Qu.:   6.0   1st Qu.:  439.5  \n Median :  42.50   Median : 114.0   Median :  81.0   Median : 2496.5  \n Mean   : 271.91   Mean   : 546.4   Mean   : 507.9   Mean   : 7266.8  \n 3rd Qu.: 191.75   3rd Qu.: 608.0   3rd Qu.: 497.5   3rd Qu.: 9517.8  \n Max.   :2599.00   Max.   :6154.0   Max.   :5519.0   Max.   :58317.0  \n   CM_Female         DOD_type        \n Min.   :   0.00   Length:90         \n 1st Qu.:   9.25   Class :character  \n Median : 154.00   Mode  :character  \n Mean   : 541.13                     \n 3rd Qu.: 533.75                     \n Max.   :6010.00"
  },
  {
    "objectID": "posts/homework2_ManiShankerKamarapu.html#anticipate-the-end-result",
    "href": "posts/homework2_ManiShankerKamarapu.html#anticipate-the-end-result",
    "title": "Homework2",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nAs you might see the data set now there is lot of data which is still untidy, that is I can see that each observation doesn’t have it’s own row and need to be tidy further. First we need decrease the number of columns and use pivot_longer() function to convert the columns into rows so each row can have different and unique observation. And then after that make the “Status” column separate and split it into two columns Status and gender so we can have a separate variable for gender."
  },
  {
    "objectID": "posts/homework2_ManiShankerKamarapu.html#pivot-the-data",
    "href": "posts/homework2_ManiShankerKamarapu.html#pivot-the-data",
    "title": "Homework2",
    "section": "Pivot the Data",
    "text": "Pivot the Data\n\n\nCode\nTotalDOD_MaritalStatus <-  pivot_longer(TotalDOD_MaritalStatus, SWC_Male:CM_Female, names_to = \"Status\", values_to = \"Count\") %>% \n  separate(col = \"Status\", into= c(\"Status\", \"Gender\"), \"_\") %>%\n  arrange(\"Pay Grade\")\n\nTotalDOD_MaritalStatus\n\n\n# A tibble: 720 × 5\n   `Pay Grade` DOD_type Status Gender Count\n   <chr>       <chr>    <chr>  <chr>  <dbl>\n 1 E-1         AirForce SWC    Male    7721\n 2 E-1         AirForce SWC    Female  1550\n 3 E-1         AirForce SC     Male      27\n 4 E-1         AirForce SC     Female     5\n 5 E-1         AirForce JSM    Male      49\n 6 E-1         AirForce JSM    Female    27\n 7 E-1         AirForce CM     Male    1064\n 8 E-1         AirForce CM     Female   178\n 9 E-2         AirForce SWC    Male    4380\n10 E-2         AirForce SWC    Female  1010\n# … with 710 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n\n\n\n\nPivot longer function\n\n\n\nIt is used to convert columns into rows so we can have each observation for each row.\n\n\n\n\n\n\n\n\nSeparate function\n\n\n\nIt is an important function used to split one column into two or more columns by using sep parameter.\n\n\nNow from the table we can see that each variable has it’s own column and each observation has it’s own row, so now the data set is tidy.\n\nDescribe the final dimensions\n\n\nCode\ndim(TotalDOD_MaritalStatus)\n\n\n[1] 720   5\n\n\nNow from the tidy data set, the final dimensions are 720 rows and 5 columns. The column variables are “Pay Grade”, “DOD_type”, “Status”, “Gender” and “Count”. The pay grade represents the level and stats of the people, the DOD_type tells us about the type of service the person is enrolled in and Status gives us the Marital status of the person and Count represents the count of people enrolled in different services based on their marital status and distinguishing by their pay grade."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge3.html",
    "href": "posts/SaaradhaaM_Challenge3.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge3.html#reading-in-data",
    "href": "posts/SaaradhaaM_Challenge3.html#reading-in-data",
    "title": "Challenge 3",
    "section": "Reading in data",
    "text": "Reading in data\nI will be working with the households dataset.\n\n\nCode\n# Reading in data.\nhouseholds <-read_excel(\"_data/USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\", skip=4)\nhouseholds\n\n\n# A tibble: 383 × 16\n   ...1      ...2  Total Under…¹ $15,0…² $25,0…³ $35,0…⁴ $50,0…⁵ $75,0…⁶ $100,…⁷\n   <chr>     <chr> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 ALL RACES <NA>     NA    NA      NA      NA      NA      NA      NA      NA  \n 2 2019      1284…   100     9.1     8       8.3    11.7    16.5    12.3    15.5\n 3 2018      1285…   100    10.1     8.8     8.7    12      17      12.5    15  \n 4 2017 2    1276…   100    10       9.1     9.2    12      16.4    12.4    14.7\n 5 2017      1275…   100    10.1     9.1     9.2    11.9    16.3    12.6    14.8\n 6 2016      1262…   100    10.4     9       9.2    12.3    16.7    12.2    15  \n 7 2015      1258…   100    10.6    10       9.6    12.1    16.1    12.4    14.9\n 8 2014      1245…   100    11.4    10.5     9.6    12.6    16.4    12.1    14  \n 9 2013 3    1239…   100    11.4    10.3     9.5    12.5    16.8    12      13.9\n10 2013 4    1229…   100    11.3    10.4     9.7    13.1    17      12.5    13.6\n# … with 373 more rows, 6 more variables: `$150,000\\r\\nto\\r\\n$199,999` <dbl>,\n#   `$200,000 and over` <dbl>, Estimate...13 <dbl>,\n#   `Margin of error1 (±)...14` <dbl>, Estimate...15 <chr>,\n#   `Margin of error1 (±)...16` <chr>, and abbreviated variable names\n#   ¹​`Under $15,000`, ²​`$15,000\\r\\nto\\r\\n$24,999`, ³​`$25,000\\r\\nto\\r\\n$34,999`,\n#   ⁴​`$35,000\\r\\nto\\r\\n$49,999`, ⁵​`$50,000\\r\\nto\\r\\n$74,999`,\n#   ⁶​`$75,000\\r\\nto\\r\\n$99,999`, ⁷​`$100,000\\r\\nto\\r\\n$149,999`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\nBrief description of data\nWhen reading in the data, I skipped the first four rows (they just describe the dataset). The dataset segments households by their income levels, race and Hispanic origin of householder from 1967 to 2019. It also has an external link to https://www2.census.gov/programs-surveys/cps/techdocs/cpsmar20.pdf, which shows that the data is part of the Annual Social and Economic Supplement in the Census. I need to re-name some headers, remove “Total” (redundant column) and remove rows 353 to 383 (they’re just notes).\n\n\nCode\n# Rename column headers.\ncolnames(households)\n\n\n [1] \"...1\"                       \"...2\"                      \n [3] \"Total\"                      \"Under $15,000\"             \n [5] \"$15,000\\r\\nto\\r\\n$24,999\"   \"$25,000\\r\\nto\\r\\n$34,999\"  \n [7] \"$35,000\\r\\nto\\r\\n$49,999\"   \"$50,000\\r\\nto\\r\\n$74,999\"  \n [9] \"$75,000\\r\\nto\\r\\n$99,999\"   \"$100,000\\r\\nto\\r\\n$149,999\"\n[11] \"$150,000\\r\\nto\\r\\n$199,999\" \"$200,000 and over\"         \n[13] \"Estimate...13\"              \"Margin of error1 (±)...14\" \n[15] \"Estimate...15\"              \"Margin of error1 (±)...16\" \n\n\nCode\nhouseholds <- rename(households, \"year\" = \"...1\", \"num_thousands\" = \"...2\", \"estimated_median_income\" = \"Estimate...13\", \"median_moe\" = \"Margin of error1 (±)...14\", \"estimated_mean_income\" = \"Estimate...15\", \"mean_moe\" = \"Margin of error1 (±)...16\")\n# Remove \"Total\" column.\nhouseholds <- households[,-3]\n# Remove rows 353-383.\nhouseholds <- households[-c(353:383),]\n# Which rows have missing values? This tells me how many rows of races there are.\nwhich(rowSums(is.na(households))>0)\n\n\n [1]   1  57  78 114 135 166 187 208 244 265 286 302"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge3.html#anticipate-end-result-and-find-current-and-future-data-dimensions.",
    "href": "posts/SaaradhaaM_Challenge3.html#anticipate-end-result-and-find-current-and-future-data-dimensions.",
    "title": "Challenge 3",
    "section": "Anticipate end result and find current and future data dimensions.",
    "text": "Anticipate end result and find current and future data dimensions.\nNow the dataset is a lot cleaner. We can see that in the “year” column, there are rows of races (N = 12). Race should actually be entered as a separate column, but I don’t know how to select specific rows in the “year” column to create a new column. I’ll remove those rows just for the purposes of working through this exercise.\n\n\nCode\n# Remove race rows.\nhouseholds_new <- households[-c(1,57,78,114,135,166,187,208,244,265,286,302),]\ndim(households_new)\n\n\n[1] 340  15\n\n\nThe current dimensions are 340 rows and 15 columns. I would like to shift all the income categories into an “income” column, so this should give me a lot more rows and 8 columns.\n\nPivoting dataset\n\n\nCode\n# Attempt pivotlonger().\nhouseholds_new <- pivot_longer(households_new, cols = contains(\"$\"), names_to = \"income\", values_to = \"proportion\")\nhouseholds_new\n\n\n# A tibble: 3,060 × 8\n   year  num_thousands estimated_median…¹ media…² estim…³ mean_…⁴ income propo…⁵\n   <chr> <chr>                      <dbl>   <dbl> <chr>   <chr>   <chr>    <dbl>\n 1 2019  128451                     68703     904 98088   1042    \"Unde…     9.1\n 2 2019  128451                     68703     904 98088   1042    \"$15,…     8  \n 3 2019  128451                     68703     904 98088   1042    \"$25,…     8.3\n 4 2019  128451                     68703     904 98088   1042    \"$35,…    11.7\n 5 2019  128451                     68703     904 98088   1042    \"$50,…    16.5\n 6 2019  128451                     68703     904 98088   1042    \"$75,…    12.3\n 7 2019  128451                     68703     904 98088   1042    \"$100…    15.5\n 8 2019  128451                     68703     904 98088   1042    \"$150…     8.3\n 9 2019  128451                     68703     904 98088   1042    \"$200…    10.3\n10 2018  128579                     64324     704 91652   914     \"Unde…    10.1\n# … with 3,050 more rows, and abbreviated variable names\n#   ¹​estimated_median_income, ²​median_moe, ³​estimated_mean_income, ⁴​mean_moe,\n#   ⁵​proportion\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow we suddenly have >3000 rows. This is because the columns estimated_median_income, median_moe, estimated_mean_income and mean_moe are the same for each year (regardless of income bracket, which we’ve just pivoted into a new column). So I’m going to split the data into two tables to make it easier to understand.\n\n\nCode\n# Creating table 1 by removing appropriate columns.\nhouseholds_1 <- households_new[,-c(3:6)]\n\n# Changing num_thousands to numeric so that the next argument runs properly.\nhouseholds_1$num_thousands <- as.numeric(households_1$num_thousands)\n\n# Merging 2 columns into 1.\nhouseholds_1 <- households_1 %>% mutate(count_thousands = `num_thousands`*(`proportion`/100))\n\n# Removing the 2 old columns.\nhouseholds_1 <- households_1[,-c(2,4)]\n\n\n\n\nCode\n# Creating table 2 by removing appropriate columns.\nhouseholds_2 <- households_new[,-c(2, 7:8)]\n\n# Remove duplicate rows in table 2.\nhouseholds_2 %>% distinct()\n\n\n# A tibble: 340 × 5\n   year   estimated_median_income median_moe estimated_mean_income mean_moe\n   <chr>                    <dbl>      <dbl> <chr>                 <chr>   \n 1 2019                     68703        904 98088                 1042    \n 2 2018                     64324        704 91652                 914     \n 3 2017 2                   63761        552 91406                 979     \n 4 2017                     64007        575 89922                 892     \n 5 2016                     62898        764 88578                 822     \n 6 2015                     60987        570 85533                 715     \n 7 2014                     58001        697 81870                 793     \n 8 2013 3                   58904       1183 82660                 1201    \n 9 2013 4                   57095        499 79852                 902     \n10 2012                     56912        384 79510                 773     \n# … with 330 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow we have 2 tables that are relatively easier to comprehend than what we had at the start. This is a work in progress - I want to figure out how to add the race column, and also to interpret the tables I’ve created."
  },
  {
    "objectID": "posts/challenge2_nickboonstra.html",
    "href": "posts/challenge2_nickboonstra.html",
    "title": "Nick Boonstra Challenge 2",
    "section": "",
    "text": "For today’s challenge, I will be reading in and wrangling data from the “hotel_bookings” dataset.\n\n\nFirst, my R setup chunk:\n\n\nCode\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_nickboonstra.html#reading-in-the-data",
    "href": "posts/challenge2_nickboonstra.html#reading-in-the-data",
    "title": "Nick Boonstra Challenge 2",
    "section": "Reading in the data",
    "text": "Reading in the data\nReading in the data was a fairly straightforward process:\n\n\nCode\nhotels<-read_csv(\"_data/hotel_bookings.csv\")\nhotels\n\n\n# A tibble: 119,390 × 32\n   hotel  is_ca…¹ lead_…² arriv…³ arriv…⁴ arriv…⁵ arriv…⁶ stays…⁷ stays…⁸ adults\n   <chr>    <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 Resor…       0     342    2015 July         27       1       0       0      2\n 2 Resor…       0     737    2015 July         27       1       0       0      2\n 3 Resor…       0       7    2015 July         27       1       0       1      1\n 4 Resor…       0      13    2015 July         27       1       0       1      1\n 5 Resor…       0      14    2015 July         27       1       0       2      2\n 6 Resor…       0      14    2015 July         27       1       0       2      2\n 7 Resor…       0       0    2015 July         27       1       0       2      2\n 8 Resor…       0       9    2015 July         27       1       0       2      2\n 9 Resor…       1      85    2015 July         27       1       0       3      2\n10 Resor…       1      75    2015 July         27       1       0       3      2\n# … with 119,380 more rows, 22 more variables: children <dbl>, babies <dbl>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, booking_changes <dbl>,\n#   deposit_type <chr>, agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\nTransformations\nAfter reading in the data, I made a few transformations:\n\n\nCode\nhotels<-hotels %>% \n  rename(is_cancelled = is_canceled) %>%  ## i'm petty\n  mutate(booking_dummy = case_when( ## dummy var for whether or not changes were made\n    booking_changes == 0 ~ 0,\n    T ~ 1\n  )) %>% \n  mutate(arrival_date_month_num = case_when( ## numerical variable for months\n    arrival_date_month == \"January\" ~ 1,\n    arrival_date_month == \"February\" ~ 2,\n    arrival_date_month == \"March\" ~ 3,\n    arrival_date_month == \"April\" ~ 4,\n    arrival_date_month == \"May\" ~ 5,\n    arrival_date_month == \"June\" ~ 6,\n    arrival_date_month == \"July\" ~ 7,\n    arrival_date_month == \"August\" ~ 8,\n    arrival_date_month == \"September\" ~ 9,\n    arrival_date_month == \"October\" ~ 10,\n    arrival_date_month == \"November\" ~ 11,\n    arrival_date_month == \"December\" ~ 12\n  ))\n\n\nI found that this data set only required fairly minimal/minor transformations. Firstly, I renamed the “is_canceled” variable to “is_cancelled” primarily because I’m petty, and I knew I would want to spell it with the double “L” the whole time. Next, I created a dummy variable called “booking_dummy” for whether or not any changes were made to a booking, regardless of how many such changes there were. Lastly, I created a variable named “arrival_date_month_num” to assign the corresponding number to each month as named in the “arrival_date_month” column."
  },
  {
    "objectID": "posts/challenge2_nickboonstra.html#describing-the-data",
    "href": "posts/challenge2_nickboonstra.html#describing-the-data",
    "title": "Nick Boonstra Challenge 2",
    "section": "Describing the data",
    "text": "Describing the data\nBefore I started summarising, I wanted to get a sense of what the data “looked like,” so to speak:\n\n\nCode\nnames(hotels)\n\n\n [1] \"hotel\"                          \"is_cancelled\"                  \n [3] \"lead_time\"                      \"arrival_date_year\"             \n [5] \"arrival_date_month\"             \"arrival_date_week_number\"      \n [7] \"arrival_date_day_of_month\"      \"stays_in_weekend_nights\"       \n [9] \"stays_in_week_nights\"           \"adults\"                        \n[11] \"children\"                       \"babies\"                        \n[13] \"meal\"                           \"country\"                       \n[15] \"market_segment\"                 \"distribution_channel\"          \n[17] \"is_repeated_guest\"              \"previous_cancellations\"        \n[19] \"previous_bookings_not_canceled\" \"reserved_room_type\"            \n[21] \"assigned_room_type\"             \"booking_changes\"               \n[23] \"deposit_type\"                   \"agent\"                         \n[25] \"company\"                        \"days_in_waiting_list\"          \n[27] \"customer_type\"                  \"adr\"                           \n[29] \"required_car_parking_spaces\"    \"total_of_special_requests\"     \n[31] \"reservation_status\"             \"reservation_status_date\"       \n[33] \"booking_dummy\"                  \"arrival_date_month_num\"        \n\n\nCode\ncount(hotels)\n\n\n# A tibble: 1 × 1\n       n\n   <int>\n1 119390\n\n\nCode\ncount(hotels,hotel)\n\n\n# A tibble: 2 × 2\n  hotel            n\n  <chr>        <int>\n1 City Hotel   79330\n2 Resort Hotel 40060\n\n\nCode\ncount(hotels,country)\n\n\n# A tibble: 178 × 2\n   country     n\n   <chr>   <int>\n 1 ABW         2\n 2 AGO       362\n 3 AIA         1\n 4 ALB        12\n 5 AND         7\n 6 ARE        51\n 7 ARG       214\n 8 ARM         8\n 9 ASM         1\n10 ATA         2\n# … with 168 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\narrivals<-xtabs(~arrival_date_year+arrival_date_month_num,hotels)\narrivals\n\n\n                 arrival_date_month_num\narrival_date_year    1    2    3    4    5    6    7    8    9   10   11   12\n             2015    0    0    0    0    0    0 2776 3889 5114 4957 2340 2920\n             2016 2248 3891 4824 5428 5478 5292 4572 5063 5394 6203 4454 3860\n             2017 3681 4177 4970 5661 6313 5647 5313 4925    0    0    0    0\n\n\nThis data set appears to describe hotel bookings from across a number of different countries and kinds of hotels, in the time range between July 2015 and August 2018. Each observation appears to be a single booking, with a range of information about each booking tracked in each column."
  },
  {
    "objectID": "posts/challenge2_nickboonstra.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_nickboonstra.html#provide-grouped-summary-statistics",
    "title": "Nick Boonstra Challenge 2",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nAs I looked through the data, I found myself gravitating toward the information about booking changes and cancellations.\n\n\nCode\nhotels %>% \n  group_by(is_cancelled) %>% \n  summarise(mean(booking_changes,na.rm=T))\n\n\n# A tibble: 2 × 2\n  is_cancelled `mean(booking_changes, na.rm = T)`\n         <dbl>                              <dbl>\n1            0                             0.293 \n2            1                             0.0983\n\n\nCode\nhotels %>% \n  group_by(is_cancelled) %>%\n  summarise(median(booking_changes))\n\n\n# A tibble: 2 × 2\n  is_cancelled `median(booking_changes)`\n         <dbl>                     <dbl>\n1            0                         0\n2            1                         0\n\n\nClearly, most bookings did not have any changes to their booking, demonstrated by the fact that the median number of changes for both cancelled and non-cancelled bookings was 0. However, it is interesting to observe that non-cancelled bookings tended to have more booking changes performed, suggesting that making changes to a booking may have increased the likelihood of that booking not having to be cancelled – a boon for hotels and travel agencies, if that extra bit of work is all it takes to retain a customer.\n\nVisualizing and Interpreting\nThese observations can be seen much more clearly in a graphic visualization:\n\n\nCode\nggplot(hotels,aes(x=factor(booking_dummy),fill=factor(is_cancelled))) +\n  geom_bar() +\n  theme_bw() +\n  labs(title = \"Cancellations by changes in booking\", x = \"Change in booking?\", y= \"Number of bookings\",\n       fill = \"Booking cancelled?\")\n\n\n\n\n\nThis bar graph utilizes the “booking_dummy” variable, easily dividing all bookings into those that had changes performed and those that didn’t. As can be seen, there were more bookings cancelled without changes being made than there were total bookings with changes! Additionally, a much smaller proportion of bookings with changes ended up being cancelled when compared to bookings without any changes made (though cancellations accounted for less than half of each group in the end). Of course, it is hard to make broad generalizations without knowing more of the story told by this data. However, on the face of things it looks as though a case could be made for flexibility with regards to changing bookings as a strong protection against customers cancelling reservations."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html",
    "href": "posts/SaaradhaaM_Challenge2.html",
    "title": "Challenge 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html#reading-in-the-data",
    "href": "posts/SaaradhaaM_Challenge2.html#reading-in-the-data",
    "title": "Challenge 2",
    "section": "Reading in the data",
    "text": "Reading in the data\n\n\nCode\n# Read in and view the dataset.\nhotel <- read.csv(\"_data/hotel_bookings.csv\")\nhotel"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html#description-of-data",
    "href": "posts/SaaradhaaM_Challenge2.html#description-of-data",
    "title": "Challenge 2",
    "section": "Description of data",
    "text": "Description of data\n\n\nCode\n# Get rows and columns.\ndim(hotel)\n\n\n[1] 119390     32\n\n\nCode\n# Find which columns have missing data.\nwhich(colSums(is.na(hotel))>0)\n\n\nchildren \n      11 \n\n\nIn the hotel bookings dataset, there are 119390 cases and 32 columns. Only the children column has missing data (N = 11). Interesting columns include assigned room type, previous cancellations, days in waiting list and is_canceled. There are also columns for country and hotel, indicating that the data was likely gathered by surveying different hotels around the world."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html#grouped-summary-statistics-1",
    "href": "posts/SaaradhaaM_Challenge2.html#grouped-summary-statistics-1",
    "title": "Challenge 2",
    "section": "Grouped summary statistics #1",
    "text": "Grouped summary statistics #1\nI first want to examine the relationship between number of days in the waiting list and whether the booking was cancelled.\n\n\nCode\n# Check if is_canceled is binary.\napply(hotel,2,function(x) { all(x %in% 0:1) })\n\n\n                         hotel                    is_canceled \n                         FALSE                           TRUE \n                     lead_time              arrival_date_year \n                         FALSE                          FALSE \n            arrival_date_month       arrival_date_week_number \n                         FALSE                          FALSE \n     arrival_date_day_of_month        stays_in_weekend_nights \n                         FALSE                          FALSE \n          stays_in_week_nights                         adults \n                         FALSE                          FALSE \n                      children                         babies \n                         FALSE                          FALSE \n                          meal                        country \n                         FALSE                          FALSE \n                market_segment           distribution_channel \n                         FALSE                          FALSE \n             is_repeated_guest         previous_cancellations \n                          TRUE                          FALSE \nprevious_bookings_not_canceled             reserved_room_type \n                         FALSE                          FALSE \n            assigned_room_type                booking_changes \n                         FALSE                          FALSE \n                  deposit_type                          agent \n                         FALSE                          FALSE \n                       company           days_in_waiting_list \n                         FALSE                          FALSE \n                 customer_type                            adr \n                         FALSE                          FALSE \n   required_car_parking_spaces      total_of_special_requests \n                         FALSE                          FALSE \n            reservation_status        reservation_status_date \n                         FALSE                          FALSE \n\n\nCode\n# Check mean and median for days in waiting list.\nsummarise(hotel, diwl_mean = mean(days_in_waiting_list), diwl_median = median(days_in_waiting_list))\n\n\n\n\n  \n\n\n\nCode\n# Check mean of is_canceled, grouped by days in waiting list.\nhotel %>%\n  group_by(days_in_waiting_list) %>%\n  select(`is_canceled`) %>%\nsummarise(is_canceled_mean = mean(is_canceled))\n\n\n\n\n  \n\n\n\nCommon sense tells me that those who wait longer are more likely to cancel their reservations, but I want to check if this can actually be observed in our data. The code chunk above demonstrates that is_canceled is a binary variable (with values 0 and 1). The tables generated also show that on average, people spend about 2 days on the waiting list. However, some people were on the waiting list for over a year!\nThe mean cancellation rate is 1 for those who waited 391 days (understandable), and 0.36 for those who didn’t wait at all."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge2.html#grouped-summary-statistics-1-1",
    "href": "posts/SaaradhaaM_Challenge2.html#grouped-summary-statistics-1-1",
    "title": "Challenge 2",
    "section": "Grouped summary statistics #1",
    "text": "Grouped summary statistics #1\nI also want to examine the relationship between assigned room type and previous cancellations.\n\n\nCode\n# Check median previous cancellations, grouped by assigned room type.\nhotel %>%\n  group_by(assigned_room_type) %>%\n  select(previous_cancellations) %>%\nsummarise(previous_cancellations_median = median(previous_cancellations))\n\n\n\n\n  \n\n\n\nMost people assigned types A to K and type P were not likely to have previously cancelled their bookings. However, most people assigned type L were like to have previously cancelled their bookings on one occasion - if we can find the data source, it would be interesting to uncover how type L differs from the other room types (were these people given smaller rooms?)."
  },
  {
    "objectID": "posts/challenge4_solutions.html",
    "href": "posts/challenge4_solutions.html",
    "title": "Challenge 4 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_solutions.html#challenge-overview",
    "href": "posts/challenge4_solutions.html#challenge-overview",
    "title": "Challenge 4 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations\n\nTwo tidyverse packages will be used heavily today: lubridate (which is not automatically loaded) and stringr (which is part of core tidyverse).\n\nABC Poll ⭐Eggs ⭐⭐Fed Rates ⭐⭐⭐Hotel Bookings ⭐⭐⭐⭐Debt ⭐⭐⭐⭐⭐\n\n\n\nabc_poll_orig<-read_csv(\"_data/abc_poll_2021.csv\")\n\n# political questions\nabc_poll_orig%>%\n  select(starts_with(\"Q\"))%>%\n  colnames(.)\n\n [1] \"Q1_a\" \"Q1_b\" \"Q1_c\" \"Q1_d\" \"Q1_e\" \"Q1_f\" \"Q2\"   \"Q3\"   \"Q4\"   \"Q5\"  \n[11] \"QPID\"\n\n# all but one demographer\nabc_poll_orig%>%\n  select(starts_with(\"pp\"))%>%\n  colnames(.)\n\n [1] \"ppage\"    \"ppeduc5\"  \"ppeducat\" \"ppgender\" \"ppethm\"   \"pphhsize\"\n [7] \"ppinc7\"   \"ppmarit5\" \"ppmsacat\" \"ppreg4\"   \"pprent\"   \"ppstaten\"\n[13] \"PPWORKA\"  \"ppemploy\"\n\n# national poll\nn_distinct(abc_poll_orig$ppstaten)\n\n[1] 49\n\n\nThe ABC Poll appears to be a national sample survey (presumably from 2019) with 527 respondents. There are 10 political attitudes questions, plus party identification, in addition to 15 demographic variables (some with re-coded information) and 5 survey administration variables.\n\nprint(summarytools::dfSummary(abc_poll_orig,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nabc_poll_orig\nDimensions: 527 x 31\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      id\n[numeric]\n      Mean (sd) : 7230264 (152.3)min ≤ med ≤ max:7230001 ≤ 7230264 ≤ 7230527IQR (CV) : 263 (0)\n      527 distinct values\n      \n      0\n(0.0%)\n    \n    \n      xspanish\n[character]\n      1. English2. Spanish\n      514(97.5%)13(2.5%)\n      \n      0\n(0.0%)\n    \n    \n      complete_status\n[character]\n      1. qualified\n      527(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      ppage\n[numeric]\n      Mean (sd) : 53.4 (17.1)min ≤ med ≤ max:18 ≤ 55 ≤ 91IQR (CV) : 27 (0.3)\n      72 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ppeduc5\n[character]\n      1. NA2. High school graduate (hig3. NA4. No high school diploma or5. Some college or Associate\n      108(20.5%)133(25.2%)99(18.8%)29(5.5%)158(30.0%)\n      \n      0\n(0.0%)\n    \n    \n      ppeducat\n[character]\n      1. Bachelors degree or highe2. High school3. Less than high school4. Some college\n      207(39.3%)133(25.2%)29(5.5%)158(30.0%)\n      \n      0\n(0.0%)\n    \n    \n      ppgender\n[character]\n      1. Female2. Male\n      254(48.2%)273(51.8%)\n      \n      0\n(0.0%)\n    \n    \n      ppethm\n[character]\n      1. 2+ Races, Non-Hispanic2. Black, Non-Hispanic3. Hispanic4. Other, Non-Hispanic5. White, Non-Hispanic\n      21(4.0%)27(5.1%)51(9.7%)24(4.6%)404(76.7%)\n      \n      0\n(0.0%)\n    \n    \n      pphhsize\n[character]\n      1. 12. 23. 34. 45. 56. 6 or more\n      80(15.2%)219(41.6%)102(19.4%)76(14.4%)35(6.6%)15(2.8%)\n      \n      0\n(0.0%)\n    \n    \n      ppinc7\n[character]\n      1. $10,000 to $24,9992. $100,000 to $149,9993. $150,000 or more4. $25,000 to $49,9995. $50,000 to $74,9996. $75,000 to $99,9997. Less than $10,000\n      32(6.1%)105(19.9%)137(26.0%)82(15.6%)85(16.1%)69(13.1%)17(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      ppmarit5\n[character]\n      1. Divorced2. Never married3. Now Married4. Separated5. Widowed\n      43(8.2%)111(21.1%)337(63.9%)8(1.5%)28(5.3%)\n      \n      0\n(0.0%)\n    \n    \n      ppmsacat\n[character]\n      1. Metro area2. Non-metro area\n      448(85.0%)79(15.0%)\n      \n      0\n(0.0%)\n    \n    \n      ppreg4\n[character]\n      1. MidWest2. NorthEast3. South4. West\n      118(22.4%)93(17.6%)190(36.1%)126(23.9%)\n      \n      0\n(0.0%)\n    \n    \n      pprent\n[character]\n      1. Occupied without payment 2. Owned or being bought by 3. Rented for cash\n      10(1.9%)406(77.0%)111(21.1%)\n      \n      0\n(0.0%)\n    \n    \n      ppstaten\n[character]\n      1. California2. Texas3. Florida4. Pennsylvania5. Illinois6. New Jersey7. Ohio8. Michigan9. New York10. Washington[ 39 others ]\n      51(9.7%)42(8.0%)34(6.5%)28(5.3%)23(4.4%)21(4.0%)21(4.0%)18(3.4%)18(3.4%)18(3.4%)253(48.0%)\n      \n      0\n(0.0%)\n    \n    \n      PPWORKA\n[character]\n      1. Currently laid off2. Employed full-time (by so3. Employed part-time (by so4. Full Time Student5. Homemaker6. On furlough7. Other8. Retired9. Self-employed\n      13(2.5%)220(41.7%)31(5.9%)8(1.5%)37(7.0%)1(0.2%)20(3.8%)165(31.3%)32(6.1%)\n      \n      0\n(0.0%)\n    \n    \n      ppemploy\n[character]\n      1. Not working2. Working full-time3. Working part-time\n      221(41.9%)245(46.5%)61(11.6%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_a\n[character]\n      1. Approve2. Disapprove3. Skipped\n      329(62.4%)193(36.6%)5(0.9%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_b\n[character]\n      1. Approve2. Disapprove3. Skipped\n      192(36.4%)322(61.1%)13(2.5%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_c\n[character]\n      1. Approve2. Disapprove3. Skipped\n      272(51.6%)248(47.1%)7(1.3%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_d\n[character]\n      1. Approve2. Disapprove3. Skipped\n      192(36.4%)321(60.9%)14(2.7%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_e\n[character]\n      1. Approve2. Disapprove3. Skipped\n      212(40.2%)301(57.1%)14(2.7%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_f\n[character]\n      1. Approve2. Disapprove3. Skipped\n      281(53.3%)230(43.6%)16(3.0%)\n      \n      0\n(0.0%)\n    \n    \n      Q2\n[character]\n      1. Not concerned at all2. Not so concerned3. Somewhat concerned4. Very concerned\n      65(12.3%)147(27.9%)221(41.9%)94(17.8%)\n      \n      0\n(0.0%)\n    \n    \n      Q3\n[character]\n      1. No2. Skipped3. Yes\n      107(20.3%)5(0.9%)415(78.7%)\n      \n      0\n(0.0%)\n    \n    \n      Q4\n[character]\n      1. Excellent2. Good3. Not so good4. Poor5. Skipped\n      60(11.4%)215(40.8%)97(18.4%)149(28.3%)6(1.1%)\n      \n      0\n(0.0%)\n    \n    \n      Q5\n[character]\n      1. Optimistic2. Pessimistic3. Skipped\n      229(43.5%)295(56.0%)3(0.6%)\n      \n      0\n(0.0%)\n    \n    \n      QPID\n[character]\n      1. A Democrat2. A Republican3. An Independent4. Skipped5. Something else\n      176(33.4%)152(28.8%)168(31.9%)3(0.6%)28(5.3%)\n      \n      0\n(0.0%)\n    \n    \n      ABCAGE\n[character]\n      1. 18-292. 30-493. 50-644. 65+\n      60(11.4%)148(28.1%)157(29.8%)162(30.7%)\n      \n      0\n(0.0%)\n    \n    \n      Contact\n[character]\n      1. No, I am not willing to b2. Yes, I am willing to be i\n      355(67.4%)172(32.6%)\n      \n      0\n(0.0%)\n    \n    \n      weights_pid\n[numeric]\n      Mean (sd) : 1 (0.6)min ≤ med ≤ max:0.3 ≤ 0.8 ≤ 6.3IQR (CV) : 0.5 (0.6)\n      453 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-22\n\n\n\n\nMutate PartyID\n\n\nThere are lots of string variables that might need to be modified for analysis or visualization. For example, the party id variable has “A Democrat” not the more standard language. Plus, there is a response “skipped” that should be treated as missing data. Lets see if we can fix it.\n\n#starting point\ntable(abc_poll_orig$QPID)\n\n\n    A Democrat   A Republican An Independent        Skipped Something else \n           176            152            168              3             28 \n\n#mutate\nabc_poll<-abc_poll_orig%>%\n  mutate(partyid = str_remove(QPID, \"A \"),\n         partyid = na_if(partyid, \"Skipped\"))%>%\n  select(-QPID)\n\n#sanity check\ntable(abc_poll$partyid)\n\n\nAn Independent       Democrat     Republican Something else \n           168            176            152             28 \n\n\n\nEthnic Identity\nThe ethnic identity variable is long and could be tough to include in graphs, lets see if we can modify it - but we would need to include a table note to explain what the data labels mean (e.g., that racial labels mean non-hispanic, and that hispanic responses don’t indicate race.)\n\n#starting point\ntable(abc_poll$ppethm)\n\n\n2+ Races, Non-Hispanic    Black, Non-Hispanic               Hispanic \n                    21                     27                     51 \n   Other, Non-Hispanic    White, Non-Hispanic \n                    24                    404 \n\n#mutate\nabc_poll<-abc_poll%>%\n  mutate(ethnic = str_remove(ppethm, \", Non-Hispanic\"))%>%\n  select(-ppethm)\n\n#sanity check\ntable(abc_poll$ethnic)\n\n\n2+ Races    Black Hispanic    Other    White \n      21       27       51       24      404 \n\n\n\n\nRemoving “Skipped”\nWhat about the political variables that all have “Skipped” - a value that should probably be replaced with NA for analysis. Lets use the across function to make this easier.\n\nabc_poll<-abc_poll%>%\n  mutate(across(starts_with(\"Q\"), ~ na_if(.x, \"Skipped\")))\n\nmap(select(abc_poll, starts_with(\"Q1\")), table)\n\n$Q1_a\n\n   Approve Disapprove \n       329        193 \n\n$Q1_b\n\n   Approve Disapprove \n       192        322 \n\n$Q1_c\n\n   Approve Disapprove \n       272        248 \n\n$Q1_d\n\n   Approve Disapprove \n       192        321 \n\n$Q1_e\n\n   Approve Disapprove \n       212        301 \n\n$Q1_f\n\n   Approve Disapprove \n       281        230 \n\n\n\n\nFactor order\nFinally, what if you would like the categories of your variable to appear in a specific order, like the education variable that is currently in alphabetical order?\n\n\n\n\n\n\nfactor()\n\n\n\nThe factor variable type links variable labels to an underlying numeric order, and allows you to maintain the specified order for tables and graphics. Character strings always appear in alphabetical order.\n\n\n\ntable(abc_poll$ppeducat)\n\n\nBachelors degree or higher                High school \n                       207                        133 \n     Less than high school               Some college \n                        29                        158 \n\nedulabs <- unique(abc_poll$ppeducat)\nedulabs\n\n[1] \"High school\"                \"Bachelors degree or higher\"\n[3] \"Some college\"               \"Less than high school\"     \n\nabc_poll<-abc_poll%>%\n  mutate(educ = factor(ppeducat, \n                       labels=edulabs[c(4,1,3,2)]))%>%\n  select(-ppeducat)\nrm(edulabs)\n\ntable(abc_poll$educ)\n\n\n     Less than high school                High school \n                       207                        133 \n              Some college Bachelors degree or higher \n                        29                        158 \n\n\n\n\n\n\n\n\nThis section builds on the code available in the solution to Challenge 3, where we pivoted the organic eggs pricing data. The data reports the average price per carton paid to the farmer or producer for organic eggs (and organic chicken), reported monthly from 2004 to 20013. Average price is reported by carton type, which can vary in both size (x-large or large) and quantity (half-dozen or dozen.)\n\nRead Data\nWe are reading in half of the data from this workbook - the other half contains information about the price of organic chicken.\n\neggs_orig<-read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dozen\",\n                               \"xlarge_halfdozen\", \"large_dozen\",\n                               \"large_halfdozen\")\n                 )\n\n\n\nClean and Mutate\nWe are going to be removing the note from the first column of the data, and splitting the year and month, and pivoting into long format prior to transforming the year and month columns into a date.\n\neggs<-eggs_orig%>%\n  mutate(date = str_remove(date, \" /1\"))%>%\n  separate(date, into=c(\"month\", \"year\"), sep=\" \")%>%\n  fill(year) %>%\n  pivot_longer(cols=contains(\"large\"),\n               names_to = c(\"size\", \"quantity\"),\n               names_sep=\"_\",\n               values_to = \"price\")\n\nNow, we need to create a date from a month and year. I can see that the months are a mix of long month name and 3 character month (for January), and the years are four digit years. Do I need to adjust the string for month manually, or can lubridate fix things for me?\nI’m going to combine the month with the now complete year column, and the parse the “month-year” format using my().\n\neggs<-eggs%>%\n  mutate(date = str_c(month, year, sep=\" \"),\n         date = my(date))\n\nselect(eggs, month, year, date)\n\n\n\n  \n\n\n\nInteresting - lubridate automatically fills in the first day of the month. Maybe we would prefer the last day, or even the middle of the month?\n\neggs<-eggs%>%\n  mutate(date = str_c(month, \"15\", year, sep=\"-\"),\n         date = mdy(date))\n\nselect(eggs, month, year, date)\n\n\n\n  \n\n\n\n\n\n\nThis data set runs from July 1954 to March 2017, and includes daily macroeconomic indicators related to the effective federal funds rate - or the interest rate at which banks lend money to each other in order to meet mandated reserve requirements.\nA single case is a year-month-day, and there are 7 values that can be pivoted or not depending on the needs of the analyst. 4 values are related to the federal funds rate: target, upper target, lower target, and effective), while 3 are related macroeconomic indicators (inflation, \\(\\bigtriangleup\\) GDP, and unemployment rate.)\nFor now, lets just focus on mutating the date.\n\nfed_rates_orig<-read_csv(\"_data/FedFundsRate.csv\")\n\nfed_rates_orig\n\n\n\n  \n\n\n\nOnce again, it looks like we will need to combine the year, month and date using stringr::str_c(), then we can use lubridate to transform into a date.\n\nfed_rates<-fed_rates_orig%>%\n  mutate(date = str_c(Year, Month, Day, sep=\"-\"),\n         date = ymd(date))\n\nsummary(fed_rates$date)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"1954-07-01\" \"1973-04-23\" \"1987-12-16\" \"1987-02-25\" \"2001-06-07\" \"2017-03-16\" \n\n\n\n\n\n\n\n\nGoing Further\n\n\n\nYou can now go through and figure out whether there are patterns in the missing-ness of specific indicators by date (maybe the values are only measured once a month or once a quarter, and we need to use fill(), or maybe there is something else going on?)\n\n\n\n\nThis data set contains 119,390 hotel bookings from two hotels (“City Hotel” and “Resort Hotel”) with an arrival date between July 2015 and August 2017 (more detail needed), including bookings that were later cancelled. See Solution Set 2 for additional details. The data are a de-identified extract of real hotel demand data, made available by the authors.\n\nbookings_orig<-read_csv(\"_data/hotel_bookings.csv\")\n\nselect(bookings_orig, starts_with(\"arrival\"))\n\n\n\n  \n\n\n\nLast time we looked at these data, I went to pretty extraordinary lengths to confirm the dates covered by the data. Lets see how much easier that is if we set the date to a date type variable instead! Those are long variable names, thank goodness we can get rid of them. Note that we only need three pieces of information out of the four provided.\nLook how I can mess around with the format, and lubridate still recovers the date!\n\nbookings<-bookings_orig%>%\n  mutate(date_arrival = str_c(arrival_date_day_of_month,\n                              arrival_date_month,\n                              arrival_date_year, sep=\"/\"),\n         date_arrival = dmy(date_arrival))%>%\n  select(-starts_with(\"arrival\"))\n\nsummary(bookings$date_arrival)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2015-07-01\" \"2016-03-13\" \"2016-09-06\" \"2016-08-28\" \"2017-03-18\" \"2017-08-31\" \n\n\nThere are other relevant time variables in the data set that may be worth exploring. For example, we are given a lead time measure in days (integer), but we could recover a date with lubridate. This would allow us to more easily visually explore, for example, if some people were more likely to make bookings over the winter for summer trips, but in fall for winter trips - or some other seasonal pattern.\n\nbookings<-bookings%>%\n  mutate(date_booking = date_arrival-days(lead_time))\n\nsummary(bookings$date_booking)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2013-06-24\" \"2015-11-28\" \"2016-05-04\" \"2016-05-16\" \"2016-12-09\" \"2017-08-31\" \n\n\nWe can also go in the reverse order. So if we wanted to know how many days before a booking there was last a change in the reservation status, we can generate this by comparing arrival date to reservation status date.\n\nsummary(bookings$reservation_status_date)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2014-10-17\" \"2016-02-01\" \"2016-08-07\" \"2016-07-30\" \"2017-02-08\" \"2017-09-14\" \n\nbookings<-bookings%>%\n  mutate(change_days = interval(reservation_status_date,\n                                date_arrival),\n         change_days = change_days %/% days(1))\n\nsummary(bookings$change_days)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -69.00   -3.00   -1.00   29.68   26.00  526.00 \n\n\n\n\nThis data set runs from the first quarter of 2003 to the second quarter of 2021, and includes quarterly measures of the total amount of household debt associated with 6 different types of loans - mortgage,HE revolving, auto, credit card, student, and other - plus a total household debt including all 6 loan types. This is another fantastic macroeconomic data product from the New York Federal Reserve. Detailed notes on the website reveal that the data are from an Equifax, and explain why data prior to 2003 is no longer part of the primary data publication.\n\ndebt_orig<-read_excel(\"_data/debt_in_trillions.xlsx\")\n\ndebt_orig\n\n\n\n  \n\n\n\nA single case is a year-quarter, and there are 6 (or 7) values that can be pivoted or not depending on the needs of the analyst. The tricky part is figuring out how to tell R to treat the quarters as a date! We could take the long road and separate the year and quarter information, then fix the year to be numeric, recombine, etc. But lets use the more complex formats option of parse_date plus a little regular expression style knowledge and read the information directly.\n\ndebt<-debt_orig%>%\n  mutate(date = parse_date_time(`Year and Quarter`, \n                           orders=\"yq\"))\n\nsummary(debt$year)\n\nLength  Class   Mode \n     0   NULL   NULL \n\n\nWow, isn’t that super simple!"
  },
  {
    "objectID": "posts/challenge4_solutions.html#removing-skipped",
    "href": "posts/challenge4_solutions.html#removing-skipped",
    "title": "Challenge 4 Solutions",
    "section": "Removing “Skipped”",
    "text": "Removing “Skipped”\nWhat about the political variables that all have “Skipped” - a value that should probably be replaced with NA for analysis. Lets use the across function to make this easier.\n\nabc_poll<-abc_poll%>%\n  mutate(across(starts_with(\"Q\"), ~ na_if(.x, \"Skipped\")))\n\nmap(select(abc_poll, starts_with(\"Q1\")), table)\n\n$Q1_a\n\n   Approve Disapprove \n       329        193 \n\n$Q1_b\n\n   Approve Disapprove \n       192        322 \n\n$Q1_c\n\n   Approve Disapprove \n       272        248 \n\n$Q1_d\n\n   Approve Disapprove \n       192        321 \n\n$Q1_e\n\n   Approve Disapprove \n       212        301 \n\n$Q1_f\n\n   Approve Disapprove \n       281        230"
  },
  {
    "objectID": "posts/challenge4_solutions.html#factor-order",
    "href": "posts/challenge4_solutions.html#factor-order",
    "title": "Challenge 4 Solutions",
    "section": "Factor order",
    "text": "Factor order\nFinally, what if you would like the categories of your variable to appear in a specific order, like the education variable that is currently in alphabetical order?\n\n\n\n\n\n\nfactor()\n\n\n\nThe factor variable type links variable labels to an underlying numeric order, and allows you to maintain the specified order for tables and graphics. Character strings always appear in alphabetical order.\n\n\n\ntable(abc_poll$ppeducat)\n\n\nBachelors degree or higher                High school \n                       207                        133 \n     Less than high school               Some college \n                        29                        158 \n\nedulabs <- unique(abc_poll$ppeducat)\nedulabs\n\n[1] \"High school\"                \"Bachelors degree or higher\"\n[3] \"Some college\"               \"Less than high school\"     \n\nabc_poll<-abc_poll%>%\n  mutate(educ = factor(ppeducat, \n                       labels=edulabs[c(4,1,3,2)]))%>%\n  select(-ppeducat)\nrm(edulabs)\n\ntable(abc_poll$educ)\n\n\n     Less than high school                High school \n                       207                        133 \n              Some college Bachelors degree or higher \n                        29                        158"
  },
  {
    "objectID": "posts/challenge3_LindsayJones.html",
    "href": "posts/challenge3_LindsayJones.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_LindsayJones.html#challenge-overview",
    "href": "posts/challenge3_LindsayJones.html#challenge-overview",
    "title": "Challenge 3",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_LindsayJones.html#read-in-data",
    "href": "posts/challenge3_LindsayJones.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\nCode\nanimal_weight<-read_csv(\"_data/animal_weight.csv\",\n                        show_col_types = FALSE)\nprint(animal_weight)\n\n\n# A tibble: 9 × 17\n  IPCC A…¹ Cattl…² Cattl…³ Buffa…⁴ Swine…⁵ Swine…⁶ Chick…⁷ Chick…⁸ Ducks Turkeys\n  <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl>\n1 Indian …     275     110     295      28      28     0.9     1.8   2.7     6.8\n2 Eastern…     550     391     380      50     180     0.9     1.8   2.7     6.8\n3 Africa       275     173     380      28      28     0.9     1.8   2.7     6.8\n4 Oceania      500     330     380      45     180     0.9     1.8   2.7     6.8\n5 Western…     600     420     380      50     198     0.9     1.8   2.7     6.8\n6 Latin A…     400     305     380      28      28     0.9     1.8   2.7     6.8\n7 Asia         350     391     380      50     180     0.9     1.8   2.7     6.8\n8 Middle …     275     173     380      28      28     0.9     1.8   2.7     6.8\n9 Norther…     604     389     380      46     198     0.9     1.8   2.7     6.8\n# … with 7 more variables: Sheep <dbl>, Goats <dbl>, Horses <dbl>, Asses <dbl>,\n#   Mules <dbl>, Camels <dbl>, Llamas <dbl>, and abbreviated variable names\n#   ¹​`IPCC Area`, ²​`Cattle - dairy`, ³​`Cattle - non-dairy`, ⁴​Buffaloes,\n#   ⁵​`Swine - market`, ⁶​`Swine - breeding`, ⁷​`Chicken - Broilers`,\n#   ⁸​`Chicken - Layers`\n# ℹ Use `colnames()` to see all variable names\n\n\n\nBriefly describe the data\nThis data set contains the average weight of 17 different groups of animals in 9 regions of the world. This data set is not tidy because the variables (animal species and average weight) are not set as the columns. Using pivot_longer will fix that.\n\n\nChallenge: Pivot the Chosen Data\n\n\nCode\naw_pivot <- pivot_longer(animal_weight, \n             \"Cattle - dairy\":\"Llamas\", \n             names_to = \"Species\",\n             values_to = \"Weight\")\nprint(aw_pivot)\n\n\n# A tibble: 144 × 3\n   `IPCC Area`         Species            Weight\n   <chr>               <chr>               <dbl>\n 1 Indian Subcontinent Cattle - dairy      275  \n 2 Indian Subcontinent Cattle - non-dairy  110  \n 3 Indian Subcontinent Buffaloes           295  \n 4 Indian Subcontinent Swine - market       28  \n 5 Indian Subcontinent Swine - breeding     28  \n 6 Indian Subcontinent Chicken - Broilers    0.9\n 7 Indian Subcontinent Chicken - Layers      1.8\n 8 Indian Subcontinent Ducks                 2.7\n 9 Indian Subcontinent Turkeys               6.8\n10 Indian Subcontinent Sheep                28  \n# … with 134 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThis format makes it more difficult to look at the data for each country, but this could be solved using a few different functions. If we wanted to examine both types of cattle in every country:\n\n\nCode\naw_pivot %>%\n  filter(grepl('Cattle', Species))\n\n\n# A tibble: 18 × 3\n   `IPCC Area`         Species            Weight\n   <chr>               <chr>               <dbl>\n 1 Indian Subcontinent Cattle - dairy        275\n 2 Indian Subcontinent Cattle - non-dairy    110\n 3 Eastern Europe      Cattle - dairy        550\n 4 Eastern Europe      Cattle - non-dairy    391\n 5 Africa              Cattle - dairy        275\n 6 Africa              Cattle - non-dairy    173\n 7 Oceania             Cattle - dairy        500\n 8 Oceania             Cattle - non-dairy    330\n 9 Western Europe      Cattle - dairy        600\n10 Western Europe      Cattle - non-dairy    420\n11 Latin America       Cattle - dairy        400\n12 Latin America       Cattle - non-dairy    305\n13 Asia                Cattle - dairy        350\n14 Asia                Cattle - non-dairy    391\n15 Middle east         Cattle - dairy        275\n16 Middle east         Cattle - non-dairy    173\n17 Northern America    Cattle - dairy        604\n18 Northern America    Cattle - non-dairy    389"
  },
  {
    "objectID": "posts/challenge1_Akhilesh.html",
    "href": "posts/challenge1_Akhilesh.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(Hmisc)\nlibrary(psych)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_Akhilesh.html#challenge-overview",
    "href": "posts/challenge1_Akhilesh.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_Akhilesh.html#read-in-the-data",
    "href": "posts/challenge1_Akhilesh.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\n# \nrailroad <- read.csv('_data/railroad_2012_clean_county.csv', stringsAsFactors = TRUE, header = TRUE)\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_Akhilesh.html#describe-the-data",
    "href": "posts/challenge1_Akhilesh.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\n# check first 6 rows in the dataset, to get primary understading of the dataset structure\n# check first 6 rows in the dataset, to get primary understanding of the dataframe structure\n\nhead(railroad)\n\n\n  state               county total_employees\n1    AE                  APO               2\n2    AK            ANCHORAGE               7\n3    AK FAIRBANKS NORTH STAR               2\n4    AK               JUNEAU               3\n5    AK    MATANUSKA-SUSITNA               2\n6    AK                SITKA               1\n\n\nCode\nView(railroad)\ndim(railroad) # numbers of rows. columns\n\n\n[1] 2930    3\n\n\nCode\n#Dataset Description\n  # \"state\" column contains name of different states from United States of America in upper case, abbreviated format\n  # \"county\" column contains names of different counties, for the states in 'state' column, in upper case format\n  # \"total_employees\" column contains state-county wise number of railroad employees.\n\n\n# struture of railroad dataset\n\nstr(railroad)\n\n\n'data.frame':   2930 obs. of  3 variables:\n $ state          : Factor w/ 53 levels \"AE\",\"AK\",\"AL\",..: 1 2 2 2 2 2 2 3 3 3 ...\n $ county         : Factor w/ 1709 levels \"ABBEVILLE\",\"ACADIA\",..: 44 33 524 794 963 1415 1417 78 85 95 ...\n $ total_employees: int  2 7 2 3 2 1 88 102 143 1 ...\n\n\nCode\n# Output\n\n  # 'data.frame':   2930 obs. of  3 variables:\n  #  $ state          : Factor w/ 53 levels \"AE\",\"AK\",\"AL\",..: 1 2 2 2 2 2 2 3 3 3 ...\n  #  $ county         : Factor w/ 1709 levels \"ABBEVILLE\",\"ACADIA\",..: 44 33 524 794 963 1415 1417 78 85 95 ...\n  #  $ total_employees: int  2 7 2 3 2 1 88 102 143 1 ...\n\n#summary\n\n  # railroad dataset has 2930 rows and 3 columns\n  # State column is a factor class; as converted from character class during file read command read.csv() and it has 53 levels.\n  # county column has 1709 levels\n\n# Missing Values, check NA value\n\nsum(is.na(railroad))\n\n\n[1] 0\n\n\nCode\n# check column wise NA values\n\nsum(is.na(railroad$state))\n\n\n[1] 0\n\n\nCode\nsum(is.na(railroad$county))\n\n\n[1] 0\n\n\nCode\nsum(is.na(railroad$total_employees))\n\n\n[1] 0\n\n\nCode\n# Missing Value, check NULL value\n\nsum(is.null(railroad))\n\n\n[1] 0\n\n\nCode\n# check column wise NULL values\n\nsum(is.null(railroad$state))\n\n\n[1] 0\n\n\nCode\nsum(is.null(railroad$county))\n\n\n[1] 0\n\n\nCode\nsum(is.null(railroad$total_employees))\n\n\n[1] 0\n\n\nCode\n# column, row names of the dataset\n\ncolnames(railroad)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n\nCode\nrow.names(railroad)\n\n\n   [1] \"1\"    \"2\"    \"3\"    \"4\"    \"5\"    \"6\"    \"7\"    \"8\"    \"9\"    \"10\"  \n  [11] \"11\"   \"12\"   \"13\"   \"14\"   \"15\"   \"16\"   \"17\"   \"18\"   \"19\"   \"20\"  \n  [21] \"21\"   \"22\"   \"23\"   \"24\"   \"25\"   \"26\"   \"27\"   \"28\"   \"29\"   \"30\"  \n  [31] \"31\"   \"32\"   \"33\"   \"34\"   \"35\"   \"36\"   \"37\"   \"38\"   \"39\"   \"40\"  \n  [41] \"41\"   \"42\"   \"43\"   \"44\"   \"45\"   \"46\"   \"47\"   \"48\"   \"49\"   \"50\"  \n  [51] \"51\"   \"52\"   \"53\"   \"54\"   \"55\"   \"56\"   \"57\"   \"58\"   \"59\"   \"60\"  \n  [61] \"61\"   \"62\"   \"63\"   \"64\"   \"65\"   \"66\"   \"67\"   \"68\"   \"69\"   \"70\"  \n  [71] \"71\"   \"72\"   \"73\"   \"74\"   \"75\"   \"76\"   \"77\"   \"78\"   \"79\"   \"80\"  \n  [81] \"81\"   \"82\"   \"83\"   \"84\"   \"85\"   \"86\"   \"87\"   \"88\"   \"89\"   \"90\"  \n  [91] \"91\"   \"92\"   \"93\"   \"94\"   \"95\"   \"96\"   \"97\"   \"98\"   \"99\"   \"100\" \n [101] \"101\"  \"102\"  \"103\"  \"104\"  \"105\"  \"106\"  \"107\"  \"108\"  \"109\"  \"110\" \n [111] \"111\"  \"112\"  \"113\"  \"114\"  \"115\"  \"116\"  \"117\"  \"118\"  \"119\"  \"120\" \n [121] \"121\"  \"122\"  \"123\"  \"124\"  \"125\"  \"126\"  \"127\"  \"128\"  \"129\"  \"130\" \n [131] \"131\"  \"132\"  \"133\"  \"134\"  \"135\"  \"136\"  \"137\"  \"138\"  \"139\"  \"140\" \n [141] \"141\"  \"142\"  \"143\"  \"144\"  \"145\"  \"146\"  \"147\"  \"148\"  \"149\"  \"150\" \n [151] \"151\"  \"152\"  \"153\"  \"154\"  \"155\"  \"156\"  \"157\"  \"158\"  \"159\"  \"160\" \n [161] \"161\"  \"162\"  \"163\"  \"164\"  \"165\"  \"166\"  \"167\"  \"168\"  \"169\"  \"170\" \n [171] \"171\"  \"172\"  \"173\"  \"174\"  \"175\"  \"176\"  \"177\"  \"178\"  \"179\"  \"180\" \n [181] \"181\"  \"182\"  \"183\"  \"184\"  \"185\"  \"186\"  \"187\"  \"188\"  \"189\"  \"190\" \n [191] \"191\"  \"192\"  \"193\"  \"194\"  \"195\"  \"196\"  \"197\"  \"198\"  \"199\"  \"200\" \n [201] \"201\"  \"202\"  \"203\"  \"204\"  \"205\"  \"206\"  \"207\"  \"208\"  \"209\"  \"210\" \n [211] \"211\"  \"212\"  \"213\"  \"214\"  \"215\"  \"216\"  \"217\"  \"218\"  \"219\"  \"220\" \n [221] \"221\"  \"222\"  \"223\"  \"224\"  \"225\"  \"226\"  \"227\"  \"228\"  \"229\"  \"230\" \n [231] \"231\"  \"232\"  \"233\"  \"234\"  \"235\"  \"236\"  \"237\"  \"238\"  \"239\"  \"240\" \n [241] \"241\"  \"242\"  \"243\"  \"244\"  \"245\"  \"246\"  \"247\"  \"248\"  \"249\"  \"250\" \n [251] \"251\"  \"252\"  \"253\"  \"254\"  \"255\"  \"256\"  \"257\"  \"258\"  \"259\"  \"260\" \n [261] \"261\"  \"262\"  \"263\"  \"264\"  \"265\"  \"266\"  \"267\"  \"268\"  \"269\"  \"270\" \n [271] \"271\"  \"272\"  \"273\"  \"274\"  \"275\"  \"276\"  \"277\"  \"278\"  \"279\"  \"280\" \n [281] \"281\"  \"282\"  \"283\"  \"284\"  \"285\"  \"286\"  \"287\"  \"288\"  \"289\"  \"290\" \n [291] \"291\"  \"292\"  \"293\"  \"294\"  \"295\"  \"296\"  \"297\"  \"298\"  \"299\"  \"300\" \n [301] \"301\"  \"302\"  \"303\"  \"304\"  \"305\"  \"306\"  \"307\"  \"308\"  \"309\"  \"310\" \n [311] \"311\"  \"312\"  \"313\"  \"314\"  \"315\"  \"316\"  \"317\"  \"318\"  \"319\"  \"320\" \n [321] \"321\"  \"322\"  \"323\"  \"324\"  \"325\"  \"326\"  \"327\"  \"328\"  \"329\"  \"330\" \n [331] \"331\"  \"332\"  \"333\"  \"334\"  \"335\"  \"336\"  \"337\"  \"338\"  \"339\"  \"340\" \n [341] \"341\"  \"342\"  \"343\"  \"344\"  \"345\"  \"346\"  \"347\"  \"348\"  \"349\"  \"350\" \n [351] \"351\"  \"352\"  \"353\"  \"354\"  \"355\"  \"356\"  \"357\"  \"358\"  \"359\"  \"360\" \n [361] \"361\"  \"362\"  \"363\"  \"364\"  \"365\"  \"366\"  \"367\"  \"368\"  \"369\"  \"370\" \n [371] \"371\"  \"372\"  \"373\"  \"374\"  \"375\"  \"376\"  \"377\"  \"378\"  \"379\"  \"380\" \n [381] \"381\"  \"382\"  \"383\"  \"384\"  \"385\"  \"386\"  \"387\"  \"388\"  \"389\"  \"390\" \n [391] \"391\"  \"392\"  \"393\"  \"394\"  \"395\"  \"396\"  \"397\"  \"398\"  \"399\"  \"400\" \n [401] \"401\"  \"402\"  \"403\"  \"404\"  \"405\"  \"406\"  \"407\"  \"408\"  \"409\"  \"410\" \n [411] \"411\"  \"412\"  \"413\"  \"414\"  \"415\"  \"416\"  \"417\"  \"418\"  \"419\"  \"420\" \n [421] \"421\"  \"422\"  \"423\"  \"424\"  \"425\"  \"426\"  \"427\"  \"428\"  \"429\"  \"430\" \n [431] \"431\"  \"432\"  \"433\"  \"434\"  \"435\"  \"436\"  \"437\"  \"438\"  \"439\"  \"440\" \n [441] \"441\"  \"442\"  \"443\"  \"444\"  \"445\"  \"446\"  \"447\"  \"448\"  \"449\"  \"450\" \n [451] \"451\"  \"452\"  \"453\"  \"454\"  \"455\"  \"456\"  \"457\"  \"458\"  \"459\"  \"460\" \n [461] \"461\"  \"462\"  \"463\"  \"464\"  \"465\"  \"466\"  \"467\"  \"468\"  \"469\"  \"470\" \n [471] \"471\"  \"472\"  \"473\"  \"474\"  \"475\"  \"476\"  \"477\"  \"478\"  \"479\"  \"480\" \n [481] \"481\"  \"482\"  \"483\"  \"484\"  \"485\"  \"486\"  \"487\"  \"488\"  \"489\"  \"490\" \n [491] \"491\"  \"492\"  \"493\"  \"494\"  \"495\"  \"496\"  \"497\"  \"498\"  \"499\"  \"500\" \n [501] \"501\"  \"502\"  \"503\"  \"504\"  \"505\"  \"506\"  \"507\"  \"508\"  \"509\"  \"510\" \n [511] \"511\"  \"512\"  \"513\"  \"514\"  \"515\"  \"516\"  \"517\"  \"518\"  \"519\"  \"520\" \n [521] \"521\"  \"522\"  \"523\"  \"524\"  \"525\"  \"526\"  \"527\"  \"528\"  \"529\"  \"530\" \n [531] \"531\"  \"532\"  \"533\"  \"534\"  \"535\"  \"536\"  \"537\"  \"538\"  \"539\"  \"540\" \n [541] \"541\"  \"542\"  \"543\"  \"544\"  \"545\"  \"546\"  \"547\"  \"548\"  \"549\"  \"550\" \n [551] \"551\"  \"552\"  \"553\"  \"554\"  \"555\"  \"556\"  \"557\"  \"558\"  \"559\"  \"560\" \n [561] \"561\"  \"562\"  \"563\"  \"564\"  \"565\"  \"566\"  \"567\"  \"568\"  \"569\"  \"570\" \n [571] \"571\"  \"572\"  \"573\"  \"574\"  \"575\"  \"576\"  \"577\"  \"578\"  \"579\"  \"580\" \n [581] \"581\"  \"582\"  \"583\"  \"584\"  \"585\"  \"586\"  \"587\"  \"588\"  \"589\"  \"590\" \n [591] \"591\"  \"592\"  \"593\"  \"594\"  \"595\"  \"596\"  \"597\"  \"598\"  \"599\"  \"600\" \n [601] \"601\"  \"602\"  \"603\"  \"604\"  \"605\"  \"606\"  \"607\"  \"608\"  \"609\"  \"610\" \n [611] \"611\"  \"612\"  \"613\"  \"614\"  \"615\"  \"616\"  \"617\"  \"618\"  \"619\"  \"620\" \n [621] \"621\"  \"622\"  \"623\"  \"624\"  \"625\"  \"626\"  \"627\"  \"628\"  \"629\"  \"630\" \n [631] \"631\"  \"632\"  \"633\"  \"634\"  \"635\"  \"636\"  \"637\"  \"638\"  \"639\"  \"640\" \n [641] \"641\"  \"642\"  \"643\"  \"644\"  \"645\"  \"646\"  \"647\"  \"648\"  \"649\"  \"650\" \n [651] \"651\"  \"652\"  \"653\"  \"654\"  \"655\"  \"656\"  \"657\"  \"658\"  \"659\"  \"660\" \n [661] \"661\"  \"662\"  \"663\"  \"664\"  \"665\"  \"666\"  \"667\"  \"668\"  \"669\"  \"670\" \n [671] \"671\"  \"672\"  \"673\"  \"674\"  \"675\"  \"676\"  \"677\"  \"678\"  \"679\"  \"680\" \n [681] \"681\"  \"682\"  \"683\"  \"684\"  \"685\"  \"686\"  \"687\"  \"688\"  \"689\"  \"690\" \n [691] \"691\"  \"692\"  \"693\"  \"694\"  \"695\"  \"696\"  \"697\"  \"698\"  \"699\"  \"700\" \n [701] \"701\"  \"702\"  \"703\"  \"704\"  \"705\"  \"706\"  \"707\"  \"708\"  \"709\"  \"710\" \n [711] \"711\"  \"712\"  \"713\"  \"714\"  \"715\"  \"716\"  \"717\"  \"718\"  \"719\"  \"720\" \n [721] \"721\"  \"722\"  \"723\"  \"724\"  \"725\"  \"726\"  \"727\"  \"728\"  \"729\"  \"730\" \n [731] \"731\"  \"732\"  \"733\"  \"734\"  \"735\"  \"736\"  \"737\"  \"738\"  \"739\"  \"740\" \n [741] \"741\"  \"742\"  \"743\"  \"744\"  \"745\"  \"746\"  \"747\"  \"748\"  \"749\"  \"750\" \n [751] \"751\"  \"752\"  \"753\"  \"754\"  \"755\"  \"756\"  \"757\"  \"758\"  \"759\"  \"760\" \n [761] \"761\"  \"762\"  \"763\"  \"764\"  \"765\"  \"766\"  \"767\"  \"768\"  \"769\"  \"770\" \n [771] \"771\"  \"772\"  \"773\"  \"774\"  \"775\"  \"776\"  \"777\"  \"778\"  \"779\"  \"780\" \n [781] \"781\"  \"782\"  \"783\"  \"784\"  \"785\"  \"786\"  \"787\"  \"788\"  \"789\"  \"790\" \n [791] \"791\"  \"792\"  \"793\"  \"794\"  \"795\"  \"796\"  \"797\"  \"798\"  \"799\"  \"800\" \n [801] \"801\"  \"802\"  \"803\"  \"804\"  \"805\"  \"806\"  \"807\"  \"808\"  \"809\"  \"810\" \n [811] \"811\"  \"812\"  \"813\"  \"814\"  \"815\"  \"816\"  \"817\"  \"818\"  \"819\"  \"820\" \n [821] \"821\"  \"822\"  \"823\"  \"824\"  \"825\"  \"826\"  \"827\"  \"828\"  \"829\"  \"830\" \n [831] \"831\"  \"832\"  \"833\"  \"834\"  \"835\"  \"836\"  \"837\"  \"838\"  \"839\"  \"840\" \n [841] \"841\"  \"842\"  \"843\"  \"844\"  \"845\"  \"846\"  \"847\"  \"848\"  \"849\"  \"850\" \n [851] \"851\"  \"852\"  \"853\"  \"854\"  \"855\"  \"856\"  \"857\"  \"858\"  \"859\"  \"860\" \n [861] \"861\"  \"862\"  \"863\"  \"864\"  \"865\"  \"866\"  \"867\"  \"868\"  \"869\"  \"870\" \n [871] \"871\"  \"872\"  \"873\"  \"874\"  \"875\"  \"876\"  \"877\"  \"878\"  \"879\"  \"880\" \n [881] \"881\"  \"882\"  \"883\"  \"884\"  \"885\"  \"886\"  \"887\"  \"888\"  \"889\"  \"890\" \n [891] \"891\"  \"892\"  \"893\"  \"894\"  \"895\"  \"896\"  \"897\"  \"898\"  \"899\"  \"900\" \n [901] \"901\"  \"902\"  \"903\"  \"904\"  \"905\"  \"906\"  \"907\"  \"908\"  \"909\"  \"910\" \n [911] \"911\"  \"912\"  \"913\"  \"914\"  \"915\"  \"916\"  \"917\"  \"918\"  \"919\"  \"920\" \n [921] \"921\"  \"922\"  \"923\"  \"924\"  \"925\"  \"926\"  \"927\"  \"928\"  \"929\"  \"930\" \n [931] \"931\"  \"932\"  \"933\"  \"934\"  \"935\"  \"936\"  \"937\"  \"938\"  \"939\"  \"940\" \n [941] \"941\"  \"942\"  \"943\"  \"944\"  \"945\"  \"946\"  \"947\"  \"948\"  \"949\"  \"950\" \n [951] \"951\"  \"952\"  \"953\"  \"954\"  \"955\"  \"956\"  \"957\"  \"958\"  \"959\"  \"960\" \n [961] \"961\"  \"962\"  \"963\"  \"964\"  \"965\"  \"966\"  \"967\"  \"968\"  \"969\"  \"970\" \n [971] \"971\"  \"972\"  \"973\"  \"974\"  \"975\"  \"976\"  \"977\"  \"978\"  \"979\"  \"980\" \n [981] \"981\"  \"982\"  \"983\"  \"984\"  \"985\"  \"986\"  \"987\"  \"988\"  \"989\"  \"990\" \n [991] \"991\"  \"992\"  \"993\"  \"994\"  \"995\"  \"996\"  \"997\"  \"998\"  \"999\"  \"1000\"\n[1001] \"1001\" \"1002\" \"1003\" \"1004\" \"1005\" \"1006\" \"1007\" \"1008\" \"1009\" \"1010\"\n[1011] \"1011\" \"1012\" \"1013\" \"1014\" \"1015\" \"1016\" \"1017\" \"1018\" \"1019\" \"1020\"\n[1021] \"1021\" \"1022\" \"1023\" \"1024\" \"1025\" \"1026\" \"1027\" \"1028\" \"1029\" \"1030\"\n[1031] \"1031\" \"1032\" \"1033\" \"1034\" \"1035\" \"1036\" \"1037\" \"1038\" \"1039\" \"1040\"\n[1041] \"1041\" \"1042\" \"1043\" \"1044\" \"1045\" \"1046\" \"1047\" \"1048\" \"1049\" \"1050\"\n[1051] \"1051\" \"1052\" \"1053\" \"1054\" \"1055\" \"1056\" \"1057\" \"1058\" \"1059\" \"1060\"\n[1061] \"1061\" \"1062\" \"1063\" \"1064\" \"1065\" \"1066\" \"1067\" \"1068\" \"1069\" \"1070\"\n[1071] \"1071\" \"1072\" \"1073\" \"1074\" \"1075\" \"1076\" \"1077\" \"1078\" \"1079\" \"1080\"\n[1081] \"1081\" \"1082\" \"1083\" \"1084\" \"1085\" \"1086\" \"1087\" \"1088\" \"1089\" \"1090\"\n[1091] \"1091\" \"1092\" \"1093\" \"1094\" \"1095\" \"1096\" \"1097\" \"1098\" \"1099\" \"1100\"\n[1101] \"1101\" \"1102\" \"1103\" \"1104\" \"1105\" \"1106\" \"1107\" \"1108\" \"1109\" \"1110\"\n[1111] \"1111\" \"1112\" \"1113\" \"1114\" \"1115\" \"1116\" \"1117\" \"1118\" \"1119\" \"1120\"\n[1121] \"1121\" \"1122\" \"1123\" \"1124\" \"1125\" \"1126\" \"1127\" \"1128\" \"1129\" \"1130\"\n[1131] \"1131\" \"1132\" \"1133\" \"1134\" \"1135\" \"1136\" \"1137\" \"1138\" \"1139\" \"1140\"\n[1141] \"1141\" \"1142\" \"1143\" \"1144\" \"1145\" \"1146\" \"1147\" \"1148\" \"1149\" \"1150\"\n[1151] \"1151\" \"1152\" \"1153\" \"1154\" \"1155\" \"1156\" \"1157\" \"1158\" \"1159\" \"1160\"\n[1161] \"1161\" \"1162\" \"1163\" \"1164\" \"1165\" \"1166\" \"1167\" \"1168\" \"1169\" \"1170\"\n[1171] \"1171\" \"1172\" \"1173\" \"1174\" \"1175\" \"1176\" \"1177\" \"1178\" \"1179\" \"1180\"\n[1181] \"1181\" \"1182\" \"1183\" \"1184\" \"1185\" \"1186\" \"1187\" \"1188\" \"1189\" \"1190\"\n[1191] \"1191\" \"1192\" \"1193\" \"1194\" \"1195\" \"1196\" \"1197\" \"1198\" \"1199\" \"1200\"\n[1201] \"1201\" \"1202\" \"1203\" \"1204\" \"1205\" \"1206\" \"1207\" \"1208\" \"1209\" \"1210\"\n[1211] \"1211\" \"1212\" \"1213\" \"1214\" \"1215\" \"1216\" \"1217\" \"1218\" \"1219\" \"1220\"\n[1221] \"1221\" \"1222\" \"1223\" \"1224\" \"1225\" \"1226\" \"1227\" \"1228\" \"1229\" \"1230\"\n[1231] \"1231\" \"1232\" \"1233\" \"1234\" \"1235\" \"1236\" \"1237\" \"1238\" \"1239\" \"1240\"\n[1241] \"1241\" \"1242\" \"1243\" \"1244\" \"1245\" \"1246\" \"1247\" \"1248\" \"1249\" \"1250\"\n[1251] \"1251\" \"1252\" \"1253\" \"1254\" \"1255\" \"1256\" \"1257\" \"1258\" \"1259\" \"1260\"\n[1261] \"1261\" \"1262\" \"1263\" \"1264\" \"1265\" \"1266\" \"1267\" \"1268\" \"1269\" \"1270\"\n[1271] \"1271\" \"1272\" \"1273\" \"1274\" \"1275\" \"1276\" \"1277\" \"1278\" \"1279\" \"1280\"\n[1281] \"1281\" \"1282\" \"1283\" \"1284\" \"1285\" \"1286\" \"1287\" \"1288\" \"1289\" \"1290\"\n[1291] \"1291\" \"1292\" \"1293\" \"1294\" \"1295\" \"1296\" \"1297\" \"1298\" \"1299\" \"1300\"\n[1301] \"1301\" \"1302\" \"1303\" \"1304\" \"1305\" \"1306\" \"1307\" \"1308\" \"1309\" \"1310\"\n[1311] \"1311\" \"1312\" \"1313\" \"1314\" \"1315\" \"1316\" \"1317\" \"1318\" \"1319\" \"1320\"\n[1321] \"1321\" \"1322\" \"1323\" \"1324\" \"1325\" \"1326\" \"1327\" \"1328\" \"1329\" \"1330\"\n[1331] \"1331\" \"1332\" \"1333\" \"1334\" \"1335\" \"1336\" \"1337\" \"1338\" \"1339\" \"1340\"\n[1341] \"1341\" \"1342\" \"1343\" \"1344\" \"1345\" \"1346\" \"1347\" \"1348\" \"1349\" \"1350\"\n[1351] \"1351\" \"1352\" \"1353\" \"1354\" \"1355\" \"1356\" \"1357\" \"1358\" \"1359\" \"1360\"\n[1361] \"1361\" \"1362\" \"1363\" \"1364\" \"1365\" \"1366\" \"1367\" \"1368\" \"1369\" \"1370\"\n[1371] \"1371\" \"1372\" \"1373\" \"1374\" \"1375\" \"1376\" \"1377\" \"1378\" \"1379\" \"1380\"\n[1381] \"1381\" \"1382\" \"1383\" \"1384\" \"1385\" \"1386\" \"1387\" \"1388\" \"1389\" \"1390\"\n[1391] \"1391\" \"1392\" \"1393\" \"1394\" \"1395\" \"1396\" \"1397\" \"1398\" \"1399\" \"1400\"\n[1401] \"1401\" \"1402\" \"1403\" \"1404\" \"1405\" \"1406\" \"1407\" \"1408\" \"1409\" \"1410\"\n[1411] \"1411\" \"1412\" \"1413\" \"1414\" \"1415\" \"1416\" \"1417\" \"1418\" \"1419\" \"1420\"\n[1421] \"1421\" \"1422\" \"1423\" \"1424\" \"1425\" \"1426\" \"1427\" \"1428\" \"1429\" \"1430\"\n[1431] \"1431\" \"1432\" \"1433\" \"1434\" \"1435\" \"1436\" \"1437\" \"1438\" \"1439\" \"1440\"\n[1441] \"1441\" \"1442\" \"1443\" \"1444\" \"1445\" \"1446\" \"1447\" \"1448\" \"1449\" \"1450\"\n[1451] \"1451\" \"1452\" \"1453\" \"1454\" \"1455\" \"1456\" \"1457\" \"1458\" \"1459\" \"1460\"\n[1461] \"1461\" \"1462\" \"1463\" \"1464\" \"1465\" \"1466\" \"1467\" \"1468\" \"1469\" \"1470\"\n[1471] \"1471\" \"1472\" \"1473\" \"1474\" \"1475\" \"1476\" \"1477\" \"1478\" \"1479\" \"1480\"\n[1481] \"1481\" \"1482\" \"1483\" \"1484\" \"1485\" \"1486\" \"1487\" \"1488\" \"1489\" \"1490\"\n[1491] \"1491\" \"1492\" \"1493\" \"1494\" \"1495\" \"1496\" \"1497\" \"1498\" \"1499\" \"1500\"\n[1501] \"1501\" \"1502\" \"1503\" \"1504\" \"1505\" \"1506\" \"1507\" \"1508\" \"1509\" \"1510\"\n[1511] \"1511\" \"1512\" \"1513\" \"1514\" \"1515\" \"1516\" \"1517\" \"1518\" \"1519\" \"1520\"\n[1521] \"1521\" \"1522\" \"1523\" \"1524\" \"1525\" \"1526\" \"1527\" \"1528\" \"1529\" \"1530\"\n[1531] \"1531\" \"1532\" \"1533\" \"1534\" \"1535\" \"1536\" \"1537\" \"1538\" \"1539\" \"1540\"\n[1541] \"1541\" \"1542\" \"1543\" \"1544\" \"1545\" \"1546\" \"1547\" \"1548\" \"1549\" \"1550\"\n[1551] \"1551\" \"1552\" \"1553\" \"1554\" \"1555\" \"1556\" \"1557\" \"1558\" \"1559\" \"1560\"\n[1561] \"1561\" \"1562\" \"1563\" \"1564\" \"1565\" \"1566\" \"1567\" \"1568\" \"1569\" \"1570\"\n[1571] \"1571\" \"1572\" \"1573\" \"1574\" \"1575\" \"1576\" \"1577\" \"1578\" \"1579\" \"1580\"\n[1581] \"1581\" \"1582\" \"1583\" \"1584\" \"1585\" \"1586\" \"1587\" \"1588\" \"1589\" \"1590\"\n[1591] \"1591\" \"1592\" \"1593\" \"1594\" \"1595\" \"1596\" \"1597\" \"1598\" \"1599\" \"1600\"\n[1601] \"1601\" \"1602\" \"1603\" \"1604\" \"1605\" \"1606\" \"1607\" \"1608\" \"1609\" \"1610\"\n[1611] \"1611\" \"1612\" \"1613\" \"1614\" \"1615\" \"1616\" \"1617\" \"1618\" \"1619\" \"1620\"\n[1621] \"1621\" \"1622\" \"1623\" \"1624\" \"1625\" \"1626\" \"1627\" \"1628\" \"1629\" \"1630\"\n[1631] \"1631\" \"1632\" \"1633\" \"1634\" \"1635\" \"1636\" \"1637\" \"1638\" \"1639\" \"1640\"\n[1641] \"1641\" \"1642\" \"1643\" \"1644\" \"1645\" \"1646\" \"1647\" \"1648\" \"1649\" \"1650\"\n[1651] \"1651\" \"1652\" \"1653\" \"1654\" \"1655\" \"1656\" \"1657\" \"1658\" \"1659\" \"1660\"\n[1661] \"1661\" \"1662\" \"1663\" \"1664\" \"1665\" \"1666\" \"1667\" \"1668\" \"1669\" \"1670\"\n[1671] \"1671\" \"1672\" \"1673\" \"1674\" \"1675\" \"1676\" \"1677\" \"1678\" \"1679\" \"1680\"\n[1681] \"1681\" \"1682\" \"1683\" \"1684\" \"1685\" \"1686\" \"1687\" \"1688\" \"1689\" \"1690\"\n[1691] \"1691\" \"1692\" \"1693\" \"1694\" \"1695\" \"1696\" \"1697\" \"1698\" \"1699\" \"1700\"\n[1701] \"1701\" \"1702\" \"1703\" \"1704\" \"1705\" \"1706\" \"1707\" \"1708\" \"1709\" \"1710\"\n[1711] \"1711\" \"1712\" \"1713\" \"1714\" \"1715\" \"1716\" \"1717\" \"1718\" \"1719\" \"1720\"\n[1721] \"1721\" \"1722\" \"1723\" \"1724\" \"1725\" \"1726\" \"1727\" \"1728\" \"1729\" \"1730\"\n[1731] \"1731\" \"1732\" \"1733\" \"1734\" \"1735\" \"1736\" \"1737\" \"1738\" \"1739\" \"1740\"\n[1741] \"1741\" \"1742\" \"1743\" \"1744\" \"1745\" \"1746\" \"1747\" \"1748\" \"1749\" \"1750\"\n[1751] \"1751\" \"1752\" \"1753\" \"1754\" \"1755\" \"1756\" \"1757\" \"1758\" \"1759\" \"1760\"\n[1761] \"1761\" \"1762\" \"1763\" \"1764\" \"1765\" \"1766\" \"1767\" \"1768\" \"1769\" \"1770\"\n[1771] \"1771\" \"1772\" \"1773\" \"1774\" \"1775\" \"1776\" \"1777\" \"1778\" \"1779\" \"1780\"\n[1781] \"1781\" \"1782\" \"1783\" \"1784\" \"1785\" \"1786\" \"1787\" \"1788\" \"1789\" \"1790\"\n[1791] \"1791\" \"1792\" \"1793\" \"1794\" \"1795\" \"1796\" \"1797\" \"1798\" \"1799\" \"1800\"\n[1801] \"1801\" \"1802\" \"1803\" \"1804\" \"1805\" \"1806\" \"1807\" \"1808\" \"1809\" \"1810\"\n[1811] \"1811\" \"1812\" \"1813\" \"1814\" \"1815\" \"1816\" \"1817\" \"1818\" \"1819\" \"1820\"\n[1821] \"1821\" \"1822\" \"1823\" \"1824\" \"1825\" \"1826\" \"1827\" \"1828\" \"1829\" \"1830\"\n[1831] \"1831\" \"1832\" \"1833\" \"1834\" \"1835\" \"1836\" \"1837\" \"1838\" \"1839\" \"1840\"\n[1841] \"1841\" \"1842\" \"1843\" \"1844\" \"1845\" \"1846\" \"1847\" \"1848\" \"1849\" \"1850\"\n[1851] \"1851\" \"1852\" \"1853\" \"1854\" \"1855\" \"1856\" \"1857\" \"1858\" \"1859\" \"1860\"\n[1861] \"1861\" \"1862\" \"1863\" \"1864\" \"1865\" \"1866\" \"1867\" \"1868\" \"1869\" \"1870\"\n[1871] \"1871\" \"1872\" \"1873\" \"1874\" \"1875\" \"1876\" \"1877\" \"1878\" \"1879\" \"1880\"\n[1881] \"1881\" \"1882\" \"1883\" \"1884\" \"1885\" \"1886\" \"1887\" \"1888\" \"1889\" \"1890\"\n[1891] \"1891\" \"1892\" \"1893\" \"1894\" \"1895\" \"1896\" \"1897\" \"1898\" \"1899\" \"1900\"\n[1901] \"1901\" \"1902\" \"1903\" \"1904\" \"1905\" \"1906\" \"1907\" \"1908\" \"1909\" \"1910\"\n[1911] \"1911\" \"1912\" \"1913\" \"1914\" \"1915\" \"1916\" \"1917\" \"1918\" \"1919\" \"1920\"\n[1921] \"1921\" \"1922\" \"1923\" \"1924\" \"1925\" \"1926\" \"1927\" \"1928\" \"1929\" \"1930\"\n[1931] \"1931\" \"1932\" \"1933\" \"1934\" \"1935\" \"1936\" \"1937\" \"1938\" \"1939\" \"1940\"\n[1941] \"1941\" \"1942\" \"1943\" \"1944\" \"1945\" \"1946\" \"1947\" \"1948\" \"1949\" \"1950\"\n[1951] \"1951\" \"1952\" \"1953\" \"1954\" \"1955\" \"1956\" \"1957\" \"1958\" \"1959\" \"1960\"\n[1961] \"1961\" \"1962\" \"1963\" \"1964\" \"1965\" \"1966\" \"1967\" \"1968\" \"1969\" \"1970\"\n[1971] \"1971\" \"1972\" \"1973\" \"1974\" \"1975\" \"1976\" \"1977\" \"1978\" \"1979\" \"1980\"\n[1981] \"1981\" \"1982\" \"1983\" \"1984\" \"1985\" \"1986\" \"1987\" \"1988\" \"1989\" \"1990\"\n[1991] \"1991\" \"1992\" \"1993\" \"1994\" \"1995\" \"1996\" \"1997\" \"1998\" \"1999\" \"2000\"\n[2001] \"2001\" \"2002\" \"2003\" \"2004\" \"2005\" \"2006\" \"2007\" \"2008\" \"2009\" \"2010\"\n[2011] \"2011\" \"2012\" \"2013\" \"2014\" \"2015\" \"2016\" \"2017\" \"2018\" \"2019\" \"2020\"\n[2021] \"2021\" \"2022\" \"2023\" \"2024\" \"2025\" \"2026\" \"2027\" \"2028\" \"2029\" \"2030\"\n[2031] \"2031\" \"2032\" \"2033\" \"2034\" \"2035\" \"2036\" \"2037\" \"2038\" \"2039\" \"2040\"\n[2041] \"2041\" \"2042\" \"2043\" \"2044\" \"2045\" \"2046\" \"2047\" \"2048\" \"2049\" \"2050\"\n[2051] \"2051\" \"2052\" \"2053\" \"2054\" \"2055\" \"2056\" \"2057\" \"2058\" \"2059\" \"2060\"\n[2061] \"2061\" \"2062\" \"2063\" \"2064\" \"2065\" \"2066\" \"2067\" \"2068\" \"2069\" \"2070\"\n[2071] \"2071\" \"2072\" \"2073\" \"2074\" \"2075\" \"2076\" \"2077\" \"2078\" \"2079\" \"2080\"\n[2081] \"2081\" \"2082\" \"2083\" \"2084\" \"2085\" \"2086\" \"2087\" \"2088\" \"2089\" \"2090\"\n[2091] \"2091\" \"2092\" \"2093\" \"2094\" \"2095\" \"2096\" \"2097\" \"2098\" \"2099\" \"2100\"\n[2101] \"2101\" \"2102\" \"2103\" \"2104\" \"2105\" \"2106\" \"2107\" \"2108\" \"2109\" \"2110\"\n[2111] \"2111\" \"2112\" \"2113\" \"2114\" \"2115\" \"2116\" \"2117\" \"2118\" \"2119\" \"2120\"\n[2121] \"2121\" \"2122\" \"2123\" \"2124\" \"2125\" \"2126\" \"2127\" \"2128\" \"2129\" \"2130\"\n[2131] \"2131\" \"2132\" \"2133\" \"2134\" \"2135\" \"2136\" \"2137\" \"2138\" \"2139\" \"2140\"\n[2141] \"2141\" \"2142\" \"2143\" \"2144\" \"2145\" \"2146\" \"2147\" \"2148\" \"2149\" \"2150\"\n[2151] \"2151\" \"2152\" \"2153\" \"2154\" \"2155\" \"2156\" \"2157\" \"2158\" \"2159\" \"2160\"\n[2161] \"2161\" \"2162\" \"2163\" \"2164\" \"2165\" \"2166\" \"2167\" \"2168\" \"2169\" \"2170\"\n[2171] \"2171\" \"2172\" \"2173\" \"2174\" \"2175\" \"2176\" \"2177\" \"2178\" \"2179\" \"2180\"\n[2181] \"2181\" \"2182\" \"2183\" \"2184\" \"2185\" \"2186\" \"2187\" \"2188\" \"2189\" \"2190\"\n[2191] \"2191\" \"2192\" \"2193\" \"2194\" \"2195\" \"2196\" \"2197\" \"2198\" \"2199\" \"2200\"\n[2201] \"2201\" \"2202\" \"2203\" \"2204\" \"2205\" \"2206\" \"2207\" \"2208\" \"2209\" \"2210\"\n[2211] \"2211\" \"2212\" \"2213\" \"2214\" \"2215\" \"2216\" \"2217\" \"2218\" \"2219\" \"2220\"\n[2221] \"2221\" \"2222\" \"2223\" \"2224\" \"2225\" \"2226\" \"2227\" \"2228\" \"2229\" \"2230\"\n[2231] \"2231\" \"2232\" \"2233\" \"2234\" \"2235\" \"2236\" \"2237\" \"2238\" \"2239\" \"2240\"\n[2241] \"2241\" \"2242\" \"2243\" \"2244\" \"2245\" \"2246\" \"2247\" \"2248\" \"2249\" \"2250\"\n[2251] \"2251\" \"2252\" \"2253\" \"2254\" \"2255\" \"2256\" \"2257\" \"2258\" \"2259\" \"2260\"\n[2261] \"2261\" \"2262\" \"2263\" \"2264\" \"2265\" \"2266\" \"2267\" \"2268\" \"2269\" \"2270\"\n[2271] \"2271\" \"2272\" \"2273\" \"2274\" \"2275\" \"2276\" \"2277\" \"2278\" \"2279\" \"2280\"\n[2281] \"2281\" \"2282\" \"2283\" \"2284\" \"2285\" \"2286\" \"2287\" \"2288\" \"2289\" \"2290\"\n[2291] \"2291\" \"2292\" \"2293\" \"2294\" \"2295\" \"2296\" \"2297\" \"2298\" \"2299\" \"2300\"\n[2301] \"2301\" \"2302\" \"2303\" \"2304\" \"2305\" \"2306\" \"2307\" \"2308\" \"2309\" \"2310\"\n[2311] \"2311\" \"2312\" \"2313\" \"2314\" \"2315\" \"2316\" \"2317\" \"2318\" \"2319\" \"2320\"\n[2321] \"2321\" \"2322\" \"2323\" \"2324\" \"2325\" \"2326\" \"2327\" \"2328\" \"2329\" \"2330\"\n[2331] \"2331\" \"2332\" \"2333\" \"2334\" \"2335\" \"2336\" \"2337\" \"2338\" \"2339\" \"2340\"\n[2341] \"2341\" \"2342\" \"2343\" \"2344\" \"2345\" \"2346\" \"2347\" \"2348\" \"2349\" \"2350\"\n[2351] \"2351\" \"2352\" \"2353\" \"2354\" \"2355\" \"2356\" \"2357\" \"2358\" \"2359\" \"2360\"\n[2361] \"2361\" \"2362\" \"2363\" \"2364\" \"2365\" \"2366\" \"2367\" \"2368\" \"2369\" \"2370\"\n[2371] \"2371\" \"2372\" \"2373\" \"2374\" \"2375\" \"2376\" \"2377\" \"2378\" \"2379\" \"2380\"\n[2381] \"2381\" \"2382\" \"2383\" \"2384\" \"2385\" \"2386\" \"2387\" \"2388\" \"2389\" \"2390\"\n[2391] \"2391\" \"2392\" \"2393\" \"2394\" \"2395\" \"2396\" \"2397\" \"2398\" \"2399\" \"2400\"\n[2401] \"2401\" \"2402\" \"2403\" \"2404\" \"2405\" \"2406\" \"2407\" \"2408\" \"2409\" \"2410\"\n[2411] \"2411\" \"2412\" \"2413\" \"2414\" \"2415\" \"2416\" \"2417\" \"2418\" \"2419\" \"2420\"\n[2421] \"2421\" \"2422\" \"2423\" \"2424\" \"2425\" \"2426\" \"2427\" \"2428\" \"2429\" \"2430\"\n[2431] \"2431\" \"2432\" \"2433\" \"2434\" \"2435\" \"2436\" \"2437\" \"2438\" \"2439\" \"2440\"\n[2441] \"2441\" \"2442\" \"2443\" \"2444\" \"2445\" \"2446\" \"2447\" \"2448\" \"2449\" \"2450\"\n[2451] \"2451\" \"2452\" \"2453\" \"2454\" \"2455\" \"2456\" \"2457\" \"2458\" \"2459\" \"2460\"\n[2461] \"2461\" \"2462\" \"2463\" \"2464\" \"2465\" \"2466\" \"2467\" \"2468\" \"2469\" \"2470\"\n[2471] \"2471\" \"2472\" \"2473\" \"2474\" \"2475\" \"2476\" \"2477\" \"2478\" \"2479\" \"2480\"\n[2481] \"2481\" \"2482\" \"2483\" \"2484\" \"2485\" \"2486\" \"2487\" \"2488\" \"2489\" \"2490\"\n[2491] \"2491\" \"2492\" \"2493\" \"2494\" \"2495\" \"2496\" \"2497\" \"2498\" \"2499\" \"2500\"\n[2501] \"2501\" \"2502\" \"2503\" \"2504\" \"2505\" \"2506\" \"2507\" \"2508\" \"2509\" \"2510\"\n[2511] \"2511\" \"2512\" \"2513\" \"2514\" \"2515\" \"2516\" \"2517\" \"2518\" \"2519\" \"2520\"\n[2521] \"2521\" \"2522\" \"2523\" \"2524\" \"2525\" \"2526\" \"2527\" \"2528\" \"2529\" \"2530\"\n[2531] \"2531\" \"2532\" \"2533\" \"2534\" \"2535\" \"2536\" \"2537\" \"2538\" \"2539\" \"2540\"\n[2541] \"2541\" \"2542\" \"2543\" \"2544\" \"2545\" \"2546\" \"2547\" \"2548\" \"2549\" \"2550\"\n[2551] \"2551\" \"2552\" \"2553\" \"2554\" \"2555\" \"2556\" \"2557\" \"2558\" \"2559\" \"2560\"\n[2561] \"2561\" \"2562\" \"2563\" \"2564\" \"2565\" \"2566\" \"2567\" \"2568\" \"2569\" \"2570\"\n[2571] \"2571\" \"2572\" \"2573\" \"2574\" \"2575\" \"2576\" \"2577\" \"2578\" \"2579\" \"2580\"\n[2581] \"2581\" \"2582\" \"2583\" \"2584\" \"2585\" \"2586\" \"2587\" \"2588\" \"2589\" \"2590\"\n[2591] \"2591\" \"2592\" \"2593\" \"2594\" \"2595\" \"2596\" \"2597\" \"2598\" \"2599\" \"2600\"\n[2601] \"2601\" \"2602\" \"2603\" \"2604\" \"2605\" \"2606\" \"2607\" \"2608\" \"2609\" \"2610\"\n[2611] \"2611\" \"2612\" \"2613\" \"2614\" \"2615\" \"2616\" \"2617\" \"2618\" \"2619\" \"2620\"\n[2621] \"2621\" \"2622\" \"2623\" \"2624\" \"2625\" \"2626\" \"2627\" \"2628\" \"2629\" \"2630\"\n[2631] \"2631\" \"2632\" \"2633\" \"2634\" \"2635\" \"2636\" \"2637\" \"2638\" \"2639\" \"2640\"\n[2641] \"2641\" \"2642\" \"2643\" \"2644\" \"2645\" \"2646\" \"2647\" \"2648\" \"2649\" \"2650\"\n[2651] \"2651\" \"2652\" \"2653\" \"2654\" \"2655\" \"2656\" \"2657\" \"2658\" \"2659\" \"2660\"\n[2661] \"2661\" \"2662\" \"2663\" \"2664\" \"2665\" \"2666\" \"2667\" \"2668\" \"2669\" \"2670\"\n[2671] \"2671\" \"2672\" \"2673\" \"2674\" \"2675\" \"2676\" \"2677\" \"2678\" \"2679\" \"2680\"\n[2681] \"2681\" \"2682\" \"2683\" \"2684\" \"2685\" \"2686\" \"2687\" \"2688\" \"2689\" \"2690\"\n[2691] \"2691\" \"2692\" \"2693\" \"2694\" \"2695\" \"2696\" \"2697\" \"2698\" \"2699\" \"2700\"\n[2701] \"2701\" \"2702\" \"2703\" \"2704\" \"2705\" \"2706\" \"2707\" \"2708\" \"2709\" \"2710\"\n[2711] \"2711\" \"2712\" \"2713\" \"2714\" \"2715\" \"2716\" \"2717\" \"2718\" \"2719\" \"2720\"\n[2721] \"2721\" \"2722\" \"2723\" \"2724\" \"2725\" \"2726\" \"2727\" \"2728\" \"2729\" \"2730\"\n[2731] \"2731\" \"2732\" \"2733\" \"2734\" \"2735\" \"2736\" \"2737\" \"2738\" \"2739\" \"2740\"\n[2741] \"2741\" \"2742\" \"2743\" \"2744\" \"2745\" \"2746\" \"2747\" \"2748\" \"2749\" \"2750\"\n[2751] \"2751\" \"2752\" \"2753\" \"2754\" \"2755\" \"2756\" \"2757\" \"2758\" \"2759\" \"2760\"\n[2761] \"2761\" \"2762\" \"2763\" \"2764\" \"2765\" \"2766\" \"2767\" \"2768\" \"2769\" \"2770\"\n[2771] \"2771\" \"2772\" \"2773\" \"2774\" \"2775\" \"2776\" \"2777\" \"2778\" \"2779\" \"2780\"\n[2781] \"2781\" \"2782\" \"2783\" \"2784\" \"2785\" \"2786\" \"2787\" \"2788\" \"2789\" \"2790\"\n[2791] \"2791\" \"2792\" \"2793\" \"2794\" \"2795\" \"2796\" \"2797\" \"2798\" \"2799\" \"2800\"\n[2801] \"2801\" \"2802\" \"2803\" \"2804\" \"2805\" \"2806\" \"2807\" \"2808\" \"2809\" \"2810\"\n[2811] \"2811\" \"2812\" \"2813\" \"2814\" \"2815\" \"2816\" \"2817\" \"2818\" \"2819\" \"2820\"\n[2821] \"2821\" \"2822\" \"2823\" \"2824\" \"2825\" \"2826\" \"2827\" \"2828\" \"2829\" \"2830\"\n[2831] \"2831\" \"2832\" \"2833\" \"2834\" \"2835\" \"2836\" \"2837\" \"2838\" \"2839\" \"2840\"\n[2841] \"2841\" \"2842\" \"2843\" \"2844\" \"2845\" \"2846\" \"2847\" \"2848\" \"2849\" \"2850\"\n[2851] \"2851\" \"2852\" \"2853\" \"2854\" \"2855\" \"2856\" \"2857\" \"2858\" \"2859\" \"2860\"\n[2861] \"2861\" \"2862\" \"2863\" \"2864\" \"2865\" \"2866\" \"2867\" \"2868\" \"2869\" \"2870\"\n[2871] \"2871\" \"2872\" \"2873\" \"2874\" \"2875\" \"2876\" \"2877\" \"2878\" \"2879\" \"2880\"\n[2881] \"2881\" \"2882\" \"2883\" \"2884\" \"2885\" \"2886\" \"2887\" \"2888\" \"2889\" \"2890\"\n[2891] \"2891\" \"2892\" \"2893\" \"2894\" \"2895\" \"2896\" \"2897\" \"2898\" \"2899\" \"2900\"\n[2901] \"2901\" \"2902\" \"2903\" \"2904\" \"2905\" \"2906\" \"2907\" \"2908\" \"2909\" \"2910\"\n[2911] \"2911\" \"2912\" \"2913\" \"2914\" \"2915\" \"2916\" \"2917\" \"2918\" \"2919\" \"2920\"\n[2921] \"2921\" \"2922\" \"2923\" \"2924\" \"2925\" \"2926\" \"2927\" \"2928\" \"2929\" \"2930\"\n\n\nCode\n# dimension of dataset\n\ndim(railroad) # 2930 observations & 3 columns\n\n\n[1] 2930    3\n\n\nCode\n# number of rows, number of columns\n\n\nnrow(railroad)\n\n\n[1] 2930\n\n\nCode\nncol(railroad)\n\n\n[1] 3\n\n\nCode\n# Describe & Summary for descriptive analysis\n \n #describe from Hmisc Package\n\n  Hmisc::describe(railroad)\n\n\nrailroad \n\n 3  Variables      2930  Observations\n--------------------------------------------------------------------------------\nstate \n       n  missing distinct \n    2930        0       53 \n\nlowest : AE AK AL AP AR, highest: VT WA WI WV WY\n--------------------------------------------------------------------------------\ncounty \n       n  missing distinct \n    2930        0     1709 \n\nlowest : ABBEVILLE ACADIA    ACCOMACK  ADA       ADAIR    \nhighest: YOUNG     YUBA      YUMA      ZAPATA    ZAVALA   \n--------------------------------------------------------------------------------\ntotal_employees \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    2930        0      404    0.999    87.18    133.1      2.0      3.0 \n     .25      .50      .75      .90      .95 \n     7.0     21.0     65.0    193.0    357.5 \n\nlowest :    1    2    3    4    5, highest: 3249 3685 3797 4235 8207\n--------------------------------------------------------------------------------\n\n\nCode\n  #describe from psych Pakcage\n\n  psych::describe(railroad)\n\n\n                vars    n   mean     sd median trimmed    mad min  max range\nstate*             1 2930  28.72  14.03   28.0   28.91  17.79   1   53    52\ncounty*            2 2930 859.58 478.85  858.5  857.74 592.30   1 1709  1708\ntotal_employees    3 2930  87.18 283.64   21.0   37.41  25.20   1 8207  8206\n                 skew kurtosis   se\nstate*           0.00    -1.15 0.26\ncounty*          0.03    -1.11 8.85\ntotal_employees 13.42   283.73 5.24\n\n\nCode\n  # summary\n\n  summary(railroad)\n\n\n     state             county     total_employees  \n TX     : 221   WASHINGTON:  31   Min.   :   1.00  \n GA     : 152   JEFFERSON :  26   1st Qu.:   7.00  \n KY     : 119   FRANKLIN  :  24   Median :  21.00  \n MO     : 115   LINCOLN   :  24   Mean   :  87.18  \n IL     : 103   JACKSON   :  22   3rd Qu.:  65.00  \n IA     :  99   MADISON   :  19   Max.   :8207.00  \n (Other):2121   (Other)   :2784"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge1.html",
    "href": "posts/SaaradhaaM_Challenge1.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge1.html#challenge-overview",
    "href": "posts/SaaradhaaM_Challenge1.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to read in a dataset and describe the dataset using both words and any supporting information."
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge1.html#read-in-the-data",
    "href": "posts/SaaradhaaM_Challenge1.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nI will be working with the wild bird dataset.\n\n\nCode\n# Load readxl package.\nlibrary(readxl)\n#Read in and view the dataset.\nwildbird <- read_excel(\"_data/wild_bird_data.xlsx\")\nview(wildbird)"
  },
  {
    "objectID": "posts/SaaradhaaM_Challenge1.html#describe-the-data",
    "href": "posts/SaaradhaaM_Challenge1.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, our task is to provide a high level description of the data.\n\n\nCode\n# Run dim() to get the number of cases.\ndim(wildbird)\n\n\n[1] 147   2\n\n\nCode\n# There are 147 cases and 2 columns in this dataset.\n# Run view() to see what these 2 columns are.\nview(wildbird)\n\n\nThere are 147 cases in 2 columns, which are Wet Body Weight (g) and Population Size (but these are in the rows and need to be renamed). Additionally, viewing the dataset shows that there are no missing cases.\nFrom one of the columns, I can see that the data was taken from Figure 1 of a paper written by Nee and colleagues (finding this paper will probably tell me which country this data is from). The column names also show that the data was probably collected via field research with wild birds.\n\n\nCode\n#Rename columns.\nlibrary(dplyr)\nwildbird_new <- rename(wildbird, \"wet_body_weight\" = \"Reference\", \"pop_size\" = \"Taken from Figure 1 of Nee et al.\")\n#Remove the first row of data.\nwildbird_new <- wildbird_new[-1,]\n#Check that the cleaning was done correctly.\nview(wildbird_new)\n#Check the number of cases again.\ndim(wildbird_new)\n\n\n[1] 146   2\n\n\nNow that the columns are renamed and the first row is removed, we see that the true number of cases is 146.\n\n\nCode\n# Let's check the descriptive statistics.\nsummary(wildbird_new)\n\n\n wet_body_weight      pop_size        \n Length:146         Length:146        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\nCode\n# The data is in characters, so we need to convert it to numbers.\nwildbird_new$wet_body_weight <- as.numeric(wildbird_new$wet_body_weight)\nwildbird_new$pop_size <- as.numeric(wildbird_new$pop_size)\n\n\n\n\nCode\n# Now let's check the descriptive statistics again.\nlibrary(dplyr)\nsummary(wildbird_new)\n\n\n wet_body_weight       pop_size      \n Min.   :   5.459   Min.   :      5  \n 1st Qu.:  18.620   1st Qu.:   1821  \n Median :  69.232   Median :  24353  \n Mean   : 363.694   Mean   : 382874  \n 3rd Qu.: 309.826   3rd Qu.: 198515  \n Max.   :9639.845   Max.   :5093378  \n\n\nThe mean wet body weight of the wild birds analysed was about 364g, and the mean population size was close to 383000. There was also a wide range of entries in both variables. Now, let’s check if they’re correlated.\n\n\nCode\n# Running correlation.\ncor(wildbird_new$wet_body_weight,wildbird_new$pop_size)\n\n\n[1] -0.1162993\n\n\nCode\nsummary(lm(wildbird_new$wet_body_weight~wildbird_new$pop_size))\n\n\n\nCall:\nlm(formula = wildbird_new$wet_body_weight ~ wildbird_new$pop_size)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-400.1 -369.5 -275.6   -0.4 9230.6 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            4.097e+02  8.748e+01   4.683 6.47e-06 ***\nwildbird_new$pop_size -1.202e-04  8.552e-05  -1.405    0.162    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 980.3 on 144 degrees of freedom\nMultiple R-squared:  0.01353,   Adjusted R-squared:  0.006675 \nF-statistic: 1.974 on 1 and 144 DF,  p-value: 0.1621\n\n\nThey are quite weakly correlated."
  },
  {
    "objectID": "posts/challenge3_QuinnHe.html",
    "href": "posts/challenge3_QuinnHe.html",
    "title": "Challenge 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_QuinnHe.html#note",
    "href": "posts/challenge3_QuinnHe.html#note",
    "title": "Challenge 3",
    "section": "Note",
    "text": "Note\nGo to the “Eggs” label if you don’t want to see me fumble through a harder data set."
  },
  {
    "objectID": "posts/challenge3_QuinnHe.html#read-in-data",
    "href": "posts/challenge3_QuinnHe.html#read-in-data",
    "title": "Challenge 3",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\nhousehold <-read_excel(\"_data/USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\", \n                skip = 4)\n\nhousehold\n\n\n# A tibble: 383 × 16\n   ...1      ...2  Total Under…¹ $15,0…² $25,0…³ $35,0…⁴ $50,0…⁵ $75,0…⁶ $100,…⁷\n   <chr>     <chr> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 ALL RACES <NA>     NA    NA      NA      NA      NA      NA      NA      NA  \n 2 2019      1284…   100     9.1     8       8.3    11.7    16.5    12.3    15.5\n 3 2018      1285…   100    10.1     8.8     8.7    12      17      12.5    15  \n 4 2017 2    1276…   100    10       9.1     9.2    12      16.4    12.4    14.7\n 5 2017      1275…   100    10.1     9.1     9.2    11.9    16.3    12.6    14.8\n 6 2016      1262…   100    10.4     9       9.2    12.3    16.7    12.2    15  \n 7 2015      1258…   100    10.6    10       9.6    12.1    16.1    12.4    14.9\n 8 2014      1245…   100    11.4    10.5     9.6    12.6    16.4    12.1    14  \n 9 2013 3    1239…   100    11.4    10.3     9.5    12.5    16.8    12      13.9\n10 2013 4    1229…   100    11.3    10.4     9.7    13.1    17      12.5    13.6\n# … with 373 more rows, 6 more variables: `$150,000\\r\\nto\\r\\n$199,999` <dbl>,\n#   `$200,000 and over` <dbl>, Estimate...13 <dbl>,\n#   `Margin of error1 (±)...14` <dbl>, Estimate...15 <chr>,\n#   `Margin of error1 (±)...16` <chr>, and abbreviated variable names\n#   ¹​`Under $15,000`, ²​`$15,000\\r\\nto\\r\\n$24,999`, ³​`$25,000\\r\\nto\\r\\n$34,999`,\n#   ⁴​`$35,000\\r\\nto\\r\\n$49,999`, ⁵​`$50,000\\r\\nto\\r\\n$74,999`,\n#   ⁶​`$75,000\\r\\nto\\r\\n$99,999`, ⁷​`$100,000\\r\\nto\\r\\n$149,999`\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\nBriefly describe the data\nThis is a data set that contains household income by race for homeowners from 1967 to 2019. The data set is a mess with the first column containing the years listed in descending order followed by the next race in the data set. Also, the columns are listed so poorly that I had to skip some in the read-in section. One thing I notice just by combing through the data set, Asian Americans have a much higher number of people making over $200,000 in household income, but this excludes Asian Pacific Islanders. Notes at the bottom also have to be removed. Before I can really see any trends in the data, I need to clean it and organize it. Right now we do not have tidy data!\nBelow, the column names do not make any sense and it is clearly not tidy. By changing some of the names, I hope to make it easier to manipulate later on.\n\n\nCode\ncolnames(household)\n\n\n [1] \"...1\"                       \"...2\"                      \n [3] \"Total\"                      \"Under $15,000\"             \n [5] \"$15,000\\r\\nto\\r\\n$24,999\"   \"$25,000\\r\\nto\\r\\n$34,999\"  \n [7] \"$35,000\\r\\nto\\r\\n$49,999\"   \"$50,000\\r\\nto\\r\\n$74,999\"  \n [9] \"$75,000\\r\\nto\\r\\n$99,999\"   \"$100,000\\r\\nto\\r\\n$149,999\"\n[11] \"$150,000\\r\\nto\\r\\n$199,999\" \"$200,000 and over\"         \n[13] \"Estimate...13\"              \"Margin of error1 (±)...14\" \n[15] \"Estimate...15\"              \"Margin of error1 (±)...16\" \n\n\n\n\nCode\nhousehold <- household %>% \n  rename(\"year\" = \"...1\", \"num_thousands\" = \"...2\", \"median_income\" = \"Estimate...13\", \"mean_income\" = \"Estimate...15\")\n\n\nOkay, so I changed some of the names so it’s a little neater. Now, at least the columns, are easier to read, but there is still the problem of the “year” column. It contains all the races, as well as the years in the data set so it’s much more difficult to look at individual races. There are also random numbers at the end of some of the years.\n\n\nCode\nhousehold <- household %>% \n  mutate(year = str_remove(year, \" [0:28]\"))\n\nhousehold\n\n\n# A tibble: 383 × 16\n   year    num_t…¹ Total Under…² $15,0…³ $25,0…⁴ $35,0…⁵ $50,0…⁶ $75,0…⁷ $100,…⁸\n   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 ALL RA… <NA>       NA    NA      NA      NA      NA      NA      NA      NA  \n 2 2019    128451    100     9.1     8       8.3    11.7    16.5    12.3    15.5\n 3 2018    128579    100    10.1     8.8     8.7    12      17      12.5    15  \n 4 2017    127669    100    10       9.1     9.2    12      16.4    12.4    14.7\n 5 2017    127586    100    10.1     9.1     9.2    11.9    16.3    12.6    14.8\n 6 2016    126224    100    10.4     9       9.2    12.3    16.7    12.2    15  \n 7 2015    125819    100    10.6    10       9.6    12.1    16.1    12.4    14.9\n 8 2014    124587    100    11.4    10.5     9.6    12.6    16.4    12.1    14  \n 9 2013 3  123931    100    11.4    10.3     9.5    12.5    16.8    12      13.9\n10 2013 4  122952    100    11.3    10.4     9.7    13.1    17      12.5    13.6\n# … with 373 more rows, 6 more variables: `$150,000\\r\\nto\\r\\n$199,999` <dbl>,\n#   `$200,000 and over` <dbl>, median_income <dbl>,\n#   `Margin of error1 (±)...14` <dbl>, mean_income <chr>,\n#   `Margin of error1 (±)...16` <chr>, and abbreviated variable names\n#   ¹​num_thousands, ²​`Under $15,000`, ³​`$15,000\\r\\nto\\r\\n$24,999`,\n#   ⁴​`$25,000\\r\\nto\\r\\n$34,999`, ⁵​`$35,000\\r\\nto\\r\\n$49,999`,\n#   ⁶​`$50,000\\r\\nto\\r\\n$74,999`, ⁷​`$75,000\\r\\nto\\r\\n$99,999`, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nIn the above function, I try to remove the excess numbers after the years in the “years” column, but I cannot figure out why the numbers still remain. It’s necessary I remove them in the future, but for now I will push on.\n\n\nSanity Check\nHere are our dimensions for the data before it’s pivoted\n\n\nCode\ndim(household)\n\n\n[1] 383  16\n\n\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\nCode\nhousehold2 <- pivot_longer(household, cols = 4:12,\n                          names_to = \"income\",\n                          values_to = \"count\")\n\nhousehold2\n\n\n# A tibble: 3,447 × 9\n   year      num_thousands Total median_i…¹ Margi…² mean_…³ Margi…⁴ income count\n   <chr>     <chr>         <dbl>      <dbl>   <dbl> <chr>   <chr>   <chr>  <dbl>\n 1 ALL RACES <NA>             NA         NA      NA <NA>    <NA>    \"Unde…  NA  \n 2 ALL RACES <NA>             NA         NA      NA <NA>    <NA>    \"$15,…  NA  \n 3 ALL RACES <NA>             NA         NA      NA <NA>    <NA>    \"$25,…  NA  \n 4 ALL RACES <NA>             NA         NA      NA <NA>    <NA>    \"$35,…  NA  \n 5 ALL RACES <NA>             NA         NA      NA <NA>    <NA>    \"$50,…  NA  \n 6 ALL RACES <NA>             NA         NA      NA <NA>    <NA>    \"$75,…  NA  \n 7 ALL RACES <NA>             NA         NA      NA <NA>    <NA>    \"$100…  NA  \n 8 ALL RACES <NA>             NA         NA      NA <NA>    <NA>    \"$150…  NA  \n 9 ALL RACES <NA>             NA         NA      NA <NA>    <NA>    \"$200…  NA  \n10 2019      128451          100      68703     904 98088   1042    \"Unde…   9.1\n# … with 3,437 more rows, and abbreviated variable names ¹​median_income,\n#   ²​`Margin of error1 (±)...14`, ³​mean_income, ⁴​`Margin of error1 (±)...16`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWell, that turned out horribly. I’m going to leave that mistake here and move on to try and fix that. Clearly, the data is anything but tidy.\nLet’s try that again below.\n\n\nCode\nhousehold3 <- pivot_longer(household, 4:12, names_to = \"income_brackets\", values_to = \"count\")\n\n\n\n\nSwitch to Eggs Data Set\nI’m going to switch data sets because I think I am a little over my head in this data frame, from here on out I will be working with “Eggs”. Below I will run through the assignment quicker than above to save you some already spent time.\n\n\nCode\neggs <- read_excel(\"_data/organiceggpoultry.xls\",\n                   sheet = \"Data\",\n                   skip = 4,\n                   range =cell_limits(c(6,2),c(NA,6)),\n                  col_names = c(\"date\", \"xldozen\", \"xlhalf_dozen\", \"large_dozen\", \"large_half_dozen\"))\n\neggs\n\n\n# A tibble: 120 × 5\n   date      xldozen xlhalf_dozen large_dozen large_half_dozen\n   <chr>       <dbl>        <dbl>       <dbl>            <dbl>\n 1 Jan 2004     230          132         230              126 \n 2 February     230          134.        226.             128.\n 3 March        230          137         225              131 \n 4 April        234.         137         225              131 \n 5 May          236          137         225              131 \n 6 June         241          137         231.             134.\n 7 July         241          137         234.             134.\n 8 August       241          137         234.             134.\n 9 September    241          136.        234.             130.\n10 October      241          136.        234.             128.\n# … with 110 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAbove I read in the data set as an excel file. I had to look at the solutions sheet to learn how to read in the data. The only trouble I had was figuring out the “range” parameter of the function. That one I just had to copy in because I was getting an error without it, but I understand that this tells R which cells to read.\n\n\nCode\neggs %>% \n  select(\"date\")  %>% \n  distinct()\n\n\n# A tibble: 22 × 1\n   date     \n   <chr>    \n 1 Jan 2004 \n 2 February \n 3 March    \n 4 April    \n 5 May      \n 6 June     \n 7 July     \n 8 August   \n 9 September\n10 October  \n# … with 12 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow we still have the issue of notes in the names of our months. This is most evident with “Jan…”, but the ” /1” in February needs to go. This needs to be removed to make analysis later on a bit easier to look at.\n\n\nCode\neggs <- eggs %>% \n  mutate(date = str_remove(date, \" /1\"))\n\n\nNext, the January columns need to be dealt with so below I will remove the years in the “Jan” column with the separate and fill function.\n\n\nCode\neggs <- eggs %>% \n  separate(date, c(\"month\", \"year\"), convert = TRUE) %>% \n  fill(\"year\")\n  \n\neggs\n\n\n# A tibble: 120 × 6\n   month      year xldozen xlhalf_dozen large_dozen large_half_dozen\n   <chr>     <int>   <dbl>        <dbl>       <dbl>            <dbl>\n 1 Jan        2004    230          132         230              126 \n 2 February   2004    230          134.        226.             128.\n 3 March      2004    230          137         225              131 \n 4 April      2004    234.         137         225              131 \n 5 May        2004    236          137         225              131 \n 6 June       2004    241          137         231.             134.\n 7 July       2004    241          137         234.             134.\n 8 August     2004    241          137         234.             134.\n 9 September  2004    241          136.        234.             130.\n10 October    2004    241          136.        234.             128.\n# … with 110 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThere we go. Clean, easy to use data. You can see as the years progress, the price of eggs increase, though at what rate I am uncertain. Now the data set is read for some pivoting. Now let me do a sanity check to view the data dimensions before the pivot and then I will look at it after the pivot.\n\n\nCode\ndim(eggs)\n\n\n[1] 120   6\n\n\nThere are four columns with the type of eggs this data set is viewing. If we want even tidier data we can collapse these four columns into one. Lets call this new data set “eggstidy” to represent the final form of the data in this challenge.\n\n\nCode\neggstidy <- eggs %>% \n  pivot_longer(c(3:6), names_to = \"egg_type\", values_to = \"price\")\n\neggstidy\n\n\n# A tibble: 480 × 4\n   month     year egg_type         price\n   <chr>    <int> <chr>            <dbl>\n 1 Jan       2004 xldozen           230 \n 2 Jan       2004 xlhalf_dozen      132 \n 3 Jan       2004 large_dozen       230 \n 4 Jan       2004 large_half_dozen  126 \n 5 February  2004 xldozen           230 \n 6 February  2004 xlhalf_dozen      134.\n 7 February  2004 large_dozen       226.\n 8 February  2004 large_half_dozen  128.\n 9 March     2004 xldozen           230 \n10 March     2004 xlhalf_dozen      137 \n# … with 470 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nFinally! After banging my head against the wall with the household data set, reading in, cleaning, and pivoting this easier data set was much more manageable. It was helpful to look at the solution to how to read in this data set, but after that, it was easy to move on my own.\nBy pivoting the data, each row has one observation, making it tidy for future manipulation. Within each row we can look at the particular variables within that observation with ease.\nBelow the dimensions have clearly changed, adding significantly more rows, but also condensing the amount of columns.\n\n\nCode\ndim(eggstidy)\n\n\n[1] 480   4"
  },
  {
    "objectID": "posts/challenge4_instructions.html",
    "href": "posts/challenge4_instructions.html",
    "title": "Challenge 4 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_instructions.html#challenge-overview",
    "href": "posts/challenge4_instructions.html#challenge-overview",
    "title": "Challenge 4 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_instructions.html#read-in-data",
    "href": "posts/challenge4_instructions.html#read-in-data",
    "title": "Challenge 4 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ⭐\npoultry_tidy.csv⭐⭐\nFedFundsRate.csv⭐⭐⭐\nhotel_bookings.csv⭐⭐⭐⭐\ndebt_in_trillions ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "title": "Challenge 4 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Instructions",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge4_ManiShankerKamarapu.html",
    "href": "posts/challenge4_ManiShankerKamarapu.html",
    "title": "Challenge 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_ManiShankerKamarapu.html#read-in-data",
    "href": "posts/challenge4_ManiShankerKamarapu.html#read-in-data",
    "title": "Challenge 4",
    "section": "Read in data",
    "text": "Read in data\nThe data set is about the debt in trillions in different years and quarters and various types of loans. It gives the detailed description and data from 2003-2021 in four quarters.\n\n\nCode\nDebt <- read_excel(\"_data/debt_in_trillions.xlsx\") %>%\n  select(!Total)\nDebt\n\n\n# A tibble: 74 × 7\n   `Year and Quarter` Mortgage `HE Revolving` `Auto Loan` Credit…¹ Stude…² Other\n   <chr>                 <dbl>          <dbl>       <dbl>    <dbl>   <dbl> <dbl>\n 1 03:Q1                  4.94          0.242       0.641    0.688   0.241 0.478\n 2 03:Q2                  5.08          0.26        0.622    0.693   0.243 0.486\n 3 03:Q3                  5.18          0.269       0.684    0.693   0.249 0.477\n 4 03:Q4                  5.66          0.302       0.704    0.698   0.253 0.449\n 5 04:Q1                  5.84          0.328       0.72     0.695   0.260 0.446\n 6 04:Q2                  5.97          0.367       0.743    0.697   0.263 0.423\n 7 04:Q3                  6.21          0.426       0.751    0.706   0.33  0.41 \n 8 04:Q4                  6.36          0.468       0.728    0.717   0.346 0.423\n 9 05:Q1                  6.51          0.502       0.725    0.71    0.364 0.394\n10 05:Q2                  6.70          0.528       0.774    0.717   0.374 0.402\n# … with 64 more rows, and abbreviated variable names ¹​`Credit Card`,\n#   ²​`Student Loan`\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nBriefly describe the data\n\n\nCode\ndim(Debt)\n\n\n[1] 74  7\n\n\nThis will give us the dimensions of the data set and which is 74 rows and 7 columns.\n\n\nCode\nsummary(Debt)\n\n\n Year and Quarter      Mortgage       HE Revolving      Auto Loan     \n Length:74          Min.   : 4.942   Min.   :0.2420   Min.   :0.6220  \n Class :character   1st Qu.: 8.036   1st Qu.:0.4275   1st Qu.:0.7430  \n Mode  :character   Median : 8.412   Median :0.5165   Median :0.8145  \n                    Mean   : 8.274   Mean   :0.5161   Mean   :0.9309  \n                    3rd Qu.: 9.047   3rd Qu.:0.6172   3rd Qu.:1.1515  \n                    Max.   :10.442   Max.   :0.7140   Max.   :1.4150  \n  Credit Card      Student Loan        Other       \n Min.   :0.6590   Min.   :0.2407   Min.   :0.2960  \n 1st Qu.:0.6966   1st Qu.:0.5333   1st Qu.:0.3414  \n Median :0.7375   Median :0.9088   Median :0.3921  \n Mean   :0.7565   Mean   :0.9189   Mean   :0.3831  \n 3rd Qu.:0.8165   3rd Qu.:1.3022   3rd Qu.:0.4154  \n Max.   :0.9270   Max.   :1.5840   Max.   :0.4860  \n\n\nThe above observation helps us with the mean values of the debts and we can get an overall grasp of the situation through years. And further we can also draw a plot representing the mean values and type of debt."
  },
  {
    "objectID": "posts/challenge4_ManiShankerKamarapu.html#tidy-data-as-needed",
    "href": "posts/challenge4_ManiShankerKamarapu.html#tidy-data-as-needed",
    "title": "Challenge 4",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nAs you can observe that the data set is still untidy so we use two important functions to make it tidy, those are pivot longer and pivot wider. And also first I will use separate function to split year an quarter into two separate columns. Now we use pivot_longer() to convert types of debt variables into single column so we can have each observation for each row. And at last convert quarter rows into columns so we can have each variable for each column.\n\n\nCode\nDebt <- Debt %>%\n  separate(\"Year and Quarter\", into = c(\"Year\", \"Quarter\"), \":\") %>%\n  pivot_longer(Mortgage:Other, names_to = \"Type of loan\", values_to = \"Amount\") %>%\n  pivot_wider(names_from = \"Quarter\", values_from = \"Amount\")\nDebt\n\n\n# A tibble: 114 × 6\n   Year  `Type of loan`    Q1    Q2    Q3    Q4\n   <chr> <chr>          <dbl> <dbl> <dbl> <dbl>\n 1 03    Mortgage       4.94  5.08  5.18  5.66 \n 2 03    HE Revolving   0.242 0.26  0.269 0.302\n 3 03    Auto Loan      0.641 0.622 0.684 0.704\n 4 03    Credit Card    0.688 0.693 0.693 0.698\n 5 03    Student Loan   0.241 0.243 0.249 0.253\n 6 03    Other          0.478 0.486 0.477 0.449\n 7 04    Mortgage       5.84  5.97  6.21  6.36 \n 8 04    HE Revolving   0.328 0.367 0.426 0.468\n 9 04    Auto Loan      0.72  0.743 0.751 0.728\n10 04    Credit Card    0.695 0.697 0.706 0.717\n# … with 104 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\ndim(Debt)\n\n\n[1] 114   6\n\n\nCode\nsummary(Debt)\n\n\n     Year           Type of loan             Q1                Q2         \n Length:114         Length:114         Min.   : 0.2407   Min.   : 0.2429  \n Class :character   Class :character   1st Qu.: 0.4614   1st Qu.: 0.4585  \n Mode  :character   Mode  :character   Median : 0.7170   Median : 0.7340  \n                                       Mean   : 1.9518   Mean   : 1.9661  \n                                       3rd Qu.: 1.2190   3rd Qu.: 1.2260  \n                                       Max.   :10.1600   Max.   :10.4420  \n                                                                          \n       Q3               Q4         \n Min.   :0.2488   Min.   : 0.2529  \n 1st Qu.:0.4660   1st Qu.: 0.4718  \n Median :0.7316   Median : 0.7313  \n Mean   :1.9571   Mean   : 1.9783  \n 3rd Qu.:1.2055   3rd Qu.: 1.2237  \n Max.   :9.8610   Max.   :10.0430  \n NA's   :6        NA's   :6        \n\n\nThe final dimensions of the tidy data set are 114 rows and 6 columns."
  },
  {
    "objectID": "about/Rosemary.html",
    "href": "about/Rosemary.html",
    "title": "Rosemary",
    "section": "",
    "text": "PSU Political Science and Social Data Analytics"
  },
  {
    "objectID": "about/Rosemary.html#r-experience",
    "href": "about/Rosemary.html#r-experience",
    "title": "Rosemary",
    "section": "R experience",
    "text": "R experience\nUse R in research and teach R programming"
  },
  {
    "objectID": "about/Rosemary.html#research-interests",
    "href": "about/Rosemary.html#research-interests",
    "title": "Rosemary",
    "section": "Research interests",
    "text": "Research interests\nAuthoritarian politics, Survey, Text as data"
  },
  {
    "objectID": "about/Rosemary.html#hometown",
    "href": "about/Rosemary.html#hometown",
    "title": "Rosemary",
    "section": "Hometown",
    "text": "Hometown\nTianjin, China"
  },
  {
    "objectID": "about/Rosemary.html#hobbies",
    "href": "about/Rosemary.html#hobbies",
    "title": "Rosemary",
    "section": "Hobbies",
    "text": "Hobbies\nHorseback riding, journaling, cooking"
  },
  {
    "objectID": "about/Rosemary.html#fun-fact",
    "href": "about/Rosemary.html#fun-fact",
    "title": "Rosemary",
    "section": "Fun fact",
    "text": "Fun fact\nI’m a zoo parent of an African lion."
  },
  {
    "objectID": "about/WillMunson.html",
    "href": "about/WillMunson.html",
    "title": "Will Munson",
    "section": "",
    "text": "UMass Amherst - BS in Math (Concentration in statistics) Montgomery College Rockville - AS in Math Cogentiv Solutions Llc. - Database Management World Data Science Institute - Data Science Researcher"
  },
  {
    "objectID": "about/WillMunson.html#r-experience",
    "href": "about/WillMunson.html#r-experience",
    "title": "Will Munson",
    "section": "R experience",
    "text": "R experience\nR is pretty much my favorite language. I was introduced to R before I graduated high school, and started actively pursuing it by the time I started coming to UMass Amherst."
  },
  {
    "objectID": "about/WillMunson.html#research-interests",
    "href": "about/WillMunson.html#research-interests",
    "title": "Will Munson",
    "section": "Research interests",
    "text": "Research interests\nMy research interests are actually very broad, but lately, my interests are leaning more towards environmental and agricultural research. I also have interests in what affects a city’s median household income, or the wellbeing of people who live in those cities."
  },
  {
    "objectID": "about/WillMunson.html#hometown",
    "href": "about/WillMunson.html#hometown",
    "title": "Will Munson",
    "section": "Hometown",
    "text": "Hometown\nHaydenville, MA"
  },
  {
    "objectID": "about/WillMunson.html#hobbies",
    "href": "about/WillMunson.html#hobbies",
    "title": "Will Munson",
    "section": "Hobbies",
    "text": "Hobbies\n\nPainting\nDrawing\nMagnet fishing\nVideo games"
  },
  {
    "objectID": "about/WillMunson.html#fun-fact",
    "href": "about/WillMunson.html#fun-fact",
    "title": "Will Munson",
    "section": "Fun fact",
    "text": "Fun fact\nI used to be Mormon, but I left before graduating high school."
  },
  {
    "objectID": "about/JerinJacob.html",
    "href": "about/JerinJacob.html",
    "title": "Jerin Jacob",
    "section": "",
    "text": "Work: Business Consultant, Freelance (India) - March 2020 to July 2022 Managing Director, Cresoul Trading & Logistics Pvt Ltd (India) - November 2014 to December 2019 Marketing Specialist, Eastern Hazar (Saudi Arabia) - October 2012 to October 2014 Marketing Manager, Sookshamtech Integral Techno Solutions Pvt Ltd (India) - May 2011 to April 2012 Assistant Store Manager, PJ Electronics & Home Appliance (India) - June 2008 to May 2009\nEducation: MBA with Marketing & Finance Specialization, University of Kerala - 2009 to 2011 Bachelor’s degree in Physics, Mahatma Gandhi University - 2005 to 2008"
  },
  {
    "objectID": "about/JerinJacob.html#r-experience",
    "href": "about/JerinJacob.html#r-experience",
    "title": "Jerin Jacob",
    "section": "R experience",
    "text": "R experience\nBeginner"
  },
  {
    "objectID": "about/JerinJacob.html#research-interests",
    "href": "about/JerinJacob.html#research-interests",
    "title": "Jerin Jacob",
    "section": "Research interests",
    "text": "Research interests\nUse of economic, political, geographical and meteorological data to predict the demand and supply of products and services, Effectiveness of marketing and PR in business and politics, based on analyzing patterns of social media usage and other media exposure of a population, Effect of different media and public responses of leaders/ famous personalities in the decision making of common people, Business decision-making based on data"
  },
  {
    "objectID": "about/JerinJacob.html#hometown",
    "href": "about/JerinJacob.html#hometown",
    "title": "Jerin Jacob",
    "section": "Hometown",
    "text": "Hometown\nKottayam, Kerala"
  },
  {
    "objectID": "about/JerinJacob.html#hobbies",
    "href": "about/JerinJacob.html#hobbies",
    "title": "Jerin Jacob",
    "section": "Hobbies",
    "text": "Hobbies\nmusic, traveling, farming"
  },
  {
    "objectID": "about/JerinJacob.html#fun-fact",
    "href": "about/JerinJacob.html#fun-fact",
    "title": "Jerin Jacob",
    "section": "Fun fact",
    "text": "Fun fact\nWhen I am in stress, I love to sleep ;)"
  },
  {
    "objectID": "about/MaddiHertz.html",
    "href": "about/MaddiHertz.html",
    "title": "Maddi Hertz",
    "section": "",
    "text": "I am a lecturer, academic advisor, and general do-it-all person for the DACSS program. I was a part of the first cohort of DACSS students who started Fall 2020. I graduated this May and am thrilled to be able to continue working with everyone on the DACSS team.\nM.S. DACSS - May 2022\nB.A. in Economics & American Studies from UMass Lowell"
  },
  {
    "objectID": "about/MaddiHertz.html#r-experience",
    "href": "about/MaddiHertz.html#r-experience",
    "title": "Maddi Hertz",
    "section": "R experience",
    "text": "R experience\nI learned a lot during my two years in the program, but there is so much more to learn. The RStudio Conference has really thrown me for a loop; things change faster than I can keep up.\nThe next big goal is to get comfortable using tidymodels and associated packages."
  },
  {
    "objectID": "about/MaddiHertz.html#research-interests",
    "href": "about/MaddiHertz.html#research-interests",
    "title": "Maddi Hertz",
    "section": "Research interests",
    "text": "Research interests\nToo indecisive to identify my own research interests, but I’d love to give you my two cents about yours."
  },
  {
    "objectID": "about/MaddiHertz.html#hometown",
    "href": "about/MaddiHertz.html#hometown",
    "title": "Maddi Hertz",
    "section": "Hometown",
    "text": "Hometown\nAndover, MA"
  },
  {
    "objectID": "about/MaddiHertz.html#hobbies",
    "href": "about/MaddiHertz.html#hobbies",
    "title": "Maddi Hertz",
    "section": "Hobbies",
    "text": "Hobbies\nbeing stressed"
  },
  {
    "objectID": "about/MaddiHertz.html#fun-fact",
    "href": "about/MaddiHertz.html#fun-fact",
    "title": "Maddi Hertz",
    "section": "Fun fact",
    "text": "Fun fact\nThe TV show The Sopranos was named after my family (supposedly—at least according to some distant cousin)."
  },
  {
    "objectID": "about/KaushikaPotluri.html",
    "href": "about/KaushikaPotluri.html",
    "title": "KaushikaPotluri",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/KaushikaPotluri.html#educationwork-background",
    "href": "about/KaushikaPotluri.html#educationwork-background",
    "title": "KaushikaPotluri",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nI completed my undergraduate in Computer Science and Engineering at Bennett University, Delhi, India. I have work experience of 2 years now"
  },
  {
    "objectID": "about/KaushikaPotluri.html#r-experience",
    "href": "about/KaushikaPotluri.html#r-experience",
    "title": "KaushikaPotluri",
    "section": "R experience",
    "text": "R experience\nI am kind of new to R but I am trying to get there"
  },
  {
    "objectID": "about/KaushikaPotluri.html#research-interests",
    "href": "about/KaushikaPotluri.html#research-interests",
    "title": "KaushikaPotluri",
    "section": "Research interests",
    "text": "Research interests\nMy research interests are statistical analysis on data"
  },
  {
    "objectID": "about/KaushikaPotluri.html#hometown",
    "href": "about/KaushikaPotluri.html#hometown",
    "title": "KaushikaPotluri",
    "section": "Hometown",
    "text": "Hometown\nMy hometown is Hyderabad, India."
  },
  {
    "objectID": "about/KaushikaPotluri.html#hobbies",
    "href": "about/KaushikaPotluri.html#hobbies",
    "title": "KaushikaPotluri",
    "section": "Hobbies",
    "text": "Hobbies\nI love playing tennis and watching basically all sports (mostly basketball). I am into photography."
  },
  {
    "objectID": "about/KaushikaPotluri.html#fun-fact",
    "href": "about/KaushikaPotluri.html#fun-fact",
    "title": "KaushikaPotluri",
    "section": "Fun fact",
    "text": "Fun fact\nI am really short for someone who loves basketball!"
  },
  {
    "objectID": "about/TylerTewksbury.html",
    "href": "about/TylerTewksbury.html",
    "title": "Tyler Tewksbury",
    "section": "",
    "text": "2022 graduate with a B.A. in Economics from UMass Amherst. Worked as a Business Systems Analyst Intern at Epsilon in the Summer of 2022, and have been a DACSS Student Employee for over a year (Social Media currently, previously Instructional Design)."
  },
  {
    "objectID": "about/TylerTewksbury.html#r-experience",
    "href": "about/TylerTewksbury.html#r-experience",
    "title": "Tyler Tewksbury",
    "section": "R experience",
    "text": "R experience\nI have experience in R from classwork in Econometrics and Quantitative Research Methods, as well as working on 601 as an Instructional Designer."
  },
  {
    "objectID": "about/TylerTewksbury.html#research-interests",
    "href": "about/TylerTewksbury.html#research-interests",
    "title": "Tyler Tewksbury",
    "section": "Research interests",
    "text": "Research interests\nData Visualization, Autonomous Transportation"
  },
  {
    "objectID": "about/TylerTewksbury.html#hometown",
    "href": "about/TylerTewksbury.html#hometown",
    "title": "Tyler Tewksbury",
    "section": "Hometown",
    "text": "Hometown\nOriginally from Milton Massachusetts, but moved around a lot. Lived in New York, Maine, and now Amherst!"
  },
  {
    "objectID": "about/TylerTewksbury.html#hobbies",
    "href": "about/TylerTewksbury.html#hobbies",
    "title": "Tyler Tewksbury",
    "section": "Hobbies",
    "text": "Hobbies\nWeightlifting, Reading, Video Games, Crossword Puzzles, Geography."
  },
  {
    "objectID": "about/TylerTewksbury.html#fun-fact",
    "href": "about/TylerTewksbury.html#fun-fact",
    "title": "Tyler Tewksbury",
    "section": "Fun fact",
    "text": "Fun fact\nI met the creator of Pokémon in 2015! He sat at the table next to me in a restaurant in Boston."
  },
  {
    "objectID": "about/MirandaManka.html",
    "href": "about/MirandaManka.html",
    "title": "Miranda Manka",
    "section": "",
    "text": "I graduated with a degree in statistics and a minor in sociology from Virginia Tech in May 2022. I have had multiple summer internships including working for the Washington Metropolitan Area Transit Authority (Metro in DC) and for the Alliance for Academic Internal Medicine."
  },
  {
    "objectID": "about/MirandaManka.html#r-experience",
    "href": "about/MirandaManka.html#r-experience",
    "title": "Miranda Manka",
    "section": "R experience",
    "text": "R experience\nI have worked with R for around 4 years. Many of my statistics classes used R throughout my undergraduate degree."
  },
  {
    "objectID": "about/MirandaManka.html#research-interests",
    "href": "about/MirandaManka.html#research-interests",
    "title": "Miranda Manka",
    "section": "Research interests",
    "text": "Research interests\nData Analytics & Visualization, Sociology."
  },
  {
    "objectID": "about/MirandaManka.html#hometown",
    "href": "about/MirandaManka.html#hometown",
    "title": "Miranda Manka",
    "section": "Hometown",
    "text": "Hometown\nAlexandria, VA."
  },
  {
    "objectID": "about/MirandaManka.html#hobbies",
    "href": "about/MirandaManka.html#hobbies",
    "title": "Miranda Manka",
    "section": "Hobbies",
    "text": "Hobbies\nI like bowling, bike riding, traveling, and playing video games."
  },
  {
    "objectID": "about/MirandaManka.html#fun-fact",
    "href": "about/MirandaManka.html#fun-fact",
    "title": "Miranda Manka",
    "section": "Fun fact",
    "text": "Fun fact\nI have been skydiving twice."
  },
  {
    "objectID": "about/Lai_Wei.html",
    "href": "about/Lai_Wei.html",
    "title": "Lai Wei",
    "section": "",
    "text": "-undergraduate: University of Massachusetts-Amherst (2018 - 2022) B.S Mathematics, B.A Economics, B.A Japanese One-year exchange program in Japan -graduate: University of Massachusetts-Amherst (2022 - 2023) Data analysis and Computational Social Science ## R experience I do not have much experience in R except Stat 501, and I learnt JAVA from CS 121, which are all programming experience of me."
  },
  {
    "objectID": "about/Lai_Wei.html#research-interests",
    "href": "about/Lai_Wei.html#research-interests",
    "title": "Lai Wei",
    "section": "Research interests",
    "text": "Research interests\n-Geography, environmental science -Economics"
  },
  {
    "objectID": "about/Lai_Wei.html#hometown",
    "href": "about/Lai_Wei.html#hometown",
    "title": "Lai Wei",
    "section": "Hometown",
    "text": "Hometown\nI was born in China, and came to Virginia in 2017 in grade 12."
  },
  {
    "objectID": "about/Lai_Wei.html#hobbies",
    "href": "about/Lai_Wei.html#hobbies",
    "title": "Lai Wei",
    "section": "Hobbies",
    "text": "Hobbies\nReadings & learning different language and culture"
  },
  {
    "objectID": "about/Lai_Wei.html#fun-fact",
    "href": "about/Lai_Wei.html#fun-fact",
    "title": "Lai Wei",
    "section": "Fun fact",
    "text": "Fun fact\nEven though I spend more time in Massachusetts, Virginia is more like my hometown."
  },
  {
    "objectID": "about/RoyYoon.html",
    "href": "about/RoyYoon.html",
    "title": "Roy Yoon",
    "section": "",
    "text": "UMass Amherst BA Linguistics and Political Science Minor in Spanish MA National Guard"
  },
  {
    "objectID": "about/RoyYoon.html#r-experience",
    "href": "about/RoyYoon.html#r-experience",
    "title": "Roy Yoon",
    "section": "R experience",
    "text": "R experience\nBeginner 2.0"
  },
  {
    "objectID": "about/RoyYoon.html#research-interests",
    "href": "about/RoyYoon.html#research-interests",
    "title": "Roy Yoon",
    "section": "Research interests",
    "text": "Research interests\nharm reduction, international human rights"
  },
  {
    "objectID": "about/RoyYoon.html#hometown",
    "href": "about/RoyYoon.html#hometown",
    "title": "Roy Yoon",
    "section": "Hometown",
    "text": "Hometown\nWorcester, MA"
  },
  {
    "objectID": "about/RoyYoon.html#hobbies",
    "href": "about/RoyYoon.html#hobbies",
    "title": "Roy Yoon",
    "section": "Hobbies",
    "text": "Hobbies\nMusic, running, hair"
  },
  {
    "objectID": "about/RoyYoon.html#fun-fact",
    "href": "about/RoyYoon.html#fun-fact",
    "title": "Roy Yoon",
    "section": "Fun fact",
    "text": "Fun fact\nI get the no.25 from Miss Saigon with extra noodles, sate broth, and the occasional extra meat!"
  },
  {
    "objectID": "about/ZhiyuanZhou.html",
    "href": "about/ZhiyuanZhou.html",
    "title": "Zhiyuan Zhou",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/ZhiyuanZhou.html#educationwork-background",
    "href": "about/ZhiyuanZhou.html#educationwork-background",
    "title": "Zhiyuan Zhou",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nThe Ohio State University, Columbus 2017 - 2021 Bachelor of Science Major in Computer Science & Engineering Minor in Mathematics"
  },
  {
    "objectID": "about/ZhiyuanZhou.html#r-experience",
    "href": "about/ZhiyuanZhou.html#r-experience",
    "title": "Zhiyuan Zhou",
    "section": "R experience",
    "text": "R experience\nIntroductory"
  },
  {
    "objectID": "about/ZhiyuanZhou.html#research-interests",
    "href": "about/ZhiyuanZhou.html#research-interests",
    "title": "Zhiyuan Zhou",
    "section": "Research interests",
    "text": "Research interests\nBig data, AI, Cellular Automaton"
  },
  {
    "objectID": "about/ZhiyuanZhou.html#hometown",
    "href": "about/ZhiyuanZhou.html#hometown",
    "title": "Zhiyuan Zhou",
    "section": "Hometown",
    "text": "Hometown\nChengdu, Sichuan, China"
  },
  {
    "objectID": "about/ZhiyuanZhou.html#hobbies",
    "href": "about/ZhiyuanZhou.html#hobbies",
    "title": "Zhiyuan Zhou",
    "section": "Hobbies",
    "text": "Hobbies\nWalk my Dog, Video Games"
  },
  {
    "objectID": "about/ZhiyuanZhou.html#fun-fact",
    "href": "about/ZhiyuanZhou.html#fun-fact",
    "title": "Zhiyuan Zhou",
    "section": "Fun fact",
    "text": "Fun fact\nAverage Daily Steps > 10k"
  },
  {
    "objectID": "about/AnimeshSengupta.html",
    "href": "about/AnimeshSengupta.html",
    "title": "AnimeshSengupta",
    "section": "",
    "text": "High school : DLF Public School : 92.6%\nB.Tech CSE: Vellore institute of technology: 9.14 CGPA\nIntern->Sr. Software developer : Societe Generale GSC"
  },
  {
    "objectID": "about/AnimeshSengupta.html#r-experience",
    "href": "about/AnimeshSengupta.html#r-experience",
    "title": "AnimeshSengupta",
    "section": "R experience",
    "text": "R experience\nMynR journey began during my undergraduate days. I had learnt intermediary R syntax during that time. Since, I have had no industry experience. I can be considered beginner."
  },
  {
    "objectID": "about/AnimeshSengupta.html#research-interests",
    "href": "about/AnimeshSengupta.html#research-interests",
    "title": "AnimeshSengupta",
    "section": "Research interests",
    "text": "Research interests\nMy research interests include: 1. X-shot deep learning techniques to reduce learning in dearth of dataset 2. Meta agnostic learning and prototype learning and MAML to perform meta data learning 3. Generative adversarial networks to create new data."
  },
  {
    "objectID": "about/AnimeshSengupta.html#hometown",
    "href": "about/AnimeshSengupta.html#hometown",
    "title": "AnimeshSengupta",
    "section": "Hometown",
    "text": "Hometown\nI hail from New Delhi, India"
  },
  {
    "objectID": "about/AnimeshSengupta.html#hobbies",
    "href": "about/AnimeshSengupta.html#hobbies",
    "title": "AnimeshSengupta",
    "section": "Hobbies",
    "text": "Hobbies\nMy hobbies are: 1. Soccer fan - Chelsea for life 2. Numismatics 3. Bibliophile"
  },
  {
    "objectID": "about/AnimeshSengupta.html#fun-fact",
    "href": "about/AnimeshSengupta.html#fun-fact",
    "title": "AnimeshSengupta",
    "section": "Fun fact",
    "text": "Fun fact\nI am great with bonfires"
  },
  {
    "objectID": "about/ProfRolfe.html",
    "href": "about/ProfRolfe.html",
    "title": "Meredith Rolfe",
    "section": "",
    "text": "Associate Professor, Political Science | UMass Amherst Lecturer in Public Management | London School of Economics Nuffield Posdoctoral Prize Fellow | Oxford University\nPhD, Political Science | University of Chicago BA, Comparative Area Studies | Duke University"
  },
  {
    "objectID": "about/ProfRolfe.html#r-experience",
    "href": "about/ProfRolfe.html#r-experience",
    "title": "Meredith Rolfe",
    "section": "R experience",
    "text": "R experience\nI was using R before it was cool (and when it was still called S…)"
  },
  {
    "objectID": "about/ProfRolfe.html#research-interests",
    "href": "about/ProfRolfe.html#research-interests",
    "title": "Meredith Rolfe",
    "section": "Research interests",
    "text": "Research interests\nTheories that combine social interaction and cognitive micro-foundations; innovative applications of methods of any sort (networks, text, simulations, experiments, surveys…)"
  },
  {
    "objectID": "about/ProfRolfe.html#hometown",
    "href": "about/ProfRolfe.html#hometown",
    "title": "Meredith Rolfe",
    "section": "Hometown",
    "text": "Hometown\nCharlotte, NC"
  },
  {
    "objectID": "about/ProfRolfe.html#hobbies",
    "href": "about/ProfRolfe.html#hobbies",
    "title": "Meredith Rolfe",
    "section": "Hobbies",
    "text": "Hobbies\num, DACSS???"
  },
  {
    "objectID": "about/ProfRolfe.html#fun-fact",
    "href": "about/ProfRolfe.html#fun-fact",
    "title": "Meredith Rolfe",
    "section": "Fun fact",
    "text": "Fun fact\ncoffee is my favorite food"
  },
  {
    "objectID": "about/LindsayJones.html",
    "href": "about/LindsayJones.html",
    "title": "Lindsay Jones",
    "section": "",
    "text": "B.A. - Sociology, UC San Diego\nMinor in Communication\nExecutive Assistant of a retirement community from 2019-2022."
  },
  {
    "objectID": "about/LindsayJones.html#r-experience",
    "href": "about/LindsayJones.html#r-experience",
    "title": "Lindsay Jones",
    "section": "R experience",
    "text": "R experience\nYou’re looking at it!"
  },
  {
    "objectID": "about/LindsayJones.html#research-interests",
    "href": "about/LindsayJones.html#research-interests",
    "title": "Lindsay Jones",
    "section": "Research interests",
    "text": "Research interests\nSocial Network Analysis, Digital Behaviors, Social Demography"
  },
  {
    "objectID": "about/LindsayJones.html#hometown",
    "href": "about/LindsayJones.html#hometown",
    "title": "Lindsay Jones",
    "section": "Hometown",
    "text": "Hometown\nSacramento, California"
  },
  {
    "objectID": "about/LindsayJones.html#hobbies",
    "href": "about/LindsayJones.html#hobbies",
    "title": "Lindsay Jones",
    "section": "Hobbies",
    "text": "Hobbies\nLately I’ve enjoyed playing trumpet, playing Redactle, and making ice cream."
  },
  {
    "objectID": "about/LindsayJones.html#fun-fact",
    "href": "about/LindsayJones.html#fun-fact",
    "title": "Lindsay Jones",
    "section": "Fun fact",
    "text": "Fun fact\nI once made the mistake of climbing a waterfall in running shoes."
  },
  {
    "objectID": "about/EmmaRasmussen.html",
    "href": "about/EmmaRasmussen.html",
    "title": "Emma Rasmussen",
    "section": "",
    "text": "I graduated in 2021 with a B.S. in Public Health and Anthropology/Sociology and a minor in Biology from Roger Williams University. Since graduating, I completed a research internship with the Framingham Heart Study for their Brain Aging Program. In between, I have spent a lot of time working in restaurants."
  },
  {
    "objectID": "about/EmmaRasmussen.html#r-experience",
    "href": "about/EmmaRasmussen.html#r-experience",
    "title": "Emma Rasmussen",
    "section": "R experience",
    "text": "R experience\nI have no background in R"
  },
  {
    "objectID": "about/EmmaRasmussen.html#research-interests",
    "href": "about/EmmaRasmussen.html#research-interests",
    "title": "Emma Rasmussen",
    "section": "Research interests",
    "text": "Research interests\nPopulation Health, Demography, Social Determinants of Health, Environmental Health, Disaster Research, Social Movements"
  },
  {
    "objectID": "about/EmmaRasmussen.html#hometown",
    "href": "about/EmmaRasmussen.html#hometown",
    "title": "Emma Rasmussen",
    "section": "Hometown",
    "text": "Hometown\nGroton, MA"
  },
  {
    "objectID": "about/EmmaRasmussen.html#hobbies",
    "href": "about/EmmaRasmussen.html#hobbies",
    "title": "Emma Rasmussen",
    "section": "Hobbies",
    "text": "Hobbies\n\nBaking and cooking: I just got a sourdough starter which I have been experimenting with\nListening to podcasts: My favorite is Iliza Schlesinger’s podcast “Ask Iliza Anything”\nTraveling and exploring new places: I have been to Europe a few times, but since Covid I have been exploring more locally. And I’m a foodie, so I love to eat my way around new places."
  },
  {
    "objectID": "about/EmmaRasmussen.html#fun-fact",
    "href": "about/EmmaRasmussen.html#fun-fact",
    "title": "Emma Rasmussen",
    "section": "Fun fact",
    "text": "Fun fact\nThis past February, I donated stem cells through Be The Match"
  },
  {
    "objectID": "about/nickboonstra_v2.html",
    "href": "about/nickboonstra_v2.html",
    "title": "Nick Boonstra",
    "section": "",
    "text": "I started college at Boston University studying physics. Financial issues kept me from continuing, so I spent a few years in Boston working part-time and taking classes at Bunker Hill Community College. I eventually transferred to UMass Lowell, where I finished with a Bachelor’s in Liberal Arts with concentrations in Political Science and Writing. Professionally, I have experience in a few areas, including banking and journalism."
  },
  {
    "objectID": "about/nickboonstra_v2.html#r-experience",
    "href": "about/nickboonstra_v2.html#r-experience",
    "title": "Nick Boonstra",
    "section": "R experience",
    "text": "R experience\nNone before this summer! In fact, I have very little programming experience in general, beyond a rusty grasp of LaTeX and a bit of dabbling in Python years ago. However, I have been fortunate to have started working for DACSS remotely a couple before arriving in Western Mass, which provided me the opportunity to get a jump start on R."
  },
  {
    "objectID": "about/nickboonstra_v2.html#research-and-career-interests",
    "href": "about/nickboonstra_v2.html#research-and-career-interests",
    "title": "Nick Boonstra",
    "section": "Research and career interests",
    "text": "Research and career interests\nI have a wide range of career interests, from political science professor to broadcast journalist to investment banker. When it comes to political science, my research interest tend to revolve around partisanship and polarization, and particularly around understanding how to bridge the perception gap between opposite partisans in the United States and abroad."
  },
  {
    "objectID": "about/nickboonstra_v2.html#hometown",
    "href": "about/nickboonstra_v2.html#hometown",
    "title": "Nick Boonstra",
    "section": "Hometown",
    "text": "Hometown\nI grew up in Leominster, MA. After living in Boston for a few years, and in Lowell for a few more, I have just moved to Chicopee."
  },
  {
    "objectID": "about/nickboonstra_v2.html#hobbies",
    "href": "about/nickboonstra_v2.html#hobbies",
    "title": "Nick Boonstra",
    "section": "Hobbies",
    "text": "Hobbies\nI am a massive fan of West Ham United Football Club, and follow the English Premier League far more closely than any American league, though I will always be a devoted fan of the Celtics and Red Sox. I also have passing interests in rugby and cricket, and have recently been getting into Formula 1.\nBeyond sports, I’m a big indie music junkie; some of my favorite artists right now include Beirut, Fleet Foxes, and Day Wave, though I’m equally at home with Foo Fighters, Coldplay, and Indigo Girls. When I’m looking to kill time, my go-to is usually my lichess app, or maybe a crossword."
  },
  {
    "objectID": "about/nickboonstra_v2.html#fun-fact",
    "href": "about/nickboonstra_v2.html#fun-fact",
    "title": "Nick Boonstra",
    "section": "Fun fact",
    "text": "Fun fact\nI once took a one-night trip from Boston to London to see a football match! January 29, 2020, WHU 0 - 2 LIV – not a surprising scoreline but still a disappointing one. The Reds dominated the game; the closest we came to scoring was an errant cross from Trent Alexander-Arnold clattering off of his own post. In other words: Yes, we were so bad that not even Liverpool could have scored for us. But it was an incredible experience nonetheless, and I’m so glad I got to go when I did."
  },
  {
    "objectID": "about/AkhileshKumar.html",
    "href": "about/AkhileshKumar.html",
    "title": "Akhilesh Kumar Meghwal",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/AkhileshKumar.html#educationwork-background",
    "href": "about/AkhileshKumar.html#educationwork-background",
    "title": "Akhilesh Kumar Meghwal",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nI am an experienced professional with more than 13 years of progressive multi-domain experience in implementing information & emerging technology solutions on Artificial Intelligence, Machine Learning, Cloud, Geographic Information System, Drones, Internet of Things etc. I have a Post Graduate Diploma in Industrial Management (PGDIM) from National Institute of Industrial Engineering. Mumbai and a Bachelor’s of Technology (B.Tech) from National Institute of Technology, Bhopal. I am a Microsoft Azure Certified Cloud Solution Architect; I have led cloud migration projects, prepared cloud migration strategy for clients by studying existing IT systems and developing cloud solution by considering factors such as: system availability, replication, disaster recovery, total cost of implementation etc. I have designed data systems to implement uniform data standards to enable data-convergence among different Govt. program data and implement cross-functional data analytics to enable data-driven decision-making for the Government. I have led Text Analytics Implementation by training deep learning models for 22+ different Indian vernacular languages, for topic modelling, entity extraction etc. I have led implementation of several smart-city projects in urban areas, such as: AI/ML, GIS, IoT based Flood Early Warning System; Machine Learning based Heatwave Prediction Model; Smart Manhole Monitoring for Hazardous Gas & Water Overflow Detection) etc. My research interest include: Artificial Intelligence, Machine Learning, Natural Language Processing etc."
  },
  {
    "objectID": "about/AkhileshKumar.html#r-experience",
    "href": "about/AkhileshKumar.html#r-experience",
    "title": "Akhilesh Kumar Meghwal",
    "section": "R experience",
    "text": "R experience\nI have done “Data Science: Foundations using R Specialization” on Coursera. I have done few implementations in R, including:\n\nSocial Media Analytics on Twitter Data to develop word cloud, named entity extraction, text classification etc.\nRshiny Dashboard for Inventory Management of Government Residential Schools in Remote Tribals Areas"
  },
  {
    "objectID": "about/AkhileshKumar.html#research-interests",
    "href": "about/AkhileshKumar.html#research-interests",
    "title": "Akhilesh Kumar Meghwal",
    "section": "Research interests",
    "text": "Research interests\nMy research interests include implementation of Artificial Intelligence and Machine Learning in Urban Development, Education and Health Sector."
  },
  {
    "objectID": "about/AkhileshKumar.html#hometown",
    "href": "about/AkhileshKumar.html#hometown",
    "title": "Akhilesh Kumar Meghwal",
    "section": "Hometown",
    "text": "Hometown\nAjmer Rajasthan"
  },
  {
    "objectID": "about/AkhileshKumar.html#hobbies",
    "href": "about/AkhileshKumar.html#hobbies",
    "title": "Akhilesh Kumar Meghwal",
    "section": "Hobbies",
    "text": "Hobbies\nYoga, Gym, Cycling"
  },
  {
    "objectID": "about/AkhileshKumar.html#fun-fact",
    "href": "about/AkhileshKumar.html#fun-fact",
    "title": "Akhilesh Kumar Meghwal",
    "section": "Fun fact",
    "text": "Fun fact\nNormally a man speaks about 2,000 words in a day, I speak 2 M"
  },
  {
    "objectID": "about/KatiePopiela.html",
    "href": "about/KatiePopiela.html",
    "title": "Katie Popiela",
    "section": "",
    "text": "I went to Mount Holyoke College for undergrad and double-majored in History and Politics. I’ve had several admin jobs, but now I work at a dog sanctuary!!"
  },
  {
    "objectID": "about/KatiePopiela.html#r-experience",
    "href": "about/KatiePopiela.html#r-experience",
    "title": "Katie Popiela",
    "section": "R experience",
    "text": "R experience\nThis course last semester"
  },
  {
    "objectID": "about/KatiePopiela.html#research-interests",
    "href": "about/KatiePopiela.html#research-interests",
    "title": "Katie Popiela",
    "section": "Research interests",
    "text": "Research interests\nI wrote my thesis on local-level ethnic and religious violence in times of extreme political instability and I’d like to make that into something much bigger than i could on my own with primary source materials"
  },
  {
    "objectID": "about/KatiePopiela.html#hometown",
    "href": "about/KatiePopiela.html#hometown",
    "title": "Katie Popiela",
    "section": "Hometown",
    "text": "Hometown\nHawthorne, NJ but now I live outside Hartford"
  },
  {
    "objectID": "about/KatiePopiela.html#hobbies",
    "href": "about/KatiePopiela.html#hobbies",
    "title": "Katie Popiela",
    "section": "Hobbies",
    "text": "Hobbies\nPLANTS!! Painting/photography, hanging out with my dog and my fiance"
  },
  {
    "objectID": "about/KatiePopiela.html#fun-fact",
    "href": "about/KatiePopiela.html#fun-fact",
    "title": "Katie Popiela",
    "section": "Fun fact",
    "text": "Fun fact\nI have 3 snakes and one of them is albino"
  },
  {
    "objectID": "about/NJani.html",
    "href": "about/NJani.html",
    "title": "Nayan Jani",
    "section": "",
    "text": "I received my B.S. in Data Science from the University of Rhode Island this past year. Over the summer of 2021 I worked for a startup company called Bora. At Bora, the product they are trying to bring to market is an app based, self service beach chair rental company stationed at the most popular locations across the country. My job was to collect data on certain locations and then run statistical analysis on that collected data to find out which locations are the busiest based on the characteristics of beaches. While working for Bora I was in constant contact with potential clients and the CEO of the company. Working for a startup showed me how a business functions from the ground up and how to interact with clients in a professional manner."
  },
  {
    "objectID": "about/NJani.html#r-experience",
    "href": "about/NJani.html#r-experience",
    "title": "Nayan Jani",
    "section": "R experience",
    "text": "R experience\nI have been working in R and R markdown since my sophomore year of college. I have mostly used it for statistical reports and model building for data sets. I only was introduced to the tidyverse for a short period of time so I will need to practice using its functions more."
  },
  {
    "objectID": "about/NJani.html#research-interests",
    "href": "about/NJani.html#research-interests",
    "title": "Nayan Jani",
    "section": "Research interests",
    "text": "Research interests\nThe topic I explored for my Senior recitation is the use of Machine Learning in estimating the heterogeneous treatment effect using datasets that involve AIDS and breast cancer treatments. More specifically, I looked into certain meta learners that can estimate the conditional average treatment effect so that we can personalize treatment regimes. Here at Umass I hope to learn more about how to apply data science to medical treatments for people with illnesses."
  },
  {
    "objectID": "about/NJani.html#hometown",
    "href": "about/NJani.html#hometown",
    "title": "Nayan Jani",
    "section": "Hometown",
    "text": "Hometown\nWestford, MA"
  },
  {
    "objectID": "about/NJani.html#hobbies",
    "href": "about/NJani.html#hobbies",
    "title": "Nayan Jani",
    "section": "Hobbies",
    "text": "Hobbies\n\nVideo Games\nFantasy Football\nWatching Sports\nPlaying Cards and Board Games"
  },
  {
    "objectID": "about/NJani.html#fun-fact",
    "href": "about/NJani.html#fun-fact",
    "title": "Nayan Jani",
    "section": "Fun fact",
    "text": "Fun fact\nI have over 50 cousins that live in the UK!"
  },
  {
    "objectID": "about/ShoshanaBuck.html",
    "href": "about/ShoshanaBuck.html",
    "title": "Shoshana Buck",
    "section": "",
    "text": "I spent my freshman year of undergrad at Simmons University in Boston and then in 2019 I transferred to UMass Amherst for my sophomore year. I graduated this past May with a B.A in Communication and am continuing my education at UMass with the DACSS program."
  },
  {
    "objectID": "about/ShoshanaBuck.html#r-experience",
    "href": "about/ShoshanaBuck.html#r-experience",
    "title": "Shoshana Buck",
    "section": "R experience",
    "text": "R experience\nI have very limited amount of experience with R. I took a sociology course about data collection and analysis, which introduced me to research design and how data is obtained and analyzed. I used R for my final research which made me interested to learn and use R more."
  },
  {
    "objectID": "about/ShoshanaBuck.html#research-interests",
    "href": "about/ShoshanaBuck.html#research-interests",
    "title": "Shoshana Buck",
    "section": "Research interests",
    "text": "Research interests\nSome of my research interests include digital inequalities, media effects especially in children, and disinformation."
  },
  {
    "objectID": "about/ShoshanaBuck.html#hometown",
    "href": "about/ShoshanaBuck.html#hometown",
    "title": "Shoshana Buck",
    "section": "Hometown",
    "text": "Hometown\nI am originally from Framingham Massachusetts."
  },
  {
    "objectID": "about/ShoshanaBuck.html#hobbies",
    "href": "about/ShoshanaBuck.html#hobbies",
    "title": "Shoshana Buck",
    "section": "Hobbies",
    "text": "Hobbies\nI like to go to the beach, hang out with my family and friends, take my dog on walks, and listen to live music."
  },
  {
    "objectID": "about/ShoshanaBuck.html#fun-fact",
    "href": "about/ShoshanaBuck.html#fun-fact",
    "title": "Shoshana Buck",
    "section": "Fun fact",
    "text": "Fun fact\nA fun fat about me is that my first ever concert that I went to was the Cheetah Girls and Vanessa Hudgens."
  },
  {
    "objectID": "about/ManiShankerKamarapu.html",
    "href": "about/ManiShankerKamarapu.html",
    "title": "ManiShankerKamarapu",
    "section": "",
    "text": "Work: Senior Systems Engineer,Infosys Ltd - December 2019 to July 2022\nEducation: B.E with Electronics and Communication Engineering Specialization, Osmania University - 2015 to 2019"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#r-experience",
    "href": "about/ManiShankerKamarapu.html#r-experience",
    "title": "ManiShankerKamarapu",
    "section": "R experience",
    "text": "R experience\nBeginner"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#research-interests",
    "href": "about/ManiShankerKamarapu.html#research-interests",
    "title": "ManiShankerKamarapu",
    "section": "Research interests",
    "text": "Research interests\nData analytics, data communication & visualization, machine learning, and networks"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#hometown",
    "href": "about/ManiShankerKamarapu.html#hometown",
    "title": "ManiShankerKamarapu",
    "section": "Hometown",
    "text": "Hometown\nKarimnagar, Telangana, India - 505001"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#hobbies",
    "href": "about/ManiShankerKamarapu.html#hobbies",
    "title": "ManiShankerKamarapu",
    "section": "Hobbies",
    "text": "Hobbies\nMusic, playing games and long walks"
  },
  {
    "objectID": "about/ManiShankerKamarapu.html#fun-fact",
    "href": "about/ManiShankerKamarapu.html#fun-fact",
    "title": "ManiShankerKamarapu",
    "section": "Fun fact",
    "text": "Fun fact\nI love to listen music."
  },
  {
    "objectID": "about/KimDarkenwald.html",
    "href": "about/KimDarkenwald.html",
    "title": "Kimberly Darkenwald",
    "section": "",
    "text": "I currently hold a B.A. in Journalism and Mass Communication with an emphasis in public relations. Shortly after graduating college, many years ago, my grandmother fell ill and required around-the-clock. For this reason, I placed my professional life on hold to care for her. Since her passing at the age of 105 years old, I have decided to change course and pursue a career in data analytics."
  },
  {
    "objectID": "about/KimDarkenwald.html#r-experience",
    "href": "about/KimDarkenwald.html#r-experience",
    "title": "Kimberly Darkenwald",
    "section": "R experience",
    "text": "R experience\nThe DACSS 601 class will be my introduction to R and I am looking forward to learning the program."
  },
  {
    "objectID": "about/KimDarkenwald.html#research-interests",
    "href": "about/KimDarkenwald.html#research-interests",
    "title": "Kimberly Darkenwald",
    "section": "Research interests",
    "text": "Research interests\nI have a strong interest in Environmental, Social, Governance (ESG) issues."
  },
  {
    "objectID": "about/KimDarkenwald.html#hometown",
    "href": "about/KimDarkenwald.html#hometown",
    "title": "Kimberly Darkenwald",
    "section": "Hometown",
    "text": "Hometown\nValhalla, NY"
  },
  {
    "objectID": "about/KimDarkenwald.html#hobbies",
    "href": "about/KimDarkenwald.html#hobbies",
    "title": "Kimberly Darkenwald",
    "section": "Hobbies",
    "text": "Hobbies\nPlaying with my dog, sailing, skiing, anything to do with comedy."
  },
  {
    "objectID": "about/KimDarkenwald.html#fun-fact",
    "href": "about/KimDarkenwald.html#fun-fact",
    "title": "Kimberly Darkenwald",
    "section": "Fun fact",
    "text": "Fun fact\nSharks never stop growing."
  },
  {
    "objectID": "about/AdithyaParupudi.html",
    "href": "about/AdithyaParupudi.html",
    "title": "Adithya Parupudi",
    "section": "",
    "text": "save a copy of this document as FirstLast.qmd in the about folder\nreplace Your Name in yaml header with your first and last name\nsave an image (jpeg, png) to use in the about/images folder\nreplace image names in yaml header\nreplace github user name\nfill in information below, as appropriate\ncommit to github and submit pull request"
  },
  {
    "objectID": "about/AdithyaParupudi.html#educationwork-background",
    "href": "about/AdithyaParupudi.html#educationwork-background",
    "title": "Adithya Parupudi",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nI’ve graduated in 2019 with a B.Tech in Computer Science & Engineering (CSE) from JNTU, Hyderabad\nI’ve worked in Tech Mahindra for 2.8 yrs as a Oracle SOA 12c developer and production support engineer."
  },
  {
    "objectID": "about/AdithyaParupudi.html#r-experience",
    "href": "about/AdithyaParupudi.html#r-experience",
    "title": "Adithya Parupudi",
    "section": "R experience",
    "text": "R experience\nI have foundational knowledge in R, gathered from youtube and datacamp."
  },
  {
    "objectID": "about/AdithyaParupudi.html#research-interests",
    "href": "about/AdithyaParupudi.html#research-interests",
    "title": "Adithya Parupudi",
    "section": "Research interests",
    "text": "Research interests\nInterested about Psychology and Social Behaviour"
  },
  {
    "objectID": "about/AdithyaParupudi.html#hometown",
    "href": "about/AdithyaParupudi.html#hometown",
    "title": "Adithya Parupudi",
    "section": "Hometown",
    "text": "Hometown\nI’m from Hyderabad, India"
  },
  {
    "objectID": "about/AdithyaParupudi.html#hobbies",
    "href": "about/AdithyaParupudi.html#hobbies",
    "title": "Adithya Parupudi",
    "section": "Hobbies",
    "text": "Hobbies\nCooking, Anime, Workout, Photography"
  },
  {
    "objectID": "about/AdithyaParupudi.html#fun-fact",
    "href": "about/AdithyaParupudi.html#fun-fact",
    "title": "Adithya Parupudi",
    "section": "Fun fact",
    "text": "Fun fact\nI’m scared of heights and I avoid horror movies. :)"
  },
  {
    "objectID": "about/YoungsooChoi.html",
    "href": "about/YoungsooChoi.html",
    "title": "Young Soo Choi",
    "section": "",
    "text": "I had majored in Sociology and then worked for Korean Government over 10 years."
  },
  {
    "objectID": "about/YoungsooChoi.html#r-experience",
    "href": "about/YoungsooChoi.html#r-experience",
    "title": "Young Soo Choi",
    "section": "R experience",
    "text": "R experience\nNone. I have no idea about R at all."
  },
  {
    "objectID": "about/YoungsooChoi.html#research-interests",
    "href": "about/YoungsooChoi.html#research-interests",
    "title": "Young Soo Choi",
    "section": "Research interests",
    "text": "Research interests\nPublic Safety and disaster management."
  },
  {
    "objectID": "about/YoungsooChoi.html#hometown",
    "href": "about/YoungsooChoi.html#hometown",
    "title": "Young Soo Choi",
    "section": "Hometown",
    "text": "Hometown\nvarious cities in the Korea"
  },
  {
    "objectID": "about/YoungsooChoi.html#hobbies",
    "href": "about/YoungsooChoi.html#hobbies",
    "title": "Young Soo Choi",
    "section": "Hobbies",
    "text": "Hobbies\nMusic"
  },
  {
    "objectID": "about/YoungsooChoi.html#fun-fact",
    "href": "about/YoungsooChoi.html#fun-fact",
    "title": "Young Soo Choi",
    "section": "Fun fact",
    "text": "Fun fact\nI have two sons."
  },
  {
    "objectID": "about/QuinnHe.html",
    "href": "about/QuinnHe.html",
    "title": "Quinn He",
    "section": "",
    "text": "I recently graduated from the University of Massachusetts Amherst in 2021 with a BA in English and a minor in Psychology. Afterwards, I worked for one year in retail while taking courses for the DACSS certificate. I wanted to further my education and applied to the DACSS MS program, which I am starting this Fall 2022."
  },
  {
    "objectID": "about/QuinnHe.html#r-experience",
    "href": "about/QuinnHe.html#r-experience",
    "title": "Quinn He",
    "section": "R experience",
    "text": "R experience\nI have beginner/intermediate experience in R. For the year year I was working, I would study R in my spare time to keep my skills sharp. I still have much to learn when it comes to R/RStudio"
  },
  {
    "objectID": "about/QuinnHe.html#research-interests",
    "href": "about/QuinnHe.html#research-interests",
    "title": "Quinn He",
    "section": "Research interests",
    "text": "Research interests\nI have not fully honed in my research interests, but I hope to come out of the program with extensive knowledge in the technical skills of data analysis and research."
  },
  {
    "objectID": "about/QuinnHe.html#hometown",
    "href": "about/QuinnHe.html#hometown",
    "title": "Quinn He",
    "section": "Hometown",
    "text": "Hometown\nWayne, NJ"
  },
  {
    "objectID": "about/QuinnHe.html#hobbies",
    "href": "about/QuinnHe.html#hobbies",
    "title": "Quinn He",
    "section": "Hobbies",
    "text": "Hobbies\nI enjoy playing guitar, biking, reading, and cooking."
  },
  {
    "objectID": "about/QuinnHe.html#fun-fact",
    "href": "about/QuinnHe.html#fun-fact",
    "title": "Quinn He",
    "section": "Fun fact",
    "text": "Fun fact\nI met Jesse Eisenberg at an empty movie theater in NYC."
  },
  {
    "objectID": "about/AnanyaPujary.html",
    "href": "about/AnanyaPujary.html",
    "title": "Ananya Pujary",
    "section": "",
    "text": "I received a Bachelor of Arts degree in Psychology with a minor in Literary and Cultural Studies, and a Post-Graduate Diploma in Interdisciplinary Studies and Research from FLAME University, India. I learnt Python in some courses I’d taken, and have some experience with SQL and Java NetBeans from high school too."
  },
  {
    "objectID": "about/AnanyaPujary.html#r-experience",
    "href": "about/AnanyaPujary.html#r-experience",
    "title": "Ananya Pujary",
    "section": "R experience",
    "text": "R experience\nI’ve taken the Coursera courses ‘The Data Scientist’s Toolbox’ and ‘R Programming’ before joining DACSS."
  },
  {
    "objectID": "about/AnanyaPujary.html#research-interests",
    "href": "about/AnanyaPujary.html#research-interests",
    "title": "Ananya Pujary",
    "section": "Research interests",
    "text": "Research interests\nPublic Opinion Dynamics, Behavioral Data, Networks, Community Psychology"
  },
  {
    "objectID": "about/AnanyaPujary.html#hometown",
    "href": "about/AnanyaPujary.html#hometown",
    "title": "Ananya Pujary",
    "section": "Hometown",
    "text": "Hometown\nMangalore, India"
  },
  {
    "objectID": "about/AnanyaPujary.html#hobbies",
    "href": "about/AnanyaPujary.html#hobbies",
    "title": "Ananya Pujary",
    "section": "Hobbies",
    "text": "Hobbies\n\nPiano\nReading\nCycling\nArt"
  },
  {
    "objectID": "about/AnanyaPujary.html#fun-fact",
    "href": "about/AnanyaPujary.html#fun-fact",
    "title": "Ananya Pujary",
    "section": "Fun fact",
    "text": "Fun fact\nI’ve lived in the Middle East my whole life."
  },
  {
    "objectID": "about/StevenONeill.html",
    "href": "about/StevenONeill.html",
    "title": "Steve O’Neill",
    "section": "",
    "text": "I have a B.A. in Political Science from UMass and currently work here as a system administrator (A&F IT). Day-to-day I work on server administration, scripting, and endpoint management at scale. More about who we are and what we support is here. (Disclaimer: my bio hasn’t been updated on our website since I recently changed roles)\nThis summer I did an internship with Facilities & Campus Services, focusing on GIS and spatial analytics. It’s been challenging and rewarding!\nI’ve been using PostGIS, FME, and ArcGIS to visualize and model restroom supply + demand points on campus. We are looking at improving disability accessibility, gender equity, and OSHA compliance among the 1200+ public restrooms here.\nI previously worked at UMass Transit (PVTA) and was also a seasonal mechanic at Northampton Bicycle for several years."
  },
  {
    "objectID": "about/StevenONeill.html#r-experience",
    "href": "about/StevenONeill.html#r-experience",
    "title": "Steve O’Neill",
    "section": "R experience",
    "text": "R experience\nComparatively, I have less experience with R than other scripting languages (Python, Bash, and PowerShell). I do have R for Data Science on long-term loan and have been doing some of the exercises in it with plotting, etc. since earlier this summer.\nIn the course of my summer internship, I turned to R for doing data cleaning on survey results since no other language had an easy way to join dataframes based on column values containing regular expressions. The ‘fuzzyjoin’ library came to the rescue (!) and saved me a lot of time.\nCompared to other IDEs I’ve used, I appreciate how RStudio connects directly to our SQL server using an ODBC connection and lets me read tables directly into dataframes. Very neat.\nI can provide a sanitized version of the RMarkdown narrative from the data cleaning part of my internship if there is any interest."
  },
  {
    "objectID": "about/StevenONeill.html#research-interests",
    "href": "about/StevenONeill.html#research-interests",
    "title": "Steve O’Neill",
    "section": "Research interests",
    "text": "Research interests\nI’m interested in the media, public opinion, data privacy legislation, and international internet governance."
  },
  {
    "objectID": "about/StevenONeill.html#hometown",
    "href": "about/StevenONeill.html#hometown",
    "title": "Steve O’Neill",
    "section": "Hometown",
    "text": "Hometown\nI’m from Monson, Massachusetts. We are [technically] in Western Mass! Currently I live in Northampton."
  },
  {
    "objectID": "about/StevenONeill.html#hobbies",
    "href": "about/StevenONeill.html#hobbies",
    "title": "Steve O’Neill",
    "section": "Hobbies",
    "text": "Hobbies\nCycling, photography, running, and reading!"
  },
  {
    "objectID": "about/StevenONeill.html#fun-fact",
    "href": "about/StevenONeill.html#fun-fact",
    "title": "Steve O’Neill",
    "section": "Fun fact",
    "text": "Fun fact\nI’ve been in a tornado!"
  },
  {
    "objectID": "about/MekhalaKumar.html",
    "href": "about/MekhalaKumar.html",
    "title": "Mekhala Kumar",
    "section": "",
    "text": "I studied at FLAME University where I did an integrated program, earning a Bachelor’s degree and Postgraduate Diploma. My major was Applied Mathematics and minor was Economics. Through my courses, I also had programming experience in Java, Python, R and STATA and visualisation experience in Google Data Studio and Tableau.\nMoreover, I interned at the WageIndicator Foundation for two years, as an intern in Data Visualisation and Analysis team for a year and then as a co-manager of the same team in the following year."
  },
  {
    "objectID": "about/MekhalaKumar.html#r-experience",
    "href": "about/MekhalaKumar.html#r-experience",
    "title": "Mekhala Kumar",
    "section": "R experience",
    "text": "R experience\nI attended a two week R Workshop at FLAME University where we learnt about operations in R and how to handle datasets. Additionally, I utilised R for statistical analysis in a few courses at FLAME University."
  },
  {
    "objectID": "about/MekhalaKumar.html#research-interests",
    "href": "about/MekhalaKumar.html#research-interests",
    "title": "Mekhala Kumar",
    "section": "Research interests",
    "text": "Research interests\nMigration, Gender Economics, Social Justice"
  },
  {
    "objectID": "about/MekhalaKumar.html#hometown",
    "href": "about/MekhalaKumar.html#hometown",
    "title": "Mekhala Kumar",
    "section": "Hometown",
    "text": "Hometown\nBengaluru"
  },
  {
    "objectID": "about/MekhalaKumar.html#hobbies",
    "href": "about/MekhalaKumar.html#hobbies",
    "title": "Mekhala Kumar",
    "section": "Hobbies",
    "text": "Hobbies\nI enjoy practising Karate, singing, swimming and reading books."
  },
  {
    "objectID": "about/MekhalaKumar.html#fun-fact",
    "href": "about/MekhalaKumar.html#fun-fact",
    "title": "Mekhala Kumar",
    "section": "Fun fact",
    "text": "Fun fact\nI have made a few parodies, they’re on YouTube."
  },
  {
    "objectID": "about/SaaradhaaM.html",
    "href": "about/SaaradhaaM.html",
    "title": "Saaradhaa M",
    "section": "",
    "text": "I received an Honours degree in Psychology from the National University of Singapore, then worked in government and industry. I’m currently a graduate student in the DACSS program."
  },
  {
    "objectID": "about/SaaradhaaM.html#r-experience",
    "href": "about/SaaradhaaM.html#r-experience",
    "title": "Saaradhaa M",
    "section": "R Experience 💡",
    "text": "R Experience 💡\nI took Microsoft’s Introduction to R course in 2020. However, the bulk of my R experience will be coming from DACSS 601. I’m excited to learn!"
  },
  {
    "objectID": "about/SaaradhaaM.html#research-interests",
    "href": "about/SaaradhaaM.html#research-interests",
    "title": "Saaradhaa M",
    "section": "Research Interests 🔍",
    "text": "Research Interests 🔍\nGroup dynamics, implicit social cognition, inclusivity and networks in STEM"
  },
  {
    "objectID": "about/SaaradhaaM.html#hometown",
    "href": "about/SaaradhaaM.html#hometown",
    "title": "Saaradhaa M",
    "section": "Hometown 🇸🇬",
    "text": "Hometown 🇸🇬\nI was born and raised in Singapore, a small city in Asia."
  },
  {
    "objectID": "about/SaaradhaaM.html#hobbies",
    "href": "about/SaaradhaaM.html#hobbies",
    "title": "Saaradhaa M",
    "section": "Hobbies 🐾",
    "text": "Hobbies 🐾\n\nI picked up yoga during the pandemic.\nI do a cappella with my college friends.\nOther things I enjoy include travel, hiking and trying new food."
  },
  {
    "objectID": "about/SaaradhaaM.html#fun-fact",
    "href": "about/SaaradhaaM.html#fun-fact",
    "title": "Saaradhaa M",
    "section": "Fun Fact ⭐️",
    "text": "Fun Fact ⭐️\nI am left-handed, but I play string instruments right-handed because those who taught me were right-handed."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Adithya Parupudi\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnanya Pujary\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimeshSengupta\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmma Rasmussen\n\n\n\n\n\n\n\n\n\n\n\n\n\nJerin Jacob\n\n\n\n\n\n\n\n\n\n\n\n\n\nKatie Popiela\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaushikaPotluri\n\n\n\n\n\n\n\n\n\n\n\n\n\nKimberly Darkenwald\n\n\n\n\n\n\n\n\n\n\n\n\n\nLai Wei\n\n\n\n\n\n\n\n\n\n\n\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaddi Hertz\n\n\n\n\n\n\n\n\n\n\n\n\n\nManiShankerKamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\nMekhala Kumar\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiranda Manka\n\n\n\n\n\n\n\n\n\n\n\n\n\nNayan Jani\n\n\n\n\n\n\n\n\n\n\n\n\n\nNick Boonstra\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\nRosemary\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoy Yoon\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaaradhaa M\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoshana Buck\n\n\n\n\n\n\n\n\n\n\n\n\n\nSteve O’Neill\n\n\n\n\n\n\n\n\n\n\n\n\n\nTyler Tewksbury\n\n\n\n\n\n\n\n\n\n\n\n\n\nWill Munson\n\n\n\n\n\n\n\n\n\n\n\n\n\nYoung Soo Choi\n\n\n\n\n\n\n\n\n\n\n\n\n\nZhiyuan Zhou\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601 August 2022",
    "section": "",
    "text": "Challenge1_KatiePopiela\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge2_KatiePopiela\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 8 Instructions\n\n\n\n\n\n\n\nchallenge_8\n\n\nrailroads\n\n\nsnl\n\n\nfaostat\n\n\ndebt\n\n\n\n\nJoining Data\n\n\n\n\n\n\nAug 25, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 7 Instructions\n\n\n\n\n\n\n\nchallenge_7\n\n\nhotel_bookings\n\n\naustralian_marriage\n\n\nair_bnb\n\n\neggs\n\n\nabc_poll\n\n\nfaostat\n\n\nus_hh\n\n\n\n\nVisualizing Multiple Dimensions\n\n\n\n\n\n\nAug 24, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 6 Instructions\n\n\n\n\n\n\n\nchallenge_6\n\n\nhotel_bookings\n\n\nair_bnb\n\n\nfed_rate\n\n\ndebt\n\n\nusa_hh\n\n\nabc_poll\n\n\n\n\nVisualizing Time and Relationships\n\n\n\n\n\n\nAug 23, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5\n\n\n\n\n\n\n\nchallenge_5\n\n\nrailroads\n\n\nboonstra\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nNick Boonstra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5\n\n\n\n\n\n\n\nchallenge_5\n\n\ncereal\n\n\npublic_schools\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nMekhala Kumar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Akhilesh\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2022\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5\n\n\n\n\n\n\n\nchallenge_5\n\n\nusa_hh\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nSteve O’Neill\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5\n\n\n\n\n\n\n\nchallenge_5\n\n\nrailroads\n\n\ncereal\n\n\nair_bnb\n\n\npathogen_cost\n\n\naustralian_marriage\n\n\npublic_schools\n\n\nusa_hh\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5 Emma Rasmussen\n\n\n\n\n\n\n\nchallenge_5\n\n\nair_bnb\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5\n\n\n\n\n\n\n\nchallenge_5\n\n\nair_bnb\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nMiranda Manka\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5\n\n\n\n\n\n\n\nchallenge_5\n\n\nrailroads\n\n\ncereal\n\n\nair_bnb\n\n\npathogen_cost\n\n\naustralian_marriage\n\n\npublic_schools\n\n\nusa_hh\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nNayan Jani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5 Instructions\n\n\n\n\n\n\n\nchallenge_5\n\n\nrailroads\n\n\ncereal\n\n\nair_bnb\n\n\npathogen_cost\n\n\naustralian_marriage\n\n\npublic_schools\n\n\nusa_hh\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5\n\n\n\n\n\n\n\nchallenge_5\n\n\npsc\n\n\ntidyverse\n\n\nggplot2\n\n\nsummarytools\n\n\n\n\nIntro to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nSaaradhaa M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\n\nhw2\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2022\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Submission\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\nMore data wrangling: pivoting\n\n\n\n\n\n\nAug 21, 2022\n\n\nNick Boonstra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Instructions\n\n\n\n\n\n\n\nchallenge_4\n\n\nmutate\n\n\n\n\nMore data wrangling: pivoting\n\n\n\n\n\n\nAug 21, 2022\n\n\nNayan Jani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\ntidyverse\n\n\nreadxl\n\n\nlubridate\n\n\nsummarytools\n\n\ndebt_in_trillions\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nSaaradhaa M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework2\n\n\n\n\n\n\n\nhw2\n\n\nActiveDuty_MartialStatus\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nMani Shanker Kamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Solutions\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\nMore data wrangling: pivoting\n\n\n\n\n\n\nAug 21, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\nMore data wrangling: pivoting\n\n\n\n\n\n\nAug 19, 2022\n\n\nMiranda Manka\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\npoultry_tidy\n\n\n\n\nMore data wrangling: pivoting\n\n\n\n\n\n\nAug 18, 2022\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\nAnimesh Sengupta\n\n\nDebt in trillions data\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nAnimesh Sengupta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nJerin Jacob\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\nTidy data: pivoting\n\n\n\n\n\n\nAug 18, 2022\n\n\nMiranda Manka\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Instructions\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nKim Darkenwald\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Will Munson\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nWill Munson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nYoung Soo Choi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\nFederalFundsRate\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\ndebt_in_trillions\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nSteve O’Neill\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\nfedfundsrate\n\n\n\n\nMore data wrangling: pivoting\n\n\n\n\n\n\nAug 18, 2022\n\n\nAnanya Pujary\n\n\n\n\n\n\n  \n\n\n\n\nChallenge 3 Solutions\n\n\n\n\n\n\n\nchallenge_3\n\n\nsolution\n\n\n\n\nTidy Data: Pivoting\n\n\n\n\n\n\nAug 18, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Instructions\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nZhiyuan Zhou\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Instructions\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\nMore data wrangling: mutate\n\n\n\n\n\n\nAug 18, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\n\n\n\nchallenge_4\n\n\ndebt_in_trillions\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nMani Shanker Kamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nYoung Soo Choi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3 Instructions\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\nTidy Data: Pivoting\n\n\n\n\n\n\nAug 17, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nTyler Tewksbury\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\nAnimesh Sengupta\n\n\nUSA Household Data\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nAnimesh Sengupta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3 Will Munson\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\nTidy Data: Pivoting\n\n\n\n\n\n\nAug 17, 2022\n\n\nWill Munson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nJerin Jacob\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\nanimal_weights\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Solutions\n\n\n\n\n\n\n\nchallenge_2\n\n\nsolution\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 17, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\norganicpoultry\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nMekhala Kumar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\nanimal_weight\n\n\n\n\nTidy Data: Pivoting\n\n\n\n\n\n\nAug 17, 2022\n\n\nAnanya Pujary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\nAustralian_marriage_law\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nMani Shanker Kamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3 Instructions\n\n\n\n\n\n\n\nchallenge_3\n\n\nPivot\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nNayan Jani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\ntidyverse\n\n\nreadxl\n\n\ndplyr\n\n\ntidyr\n\n\nhouseholds\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nSaaradhaa M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\n\nchallenge_3\n\n\nanimal_weight\n\n\n\n\nTidy Data: Pivoting\n\n\n\n\n\n\nAug 17, 2022\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2 Instructions\n\n\n\n\n\n\n\n-hw2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nYakub Rabiutheen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nJerin jacob\n\n\n\n\n\n\n  \n\n\n\n\nChallenge 1 Solution\n\n\n\n\n\n\n\nchallenge_1\n\n\nsolution\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Will Munson\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nWill Munson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Attempt\n\n\n\n\n\n\n\nchallenge_2\n\n\nrailroads\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nFAOSTAT_cattle_dairy.csv\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMani Shanker Kamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nTyler Tewksbury\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nAnimesh Sengupta\n\n\nFAOStat_livestock data\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nAnimesh Sengupta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNick Boonstra Challenge 1 Resubmit\n\n\n\n\n\n\n\nchallenge_1\n\n\nboonstra\n\n\nweek_1\n\n\nbirds\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nNick Boonstra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nYoung Soo Choi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 16, 2022\n\n\nJerin Jacob\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nYakub Rabiutheen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nrailroads_2012_clean_county.csv\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nShoshana Buck\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 - Adithya Parupudi\n\n\n\n\n\n\n\nchallenge_2\n\n\nhw3\n\n\nfaostat_cattle_diary.csv\n\n\ndplyr\n\n\ntidyverse\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nAdithya Parupudi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nhotel_bookings\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMiranda Manka\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\ntidyverse\n\n\nbirds.csv\n\n\nhw2\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nAdithya Parupudi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nStateCounty\n\n\n\n\nData wrangling: using group() and summarise(\n\n\n\n\n\n\nAug 16, 2022\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\nFAO\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nState County dataset\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nMekhala Kumar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nhotel_bookings\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nSteve O’Neill\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nhotel_bookings\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nAnanya Pujary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\nrailroad\n\n\nquestion\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nRoy Yoon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNick Boonstra Challenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nboonstra\n\n\nweek_1\n\n\nhotels\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nNick Boonstra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\n\nchallenge_2\n\n\nhotel_bookings.csv\n\n\ntidyverse\n\n\nreadr\n\n\n\n\nData Wrangling\n\n\n\n\n\n\nAug 16, 2022\n\n\nSaaradhaa M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMekhala Kumar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nrailroad\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 15, 2022\n\n\nAnanya Pujary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\ntidyverse\n\n\nstatecounty2012\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nSteve O’Neill\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nKim Darkenwald\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nAnimesh Sengupta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Quinn He\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMani Shanker Kamarapu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nrailroads\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nLindsay Jones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nYakub Rabiutheen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 15, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nYoung Soo Choi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Roy Yoon\n\n\n\n\n\n\n\nchallenge_1\n\n\nbirds.csv\n\n\nsubmission 2\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nRoy Yoon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nwild_bird_data\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nMiranda Manka\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nKaushika Potluri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\ntidyverse\n\n\nreadxl\n\n\ndplyr\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 15, 2022\n\n\nSaaradhaa M\n\n\n\n\n\n\nNo matching items"
  }
]